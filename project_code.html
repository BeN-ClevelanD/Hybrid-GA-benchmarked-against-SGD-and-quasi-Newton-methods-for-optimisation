<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>project_code</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                messageStyle: 'none',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=37d0e86f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Comparative-Study-of-Optimisation-Algorithms-for-Feed-Forward-Neural-Networks+">Comparative Study of Optimisation Algorithms for Feed-Forward Neural Networks+<a class="anchor-link" href="#Comparative-Study-of-Optimisation-Algorithms-for-Feed-Forward-Neural-Networks+">¶</a></h1><p>Ben Cleveland - 25504843
DJ Swanevelder - 250205269</p>
<h4 id="Notes:">Notes:<a class="anchor-link" href="#Notes:">¶</a></h4><ul>
<li>Not all imports are in one place: Sometimes there are piecewise imports in each block. Purposeful, so that not all blocks have to be run in a linearly dependent order.</li>
<li>Docstrings are provided for documentation</li>
<li>Some code has been removed: Some initially tried models and such were removed a long time ago during experimentation: What remains are the important codeblocks, as well as all code that was used in final results, tests or models</li>
</ul>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=0a1e21e2">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Initial-imports-and-setup">Initial imports and setup<a class="anchor-link" href="#Initial-imports-and-setup">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f00f9b09">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span> 
<span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span> 

<span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Load CSVs</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'Training.csv'</span><span class="p">)</span>
<span class="n">val_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'Validation.csv'</span><span class="p">)</span>

<span class="c1"># Features and targets</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[[</span><span class="s1">'x1'</span><span class="p">,</span> <span class="s1">'x2'</span><span class="p">,</span> <span class="s1">'x3'</span><span class="p">,</span> <span class="s1">'x4'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">'y'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">X_val</span> <span class="o">=</span> <span class="n">val_df</span><span class="p">[[</span><span class="s1">'x1'</span><span class="p">,</span> <span class="s1">'x2'</span><span class="p">,</span> <span class="s1">'x3'</span><span class="p">,</span> <span class="s1">'x4'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">val_df</span><span class="p">[</span><span class="s1">'y'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Standardise</span>
<span class="n">scaler_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">scaler_y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler_X</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">X_val</span> <span class="o">=</span> <span class="n">scaler_X</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_val</span><span class="p">)</span>

<span class="c1"># Convert to tensors</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=a4a72b9a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Architecture-tuning-using-grid-search:">Architecture tuning using grid search:<a class="anchor-link" href="#Architecture-tuning-using-grid-search:">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8b5c4d03">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Grid Search for Neural Network Architecture Evaluation</span>

<span class="sd">This script performs a simple grid search to evaluate different fully connected</span>
<span class="sd">neural network architectures on a regression task using PyTorch. The model is trained</span>
<span class="sd">on scaled inputs from a training CSV file and validated on a separate validation CSV.</span>

<span class="sd">Steps:</span>
<span class="sd">-------</span>
<span class="sd">1. **Imports and Reproducibility**: Sets global random seeds for NumPy and PyTorch.</span>
<span class="sd">2. **Data Loading and Preprocessing**:</span>
<span class="sd">   - Loads training and validation data from CSV files: 'Training.csv' and 'Validation.csv'.</span>
<span class="sd">   - Applies `StandardScaler` separately on `X` (features) and `y` (targets).</span>
<span class="sd">   - Converts data to PyTorch tensors.</span>
<span class="sd">3. **Architecture Grid Setup**:</span>
<span class="sd">   - A manually defined grid of `(n_layers, n_units)` combinations is iterated over.</span>
<span class="sd">4. **Training and Evaluation**:</span>
<span class="sd">   - For each architecture, a model is built using `nn.Sequential`.</span>
<span class="sd">   - Weights are initialized using Xavier normal.</span>
<span class="sd">   - The model is trained using SGD for 10 epochs.</span>
<span class="sd">   - Both training and validation Mean Squared Error (MSE) are recorded.</span>
<span class="sd">5. **Results Summary**:</span>
<span class="sd">   - Results are saved in a DataFrame and sorted by validation MSE.</span>
<span class="sd">   - The best performing architecture is displayed.</span>

<span class="sd">Returns:</span>
<span class="sd">--------</span>
<span class="sd">Prints a summary table of all architectures sorted by validation MSE,</span>
<span class="sd">as well as the best-performing architecture.</span>

<span class="sd">Instructions for Modifying:</span>
<span class="sd">----------------------------</span>
<span class="sd">- **Input Data**: Replace `'Training.csv'` and `'Validation.csv'` with your own file paths if needed.</span>
<span class="sd">- **Feature Columns**: Modify `['x1','x2','x3','x4']` if your feature column names differ.</span>
<span class="sd">- **Architecture Grid**: Modify the `arch_grid` list to test other layer/unit combinations.</span>
<span class="sd">- **Epochs**: Change the number of training epochs by adjusting the `range(10)` line.</span>
<span class="sd">- **Loss and Optimizer**: Swap out `nn.MSELoss()` or `optim.SGD(...)` for alternatives (e.g., Adam).</span>
<span class="sd">- **Device**: The script currently uses CPU only. To enable GPU, change `device = torch.device("cpu")` to:</span>
<span class="sd">  `device = torch.device("cuda" if torch.cuda.is_available() else "cpu")`</span>
<span class="sd">- **Result Export**: You can write the `df` DataFrame to CSV using `df.to_csv("results.csv", index=False)` if needed.</span>

<span class="sd">Example Output:</span>
<span class="sd">---------------</span>
<span class="sd">Grid-search results (sorted by val MSE):</span>

<span class="sd">   n_layers  n_units  train_mse   val_mse</span>
<span class="sd">0         2       24   0.989429  0.987574</span>
<span class="sd">1         1       24   0.991087  0.987586</span>
<span class="sd">...</span>
<span class="sd">  Best architecture: 2 layers × 24 units  (val MSE=0.9876)</span>


<span class="sd">"""</span>

<span class="c1"># 1) Imports &amp; Global Seeding</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># reproducibility</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 2) Load &amp; Preprocess Data (CPU only)</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'Training.csv'</span><span class="p">)</span>
<span class="n">val_df</span>   <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'Validation.csv'</span><span class="p">)</span>

<span class="c1"># split out features / targets</span>
<span class="n">X_train_np</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[[</span><span class="s1">'x1'</span><span class="p">,</span><span class="s1">'x2'</span><span class="p">,</span><span class="s1">'x3'</span><span class="p">,</span><span class="s1">'x4'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">'y'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_val_np</span>   <span class="o">=</span> <span class="n">val_df</span><span class="p">[[</span><span class="s1">'x1'</span><span class="p">,</span><span class="s1">'x2'</span><span class="p">,</span><span class="s1">'x3'</span><span class="p">,</span><span class="s1">'x4'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_val_np</span>   <span class="o">=</span> <span class="n">val_df</span><span class="p">[</span><span class="s1">'y'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">)</span>
<span class="n">scaler_y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y_train_np</span><span class="p">)</span>
<span class="n">X_train_np</span> <span class="o">=</span> <span class="n">scaler_X</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">)</span>
<span class="n">X_val_np</span>   <span class="o">=</span> <span class="n">scaler_X</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val_np</span><span class="p">)</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_train_np</span><span class="p">)</span>
<span class="n">y_val_np</span>   <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_val_np</span><span class="p">)</span>

<span class="c1"># to torch tensors</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_train_np</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_val</span>   <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_val_np</span><span class="p">,</span>   <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_val</span>   <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_val_np</span><span class="p">,</span>   <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># 3) Grid of architectures to try</span>

<span class="n">arch_grid</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span><span class="p">),</span>  
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">24</span><span class="p">),</span>   
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>  
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>  
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>   
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>   
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>  
<span class="p">]</span>

<span class="c1"># 4) Training loop</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">n_units</span> <span class="ow">in</span> <span class="n">arch_grid</span><span class="p">:</span>

    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_f</span>   <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">n_units</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">n_units</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

  
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

   
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># train 10 epochs</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># evaluate</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">train_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">val_mse</span>   <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_val</span><span class="p">),</span>   <span class="n">y_val</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
        <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="n">n_layers</span><span class="p">,</span>
        <span class="s2">"n_units"</span><span class="p">:</span>    <span class="n">n_units</span><span class="p">,</span>
        <span class="s2">"train_mse"</span><span class="p">:</span>  <span class="n">train_mse</span><span class="p">,</span>
        <span class="s2">"val_mse"</span><span class="p">:</span>    <span class="n">val_mse</span>
    <span class="p">})</span>

<span class="c1"># 5) Summarize</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">"val_mse"</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Grid-search results (sorted by val MSE):</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="n">best</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">🏆  Best architecture: "</span>
      <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">n_layers</span><span class="si">}</span><span class="s2"> layers × </span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">n_units</span><span class="si">}</span><span class="s2"> units  "</span>
      <span class="sa">f</span><span class="s2">"(val MSE=</span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">val_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Grid-search results (sorted by val MSE):

   n_layers  n_units  train_mse   val_mse
0         2       24   0.989429  0.987574
1         1       24   0.991087  0.987586
2         2       20   0.989687  0.988779
3         2       16   0.991191  0.990116
4         1       16   0.995888  0.994841
5         2        8   0.995200  0.996152
6         1        8   0.998301  0.999128

🏆  Best architecture: 2.0 layers × 24.0 units  (val MSE=0.9876)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=33f1b375">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Heaptmap-for-displaying-arch:">Heaptmap for displaying arch:<a class="anchor-link" href="#Heaptmap-for-displaying-arch:">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=561f5316">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [62]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># 1) Reconstruct your grid-search results</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span> <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.987574</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span> <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.987586</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span> <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.988779</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.990116</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.994841</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.996152</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.999128</span><span class="p">},</span>
<span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># 2) Pivot into matrix form</span>
<span class="n">pivot</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s1">'n_layers'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s1">'n_units'</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s1">'val_mse'</span><span class="p">)</span>

<span class="c1"># 3) Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pivot</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'plasma'</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">)</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">'Validation MSE'</span><span class="p">)</span>

<span class="c1"># 4) Axes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pivot</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">pivot</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pivot</span><span class="o">.</span><span class="n">index</span><span class="p">)),</span> <span class="n">pivot</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Number of Units'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Number of Layers'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Validation MSE Heatmap'</span><span class="p">)</span>

<span class="c1"># 5) Annotate each cell with white text</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pivot</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pivot</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">pivot</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span>
                 <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=99a3e448">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Using-Optuna-to-do-hyperparameter-tuning-for-our-stochastic-gradient-descent-implementation:-Finding-optimal-learning-rate-and-momentum-value.">Using Optuna to do hyperparameter tuning for our stochastic gradient descent implementation: Finding optimal learning rate and momentum value.<a class="anchor-link" href="#Using-Optuna-to-do-hyperparameter-tuning-for-our-stochastic-gradient-descent-implementation:-Finding-optimal-learning-rate-and-momentum-value.">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2980ba10">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Script for hyperparameter tuning using Optuna on a fixed neural network architecture.</span>

<span class="sd">This script builds a simple feedforward neural network (FFN) with two hidden layers </span>
<span class="sd">and performs hyperparameter optimization using Optuna, specifically tuning learning </span>
<span class="sd">rate (`lr`) and momentum (`momentum`) for stochastic gradient descent (SGD). </span>
<span class="sd">The training is done using a batch size of 1 to simulate pure SGD. The goal is to </span>
<span class="sd">minimize the mean squared error (MSE) on the validation set.</span>

<span class="sd">Sections:</span>
<span class="sd">---------</span>
<span class="sd">1. Fixed Model Definition:</span>
<span class="sd">    - Uses 2 hidden layers with 24 ReLU units each (modifiable).</span>
<span class="sd">    - Xavier initialization for weights and zero initialization for biases.</span>

<span class="sd">2. Optuna Search Objective:</span>
<span class="sd">    - Tunes learning rate (log-uniform between 1e-5 and 1e-1).</span>
<span class="sd">    - Tunes momentum (uniform between 0 and 0.99).</span>
<span class="sd">    - Trains for 10 epochs using batch size = 1.</span>

<span class="sd">3. Search Execution:</span>
<span class="sd">    - Runs 20 trials of optimization.</span>
<span class="sd">    - Reports the best validation MSE and corresponding hyperparameters.</span>

<span class="sd">How to Modify:</span>
<span class="sd">--------------</span>
<span class="sd">• **Architecture**: Change the number of hidden layers or units per layer </span>
<span class="sd">  by editing `build_fixed_model()`. For example, change range(2) to range(3) for </span>
<span class="sd">  3 hidden layers or change 24 to any other value for hidden size.</span>

<span class="sd">• **Loss Function**: Change `nn.MSELoss()` to other loss functions like MAE if needed.</span>

<span class="sd">• **Search Space**: To tune additional hyperparameters (e.g., batch size, number of units),</span>
<span class="sd">  add them to the `trial.suggest_*` section in `objective()` and modify `build_fixed_model()` accordingly.</span>

<span class="sd">• **Training Epochs**: Change the loop range in `objective()` (currently 10) to control how long </span>
<span class="sd">  each trial is trained for.</span>

<span class="sd">• **Batch Size**: To switch from pure SGD to minibatch gradient descent, change </span>
<span class="sd">  `batch_size=1` to another value in the DataLoader inside `objective()`.</span>

<span class="sd">Dependencies:</span>
<span class="sd">-------------</span>
<span class="sd">- torch</span>
<span class="sd">- numpy</span>
<span class="sd">- pandas</span>
<span class="sd">- optuna</span>
<span class="sd">- sklearn</span>

<span class="sd">Expected Inputs:</span>
<span class="sd">----------------</span>
<span class="sd">- `X_train`, `y_train`: torch.Tensors (features and labels for training)</span>
<span class="sd">- `X_val`, `y_val`: torch.Tensors (features and labels for validation)</span>
<span class="sd">  These must be preloaded in memory before running this script.</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Console log showing best hyperparameters and corresponding validation MSE.</span>

<span class="sd">"""</span>


<span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># reproducibility</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 1) Fixed architecture builder</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_fixed_model</span><span class="p">():</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_f</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>   
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>        
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">24</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="mi">24</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    <span class="c1"># Xavier init</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
<span class="c1"># 2) Optuna objective – only lr &amp; momentum, fix batch_size=1</span>
<span class="k">def</span><span class="w"> </span><span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="c1"># Two params to tune</span>
    <span class="n">lr</span>       <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_loguniform</span><span class="p">(</span><span class="s2">"lr"</span><span class="p">,</span>      <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span>  <span class="s2">"momentum"</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>

    <span class="c1">#  DataLoader with batch_size=1</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

   <span class="c1"># Build model, loss &amp; optimizer</span>
    <span class="n">model</span>     <span class="o">=</span> <span class="n">build_fixed_model</span><span class="p">()</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>

    <span class="c1">#  Train for 80 epochs</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1">#  Validation</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_val</span><span class="p">),</span> <span class="n">y_val</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">val_mse</span>

<span class="c1"># 3) Run the search</span>
<span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">"minimize"</span><span class="p">)</span>
<span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 4) Report best</span>
<span class="n">best</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">=== Best Hyperparameters (pure SGD) ==="</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  lr:       </span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">]</span><span class="si">:</span><span class="s2">.5g</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  momentum: </span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">'momentum'</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best Val MSE: </span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">value</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>[I 2025-05-28 18:42:30,770] A new study created in memory with name: no-name-29cfe234-123d-4b6d-a9b0-7c3f774a1328
  0%|          | 0/20 [00:00&lt;?, ?it/s]/tmp/ipykernel_5499/2176302026.py:31: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr       = trial.suggest_loguniform("lr",      1e-5, 1e-1)
Best trial: 0. Best value: 1.01949:   5%|▌         | 1/20 [01:14&lt;23:44, 74.96s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:43:45,730] Trial 0 finished with value: 1.019486665725708 and parameters: {'lr': 0.07722597496556405, 'momentum': 0.68862897675173}. Best is trial 0 with value: 1.019486665725708.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  10%|█         | 2/20 [02:30&lt;22:32, 75.13s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:45:00,975] Trial 1 finished with value: 0.8632642030715942 and parameters: {'lr': 0.002491034301035869, 'momentum': 0.6706817605204999}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  15%|█▌        | 3/20 [03:44&lt;21:11, 74.78s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:46:15,345] Trial 2 finished with value: 1.008341908454895 and parameters: {'lr': 0.02926027577397731, 'momentum': 0.11218386033409711}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  20%|██        | 4/20 [04:59&lt;19:54, 74.65s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:47:29,802] Trial 3 finished with value: 0.9851898550987244 and parameters: {'lr': 0.00893774459776824, 'momentum': 0.5399565006032588}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  25%|██▌       | 5/20 [06:13&lt;18:39, 74.63s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:48:44,391] Trial 4 finished with value: 0.9964484572410583 and parameters: {'lr': 4.80326942588253e-05, 'momentum': 0.08237666225046819}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  30%|███       | 6/20 [07:27&lt;17:22, 74.50s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:49:58,629] Trial 5 finished with value: 0.9977979063987732 and parameters: {'lr': 1.3879333944853265e-05, 'momentum': 0.621887448671995}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  35%|███▌      | 7/20 [08:42&lt;16:09, 74.55s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 18:51:13,284] Trial 6 failed with parameters: {'lr': 0.0693368760229596, 'momentum': 0.6561466262131057} because of the following error: The value nan is not acceptable.
[W 2025-05-28 18:51:13,287] Trial 6 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  40%|████      | 8/20 [09:56&lt;14:53, 74.49s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:52:27,660] Trial 7 finished with value: 0.8716850280761719 and parameters: {'lr': 0.004083592137636449, 'momentum': 0.1948592692793183}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  45%|████▌     | 9/20 [11:11&lt;13:39, 74.50s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:53:42,167] Trial 8 finished with value: 0.9257376790046692 and parameters: {'lr': 0.009614876416667221, 'momentum': 0.39769380793625425}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  50%|█████     | 10/20 [12:26&lt;12:25, 74.55s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:54:56,846] Trial 9 finished with value: 0.9045374989509583 and parameters: {'lr': 0.0022883990537292363, 'momentum': 0.7195431992810583}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  55%|█████▌    | 11/20 [13:40&lt;11:10, 74.46s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:56:11,099] Trial 10 finished with value: 0.9899379014968872 and parameters: {'lr': 0.00010944626772812799, 'momentum': 0.3367779849354335}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  60%|██████    | 12/20 [14:54&lt;09:55, 74.45s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:57:25,513] Trial 11 finished with value: 0.9432251453399658 and parameters: {'lr': 0.0002997376745477513, 'momentum': 0.9790257780084946}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  65%|██████▌   | 13/20 [16:08&lt;08:39, 74.28s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:58:39,417] Trial 12 finished with value: 0.965964138507843 and parameters: {'lr': 0.0015662068029184029, 'momentum': 0.2451522199571834}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 13. Best value: 0.850097:  70%|███████   | 14/20 [17:22&lt;07:25, 74.24s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:59:53,570] Trial 13 finished with value: 0.8500973582267761 and parameters: {'lr': 0.0005680703916186882, 'momentum': 0.8732847818815674}. Best is trial 13 with value: 0.8500973582267761.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692:  75%|███████▌  | 15/20 [18:36&lt;06:10, 74.19s/it] </pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:01:07,647] Trial 14 finished with value: 0.7969197630882263 and parameters: {'lr': 0.0004872119511159354, 'momentum': 0.8766164307306744}. Best is trial 14 with value: 0.7969197630882263.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692:  80%|████████  | 16/20 [19:50&lt;04:56, 74.16s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:02:21,717] Trial 15 finished with value: 0.908641517162323 and parameters: {'lr': 0.00037484338849541106, 'momentum': 0.9445571334860606}. Best is trial 14 with value: 0.7969197630882263.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692:  85%|████████▌ | 17/20 [21:04&lt;03:42, 74.10s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:03:35,671] Trial 16 finished with value: 0.8646453022956848 and parameters: {'lr': 0.00048517453693837256, 'momentum': 0.8251558057957358}. Best is trial 14 with value: 0.7969197630882263.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692:  90%|█████████ | 18/20 [22:19&lt;02:28, 74.10s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:04:49,793] Trial 17 finished with value: 0.9784354567527771 and parameters: {'lr': 9.173144461117463e-05, 'momentum': 0.8073786826673773}. Best is trial 14 with value: 0.7969197630882263.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692:  95%|█████████▌| 19/20 [23:32&lt;01:14, 74.05s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:06:03,735] Trial 18 finished with value: 0.9212556481361389 and parameters: {'lr': 0.0008223197638385264, 'momentum': 0.8744406649023867}. Best is trial 14 with value: 0.7969197630882263.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692: 100%|██████████| 20/20 [24:46&lt;00:00, 74.32s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:07:17,238] Trial 19 finished with value: 0.9900429248809814 and parameters: {'lr': 0.00016307254326817386, 'momentum': 0.522156351647615}. Best is trial 14 with value: 0.7969197630882263.

=== Best Hyperparameters (pure SGD) ===
  lr:       0.00048721
  momentum: 0.8766
Best Val MSE: 0.79692
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=93d78023">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="SGD-run-using-parameters-found-with-optuna:">SGD run using parameters found with optuna:<a class="anchor-link" href="#SGD-run-using-parameters-found-with-optuna:">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=db20497a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Final Feed-Forward Network (FFN) Training Script with Best Hyperparameters (SGD)</span>

<span class="sd">Overview:</span>
<span class="sd">---------</span>
<span class="sd">This script trains a fixed-architecture FFN on preprocessed training data using</span>
<span class="sd">Stochastic Gradient Descent (SGD) with Optuna-optimized hyperparameters. It logs</span>
<span class="sd">performance metrics (MSE) per epoch, plots loss curves, and saves the model, weights,</span>
<span class="sd">and architecture metadata for future reuse.</span>

<span class="sd">Sections:</span>
<span class="sd">---------</span>
<span class="sd">1. Device Setup         – Forces CPU or uses CUDA if available.</span>
<span class="sd">2. Hyperparameter Dict  – Update this section with new best hyperparameters if needed.</span>
<span class="sd">3. Data Preparation     – Assumes X_train, y_train, X_val, y_val are already scaled and defined.</span>
<span class="sd">4. Model Construction   – Dynamically builds a model with specified layers/activation.</span>
<span class="sd">5. Training Loop        – Runs for 100 epochs with SGD, logs per-epoch MSE to CSV.</span>
<span class="sd">6. Performance Plotting – Visualises train vs val loss curves.</span>
<span class="sd">7. Checkpointing        – Saves model weights, flattened vector, and architecture metadata.</span>

<span class="sd">Customisation Instructions:</span>
<span class="sd">---------------------------</span>
<span class="sd">- To change the architecture:</span>
<span class="sd">  ↳ Edit `best_params["n_layers"]` or `["n_units"]`, and optionally the activation function.</span>
<span class="sd">- To change the training duration:</span>
<span class="sd">  ↳ Modify the `epochs` variable (currently set to 100).</span>
<span class="sd">- To use a different batch size or optimizer:</span>
<span class="sd">  ↳ Update the `best_params["batch_size"]` and/or swap out `optim.SGD` with another optimizer.</span>
<span class="sd">- To enable GPU:</span>
<span class="sd">  ↳ Uncomment the `cuda` line in device setup.</span>
<span class="sd">- To retrain with different data:</span>
<span class="sd">  ↳ Make sure to redefine `X_train`, `y_train`, `X_val`, `y_val` with properly preprocessed tensors.</span>

<span class="sd">Output:</span>
<span class="sd">-------</span>
<span class="sd">- `checkpoints_sgd/sgd_final_weights.pth`        – Trained PyTorch state_dict.</span>
<span class="sd">- `checkpoints_sgd/sgd_final_genome.npy`         – Flattened model parameter vector.</span>
<span class="sd">- `checkpoints_sgd/sgd_model_meta.json`          – Model and training metadata.</span>
<span class="sd">- `sgd_stats.csv`                                – CSV log of train/val MSE per epoch.</span>
<span class="sd">- Train vs Val loss plot displayed at end.</span>
<span class="sd">"""</span>

<span class="c1">#  Final FFN Training with Best Hyperparameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># 1) Device: CPU </span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="n">best_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"lr"</span><span class="p">:</span>         <span class="mf">0.00048721</span><span class="p">,</span>
    <span class="s2">"momentum"</span><span class="p">:</span>   <span class="mf">0.8766</span><span class="p">,</span>
    <span class="s2">"batch_size"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"n_units"</span><span class="p">:</span>    <span class="mi">24</span><span class="p">,</span>
    <span class="s2">"activation"</span><span class="p">:</span> <span class="s2">"ReLU"</span>
<span class="p">}</span>
<span class="c1"># 3) Prepare DataLoaders (assumes X_train, y_train, X_val, y_val already defined &amp; scaled)</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">val_ds</span>   <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">],</span>
                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># we'll just evaluate on the full validation set each epoch</span>
<span class="n">Xv</span><span class="p">,</span> <span class="n">yv</span> <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 4) Build the model</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">in_f</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">Act</span>  <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
    <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
    <span class="n">in_f</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 5) Optimizer &amp; loss</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                      <span class="n">lr</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span>
                      <span class="n">momentum</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"momentum"</span><span class="p">])</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">SGD_LOG_FN</span> <span class="o">=</span> <span class="s2">"sgd_stats.csv"</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">SGD_LOG_FN</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">"epoch,epoch_time,train_mse,val_mse</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1"># 6) Train for 100 epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">running</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">running</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">xb</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">train_mse</span> <span class="o">=</span> <span class="n">running</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_mse</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">Xv</span><span class="p">),</span> <span class="n">yv</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_mse</span><span class="p">)</span>
    <span class="n">epoch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">SGD_LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">epoch_time</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">train_mse</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">val_mse</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
   
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s2">3d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2"> ▶ train MSE: </span><span class="si">{</span><span class="n">train_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">val_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ Training complete!"</span><span class="p">)</span>

<span class="c1"># 7) Plot curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_losses</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Final FFN Training &amp; Validation Loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span><span class="o">,</span><span class="w"> </span><span class="nn">json</span><span class="o">,</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span>

<span class="c1"># Saving params for later re use:</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">"checkpoints_sgd"</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 1) Save the PyTorch state_dict</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
           <span class="s2">"checkpoints_sgd/sgd_final_weights.pth"</span><span class="p">)</span>

<span class="c1"># 2) Save the flat parameter vector (so you can reload via vector_to_parameters)</span>
<span class="n">genome_sgd</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"checkpoints_sgd/sgd_final_genome.npy"</span><span class="p">,</span> <span class="n">genome_sgd</span><span class="p">)</span>

<span class="c1"># 3) Save the architecture &amp; SGD hyperparams so you can rebuild the same model later</span>
<span class="n">meta_sgd</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"arch"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">],</span>
        <span class="s2">"n_units"</span><span class="p">:</span>    <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">],</span>
        <span class="s2">"activation"</span><span class="p">:</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s2">"sgd_params"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"lr"</span><span class="p">:</span>       <span class="n">best_params</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span>
        <span class="s2">"momentum"</span><span class="p">:</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"momentum"</span><span class="p">],</span>
        <span class="s2">"batch_size"</span><span class="p">:</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">],</span>
        <span class="s2">"epochs"</span><span class="p">:</span>    <span class="n">epochs</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"checkpoints_sgd/sgd_model_meta.json"</span><span class="p">,</span><span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">meta_sgd</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ SGD‐trained model and metadata saved to `checkpoints_sgd/`"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cpu

Epoch   1/100 ▶ train MSE: 1.0027, val MSE: 0.9923
Epoch   2/100 ▶ train MSE: 0.9970, val MSE: 0.9871
Epoch   3/100 ▶ train MSE: 0.9940, val MSE: 0.9912
Epoch   4/100 ▶ train MSE: 0.9920, val MSE: 0.9927
Epoch   5/100 ▶ train MSE: 0.9908, val MSE: 0.9839
Epoch   6/100 ▶ train MSE: 0.9884, val MSE: 0.9777
Epoch   7/100 ▶ train MSE: 0.9875, val MSE: 0.9817
Epoch   8/100 ▶ train MSE: 0.9840, val MSE: 0.9875
Epoch   9/100 ▶ train MSE: 0.9798, val MSE: 0.9822
Epoch  10/100 ▶ train MSE: 0.9705, val MSE: 0.9191
Epoch  11/100 ▶ train MSE: 0.9517, val MSE: 0.8930
Epoch  12/100 ▶ train MSE: 0.9389, val MSE: 0.8855
Epoch  13/100 ▶ train MSE: 0.9265, val MSE: 0.8699
Epoch  14/100 ▶ train MSE: 0.9180, val MSE: 0.8634
Epoch  15/100 ▶ train MSE: 0.9126, val MSE: 0.8345
Epoch  16/100 ▶ train MSE: 0.9068, val MSE: 0.8344
Epoch  17/100 ▶ train MSE: 0.9012, val MSE: 0.8410
Epoch  18/100 ▶ train MSE: 0.8934, val MSE: 0.8000
Epoch  19/100 ▶ train MSE: 0.8846, val MSE: 0.7904
Epoch  20/100 ▶ train MSE: 0.8795, val MSE: 0.7870
Epoch  21/100 ▶ train MSE: 0.8638, val MSE: 0.7344
Epoch  22/100 ▶ train MSE: 0.8420, val MSE: 0.7129
Epoch  23/100 ▶ train MSE: 0.8257, val MSE: 0.7024
Epoch  24/100 ▶ train MSE: 0.8131, val MSE: 0.6714
Epoch  25/100 ▶ train MSE: 0.8010, val MSE: 0.6892
Epoch  26/100 ▶ train MSE: 0.7911, val MSE: 0.6552
Epoch  27/100 ▶ train MSE: 0.7783, val MSE: 0.6357
Epoch  28/100 ▶ train MSE: 0.7695, val MSE: 0.5944
Epoch  29/100 ▶ train MSE: 0.7676, val MSE: 0.6008
Epoch  30/100 ▶ train MSE: 0.7631, val MSE: 0.6278
Epoch  31/100 ▶ train MSE: 0.7573, val MSE: 0.6090
Epoch  32/100 ▶ train MSE: 0.7562, val MSE: 0.5965
Epoch  33/100 ▶ train MSE: 0.7523, val MSE: 0.5988
Epoch  34/100 ▶ train MSE: 0.7494, val MSE: 0.5927
Epoch  35/100 ▶ train MSE: 0.7429, val MSE: 0.5939
Epoch  36/100 ▶ train MSE: 0.7417, val MSE: 0.5846
Epoch  37/100 ▶ train MSE: 0.7372, val MSE: 0.5912
Epoch  38/100 ▶ train MSE: 0.7297, val MSE: 0.5761
Epoch  39/100 ▶ train MSE: 0.7294, val MSE: 0.5668
Epoch  40/100 ▶ train MSE: 0.7289, val MSE: 0.5726
Epoch  41/100 ▶ train MSE: 0.7263, val MSE: 0.5957
Epoch  42/100 ▶ train MSE: 0.7253, val MSE: 0.5361
Epoch  43/100 ▶ train MSE: 0.7246, val MSE: 0.5359
Epoch  44/100 ▶ train MSE: 0.7210, val MSE: 0.5702
Epoch  45/100 ▶ train MSE: 0.7213, val MSE: 0.6333
Epoch  46/100 ▶ train MSE: 0.7203, val MSE: 0.5624
Epoch  47/100 ▶ train MSE: 0.7194, val MSE: 0.5596
Epoch  48/100 ▶ train MSE: 0.7186, val MSE: 0.5529
Epoch  49/100 ▶ train MSE: 0.7162, val MSE: 0.5251
Epoch  50/100 ▶ train MSE: 0.7143, val MSE: 0.5610
Epoch  51/100 ▶ train MSE: 0.7145, val MSE: 0.5601
Epoch  52/100 ▶ train MSE: 0.7142, val MSE: 0.5127
Epoch  53/100 ▶ train MSE: 0.7138, val MSE: 0.5340
Epoch  54/100 ▶ train MSE: 0.7112, val MSE: 0.5767
Epoch  55/100 ▶ train MSE: 0.7116, val MSE: 0.5227
Epoch  56/100 ▶ train MSE: 0.7125, val MSE: 0.5177
Epoch  57/100 ▶ train MSE: 0.7105, val MSE: 0.5475
Epoch  58/100 ▶ train MSE: 0.7101, val MSE: 0.5247
Epoch  59/100 ▶ train MSE: 0.7117, val MSE: 0.5013
Epoch  60/100 ▶ train MSE: 0.7096, val MSE: 0.5332
Epoch  61/100 ▶ train MSE: 0.7110, val MSE: 0.5622
Epoch  62/100 ▶ train MSE: 0.7091, val MSE: 0.5358
Epoch  63/100 ▶ train MSE: 0.7097, val MSE: 0.5416
Epoch  64/100 ▶ train MSE: 0.7086, val MSE: 0.5348
Epoch  65/100 ▶ train MSE: 0.7091, val MSE: 0.5439
Epoch  66/100 ▶ train MSE: 0.7099, val MSE: 0.5217
Epoch  67/100 ▶ train MSE: 0.7081, val MSE: 0.5677
Epoch  68/100 ▶ train MSE: 0.7086, val MSE: 0.5454
Epoch  69/100 ▶ train MSE: 0.7098, val MSE: 0.5249
Epoch  70/100 ▶ train MSE: 0.7082, val MSE: 0.5075
Epoch  71/100 ▶ train MSE: 0.7086, val MSE: 0.5346
Epoch  72/100 ▶ train MSE: 0.7065, val MSE: 0.5194
Epoch  73/100 ▶ train MSE: 0.7078, val MSE: 0.5315
Epoch  74/100 ▶ train MSE: 0.7058, val MSE: 0.5479
Epoch  75/100 ▶ train MSE: 0.7081, val MSE: 0.5534
Epoch  76/100 ▶ train MSE: 0.7062, val MSE: 0.5462
Epoch  77/100 ▶ train MSE: 0.7062, val MSE: 0.5383
Epoch  78/100 ▶ train MSE: 0.7070, val MSE: 0.5786
Epoch  79/100 ▶ train MSE: 0.7042, val MSE: 0.5318
Epoch  80/100 ▶ train MSE: 0.7064, val MSE: 0.6006
Epoch  81/100 ▶ train MSE: 0.7056, val MSE: 0.5234
Epoch  82/100 ▶ train MSE: 0.7049, val MSE: 0.5324
Epoch  83/100 ▶ train MSE: 0.7055, val MSE: 0.5010
Epoch  84/100 ▶ train MSE: 0.7041, val MSE: 0.5954
Epoch  85/100 ▶ train MSE: 0.7036, val MSE: 0.5131
Epoch  86/100 ▶ train MSE: 0.7029, val MSE: 0.5305
Epoch  87/100 ▶ train MSE: 0.7025, val MSE: 0.5180
Epoch  88/100 ▶ train MSE: 0.7027, val MSE: 0.5715
Epoch  89/100 ▶ train MSE: 0.7009, val MSE: 0.5061
Epoch  90/100 ▶ train MSE: 0.6991, val MSE: 0.5205
Epoch  91/100 ▶ train MSE: 0.6980, val MSE: 0.4923
Epoch  92/100 ▶ train MSE: 0.7001, val MSE: 0.5231
Epoch  93/100 ▶ train MSE: 0.6968, val MSE: 0.5967
Epoch  94/100 ▶ train MSE: 0.6974, val MSE: 0.5254
Epoch  95/100 ▶ train MSE: 0.6974, val MSE: 0.5126
Epoch  96/100 ▶ train MSE: 0.6959, val MSE: 0.5894
Epoch  97/100 ▶ train MSE: 0.6957, val MSE: 0.5198
Epoch  98/100 ▶ train MSE: 0.6960, val MSE: 0.5050
Epoch  99/100 ▶ train MSE: 0.6952, val MSE: 0.4812
Epoch 100/100 ▶ train MSE: 0.6956, val MSE: 0.5245

✅ Training complete!
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
✅ SGD‐trained model and metadata saved to `checkpoints_sgd/`
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e68a654a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Using-Optuna-to-do-hyperparameter-tuning-for-our-L-BFGS-implementation:-Finding-optimal-learning-rate,-max-iterations,-and-epochs.">Using Optuna to do hyperparameter tuning for our L-BFGS implementation: Finding optimal learning rate, max iterations, and epochs.<a class="anchor-link" href="#Using-Optuna-to-do-hyperparameter-tuning-for-our-L-BFGS-implementation:-Finding-optimal-learning-rate,-max-iterations,-and-epochs.">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=ce4c71e6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">L-BFGS Optimisation with Optuna for Feed-Forward Neural Network Training</span>
<span class="sd">=========================================================================</span>

<span class="sd">This script implements hyperparameter optimisation for training a Feed-Forward Neural Network (FFN)</span>
<span class="sd">using the Limited-memory BFGS (L-BFGS) algorithm. It is designed as part of a comparative study of</span>
<span class="sd">SGD, L-BFGS, and Genetic Algorithms on a multimodal loss landscape, as specified in the project brief.</span>

<span class="sd">The workflow includes:</span>
<span class="sd">1. Model architecture setup using a fixed Optuna-tuned FFN configuration.</span>
<span class="sd">2. Weight initialisation using configurable schemes (Xavier, Kaiming).</span>
<span class="sd">3. L-BFGS hyperparameter tuning via Optuna over learning rate, max_iter, and training epochs.</span>
<span class="sd">4. Final retraining of the best model with loss curves plotted for visual comparison.</span>
<span class="sd">5. Exposure of the trained model and hyperparameters for downstream use.</span>

<span class="sd">Instructions for Use</span>
<span class="sd">---------------------</span>
<span class="sd">- Ensure that `X_train`, `y_train`, `X_val`, and `y_val` tensors are pre-defined in your workspace.</span>
<span class="sd">- Execute this script after defining these tensors to perform L-BFGS hyperparameter search and training.</span>
<span class="sd">- Modify `arch_params` and `init_scheme` to experiment with different FFN configurations.</span>
<span class="sd">- Adjust the Optuna search space within `objective()` for further tuning.</span>

<span class="sd">Changing the Architecture</span>
<span class="sd">--------------------------</span>
<span class="sd">- The FFN architecture is defined by `arch_params`, which controls the number of layers, units per layer,</span>
<span class="sd">  and activation function. Only standard PyTorch activations (e.g., "ReLU", "Tanh") are supported.</span>
<span class="sd">- To change the architecture, update `arch_params` accordingly.</span>

<span class="sd">Changing the Initialisation Scheme</span>
<span class="sd">-----------------------------------</span>
<span class="sd">- Set `init_scheme` to one of: 'xavier_normal', 'xavier_uniform', 'kaiming_normal', or 'kaiming_uniform'.</span>

<span class="sd">Customising the Optuna Search</span>
<span class="sd">------------------------------</span>
<span class="sd">- The `objective()` function defines the search space:</span>
<span class="sd">    * `lr`: Learning rate (log-uniform)</span>
<span class="sd">    * `max_iter`: L-BFGS internal iterations per outer step</span>
<span class="sd">    * `epochs`: Total number of epochs to run optimizer.step()</span>
<span class="sd">- Modify the ranges or add new parameters as needed.</span>

<span class="sd">Outputs</span>
<span class="sd">-------</span>
<span class="sd">- Prints the best hyperparameters and validation MSE.</span>
<span class="sd">- Plots training and validation MSE curves.</span>
<span class="sd">- Exposes:</span>
<span class="sd">    * `best_model_lbfgs`: trained FFN model</span>
<span class="sd">    * `best_params_lbfgs`: dictionary of best Optuna hyperparameters</span>

<span class="sd">"""</span>

<span class="c1">#  L-BFGS + Optuna Hyperparameter Search on Best FFN Architecture</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>

<span class="c1"># 1) Device</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>

<span class="c1"># 2) Move data to device</span>
<span class="n">X_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 3) Best FFN architecture from Optuna + init scheme</span>
<span class="n">arch_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"n_units"</span><span class="p">:</span>    <span class="mi">24</span><span class="p">,</span>
    <span class="s2">"activation"</span><span class="p">:</span> <span class="s2">"ReLU"</span>
<span class="p">}</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="k">def</span><span class="w"> </span><span class="nf">initialize_weights</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">init_scheme</span> <span class="o">==</span> <span class="s1">'xavier_normal'</span><span class="p">:</span>
                <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">init_scheme</span> <span class="o">==</span> <span class="s1">'xavier_uniform'</span><span class="p">:</span>
                <span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">init_scheme</span> <span class="o">==</span> <span class="s1">'kaiming_normal'</span><span class="p">:</span>
                <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">init_scheme</span> <span class="o">==</span> <span class="s1">'kaiming_uniform'</span><span class="p">:</span>
                <span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_f</span> <span class="o">=</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span>  <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">initialize_weights</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># 4) Optuna objective</span>
<span class="k">def</span><span class="w"> </span><span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="n">lr</span>       <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_loguniform</span><span class="p">(</span><span class="s2">"lr"</span><span class="p">,</span>      <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s2">"max_iter"</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span>
    <span class="n">epochs</span>   <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">"epochs"</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    
    <span class="n">model</span>     <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
    
    <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y_train_dev</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
        <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">val_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
    <span class="c1"># report final validation loss</span>
    <span class="k">return</span> <span class="n">val_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># 5) Run the search</span>
<span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">"minimize"</span><span class="p">)</span>
<span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 6) Retrieve best hyperparameters</span>
<span class="n">best</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> Best L-BFGS Config:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best Validation MSE: </span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">value</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># 7) Re-train best model to record curves</span>
<span class="n">best_lr</span>       <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">]</span>
<span class="n">best_max_iter</span> <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">"max_iter"</span><span class="p">]</span>
<span class="n">best_epochs</span>   <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">"epochs"</span><span class="p">]</span>
<span class="n">model</span>     <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">best_lr</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">best_max_iter</span><span class="p">)</span>
<span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">best_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y_train_dev</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="p">(</span><span class="n">best_epochs</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">best_epochs</span><span class="si">}</span><span class="s2"> ▶ train MSE: </span><span class="si">{</span><span class="n">train_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">val_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ Final L-BFGS training complete!"</span><span class="p">)</span>

<span class="c1"># 8) Plot the loss curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_losses</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"L-BFGS Training &amp; Validation Loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 9) Expose best model &amp; params for downstream use</span>
<span class="n">best_model_lbfgs</span> <span class="o">=</span> <span class="n">model</span>
<span class="n">best_params_lbfgs</span> <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>[I 2025-05-28 14:19:21,785] A new study created in memory with name: no-name-3a70d6ab-00f9-4d41-9a48-558d13cc2b69
  0%|          | 0/20 [00:00&lt;?, ?it/s]/tmp/ipykernel_5499/59883290.py:57: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr       = trial.suggest_loguniform("lr",      1e-2, 10.0)
  5%|▌         | 1/20 [00:11&lt;03:41, 11.64s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:19:33,431] Trial 0 failed with parameters: {'lr': 2.08981369921552, 'max_iter': 50, 'epochs': 40} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:19:33,434] Trial 0 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre> 10%|█         | 2/20 [00:13&lt;01:50,  6.16s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:19:35,750] Trial 1 failed with parameters: {'lr': 2.2316699775559283, 'max_iter': 10, 'epochs': 40} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:19:35,752] Trial 1 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 2. Best value: 0.635706:  15%|█▌        | 3/20 [00:20&lt;01:44,  6.16s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:19:41,907] Trial 2 finished with value: 0.6357057094573975 and parameters: {'lr': 0.041762790769269996, 'max_iter': 10, 'epochs': 100}. Best is trial 2 with value: 0.6357057094573975.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 3. Best value: 0.431403:  20%|██        | 4/20 [00:26&lt;01:43,  6.44s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:19:48,779] Trial 3 finished with value: 0.4314032196998596 and parameters: {'lr': 0.06168203583807849, 'max_iter': 20, 'epochs': 60}. Best is trial 3 with value: 0.4314032196998596.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  25%|██▌       | 5/20 [00:34&lt;01:39,  6.65s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:19:55,807] Trial 4 finished with value: 0.3471372723579407 and parameters: {'lr': 1.3615240458927713, 'max_iter': 20, 'epochs': 60}. Best is trial 4 with value: 0.3471372723579407.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  30%|███       | 6/20 [00:39&lt;01:29,  6.36s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:20:01,597] Trial 5 finished with value: 0.4124971330165863 and parameters: {'lr': 0.10160487871637558, 'max_iter': 10, 'epochs': 100}. Best is trial 4 with value: 0.3471372723579407.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  35%|███▌      | 7/20 [01:04&lt;02:41, 12.41s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:20:26,453] Trial 6 failed with parameters: {'lr': 4.5260308447550655, 'max_iter': 50, 'epochs': 100} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:20:26,455] Trial 6 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  40%|████      | 8/20 [01:27&lt;03:10, 15.84s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:20:49,639] Trial 7 failed with parameters: {'lr': 0.011931180028593898, 'max_iter': 50, 'epochs': 80} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:20:49,642] Trial 7 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  45%|████▌     | 9/20 [01:44&lt;02:55, 15.96s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:21:05,860] Trial 8 failed with parameters: {'lr': 1.9211059060929936, 'max_iter': 50, 'epochs': 60} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:21:05,863] Trial 8 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  50%|█████     | 10/20 [01:48&lt;02:02, 12.27s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:21:09,877] Trial 9 failed with parameters: {'lr': 9.009499918610667, 'max_iter': 20, 'epochs': 40} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:21:09,879] Trial 9 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  55%|█████▌    | 11/20 [01:51&lt;01:25,  9.54s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:21:13,220] Trial 10 finished with value: 0.4225972294807434 and parameters: {'lr': 0.2792263989562056, 'max_iter': 10, 'epochs': 60}. Best is trial 4 with value: 0.3471372723579407.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  60%|██████    | 12/20 [01:59&lt;01:12,  9.01s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:21:21,020] Trial 11 failed with parameters: {'lr': 4.108316983113093, 'max_iter': 20, 'epochs': 80} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:21:21,023] Trial 11 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  65%|██████▌   | 13/20 [02:00&lt;00:46,  6.66s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:21:22,281] Trial 12 finished with value: 0.7582546472549438 and parameters: {'lr': 0.16338714361958181, 'max_iter': 10, 'epochs': 20}. Best is trial 4 with value: 0.3471372723579407.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  70%|███████   | 14/20 [02:02&lt;00:31,  5.26s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:21:24,309] Trial 13 failed with parameters: {'lr': 9.385566437479376, 'max_iter': 20, 'epochs': 20} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:21:24,312] Trial 13 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  75%|███████▌  | 15/20 [02:28&lt;00:56, 11.36s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:21:49,808] Trial 14 failed with parameters: {'lr': 7.919467830773519, 'max_iter': 50, 'epochs': 100} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:21:49,811] Trial 14 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  80%|████████  | 16/20 [02:30&lt;00:34,  8.67s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:21:52,220] Trial 15 finished with value: 0.9797548651695251 and parameters: {'lr': 0.01422571893517124, 'max_iter': 10, 'epochs': 40}. Best is trial 4 with value: 0.3471372723579407.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 16. Best value: 0.210479:  85%|████████▌ | 17/20 [02:34&lt;00:22,  7.42s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:21:56,730] Trial 16 finished with value: 0.2104787528514862 and parameters: {'lr': 0.6843929977747965, 'max_iter': 10, 'epochs': 80}. Best is trial 16 with value: 0.2104787528514862.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 16. Best value: 0.210479:  90%|█████████ | 18/20 [02:39&lt;00:12,  6.43s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:22:00,843] Trial 17 failed with parameters: {'lr': 2.7875555078900844, 'max_iter': 10, 'epochs': 80} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:22:00,846] Trial 17 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 16. Best value: 0.210479:  95%|█████████▌| 19/20 [02:44&lt;00:06,  6.26s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:22:06,714] Trial 18 failed with parameters: {'lr': 0.03772030648482473, 'max_iter': 10, 'epochs': 100} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:22:06,716] Trial 18 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 19. Best value: 0.184061: 100%|██████████| 20/20 [02:56&lt;00:00,  8.84s/it]
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:22:18,656] Trial 19 finished with value: 0.1840611696243286 and parameters: {'lr': 0.9669461523303573, 'max_iter': 20, 'epochs': 100}. Best is trial 19 with value: 0.1840611696243286.

🏆 Best L-BFGS Config:
  lr: 0.9669461523303573
  max_iter: 20
  epochs: 100
Best Validation MSE: 0.18406

Epoch 1/100 ▶ train MSE: 1.4198, val MSE: 0.9929
Epoch 20/100 ▶ train MSE: 0.7260, val MSE: 0.5253
Epoch 40/100 ▶ train MSE: 0.6450, val MSE: 0.4130
Epoch 60/100 ▶ train MSE: 0.5921, val MSE: 0.3304
Epoch 80/100 ▶ train MSE: 0.5516, val MSE: 0.2728
Epoch 100/100 ▶ train MSE: 0.5408, val MSE: 0.2614

✅ Final L-BFGS training complete!
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=9a2f126b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="LBFGS-run-using-parameters-found-with-optuna:">LBFGS run using parameters found with optuna:<a class="anchor-link" href="#LBFGS-run-using-parameters-found-with-optuna:">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=e08bd7b5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Final L-BFGS Training and Checkpointing for Feed-Forward Neural Network</span>
<span class="sd">========================================================================</span>

<span class="sd">This script performs final training of a Feed-Forward Neural Network (FFN)</span>
<span class="sd">using the L-BFGS optimiser with previously tuned hyperparameters, logs</span>
<span class="sd">training and validation losses, and saves the model weights and metadata</span>
<span class="sd">for downstream use.</span>

<span class="sd">Modules Used:</span>
<span class="sd">-------------</span>
<span class="sd">- PyTorch (`torch`, `torch.nn`, `torch.optim`)</span>
<span class="sd">- Matplotlib for loss visualization</span>
<span class="sd">- NumPy and JSON for checkpoint management</span>

<span class="sd">Workflow Overview:</span>
<span class="sd">------------------</span>
<span class="sd">1. Load preprocessed training and validation tensors (`X_train`, `y_train`, `X_val`, `y_val`).</span>
<span class="sd">2. Define and build the FFN architecture using saved best parameters.</span>
<span class="sd">3. Initialise the L-BFGS optimiser with the tuned hyperparameters.</span>
<span class="sd">4. Train the model using full-batch L-BFGS over multiple epochs.</span>
<span class="sd">5. Log epoch duration, train MSE, and validation MSE to a CSV file (`lbfgs_stats.csv`).</span>
<span class="sd">6. Plot loss curves and save figure (optional extension).</span>
<span class="sd">7. Save:</span>
<span class="sd">   - model weights (`.pth`)</span>
<span class="sd">   - final genome as flat parameter vector (`.npy`)</span>
<span class="sd">   - full metadata (`.json`) for reproducibility.</span>

<span class="sd">How to Use:</span>
<span class="sd">-----------</span>
<span class="sd">- Ensure your data tensors (`X_train`, `y_train`, `X_val`, `y_val`) are defined beforehand.</span>
<span class="sd">- Adjust `arch_params` or `best_lbfgs` dictionaries to use different model configurations or hyperparameters.</span>
<span class="sd">- This script automatically logs results and saves model checkpoints to `checkpoints_lbfgs/`.</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- `lbfgs_stats.csv`: MSE log per epoch</span>
<span class="sd">- `checkpoints_lbfgs/lbfgs_final_weights.pth`: Final model weights</span>
<span class="sd">- `checkpoints_lbfgs/lbfgs_final_genome.npy`: Flat parameter vector</span>
<span class="sd">- `checkpoints_lbfgs/lbfgs_model_meta.json`: JSON metadata (architecture + optimiser params)</span>
<span class="sd">- Final loss curves plotted using Matplotlib</span>

<span class="sd">Notes:</span>
<span class="sd">------</span>
<span class="sd">- This script assumes full-batch training (i.e., L-BFGS is called once per epoch).</span>
<span class="sd">- Make sure the GPU is available and accessible via `torch.cuda.is_available()`; otherwise, CPU will be used.</span>
<span class="sd">- Can be extended to include test-set evaluation using the trained model.</span>

<span class="sd">"""</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span><span class="o">,</span><span class="w"> </span><span class="nn">json</span><span class="o">,</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span>

<span class="c1"># 1) Device</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># 2) Best FFN architecture (from Optuna) + your L-BFGS hyperparams</span>
<span class="n">arch_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"n_units"</span><span class="p">:</span>    <span class="mi">24</span><span class="p">,</span>
    <span class="s2">"activation"</span><span class="p">:</span> <span class="s2">"ReLU"</span>
<span class="p">}</span>
<span class="n">best_lbfgs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"lr"</span><span class="p">:</span>       <span class="mf">0.9669461523303573</span><span class="p">,</span>
    <span class="s2">"max_iter"</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">"epochs"</span><span class="p">:</span>   <span class="mi">100</span>
<span class="p">}</span>

<span class="c1"># 3) Move data to device</span>
<span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_v</span><span class="p">,</span>  <span class="n">y_v</span>  <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 4) Build the model</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">in_f</span>   <span class="o">=</span> <span class="n">X_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">Act</span>    <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
    <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
    <span class="n">in_f</span>    <span class="o">=</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 5) Optimizer &amp; loss</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">best_lbfgs</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="n">best_lbfgs</span><span class="p">[</span><span class="s2">"max_iter"</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># 6) Prepare L-BFGS log file</span>
<span class="n">LOG_FN</span> <span class="o">=</span> <span class="s2">"lbfgs_stats.csv"</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">"epoch,epoch_time,train_mse,val_mse</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># 6) Train for N full-batch epochs</span>
<span class="n">epochs</span>       <span class="o">=</span> <span class="n">best_lbfgs</span><span class="p">[</span><span class="s2">"epochs"</span><span class="p">]</span>
<span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_losses</span>   <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">out</span>  <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tr</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="c1"># one L-BFGS step</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="c1"># eval on validation set</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_v</span><span class="p">),</span> <span class="n">y_v</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_mse</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">epoch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="c1"># append to CSV</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">epoch_time</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">train_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">val_mse</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s2">3d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2"> ▶ train MSE: </span><span class="si">{</span><span class="n">train_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">val_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ L-BFGS training complete!"</span><span class="p">)</span>

<span class="c1"># 7) Plot curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_losses</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Final FFN (L-BFGS) Training &amp; Validation Loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">#  Save final L-BFGS–trained model </span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">"checkpoints_lbfgs"</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 1) Save the PyTorch state_dict</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s2">"checkpoints_lbfgs/lbfgs_final_weights.pth"</span>
<span class="p">)</span>

<span class="c1"># 2) Save the flat parameter vector</span>
<span class="n">genome_lbfgs</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
    <span class="s2">"checkpoints_lbfgs/lbfgs_final_genome.npy"</span><span class="p">,</span>
    <span class="n">genome_lbfgs</span>
<span class="p">)</span>

<span class="c1"># 3) Save the architecture &amp; L-BFGS hyperparams so you can rebuild later</span>
<span class="n">meta_lbfgs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"arch"</span><span class="p">:</span> <span class="n">arch_params</span><span class="p">,</span>      <span class="c1"># your {"n_layers":…, "n_units":…, "activation":…}</span>
    <span class="s2">"lbfgs_params"</span><span class="p">:</span> <span class="n">best_lbfgs</span> <span class="c1"># your {"lr":…, "max_iter":…, "epochs":…}</span>
<span class="p">}</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"checkpoints_lbfgs/lbfgs_model_meta.json"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">meta_lbfgs</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ L-BFGS–trained model and metadata saved to `checkpoints_lbfgs/`"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Epoch   1/100 ▶ train MSE: 1.0225, val MSE: 0.9938
Epoch  20/100 ▶ train MSE: 0.7566, val MSE: 0.5818
Epoch  40/100 ▶ train MSE: 0.6345, val MSE: 0.4083
Epoch  60/100 ▶ train MSE: 0.6128, val MSE: 0.3795
Epoch  80/100 ▶ train MSE: 0.5997, val MSE: 0.3634
Epoch 100/100 ▶ train MSE: 0.5949, val MSE: 0.3593

✅ L-BFGS training complete!
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
✅ L-BFGS–trained model and metadata saved to `checkpoints_lbfgs/`
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=f8b502f8">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Vanilla-GA:-First-reasonably-successful-implementation:">Vanilla GA: First reasonably successful implementation:<a class="anchor-link" href="#Vanilla-GA:-First-reasonably-successful-implementation:">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0fc6cbde">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Genetic Algorithm (GA) for Optimising Feed-Forward Neural Network Weights</span>
<span class="sd">==========================================================================</span>

<span class="sd">This script implements a real-valued Genetic Algorithm (GA) using Blend Crossover (BLX-α)</span>
<span class="sd">and Gaussian mutation to optimise the parameters of a fixed-architecture feed-forward</span>
<span class="sd">neural network (FFN). The algorithm is applied to a regression task with MSE loss, and</span>
<span class="sd">performance is tracked across generations.</span>

<span class="sd">Key Features:</span>
<span class="sd">-------------</span>
<span class="sd">- Fixed FFN architecture using PyTorch with Xavier weight initialisation</span>
<span class="sd">- Real-valued genome representation using PyTorch parameter vectors</span>
<span class="sd">- Tournament selection (size = 3) with elitism (top 20% retained each generation)</span>
<span class="sd">- BLX-α crossover with α = 0.3 for diversity-preserving recombination</span>
<span class="sd">- Gaussian mutation applied per gene with probability `mutation_p` and std dev `mutation_sd`</span>
<span class="sd">- Reproducible results via fixed seeds for NumPy and PyTorch</span>
<span class="sd">- Final best model reconstructed and evaluated</span>
<span class="sd">- MSE loss curves plotted across generations</span>

<span class="sd">Instructions:</span>
<span class="sd">-------------</span>
<span class="sd">1. Ensure that `X_train`, `y_train`, `X_val`, and `y_val` are defined as PyTorch tensors.</span>
<span class="sd">2. Adjust architecture via the `arch` dictionary.</span>
<span class="sd">3. Modify genetic algorithm hyperparameters (e.g., `pop_size`, `generations`, `mutation_p`) as needed.</span>
<span class="sd">4. Run the script to execute the full evolutionary cycle and view results.</span>

<span class="sd">Hyperparameters:</span>
<span class="sd">----------------</span>
<span class="sd">- `pop_size`: Number of individuals in the population</span>
<span class="sd">- `generations`: Number of generations to evolve</span>
<span class="sd">- `elite_frac`: Proportion of top individuals carried over each generation</span>
<span class="sd">- `tourn_size`: Tournament size for selection</span>
<span class="sd">- `mutation_p`: Probability of mutating each gene</span>
<span class="sd">- `mutation_sd`: Standard deviation of mutation noise</span>
<span class="sd">- `blx_alpha`: BLX-α parameter controlling crossover range</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Printed train/validation MSE every 100 generations</span>
<span class="sd">- Final best model with evaluation on training and validation sets</span>
<span class="sd">- Plot of training and validation MSE vs. generation</span>
<span class="sd">- Internal genome evolution stored in memory only (can be extended to save)</span>


<span class="sd">"""</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">skPCA</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>
<span class="c1">#  Repro &amp; Device</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1"># Data to device</span>
<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">#   Fixed Architecture + Xavier init </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"ReLU"</span><span class="p">)</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="n">criterion</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span><span class="p">,</span> <span class="n">in_f</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>

<span class="c1">#  GA Hyperparams</span>
<span class="n">pop_size</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">generations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">elite_frac</span>  <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tourn_size</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">mutation_p</span>  <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">mutation_sd</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">blx_alpha</span>   <span class="o">=</span> <span class="mf">0.3</span>  

<span class="c1">#  Init Population</span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pop_size</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>

<span class="c1"># visualize initial population </span>
<span class="n">pop_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>          
<span class="n">genome_len</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#  5) Tournament Selection</span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pop_size</span><span class="p">,</span> <span class="n">tourn_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>
<span class="c1">#  6) Evolution </span>
<span class="n">train_curve</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_curve</span>   <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># Fitness eval</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="c1"># record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="c1">#Diagnostic code</span>
    <span class="n">best_gen</span>   <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">norm_best</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">best_gen</span><span class="p">)</span>
    <span class="n">best_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_best</span><span class="p">)</span>
    <span class="n">prev_best_gen</span> <span class="o">=</span> <span class="n">best_gen</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">tr_mse</span>   <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">m_best</span>   <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">va_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tr_mse</span><span class="p">)</span>
    <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">va_mse</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gen</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">gen</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2"> ▶ train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1">#  Elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elites</span>  <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]]</span>
    <span class="n">pop_size</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
    <span class="n">elite_frac</span> <span class="o">=</span> <span class="mf">0.2</span>            
    <span class="n">elite_n</span>    <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
    <span class="n">elites</span>     <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">]</span>
    <span class="c1"># selection probabilities</span>
    <span class="n">p_elite</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_tourn</span>   <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># p_random = 0.1  # implicit: 1 - (p_elite + p_tourn)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># pure tournament selection</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="c1"># pure elitism</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span><span class="p">:</span>
            <span class="c1"># exploit: uniform from elites</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="k">elif</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span> <span class="o">+</span> <span class="n">p_tourn</span><span class="p">:</span>
            <span class="c1"># competition: standard tournament over full pop</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># pure exploration: uniform from entire pop</span>
            <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">pop_size</span><span class="p">)]</span>
    <span class="c1">#  Reproduce via BLX-α + mutation</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">pop_size</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="c1"># BLX-α crossover</span>
        <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># mutation</span>
        <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mutation_p</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mutation_sd</span>
        <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1"># ─── 7) Final Best Model ───────────────────────────────────────────────────────</span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))]</span>
<span class="n">best_model_ga</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                     <span class="n">best_model_ga</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">best_model_ga</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ GA done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="c1"># 8) Plot </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA (w/ BLX-α) Optimization of FFN Weights"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen  1/2000 ▶ train MSE: 1.0086, val MSE: 1.0063
Gen 100/2000 ▶ train MSE: 0.9933, val MSE: 0.9912
Gen 200/2000 ▶ train MSE: 0.9892, val MSE: 0.9848
Gen 300/2000 ▶ train MSE: 0.9840, val MSE: 0.9778
Gen 400/2000 ▶ train MSE: 0.9783, val MSE: 0.9704
Gen 500/2000 ▶ train MSE: 0.9736, val MSE: 0.9633
Gen 600/2000 ▶ train MSE: 0.9697, val MSE: 0.9576
Gen 700/2000 ▶ train MSE: 0.9661, val MSE: 0.9520
Gen 800/2000 ▶ train MSE: 0.9629, val MSE: 0.9459
Gen 900/2000 ▶ train MSE: 0.9605, val MSE: 0.9425
Gen 1000/2000 ▶ train MSE: 0.9582, val MSE: 0.9379
Gen 1100/2000 ▶ train MSE: 0.9561, val MSE: 0.9344
Gen 1200/2000 ▶ train MSE: 0.9541, val MSE: 0.9314
Gen 1300/2000 ▶ train MSE: 0.9513, val MSE: 0.9272
Gen 1400/2000 ▶ train MSE: 0.9486, val MSE: 0.9223
Gen 1500/2000 ▶ train MSE: 0.9459, val MSE: 0.9175
Gen 1600/2000 ▶ train MSE: 0.9430, val MSE: 0.9129
Gen 1700/2000 ▶ train MSE: 0.9401, val MSE: 0.9082
Gen 1800/2000 ▶ train MSE: 0.9375, val MSE: 0.9038
Gen 1900/2000 ▶ train MSE: 0.9340, val MSE: 0.8980
Gen 2000/2000 ▶ train MSE: 0.9304, val MSE: 0.8929

✅ GA done!  Final Train MSE: 0.9308, Val MSE: 0.8929
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=c444ac3f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Same-GA-implementation:-Changing-alpha-value-for-blx-crossover-from-0.3-to-0.5:">Same GA implementation: Changing alpha value for blx crossover from 0.3 to 0.5:<a class="anchor-link" href="#Same-GA-implementation:-Changing-alpha-value-for-blx-crossover-from-0.3-to-0.5:">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=04819981">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Genetic Algorithm (GA) for Optimising Feed-Forward Neural Network Weights</span>
<span class="sd">==========================================================================</span>

<span class="sd">This script implements a real-valued Genetic Algorithm (GA) using Blend Crossover (BLX-α)</span>
<span class="sd">and Gaussian mutation to optimise the parameters of a fixed-architecture feed-forward</span>
<span class="sd">neural network (FFN). The algorithm is applied to a regression task with MSE loss, and</span>
<span class="sd">performance is tracked across generations.</span>

<span class="sd">Key Features:</span>
<span class="sd">-------------</span>
<span class="sd">- Fixed FFN architecture using PyTorch with Xavier weight initialisation</span>
<span class="sd">- Real-valued genome representation using PyTorch parameter vectors</span>
<span class="sd">- Tournament selection (size = 3) with elitism (top 20% retained each generation)</span>
<span class="sd">- BLX-α crossover with α = 0.5 for diversity-preserving recombination</span>
<span class="sd">- Gaussian mutation applied per gene with probability `mutation_p` and std dev `mutation_sd`</span>
<span class="sd">- Reproducible results via fixed seeds for NumPy and PyTorch</span>
<span class="sd">- Final best model reconstructed and evaluated</span>
<span class="sd">- MSE loss curves plotted across generations</span>

<span class="sd">Instructions:</span>
<span class="sd">-------------</span>
<span class="sd">1. Ensure that `X_train`, `y_train`, `X_val`, and `y_val` are defined as PyTorch tensors.</span>
<span class="sd">2. Adjust architecture via the `arch` dictionary.</span>
<span class="sd">3. Modify genetic algorithm hyperparameters (e.g., `pop_size`, `generations`, `mutation_p`) as needed.</span>
<span class="sd">4. Run the script to execute the full evolutionary cycle and view results.</span>

<span class="sd">Hyperparameters:</span>
<span class="sd">----------------</span>
<span class="sd">- `pop_size`: Number of individuals in the population</span>
<span class="sd">- `generations`: Number of generations to evolve</span>
<span class="sd">- `elite_frac`: Proportion of top individuals carried over each generation</span>
<span class="sd">- `tourn_size`: Tournament size for selection</span>
<span class="sd">- `mutation_p`: Probability of mutating each gene</span>
<span class="sd">- `mutation_sd`: Standard deviation of mutation noise</span>
<span class="sd">- `blx_alpha`: BLX-α parameter controlling crossover range</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Printed train/validation MSE every 100 generations</span>
<span class="sd">- Final best model with evaluation on training and validation sets</span>
<span class="sd">- Plot of training and validation MSE vs. generation</span>
<span class="sd">- Internal genome evolution stored in memory only (can be extended to save)</span>


<span class="sd">"""</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">skPCA</span>

<span class="c1">#  0) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1">#  1) Data to device </span>
<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">#  2) Fixed Architecture + Xavier init </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"ReLU"</span><span class="p">)</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="n">criterion</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span><span class="p">,</span> <span class="n">in_f</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>

<span class="c1">#  3) GA Hyperparams </span>
<span class="n">pop_size</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">generations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">elite_frac</span>  <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tourn_size</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">mutation_p</span>  <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">mutation_sd</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">blx_alpha</span>   <span class="o">=</span> <span class="mf">0.5</span>  

<span class="c1">#  4) Init Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pop_size</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>

<span class="c1">#  visualize initial population </span>
<span class="n">pop_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>          
<span class="n">genome_len</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span>

<span class="c1">#  5) Tournament Selection </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pop_size</span><span class="p">,</span> <span class="n">tourn_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>
<span class="c1">#  6) Evolution </span>
<span class="n">train_curve</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_curve</span>   <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># a) Fitness eval</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gen</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">gen</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2"> ▶ train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">tr_mse</span>   <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">m_best</span>   <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">va_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tr_mse</span><span class="p">)</span>
    <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">va_mse</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2"> ▶ train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># Elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elites</span>  <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]]</span>
    <span class="n">pop_size</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
    <span class="n">elite_frac</span> <span class="o">=</span> <span class="mf">0.2</span>             
    <span class="n">elite_n</span>    <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
    <span class="n">elites</span>     <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">]</span>
    <span class="c1"># selection probabilities</span>
    <span class="n">p_elite</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_tourn</span>   <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># p_random = 0.1  # implicit: 1 - (p_elite + p_tourn)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># pure tournament selection</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="c1"># pure elitism</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span><span class="p">:</span>
            <span class="c1"># exploit: uniform from elites</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="k">elif</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span> <span class="o">+</span> <span class="n">p_tourn</span><span class="p">:</span>
            <span class="c1"># competition: standard tournament over full pop</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># pure exploration: uniform from entire pop</span>
            <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">pop_size</span><span class="p">)]</span>
    <span class="c1"># c) Reproduce via BLX-α + mutation</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">pop_size</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="c1"># BLX-α crossover</span>
        <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># mutation applied</span>
        <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mutation_p</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mutation_sd</span>
        <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1">#  7) Final Best Model </span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))]</span>
<span class="n">best_model_ga</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                     <span class="n">best_model_ga</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">best_model_ga</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ GA done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  8) Plot </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA (w/ BLX-α) Optimization of FFN Weights"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen  1/2000 ▶ train MSE: 0.9623, val MSE: 0.9498
Gen  1/2000 ▶ train MSE: 1.0086, val MSE: 1.0063
Gen  2/2000 ▶ train MSE: 1.0086, val MSE: 1.0063
Gen  3/2000 ▶ train MSE: 1.0086, val MSE: 1.0063
Gen  4/2000 ▶ train MSE: 1.0086, val MSE: 1.0063
Gen  5/2000 ▶ train MSE: 1.0086, val MSE: 1.0063
Gen  6/2000 ▶ train MSE: 1.0086, val MSE: 1.0063
Gen  7/2000 ▶ train MSE: 1.0086, val MSE: 1.0063
Gen  8/2000 ▶ train MSE: 1.0086, val MSE: 1.0063
Gen  9/2000 ▶ train MSE: 1.0086, val MSE: 1.0063
Gen 10/2000 ▶ train MSE: 1.0086, val MSE: 1.0063
Gen 11/2000 ▶ train MSE: 1.0080, val MSE: 1.0096
Gen 12/2000 ▶ train MSE: 1.0080, val MSE: 1.0096
Gen 13/2000 ▶ train MSE: 1.0080, val MSE: 1.0096
Gen 14/2000 ▶ train MSE: 1.0047, val MSE: 1.0029
Gen 15/2000 ▶ train MSE: 1.0047, val MSE: 1.0029
Gen 16/2000 ▶ train MSE: 1.0047, val MSE: 1.0029
Gen 17/2000 ▶ train MSE: 1.0047, val MSE: 1.0029
Gen 18/2000 ▶ train MSE: 1.0047, val MSE: 1.0029
Gen 19/2000 ▶ train MSE: 1.0047, val MSE: 1.0029
Gen 20/2000 ▶ train MSE: 1.0047, val MSE: 1.0029
Gen 21/2000 ▶ train MSE: 1.0047, val MSE: 1.0029
Gen 22/2000 ▶ train MSE: 1.0047, val MSE: 1.0029
Gen 23/2000 ▶ train MSE: 1.0047, val MSE: 1.0029
Gen 24/2000 ▶ train MSE: 1.0047, val MSE: 1.0029
Gen 25/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 26/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 27/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 28/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 29/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 30/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 31/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 32/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 33/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 34/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 35/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 36/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 37/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 38/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 39/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 40/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 41/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 42/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 43/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 44/2000 ▶ train MSE: 1.0010, val MSE: 1.0012
Gen 45/2000 ▶ train MSE: 1.0010, val MSE: 0.9990
Gen 46/2000 ▶ train MSE: 1.0008, val MSE: 0.9994
Gen 47/2000 ▶ train MSE: 1.0008, val MSE: 0.9994
Gen 48/2000 ▶ train MSE: 1.0008, val MSE: 0.9994
Gen 49/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 50/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 51/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 52/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 53/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 54/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 55/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 56/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 57/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 58/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 59/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 60/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 61/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 62/2000 ▶ train MSE: 0.9997, val MSE: 1.0002
Gen 63/2000 ▶ train MSE: 0.9996, val MSE: 1.0013
Gen 64/2000 ▶ train MSE: 0.9996, val MSE: 1.0013
Gen 65/2000 ▶ train MSE: 0.9996, val MSE: 1.0013
Gen 66/2000 ▶ train MSE: 0.9996, val MSE: 1.0013
Gen 67/2000 ▶ train MSE: 0.9996, val MSE: 1.0013
Gen 68/2000 ▶ train MSE: 0.9991, val MSE: 0.9998
Gen 69/2000 ▶ train MSE: 0.9991, val MSE: 0.9998
Gen 70/2000 ▶ train MSE: 0.9991, val MSE: 0.9998
Gen 71/2000 ▶ train MSE: 0.9991, val MSE: 0.9998
Gen 72/2000 ▶ train MSE: 0.9991, val MSE: 0.9998
Gen 73/2000 ▶ train MSE: 0.9991, val MSE: 0.9998
Gen 74/2000 ▶ train MSE: 0.9988, val MSE: 0.9994
Gen 75/2000 ▶ train MSE: 0.9988, val MSE: 0.9994
Gen 76/2000 ▶ train MSE: 0.9985, val MSE: 0.9987
Gen 77/2000 ▶ train MSE: 0.9985, val MSE: 0.9987
Gen 78/2000 ▶ train MSE: 0.9985, val MSE: 0.9987
Gen 79/2000 ▶ train MSE: 0.9985, val MSE: 0.9987
Gen 80/2000 ▶ train MSE: 0.9985, val MSE: 0.9987
Gen 81/2000 ▶ train MSE: 0.9981, val MSE: 0.9985
Gen 82/2000 ▶ train MSE: 0.9981, val MSE: 0.9985
Gen 83/2000 ▶ train MSE: 0.9981, val MSE: 0.9985
Gen 84/2000 ▶ train MSE: 0.9981, val MSE: 0.9985
Gen 85/2000 ▶ train MSE: 0.9981, val MSE: 0.9985
Gen 86/2000 ▶ train MSE: 0.9980, val MSE: 0.9980
Gen 87/2000 ▶ train MSE: 0.9980, val MSE: 0.9980
Gen 88/2000 ▶ train MSE: 0.9980, val MSE: 0.9980
Gen 89/2000 ▶ train MSE: 0.9980, val MSE: 0.9980
Gen 90/2000 ▶ train MSE: 0.9980, val MSE: 0.9980
Gen 91/2000 ▶ train MSE: 0.9980, val MSE: 0.9980
Gen 92/2000 ▶ train MSE: 0.9979, val MSE: 0.9982
Gen 93/2000 ▶ train MSE: 0.9978, val MSE: 0.9977
Gen 94/2000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 95/2000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 96/2000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 97/2000 ▶ train MSE: 0.9974, val MSE: 0.9968
Gen 98/2000 ▶ train MSE: 0.9971, val MSE: 0.9972
Gen 99/2000 ▶ train MSE: 0.9971, val MSE: 0.9972
Gen 100/2000 ▶ train MSE: 0.9971, val MSE: 0.9972
Gen 100/2000 ▶ train MSE: 0.9971, val MSE: 0.9967
Gen 101/2000 ▶ train MSE: 0.9965, val MSE: 0.9970
Gen 102/2000 ▶ train MSE: 0.9965, val MSE: 0.9970
Gen 103/2000 ▶ train MSE: 0.9965, val MSE: 0.9970
Gen 104/2000 ▶ train MSE: 0.9965, val MSE: 0.9970
Gen 105/2000 ▶ train MSE: 0.9963, val MSE: 0.9967
Gen 106/2000 ▶ train MSE: 0.9963, val MSE: 0.9959
Gen 107/2000 ▶ train MSE: 0.9961, val MSE: 0.9956
Gen 108/2000 ▶ train MSE: 0.9961, val MSE: 0.9956
Gen 109/2000 ▶ train MSE: 0.9960, val MSE: 0.9970
Gen 110/2000 ▶ train MSE: 0.9960, val MSE: 0.9949
Gen 111/2000 ▶ train MSE: 0.9958, val MSE: 0.9963
Gen 112/2000 ▶ train MSE: 0.9958, val MSE: 0.9963
Gen 113/2000 ▶ train MSE: 0.9955, val MSE: 0.9942
Gen 114/2000 ▶ train MSE: 0.9955, val MSE: 0.9956
Gen 115/2000 ▶ train MSE: 0.9951, val MSE: 0.9937
Gen 116/2000 ▶ train MSE: 0.9951, val MSE: 0.9937
Gen 117/2000 ▶ train MSE: 0.9951, val MSE: 0.9937
Gen 118/2000 ▶ train MSE: 0.9951, val MSE: 0.9937
Gen 119/2000 ▶ train MSE: 0.9951, val MSE: 0.9937
Gen 120/2000 ▶ train MSE: 0.9949, val MSE: 0.9935
Gen 121/2000 ▶ train MSE: 0.9949, val MSE: 0.9935
Gen 122/2000 ▶ train MSE: 0.9949, val MSE: 0.9935
Gen 123/2000 ▶ train MSE: 0.9947, val MSE: 0.9938
Gen 124/2000 ▶ train MSE: 0.9944, val MSE: 0.9932
Gen 125/2000 ▶ train MSE: 0.9940, val MSE: 0.9930
Gen 126/2000 ▶ train MSE: 0.9940, val MSE: 0.9930
Gen 127/2000 ▶ train MSE: 0.9940, val MSE: 0.9930
Gen 128/2000 ▶ train MSE: 0.9939, val MSE: 0.9914
Gen 129/2000 ▶ train MSE: 0.9939, val MSE: 0.9914
Gen 130/2000 ▶ train MSE: 0.9939, val MSE: 0.9914
Gen 131/2000 ▶ train MSE: 0.9938, val MSE: 0.9917
Gen 132/2000 ▶ train MSE: 0.9937, val MSE: 0.9925
Gen 133/2000 ▶ train MSE: 0.9937, val MSE: 0.9925
Gen 134/2000 ▶ train MSE: 0.9937, val MSE: 0.9925
Gen 135/2000 ▶ train MSE: 0.9935, val MSE: 0.9910
Gen 136/2000 ▶ train MSE: 0.9935, val MSE: 0.9910
Gen 137/2000 ▶ train MSE: 0.9932, val MSE: 0.9916
Gen 138/2000 ▶ train MSE: 0.9932, val MSE: 0.9916
Gen 139/2000 ▶ train MSE: 0.9932, val MSE: 0.9916
Gen 140/2000 ▶ train MSE: 0.9932, val MSE: 0.9916
Gen 141/2000 ▶ train MSE: 0.9932, val MSE: 0.9916
Gen 142/2000 ▶ train MSE: 0.9929, val MSE: 0.9902
Gen 143/2000 ▶ train MSE: 0.9928, val MSE: 0.9909
Gen 144/2000 ▶ train MSE: 0.9928, val MSE: 0.9908
Gen 145/2000 ▶ train MSE: 0.9926, val MSE: 0.9900
Gen 146/2000 ▶ train MSE: 0.9926, val MSE: 0.9909
Gen 147/2000 ▶ train MSE: 0.9926, val MSE: 0.9909
Gen 148/2000 ▶ train MSE: 0.9924, val MSE: 0.9905
Gen 149/2000 ▶ train MSE: 0.9924, val MSE: 0.9905
Gen 150/2000 ▶ train MSE: 0.9924, val MSE: 0.9905
Gen 151/2000 ▶ train MSE: 0.9918, val MSE: 0.9898
Gen 152/2000 ▶ train MSE: 0.9918, val MSE: 0.9898
Gen 153/2000 ▶ train MSE: 0.9918, val MSE: 0.9898
Gen 154/2000 ▶ train MSE: 0.9918, val MSE: 0.9898
Gen 155/2000 ▶ train MSE: 0.9918, val MSE: 0.9895
Gen 156/2000 ▶ train MSE: 0.9918, val MSE: 0.9895
Gen 157/2000 ▶ train MSE: 0.9917, val MSE: 0.9894
Gen 158/2000 ▶ train MSE: 0.9917, val MSE: 0.9894
Gen 159/2000 ▶ train MSE: 0.9913, val MSE: 0.9890
Gen 160/2000 ▶ train MSE: 0.9913, val MSE: 0.9890
Gen 161/2000 ▶ train MSE: 0.9911, val MSE: 0.9888
Gen 162/2000 ▶ train MSE: 0.9911, val MSE: 0.9888
Gen 163/2000 ▶ train MSE: 0.9911, val MSE: 0.9888
Gen 164/2000 ▶ train MSE: 0.9911, val MSE: 0.9888
Gen 165/2000 ▶ train MSE: 0.9911, val MSE: 0.9888
Gen 166/2000 ▶ train MSE: 0.9909, val MSE: 0.9879
Gen 167/2000 ▶ train MSE: 0.9909, val MSE: 0.9885
Gen 168/2000 ▶ train MSE: 0.9908, val MSE: 0.9873
Gen 169/2000 ▶ train MSE: 0.9908, val MSE: 0.9883
Gen 170/2000 ▶ train MSE: 0.9907, val MSE: 0.9883
Gen 171/2000 ▶ train MSE: 0.9904, val MSE: 0.9864
Gen 172/2000 ▶ train MSE: 0.9904, val MSE: 0.9864
Gen 173/2000 ▶ train MSE: 0.9904, val MSE: 0.9864
Gen 174/2000 ▶ train MSE: 0.9903, val MSE: 0.9870
Gen 175/2000 ▶ train MSE: 0.9903, val MSE: 0.9870
Gen 176/2000 ▶ train MSE: 0.9902, val MSE: 0.9869
Gen 177/2000 ▶ train MSE: 0.9901, val MSE: 0.9863
Gen 178/2000 ▶ train MSE: 0.9901, val MSE: 0.9863
Gen 179/2000 ▶ train MSE: 0.9901, val MSE: 0.9863
Gen 180/2000 ▶ train MSE: 0.9900, val MSE: 0.9865
Gen 181/2000 ▶ train MSE: 0.9898, val MSE: 0.9854
Gen 182/2000 ▶ train MSE: 0.9898, val MSE: 0.9854
Gen 183/2000 ▶ train MSE: 0.9898, val MSE: 0.9854
Gen 184/2000 ▶ train MSE: 0.9898, val MSE: 0.9854
Gen 185/2000 ▶ train MSE: 0.9897, val MSE: 0.9854
Gen 186/2000 ▶ train MSE: 0.9897, val MSE: 0.9854
Gen 187/2000 ▶ train MSE: 0.9897, val MSE: 0.9854
Gen 188/2000 ▶ train MSE: 0.9895, val MSE: 0.9851
Gen 189/2000 ▶ train MSE: 0.9895, val MSE: 0.9851
Gen 190/2000 ▶ train MSE: 0.9895, val MSE: 0.9851
Gen 191/2000 ▶ train MSE: 0.9895, val MSE: 0.9851
Gen 192/2000 ▶ train MSE: 0.9889, val MSE: 0.9836
Gen 193/2000 ▶ train MSE: 0.9889, val MSE: 0.9836
Gen 194/2000 ▶ train MSE: 0.9889, val MSE: 0.9836
Gen 195/2000 ▶ train MSE: 0.9889, val MSE: 0.9836
Gen 196/2000 ▶ train MSE: 0.9889, val MSE: 0.9836
Gen 197/2000 ▶ train MSE: 0.9888, val MSE: 0.9845
Gen 198/2000 ▶ train MSE: 0.9887, val MSE: 0.9842
Gen 199/2000 ▶ train MSE: 0.9887, val MSE: 0.9842
Gen 200/2000 ▶ train MSE: 0.9887, val MSE: 0.9842
Gen 200/2000 ▶ train MSE: 0.9887, val MSE: 0.9842
Gen 201/2000 ▶ train MSE: 0.9886, val MSE: 0.9854
Gen 202/2000 ▶ train MSE: 0.9886, val MSE: 0.9854
Gen 203/2000 ▶ train MSE: 0.9882, val MSE: 0.9835
Gen 204/2000 ▶ train MSE: 0.9882, val MSE: 0.9835
Gen 205/2000 ▶ train MSE: 0.9881, val MSE: 0.9828
Gen 206/2000 ▶ train MSE: 0.9881, val MSE: 0.9828
Gen 207/2000 ▶ train MSE: 0.9879, val MSE: 0.9830
Gen 208/2000 ▶ train MSE: 0.9879, val MSE: 0.9830
Gen 209/2000 ▶ train MSE: 0.9879, val MSE: 0.9830
Gen 210/2000 ▶ train MSE: 0.9879, val MSE: 0.9830
Gen 211/2000 ▶ train MSE: 0.9878, val MSE: 0.9818
Gen 212/2000 ▶ train MSE: 0.9877, val MSE: 0.9822
Gen 213/2000 ▶ train MSE: 0.9876, val MSE: 0.9823
Gen 214/2000 ▶ train MSE: 0.9874, val MSE: 0.9823
Gen 215/2000 ▶ train MSE: 0.9871, val MSE: 0.9819
Gen 216/2000 ▶ train MSE: 0.9869, val MSE: 0.9823
Gen 217/2000 ▶ train MSE: 0.9869, val MSE: 0.9823
Gen 218/2000 ▶ train MSE: 0.9869, val MSE: 0.9823
Gen 219/2000 ▶ train MSE: 0.9869, val MSE: 0.9814
Gen 220/2000 ▶ train MSE: 0.9864, val MSE: 0.9805
Gen 221/2000 ▶ train MSE: 0.9864, val MSE: 0.9805
Gen 222/2000 ▶ train MSE: 0.9864, val MSE: 0.9805
Gen 223/2000 ▶ train MSE: 0.9862, val MSE: 0.9795
Gen 224/2000 ▶ train MSE: 0.9862, val MSE: 0.9795
Gen 225/2000 ▶ train MSE: 0.9861, val MSE: 0.9802
Gen 226/2000 ▶ train MSE: 0.9861, val MSE: 0.9802
Gen 227/2000 ▶ train MSE: 0.9855, val MSE: 0.9788
Gen 228/2000 ▶ train MSE: 0.9855, val MSE: 0.9788
Gen 229/2000 ▶ train MSE: 0.9855, val MSE: 0.9788
Gen 230/2000 ▶ train MSE: 0.9855, val MSE: 0.9788
Gen 231/2000 ▶ train MSE: 0.9855, val MSE: 0.9788
Gen 232/2000 ▶ train MSE: 0.9854, val MSE: 0.9788
Gen 233/2000 ▶ train MSE: 0.9854, val MSE: 0.9788
Gen 234/2000 ▶ train MSE: 0.9854, val MSE: 0.9788
Gen 235/2000 ▶ train MSE: 0.9851, val MSE: 0.9783
Gen 236/2000 ▶ train MSE: 0.9851, val MSE: 0.9783
Gen 237/2000 ▶ train MSE: 0.9847, val MSE: 0.9772
Gen 238/2000 ▶ train MSE: 0.9846, val MSE: 0.9766
Gen 239/2000 ▶ train MSE: 0.9846, val MSE: 0.9766
Gen 240/2000 ▶ train MSE: 0.9846, val MSE: 0.9774
Gen 241/2000 ▶ train MSE: 0.9840, val MSE: 0.9765
Gen 242/2000 ▶ train MSE: 0.9840, val MSE: 0.9765
Gen 243/2000 ▶ train MSE: 0.9840, val MSE: 0.9765
Gen 244/2000 ▶ train MSE: 0.9837, val MSE: 0.9762
Gen 245/2000 ▶ train MSE: 0.9837, val MSE: 0.9762
Gen 246/2000 ▶ train MSE: 0.9836, val MSE: 0.9768
Gen 247/2000 ▶ train MSE: 0.9834, val MSE: 0.9759
Gen 248/2000 ▶ train MSE: 0.9834, val MSE: 0.9759
Gen 249/2000 ▶ train MSE: 0.9834, val MSE: 0.9759
Gen 250/2000 ▶ train MSE: 0.9833, val MSE: 0.9761
Gen 251/2000 ▶ train MSE: 0.9833, val MSE: 0.9761
Gen 252/2000 ▶ train MSE: 0.9833, val MSE: 0.9754
Gen 253/2000 ▶ train MSE: 0.9829, val MSE: 0.9756
Gen 254/2000 ▶ train MSE: 0.9828, val MSE: 0.9751
Gen 255/2000 ▶ train MSE: 0.9828, val MSE: 0.9751
Gen 256/2000 ▶ train MSE: 0.9828, val MSE: 0.9751
Gen 257/2000 ▶ train MSE: 0.9824, val MSE: 0.9753
Gen 258/2000 ▶ train MSE: 0.9824, val MSE: 0.9753
Gen 259/2000 ▶ train MSE: 0.9824, val MSE: 0.9753
Gen 260/2000 ▶ train MSE: 0.9824, val MSE: 0.9753
Gen 261/2000 ▶ train MSE: 0.9824, val MSE: 0.9734
Gen 262/2000 ▶ train MSE: 0.9822, val MSE: 0.9734
Gen 263/2000 ▶ train MSE: 0.9822, val MSE: 0.9734
Gen 264/2000 ▶ train MSE: 0.9821, val MSE: 0.9722
Gen 265/2000 ▶ train MSE: 0.9821, val MSE: 0.9722
Gen 266/2000 ▶ train MSE: 0.9819, val MSE: 0.9740
Gen 267/2000 ▶ train MSE: 0.9818, val MSE: 0.9726
Gen 268/2000 ▶ train MSE: 0.9814, val MSE: 0.9725
Gen 269/2000 ▶ train MSE: 0.9814, val MSE: 0.9725
Gen 270/2000 ▶ train MSE: 0.9814, val MSE: 0.9725
Gen 271/2000 ▶ train MSE: 0.9813, val MSE: 0.9727
Gen 272/2000 ▶ train MSE: 0.9812, val MSE: 0.9705
Gen 273/2000 ▶ train MSE: 0.9811, val MSE: 0.9725
Gen 274/2000 ▶ train MSE: 0.9810, val MSE: 0.9730
Gen 275/2000 ▶ train MSE: 0.9810, val MSE: 0.9720
Gen 276/2000 ▶ train MSE: 0.9810, val MSE: 0.9720
Gen 277/2000 ▶ train MSE: 0.9804, val MSE: 0.9715
Gen 278/2000 ▶ train MSE: 0.9804, val MSE: 0.9715
Gen 279/2000 ▶ train MSE: 0.9804, val MSE: 0.9715
Gen 280/2000 ▶ train MSE: 0.9804, val MSE: 0.9715
Gen 281/2000 ▶ train MSE: 0.9800, val MSE: 0.9696
Gen 282/2000 ▶ train MSE: 0.9800, val MSE: 0.9696
Gen 283/2000 ▶ train MSE: 0.9800, val MSE: 0.9696
Gen 284/2000 ▶ train MSE: 0.9800, val MSE: 0.9696
Gen 285/2000 ▶ train MSE: 0.9800, val MSE: 0.9696
Gen 286/2000 ▶ train MSE: 0.9800, val MSE: 0.9696
Gen 287/2000 ▶ train MSE: 0.9798, val MSE: 0.9696
Gen 288/2000 ▶ train MSE: 0.9798, val MSE: 0.9696
Gen 289/2000 ▶ train MSE: 0.9798, val MSE: 0.9696
Gen 290/2000 ▶ train MSE: 0.9798, val MSE: 0.9696
Gen 291/2000 ▶ train MSE: 0.9792, val MSE: 0.9690
Gen 292/2000 ▶ train MSE: 0.9792, val MSE: 0.9690
Gen 293/2000 ▶ train MSE: 0.9789, val MSE: 0.9688
Gen 294/2000 ▶ train MSE: 0.9789, val MSE: 0.9688
Gen 295/2000 ▶ train MSE: 0.9789, val MSE: 0.9688
Gen 296/2000 ▶ train MSE: 0.9789, val MSE: 0.9688
Gen 297/2000 ▶ train MSE: 0.9789, val MSE: 0.9688
Gen 298/2000 ▶ train MSE: 0.9784, val MSE: 0.9675
Gen 299/2000 ▶ train MSE: 0.9784, val MSE: 0.9675
Gen 300/2000 ▶ train MSE: 0.9784, val MSE: 0.9675
Gen 300/2000 ▶ train MSE: 0.9784, val MSE: 0.9675
Gen 301/2000 ▶ train MSE: 0.9784, val MSE: 0.9675
Gen 302/2000 ▶ train MSE: 0.9784, val MSE: 0.9675
Gen 303/2000 ▶ train MSE: 0.9784, val MSE: 0.9675
Gen 304/2000 ▶ train MSE: 0.9784, val MSE: 0.9675
Gen 305/2000 ▶ train MSE: 0.9784, val MSE: 0.9675
Gen 306/2000 ▶ train MSE: 0.9784, val MSE: 0.9675
Gen 307/2000 ▶ train MSE: 0.9783, val MSE: 0.9673
Gen 308/2000 ▶ train MSE: 0.9783, val MSE: 0.9673
Gen 309/2000 ▶ train MSE: 0.9783, val MSE: 0.9673
Gen 310/2000 ▶ train MSE: 0.9783, val MSE: 0.9673
Gen 311/2000 ▶ train MSE: 0.9781, val MSE: 0.9673
Gen 312/2000 ▶ train MSE: 0.9781, val MSE: 0.9673
Gen 313/2000 ▶ train MSE: 0.9781, val MSE: 0.9665
Gen 314/2000 ▶ train MSE: 0.9777, val MSE: 0.9673
Gen 315/2000 ▶ train MSE: 0.9777, val MSE: 0.9673
Gen 316/2000 ▶ train MSE: 0.9777, val MSE: 0.9673
Gen 317/2000 ▶ train MSE: 0.9777, val MSE: 0.9673
Gen 318/2000 ▶ train MSE: 0.9777, val MSE: 0.9673
Gen 319/2000 ▶ train MSE: 0.9777, val MSE: 0.9673
Gen 320/2000 ▶ train MSE: 0.9777, val MSE: 0.9673
Gen 321/2000 ▶ train MSE: 0.9777, val MSE: 0.9663
Gen 322/2000 ▶ train MSE: 0.9777, val MSE: 0.9663
Gen 323/2000 ▶ train MSE: 0.9776, val MSE: 0.9648
Gen 324/2000 ▶ train MSE: 0.9773, val MSE: 0.9666
Gen 325/2000 ▶ train MSE: 0.9773, val MSE: 0.9651
Gen 326/2000 ▶ train MSE: 0.9773, val MSE: 0.9651
Gen 327/2000 ▶ train MSE: 0.9773, val MSE: 0.9651
Gen 328/2000 ▶ train MSE: 0.9773, val MSE: 0.9651
Gen 329/2000 ▶ train MSE: 0.9773, val MSE: 0.9661
Gen 330/2000 ▶ train MSE: 0.9771, val MSE: 0.9648
Gen 331/2000 ▶ train MSE: 0.9769, val MSE: 0.9656
Gen 332/2000 ▶ train MSE: 0.9765, val MSE: 0.9645
Gen 333/2000 ▶ train MSE: 0.9765, val MSE: 0.9645
Gen 334/2000 ▶ train MSE: 0.9765, val MSE: 0.9645
Gen 335/2000 ▶ train MSE: 0.9765, val MSE: 0.9645
Gen 336/2000 ▶ train MSE: 0.9765, val MSE: 0.9645
Gen 337/2000 ▶ train MSE: 0.9765, val MSE: 0.9645
Gen 338/2000 ▶ train MSE: 0.9764, val MSE: 0.9638
Gen 339/2000 ▶ train MSE: 0.9764, val MSE: 0.9634
Gen 340/2000 ▶ train MSE: 0.9762, val MSE: 0.9621
Gen 341/2000 ▶ train MSE: 0.9762, val MSE: 0.9621
Gen 342/2000 ▶ train MSE: 0.9757, val MSE: 0.9638
Gen 343/2000 ▶ train MSE: 0.9757, val MSE: 0.9638
Gen 344/2000 ▶ train MSE: 0.9757, val MSE: 0.9638
Gen 345/2000 ▶ train MSE: 0.9755, val MSE: 0.9634
Gen 346/2000 ▶ train MSE: 0.9755, val MSE: 0.9637
Gen 347/2000 ▶ train MSE: 0.9750, val MSE: 0.9629
Gen 348/2000 ▶ train MSE: 0.9750, val MSE: 0.9629
Gen 349/2000 ▶ train MSE: 0.9750, val MSE: 0.9629
Gen 350/2000 ▶ train MSE: 0.9750, val MSE: 0.9629
Gen 351/2000 ▶ train MSE: 0.9750, val MSE: 0.9617
Gen 352/2000 ▶ train MSE: 0.9750, val MSE: 0.9617
Gen 353/2000 ▶ train MSE: 0.9743, val MSE: 0.9602
Gen 354/2000 ▶ train MSE: 0.9743, val MSE: 0.9602
Gen 355/2000 ▶ train MSE: 0.9738, val MSE: 0.9594
Gen 356/2000 ▶ train MSE: 0.9738, val MSE: 0.9594
Gen 357/2000 ▶ train MSE: 0.9738, val MSE: 0.9594
Gen 358/2000 ▶ train MSE: 0.9738, val MSE: 0.9594
Gen 359/2000 ▶ train MSE: 0.9738, val MSE: 0.9594
Gen 360/2000 ▶ train MSE: 0.9738, val MSE: 0.9594
Gen 361/2000 ▶ train MSE: 0.9725, val MSE: 0.9573
Gen 362/2000 ▶ train MSE: 0.9725, val MSE: 0.9573
Gen 363/2000 ▶ train MSE: 0.9725, val MSE: 0.9573
Gen 364/2000 ▶ train MSE: 0.9724, val MSE: 0.9560
Gen 365/2000 ▶ train MSE: 0.9724, val MSE: 0.9560
Gen 366/2000 ▶ train MSE: 0.9724, val MSE: 0.9560
Gen 367/2000 ▶ train MSE: 0.9724, val MSE: 0.9560
Gen 368/2000 ▶ train MSE: 0.9708, val MSE: 0.9558
Gen 369/2000 ▶ train MSE: 0.9708, val MSE: 0.9558
Gen 370/2000 ▶ train MSE: 0.9708, val MSE: 0.9558
Gen 371/2000 ▶ train MSE: 0.9708, val MSE: 0.9558
Gen 372/2000 ▶ train MSE: 0.9708, val MSE: 0.9558
Gen 373/2000 ▶ train MSE: 0.9708, val MSE: 0.9558
Gen 374/2000 ▶ train MSE: 0.9708, val MSE: 0.9558
Gen 375/2000 ▶ train MSE: 0.9705, val MSE: 0.9563
Gen 376/2000 ▶ train MSE: 0.9705, val MSE: 0.9563
Gen 377/2000 ▶ train MSE: 0.9705, val MSE: 0.9563
Gen 378/2000 ▶ train MSE: 0.9705, val MSE: 0.9563
Gen 379/2000 ▶ train MSE: 0.9700, val MSE: 0.9539
Gen 380/2000 ▶ train MSE: 0.9700, val MSE: 0.9539
Gen 381/2000 ▶ train MSE: 0.9697, val MSE: 0.9538
Gen 382/2000 ▶ train MSE: 0.9693, val MSE: 0.9519
Gen 383/2000 ▶ train MSE: 0.9693, val MSE: 0.9519
Gen 384/2000 ▶ train MSE: 0.9693, val MSE: 0.9519
Gen 385/2000 ▶ train MSE: 0.9693, val MSE: 0.9519
Gen 386/2000 ▶ train MSE: 0.9693, val MSE: 0.9519
Gen 387/2000 ▶ train MSE: 0.9690, val MSE: 0.9519
Gen 388/2000 ▶ train MSE: 0.9686, val MSE: 0.9489
Gen 389/2000 ▶ train MSE: 0.9686, val MSE: 0.9489
Gen 390/2000 ▶ train MSE: 0.9686, val MSE: 0.9489
Gen 391/2000 ▶ train MSE: 0.9686, val MSE: 0.9489
Gen 392/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 393/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 394/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 395/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 396/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 397/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 398/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 399/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 400/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 400/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 401/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 402/2000 ▶ train MSE: 0.9678, val MSE: 0.9495
Gen 403/2000 ▶ train MSE: 0.9676, val MSE: 0.9502
Gen 404/2000 ▶ train MSE: 0.9672, val MSE: 0.9487
Gen 405/2000 ▶ train MSE: 0.9672, val MSE: 0.9487
Gen 406/2000 ▶ train MSE: 0.9665, val MSE: 0.9484
Gen 407/2000 ▶ train MSE: 0.9665, val MSE: 0.9484
Gen 408/2000 ▶ train MSE: 0.9665, val MSE: 0.9484
Gen 409/2000 ▶ train MSE: 0.9665, val MSE: 0.9484
Gen 410/2000 ▶ train MSE: 0.9665, val MSE: 0.9484
Gen 411/2000 ▶ train MSE: 0.9665, val MSE: 0.9484
Gen 412/2000 ▶ train MSE: 0.9665, val MSE: 0.9484
Gen 413/2000 ▶ train MSE: 0.9665, val MSE: 0.9484
Gen 414/2000 ▶ train MSE: 0.9665, val MSE: 0.9484
Gen 415/2000 ▶ train MSE: 0.9665, val MSE: 0.9484
Gen 416/2000 ▶ train MSE: 0.9665, val MSE: 0.9484
Gen 417/2000 ▶ train MSE: 0.9664, val MSE: 0.9465
Gen 418/2000 ▶ train MSE: 0.9659, val MSE: 0.9463
Gen 419/2000 ▶ train MSE: 0.9659, val MSE: 0.9463
Gen 420/2000 ▶ train MSE: 0.9659, val MSE: 0.9463
Gen 421/2000 ▶ train MSE: 0.9659, val MSE: 0.9463
Gen 422/2000 ▶ train MSE: 0.9659, val MSE: 0.9463
Gen 423/2000 ▶ train MSE: 0.9652, val MSE: 0.9461
Gen 424/2000 ▶ train MSE: 0.9652, val MSE: 0.9461
Gen 425/2000 ▶ train MSE: 0.9652, val MSE: 0.9461
Gen 426/2000 ▶ train MSE: 0.9652, val MSE: 0.9461
Gen 427/2000 ▶ train MSE: 0.9652, val MSE: 0.9461
Gen 428/2000 ▶ train MSE: 0.9649, val MSE: 0.9453
Gen 429/2000 ▶ train MSE: 0.9649, val MSE: 0.9453
Gen 430/2000 ▶ train MSE: 0.9649, val MSE: 0.9453
Gen 431/2000 ▶ train MSE: 0.9649, val MSE: 0.9459
Gen 432/2000 ▶ train MSE: 0.9649, val MSE: 0.9459
Gen 433/2000 ▶ train MSE: 0.9649, val MSE: 0.9459
Gen 434/2000 ▶ train MSE: 0.9649, val MSE: 0.9459
Gen 435/2000 ▶ train MSE: 0.9649, val MSE: 0.9459
Gen 436/2000 ▶ train MSE: 0.9649, val MSE: 0.9459
Gen 437/2000 ▶ train MSE: 0.9649, val MSE: 0.9459
Gen 438/2000 ▶ train MSE: 0.9649, val MSE: 0.9459
Gen 439/2000 ▶ train MSE: 0.9649, val MSE: 0.9436
Gen 440/2000 ▶ train MSE: 0.9649, val MSE: 0.9436
Gen 441/2000 ▶ train MSE: 0.9648, val MSE: 0.9456
Gen 442/2000 ▶ train MSE: 0.9646, val MSE: 0.9444
Gen 443/2000 ▶ train MSE: 0.9645, val MSE: 0.9439
Gen 444/2000 ▶ train MSE: 0.9645, val MSE: 0.9439
Gen 445/2000 ▶ train MSE: 0.9645, val MSE: 0.9439
Gen 446/2000 ▶ train MSE: 0.9644, val MSE: 0.9440
Gen 447/2000 ▶ train MSE: 0.9644, val MSE: 0.9442
Gen 448/2000 ▶ train MSE: 0.9644, val MSE: 0.9442
Gen 449/2000 ▶ train MSE: 0.9644, val MSE: 0.9442
Gen 450/2000 ▶ train MSE: 0.9641, val MSE: 0.9435
Gen 451/2000 ▶ train MSE: 0.9641, val MSE: 0.9435
Gen 452/2000 ▶ train MSE: 0.9641, val MSE: 0.9435
Gen 453/2000 ▶ train MSE: 0.9641, val MSE: 0.9435
Gen 454/2000 ▶ train MSE: 0.9640, val MSE: 0.9436
Gen 455/2000 ▶ train MSE: 0.9640, val MSE: 0.9436
Gen 456/2000 ▶ train MSE: 0.9640, val MSE: 0.9436
Gen 457/2000 ▶ train MSE: 0.9640, val MSE: 0.9436
Gen 458/2000 ▶ train MSE: 0.9636, val MSE: 0.9424
Gen 459/2000 ▶ train MSE: 0.9633, val MSE: 0.9418
Gen 460/2000 ▶ train MSE: 0.9633, val MSE: 0.9418
Gen 461/2000 ▶ train MSE: 0.9633, val MSE: 0.9418
Gen 462/2000 ▶ train MSE: 0.9633, val MSE: 0.9418
Gen 463/2000 ▶ train MSE: 0.9632, val MSE: 0.9420
Gen 464/2000 ▶ train MSE: 0.9632, val MSE: 0.9435
Gen 465/2000 ▶ train MSE: 0.9632, val MSE: 0.9435
Gen 466/2000 ▶ train MSE: 0.9631, val MSE: 0.9431
Gen 467/2000 ▶ train MSE: 0.9630, val MSE: 0.9419
Gen 468/2000 ▶ train MSE: 0.9628, val MSE: 0.9417
Gen 469/2000 ▶ train MSE: 0.9628, val MSE: 0.9419
Gen 470/2000 ▶ train MSE: 0.9627, val MSE: 0.9402
Gen 471/2000 ▶ train MSE: 0.9627, val MSE: 0.9402
Gen 472/2000 ▶ train MSE: 0.9626, val MSE: 0.9403
Gen 473/2000 ▶ train MSE: 0.9624, val MSE: 0.9409
Gen 474/2000 ▶ train MSE: 0.9624, val MSE: 0.9409
Gen 475/2000 ▶ train MSE: 0.9624, val MSE: 0.9409
Gen 476/2000 ▶ train MSE: 0.9624, val MSE: 0.9408
Gen 477/2000 ▶ train MSE: 0.9624, val MSE: 0.9408
Gen 478/2000 ▶ train MSE: 0.9624, val MSE: 0.9399
Gen 479/2000 ▶ train MSE: 0.9624, val MSE: 0.9399
Gen 480/2000 ▶ train MSE: 0.9623, val MSE: 0.9405
Gen 481/2000 ▶ train MSE: 0.9623, val MSE: 0.9404
Gen 482/2000 ▶ train MSE: 0.9622, val MSE: 0.9408
Gen 483/2000 ▶ train MSE: 0.9621, val MSE: 0.9400
Gen 484/2000 ▶ train MSE: 0.9618, val MSE: 0.9398
Gen 485/2000 ▶ train MSE: 0.9618, val MSE: 0.9398
Gen 486/2000 ▶ train MSE: 0.9618, val MSE: 0.9397
Gen 487/2000 ▶ train MSE: 0.9618, val MSE: 0.9397
Gen 488/2000 ▶ train MSE: 0.9617, val MSE: 0.9398
Gen 489/2000 ▶ train MSE: 0.9617, val MSE: 0.9398
Gen 490/2000 ▶ train MSE: 0.9616, val MSE: 0.9406
Gen 491/2000 ▶ train MSE: 0.9616, val MSE: 0.9394
Gen 492/2000 ▶ train MSE: 0.9616, val MSE: 0.9394
Gen 493/2000 ▶ train MSE: 0.9614, val MSE: 0.9402
Gen 494/2000 ▶ train MSE: 0.9613, val MSE: 0.9394
Gen 495/2000 ▶ train MSE: 0.9613, val MSE: 0.9394
Gen 496/2000 ▶ train MSE: 0.9613, val MSE: 0.9394
Gen 497/2000 ▶ train MSE: 0.9613, val MSE: 0.9394
Gen 498/2000 ▶ train MSE: 0.9613, val MSE: 0.9394
Gen 499/2000 ▶ train MSE: 0.9613, val MSE: 0.9394
Gen 500/2000 ▶ train MSE: 0.9613, val MSE: 0.9394
Gen 500/2000 ▶ train MSE: 0.9613, val MSE: 0.9394
Gen 501/2000 ▶ train MSE: 0.9613, val MSE: 0.9405
Gen 502/2000 ▶ train MSE: 0.9613, val MSE: 0.9405
Gen 503/2000 ▶ train MSE: 0.9612, val MSE: 0.9407
Gen 504/2000 ▶ train MSE: 0.9612, val MSE: 0.9398
Gen 505/2000 ▶ train MSE: 0.9612, val MSE: 0.9398
Gen 506/2000 ▶ train MSE: 0.9611, val MSE: 0.9393
Gen 507/2000 ▶ train MSE: 0.9610, val MSE: 0.9399
Gen 508/2000 ▶ train MSE: 0.9608, val MSE: 0.9384
Gen 509/2000 ▶ train MSE: 0.9607, val MSE: 0.9385
Gen 510/2000 ▶ train MSE: 0.9607, val MSE: 0.9385
Gen 511/2000 ▶ train MSE: 0.9606, val MSE: 0.9394
Gen 512/2000 ▶ train MSE: 0.9606, val MSE: 0.9394
Gen 513/2000 ▶ train MSE: 0.9606, val MSE: 0.9394
Gen 514/2000 ▶ train MSE: 0.9606, val MSE: 0.9394
Gen 515/2000 ▶ train MSE: 0.9605, val MSE: 0.9392
Gen 516/2000 ▶ train MSE: 0.9604, val MSE: 0.9389
Gen 517/2000 ▶ train MSE: 0.9603, val MSE: 0.9377
Gen 518/2000 ▶ train MSE: 0.9603, val MSE: 0.9377
Gen 519/2000 ▶ train MSE: 0.9603, val MSE: 0.9377
Gen 520/2000 ▶ train MSE: 0.9602, val MSE: 0.9376
Gen 521/2000 ▶ train MSE: 0.9602, val MSE: 0.9376
Gen 522/2000 ▶ train MSE: 0.9602, val MSE: 0.9376
Gen 523/2000 ▶ train MSE: 0.9602, val MSE: 0.9392
Gen 524/2000 ▶ train MSE: 0.9600, val MSE: 0.9383
Gen 525/2000 ▶ train MSE: 0.9600, val MSE: 0.9387
Gen 526/2000 ▶ train MSE: 0.9600, val MSE: 0.9387
Gen 527/2000 ▶ train MSE: 0.9599, val MSE: 0.9388
Gen 528/2000 ▶ train MSE: 0.9599, val MSE: 0.9388
Gen 529/2000 ▶ train MSE: 0.9599, val MSE: 0.9388
Gen 530/2000 ▶ train MSE: 0.9599, val MSE: 0.9388
Gen 531/2000 ▶ train MSE: 0.9598, val MSE: 0.9384
Gen 532/2000 ▶ train MSE: 0.9598, val MSE: 0.9384
Gen 533/2000 ▶ train MSE: 0.9597, val MSE: 0.9384
Gen 534/2000 ▶ train MSE: 0.9597, val MSE: 0.9384
Gen 535/2000 ▶ train MSE: 0.9597, val MSE: 0.9384
Gen 536/2000 ▶ train MSE: 0.9597, val MSE: 0.9384
Gen 537/2000 ▶ train MSE: 0.9597, val MSE: 0.9381
Gen 538/2000 ▶ train MSE: 0.9595, val MSE: 0.9379
Gen 539/2000 ▶ train MSE: 0.9595, val MSE: 0.9379
Gen 540/2000 ▶ train MSE: 0.9595, val MSE: 0.9390
Gen 541/2000 ▶ train MSE: 0.9595, val MSE: 0.9390
Gen 542/2000 ▶ train MSE: 0.9595, val MSE: 0.9390
Gen 543/2000 ▶ train MSE: 0.9593, val MSE: 0.9363
Gen 544/2000 ▶ train MSE: 0.9593, val MSE: 0.9366
Gen 545/2000 ▶ train MSE: 0.9593, val MSE: 0.9366
Gen 546/2000 ▶ train MSE: 0.9588, val MSE: 0.9356
Gen 547/2000 ▶ train MSE: 0.9588, val MSE: 0.9356
Gen 548/2000 ▶ train MSE: 0.9588, val MSE: 0.9356
Gen 549/2000 ▶ train MSE: 0.9588, val MSE: 0.9356
Gen 550/2000 ▶ train MSE: 0.9588, val MSE: 0.9356
Gen 551/2000 ▶ train MSE: 0.9588, val MSE: 0.9356
Gen 552/2000 ▶ train MSE: 0.9588, val MSE: 0.9356
Gen 553/2000 ▶ train MSE: 0.9588, val MSE: 0.9356
Gen 554/2000 ▶ train MSE: 0.9588, val MSE: 0.9366
Gen 555/2000 ▶ train MSE: 0.9586, val MSE: 0.9350
Gen 556/2000 ▶ train MSE: 0.9582, val MSE: 0.9355
Gen 557/2000 ▶ train MSE: 0.9582, val MSE: 0.9355
Gen 558/2000 ▶ train MSE: 0.9582, val MSE: 0.9355
Gen 559/2000 ▶ train MSE: 0.9582, val MSE: 0.9352
Gen 560/2000 ▶ train MSE: 0.9582, val MSE: 0.9352
Gen 561/2000 ▶ train MSE: 0.9582, val MSE: 0.9352
Gen 562/2000 ▶ train MSE: 0.9582, val MSE: 0.9352
Gen 563/2000 ▶ train MSE: 0.9580, val MSE: 0.9346
Gen 564/2000 ▶ train MSE: 0.9580, val MSE: 0.9346
Gen 565/2000 ▶ train MSE: 0.9580, val MSE: 0.9346
Gen 566/2000 ▶ train MSE: 0.9576, val MSE: 0.9340
Gen 567/2000 ▶ train MSE: 0.9576, val MSE: 0.9340
Gen 568/2000 ▶ train MSE: 0.9576, val MSE: 0.9340
Gen 569/2000 ▶ train MSE: 0.9576, val MSE: 0.9340
Gen 570/2000 ▶ train MSE: 0.9574, val MSE: 0.9339
Gen 571/2000 ▶ train MSE: 0.9574, val MSE: 0.9339
Gen 572/2000 ▶ train MSE: 0.9574, val MSE: 0.9339
Gen 573/2000 ▶ train MSE: 0.9573, val MSE: 0.9342
Gen 574/2000 ▶ train MSE: 0.9570, val MSE: 0.9335
Gen 575/2000 ▶ train MSE: 0.9570, val MSE: 0.9335
Gen 576/2000 ▶ train MSE: 0.9570, val MSE: 0.9335
Gen 577/2000 ▶ train MSE: 0.9570, val MSE: 0.9335
Gen 578/2000 ▶ train MSE: 0.9570, val MSE: 0.9335
Gen 579/2000 ▶ train MSE: 0.9568, val MSE: 0.9317
Gen 580/2000 ▶ train MSE: 0.9568, val MSE: 0.9317
Gen 581/2000 ▶ train MSE: 0.9568, val MSE: 0.9317
Gen 582/2000 ▶ train MSE: 0.9568, val MSE: 0.9317
Gen 583/2000 ▶ train MSE: 0.9567, val MSE: 0.9319
Gen 584/2000 ▶ train MSE: 0.9567, val MSE: 0.9319
Gen 585/2000 ▶ train MSE: 0.9562, val MSE: 0.9312
Gen 586/2000 ▶ train MSE: 0.9562, val MSE: 0.9312
Gen 587/2000 ▶ train MSE: 0.9562, val MSE: 0.9312
Gen 588/2000 ▶ train MSE: 0.9562, val MSE: 0.9312
Gen 589/2000 ▶ train MSE: 0.9562, val MSE: 0.9312
Gen 590/2000 ▶ train MSE: 0.9559, val MSE: 0.9307
Gen 591/2000 ▶ train MSE: 0.9559, val MSE: 0.9307
Gen 592/2000 ▶ train MSE: 0.9559, val MSE: 0.9307
Gen 593/2000 ▶ train MSE: 0.9559, val MSE: 0.9307
Gen 594/2000 ▶ train MSE: 0.9559, val MSE: 0.9310
Gen 595/2000 ▶ train MSE: 0.9559, val MSE: 0.9312
Gen 596/2000 ▶ train MSE: 0.9559, val MSE: 0.9312
Gen 597/2000 ▶ train MSE: 0.9558, val MSE: 0.9316
Gen 598/2000 ▶ train MSE: 0.9557, val MSE: 0.9302
Gen 599/2000 ▶ train MSE: 0.9557, val MSE: 0.9302
Gen 600/2000 ▶ train MSE: 0.9557, val MSE: 0.9302
Gen 600/2000 ▶ train MSE: 0.9556, val MSE: 0.9303
Gen 601/2000 ▶ train MSE: 0.9553, val MSE: 0.9302
Gen 602/2000 ▶ train MSE: 0.9553, val MSE: 0.9302
Gen 603/2000 ▶ train MSE: 0.9553, val MSE: 0.9297
Gen 604/2000 ▶ train MSE: 0.9552, val MSE: 0.9313
Gen 605/2000 ▶ train MSE: 0.9552, val MSE: 0.9313
Gen 606/2000 ▶ train MSE: 0.9551, val MSE: 0.9305
Gen 607/2000 ▶ train MSE: 0.9549, val MSE: 0.9293
Gen 608/2000 ▶ train MSE: 0.9549, val MSE: 0.9293
Gen 609/2000 ▶ train MSE: 0.9549, val MSE: 0.9286
Gen 610/2000 ▶ train MSE: 0.9549, val MSE: 0.9303
Gen 611/2000 ▶ train MSE: 0.9547, val MSE: 0.9298
Gen 612/2000 ▶ train MSE: 0.9547, val MSE: 0.9298
Gen 613/2000 ▶ train MSE: 0.9547, val MSE: 0.9298
Gen 614/2000 ▶ train MSE: 0.9547, val MSE: 0.9292
Gen 615/2000 ▶ train MSE: 0.9547, val MSE: 0.9292
Gen 616/2000 ▶ train MSE: 0.9545, val MSE: 0.9285
Gen 617/2000 ▶ train MSE: 0.9545, val MSE: 0.9285
Gen 618/2000 ▶ train MSE: 0.9544, val MSE: 0.9289
Gen 619/2000 ▶ train MSE: 0.9542, val MSE: 0.9290
Gen 620/2000 ▶ train MSE: 0.9542, val MSE: 0.9290
Gen 621/2000 ▶ train MSE: 0.9542, val MSE: 0.9290
Gen 622/2000 ▶ train MSE: 0.9542, val MSE: 0.9290
Gen 623/2000 ▶ train MSE: 0.9540, val MSE: 0.9286
Gen 624/2000 ▶ train MSE: 0.9540, val MSE: 0.9286
Gen 625/2000 ▶ train MSE: 0.9540, val MSE: 0.9295
Gen 626/2000 ▶ train MSE: 0.9540, val MSE: 0.9290
Gen 627/2000 ▶ train MSE: 0.9540, val MSE: 0.9297
Gen 628/2000 ▶ train MSE: 0.9538, val MSE: 0.9282
Gen 629/2000 ▶ train MSE: 0.9537, val MSE: 0.9285
Gen 630/2000 ▶ train MSE: 0.9537, val MSE: 0.9284
Gen 631/2000 ▶ train MSE: 0.9536, val MSE: 0.9282
Gen 632/2000 ▶ train MSE: 0.9534, val MSE: 0.9287
Gen 633/2000 ▶ train MSE: 0.9534, val MSE: 0.9287
Gen 634/2000 ▶ train MSE: 0.9534, val MSE: 0.9287
Gen 635/2000 ▶ train MSE: 0.9534, val MSE: 0.9287
Gen 636/2000 ▶ train MSE: 0.9534, val MSE: 0.9287
Gen 637/2000 ▶ train MSE: 0.9534, val MSE: 0.9287
Gen 638/2000 ▶ train MSE: 0.9534, val MSE: 0.9280
Gen 639/2000 ▶ train MSE: 0.9534, val MSE: 0.9280
Gen 640/2000 ▶ train MSE: 0.9534, val MSE: 0.9280
Gen 641/2000 ▶ train MSE: 0.9533, val MSE: 0.9287
Gen 642/2000 ▶ train MSE: 0.9532, val MSE: 0.9285
Gen 643/2000 ▶ train MSE: 0.9532, val MSE: 0.9285
Gen 644/2000 ▶ train MSE: 0.9532, val MSE: 0.9273
Gen 645/2000 ▶ train MSE: 0.9532, val MSE: 0.9273
Gen 646/2000 ▶ train MSE: 0.9530, val MSE: 0.9287
Gen 647/2000 ▶ train MSE: 0.9530, val MSE: 0.9287
Gen 648/2000 ▶ train MSE: 0.9530, val MSE: 0.9280
Gen 649/2000 ▶ train MSE: 0.9530, val MSE: 0.9280
Gen 650/2000 ▶ train MSE: 0.9530, val MSE: 0.9278
Gen 651/2000 ▶ train MSE: 0.9530, val MSE: 0.9278
Gen 652/2000 ▶ train MSE: 0.9529, val MSE: 0.9279
Gen 653/2000 ▶ train MSE: 0.9529, val MSE: 0.9279
Gen 654/2000 ▶ train MSE: 0.9527, val MSE: 0.9267
Gen 655/2000 ▶ train MSE: 0.9527, val MSE: 0.9267
Gen 656/2000 ▶ train MSE: 0.9527, val MSE: 0.9267
Gen 657/2000 ▶ train MSE: 0.9527, val MSE: 0.9277
Gen 658/2000 ▶ train MSE: 0.9527, val MSE: 0.9277
Gen 659/2000 ▶ train MSE: 0.9525, val MSE: 0.9282
Gen 660/2000 ▶ train MSE: 0.9525, val MSE: 0.9282
Gen 661/2000 ▶ train MSE: 0.9525, val MSE: 0.9282
Gen 662/2000 ▶ train MSE: 0.9525, val MSE: 0.9282
Gen 663/2000 ▶ train MSE: 0.9525, val MSE: 0.9282
Gen 664/2000 ▶ train MSE: 0.9524, val MSE: 0.9273
Gen 665/2000 ▶ train MSE: 0.9524, val MSE: 0.9273
Gen 666/2000 ▶ train MSE: 0.9524, val MSE: 0.9267
Gen 667/2000 ▶ train MSE: 0.9524, val MSE: 0.9267
Gen 668/2000 ▶ train MSE: 0.9524, val MSE: 0.9267
Gen 669/2000 ▶ train MSE: 0.9519, val MSE: 0.9259
Gen 670/2000 ▶ train MSE: 0.9519, val MSE: 0.9259
Gen 671/2000 ▶ train MSE: 0.9519, val MSE: 0.9259
Gen 672/2000 ▶ train MSE: 0.9519, val MSE: 0.9259
Gen 673/2000 ▶ train MSE: 0.9519, val MSE: 0.9259
Gen 674/2000 ▶ train MSE: 0.9519, val MSE: 0.9259
Gen 675/2000 ▶ train MSE: 0.9519, val MSE: 0.9259
Gen 676/2000 ▶ train MSE: 0.9519, val MSE: 0.9259
Gen 677/2000 ▶ train MSE: 0.9518, val MSE: 0.9258
Gen 678/2000 ▶ train MSE: 0.9518, val MSE: 0.9258
Gen 679/2000 ▶ train MSE: 0.9518, val MSE: 0.9258
Gen 680/2000 ▶ train MSE: 0.9518, val MSE: 0.9258
Gen 681/2000 ▶ train MSE: 0.9518, val MSE: 0.9258
Gen 682/2000 ▶ train MSE: 0.9518, val MSE: 0.9258
Gen 683/2000 ▶ train MSE: 0.9518, val MSE: 0.9258
Gen 684/2000 ▶ train MSE: 0.9518, val MSE: 0.9258
Gen 685/2000 ▶ train MSE: 0.9518, val MSE: 0.9258
Gen 686/2000 ▶ train MSE: 0.9518, val MSE: 0.9258
Gen 687/2000 ▶ train MSE: 0.9518, val MSE: 0.9258
Gen 688/2000 ▶ train MSE: 0.9517, val MSE: 0.9271
Gen 689/2000 ▶ train MSE: 0.9517, val MSE: 0.9271
Gen 690/2000 ▶ train MSE: 0.9517, val MSE: 0.9271
Gen 691/2000 ▶ train MSE: 0.9516, val MSE: 0.9244
Gen 692/2000 ▶ train MSE: 0.9516, val MSE: 0.9254
Gen 693/2000 ▶ train MSE: 0.9515, val MSE: 0.9258
Gen 694/2000 ▶ train MSE: 0.9515, val MSE: 0.9258
Gen 695/2000 ▶ train MSE: 0.9513, val MSE: 0.9248
Gen 696/2000 ▶ train MSE: 0.9512, val MSE: 0.9240
Gen 697/2000 ▶ train MSE: 0.9512, val MSE: 0.9254
Gen 698/2000 ▶ train MSE: 0.9512, val MSE: 0.9254
Gen 699/2000 ▶ train MSE: 0.9510, val MSE: 0.9240
Gen 700/2000 ▶ train MSE: 0.9510, val MSE: 0.9240
Gen 700/2000 ▶ train MSE: 0.9509, val MSE: 0.9241
Gen 701/2000 ▶ train MSE: 0.9509, val MSE: 0.9248
Gen 702/2000 ▶ train MSE: 0.9507, val MSE: 0.9250
Gen 703/2000 ▶ train MSE: 0.9505, val MSE: 0.9237
Gen 704/2000 ▶ train MSE: 0.9504, val MSE: 0.9240
Gen 705/2000 ▶ train MSE: 0.9502, val MSE: 0.9234
Gen 706/2000 ▶ train MSE: 0.9502, val MSE: 0.9234
Gen 707/2000 ▶ train MSE: 0.9501, val MSE: 0.9232
Gen 708/2000 ▶ train MSE: 0.9501, val MSE: 0.9232
Gen 709/2000 ▶ train MSE: 0.9501, val MSE: 0.9232
Gen 710/2000 ▶ train MSE: 0.9501, val MSE: 0.9232
Gen 711/2000 ▶ train MSE: 0.9497, val MSE: 0.9220
Gen 712/2000 ▶ train MSE: 0.9497, val MSE: 0.9220
Gen 713/2000 ▶ train MSE: 0.9497, val MSE: 0.9220
Gen 714/2000 ▶ train MSE: 0.9497, val MSE: 0.9222
Gen 715/2000 ▶ train MSE: 0.9497, val MSE: 0.9222
Gen 716/2000 ▶ train MSE: 0.9497, val MSE: 0.9222
Gen 717/2000 ▶ train MSE: 0.9496, val MSE: 0.9223
Gen 718/2000 ▶ train MSE: 0.9495, val MSE: 0.9210
Gen 719/2000 ▶ train MSE: 0.9495, val MSE: 0.9202
Gen 720/2000 ▶ train MSE: 0.9495, val MSE: 0.9208
Gen 721/2000 ▶ train MSE: 0.9495, val MSE: 0.9208
Gen 722/2000 ▶ train MSE: 0.9494, val MSE: 0.9215
Gen 723/2000 ▶ train MSE: 0.9494, val MSE: 0.9215
Gen 724/2000 ▶ train MSE: 0.9494, val MSE: 0.9215
Gen 725/2000 ▶ train MSE: 0.9492, val MSE: 0.9212
Gen 726/2000 ▶ train MSE: 0.9492, val MSE: 0.9212
Gen 727/2000 ▶ train MSE: 0.9491, val MSE: 0.9214
Gen 728/2000 ▶ train MSE: 0.9491, val MSE: 0.9214
Gen 729/2000 ▶ train MSE: 0.9491, val MSE: 0.9214
Gen 730/2000 ▶ train MSE: 0.9488, val MSE: 0.9204
Gen 731/2000 ▶ train MSE: 0.9488, val MSE: 0.9204
Gen 732/2000 ▶ train MSE: 0.9488, val MSE: 0.9204
Gen 733/2000 ▶ train MSE: 0.9488, val MSE: 0.9204
Gen 734/2000 ▶ train MSE: 0.9488, val MSE: 0.9204
Gen 735/2000 ▶ train MSE: 0.9487, val MSE: 0.9210
Gen 736/2000 ▶ train MSE: 0.9487, val MSE: 0.9210
Gen 737/2000 ▶ train MSE: 0.9487, val MSE: 0.9210
Gen 738/2000 ▶ train MSE: 0.9485, val MSE: 0.9206
Gen 739/2000 ▶ train MSE: 0.9484, val MSE: 0.9198
Gen 740/2000 ▶ train MSE: 0.9484, val MSE: 0.9198
Gen 741/2000 ▶ train MSE: 0.9484, val MSE: 0.9198
Gen 742/2000 ▶ train MSE: 0.9484, val MSE: 0.9198
Gen 743/2000 ▶ train MSE: 0.9484, val MSE: 0.9198
Gen 744/2000 ▶ train MSE: 0.9480, val MSE: 0.9201
Gen 745/2000 ▶ train MSE: 0.9480, val MSE: 0.9201
Gen 746/2000 ▶ train MSE: 0.9480, val MSE: 0.9201
Gen 747/2000 ▶ train MSE: 0.9480, val MSE: 0.9201
Gen 748/2000 ▶ train MSE: 0.9480, val MSE: 0.9201
Gen 749/2000 ▶ train MSE: 0.9480, val MSE: 0.9201
Gen 750/2000 ▶ train MSE: 0.9480, val MSE: 0.9201
Gen 751/2000 ▶ train MSE: 0.9476, val MSE: 0.9196
Gen 752/2000 ▶ train MSE: 0.9476, val MSE: 0.9196
Gen 753/2000 ▶ train MSE: 0.9476, val MSE: 0.9196
Gen 754/2000 ▶ train MSE: 0.9476, val MSE: 0.9196
Gen 755/2000 ▶ train MSE: 0.9476, val MSE: 0.9196
Gen 756/2000 ▶ train MSE: 0.9476, val MSE: 0.9196
Gen 757/2000 ▶ train MSE: 0.9476, val MSE: 0.9196
Gen 758/2000 ▶ train MSE: 0.9476, val MSE: 0.9196
Gen 759/2000 ▶ train MSE: 0.9476, val MSE: 0.9196
Gen 760/2000 ▶ train MSE: 0.9475, val MSE: 0.9196
Gen 761/2000 ▶ train MSE: 0.9475, val MSE: 0.9199
Gen 762/2000 ▶ train MSE: 0.9474, val MSE: 0.9196
Gen 763/2000 ▶ train MSE: 0.9472, val MSE: 0.9194
Gen 764/2000 ▶ train MSE: 0.9472, val MSE: 0.9194
Gen 765/2000 ▶ train MSE: 0.9472, val MSE: 0.9194
Gen 766/2000 ▶ train MSE: 0.9472, val MSE: 0.9194
Gen 767/2000 ▶ train MSE: 0.9472, val MSE: 0.9194
Gen 768/2000 ▶ train MSE: 0.9472, val MSE: 0.9194
Gen 769/2000 ▶ train MSE: 0.9472, val MSE: 0.9194
Gen 770/2000 ▶ train MSE: 0.9472, val MSE: 0.9194
Gen 771/2000 ▶ train MSE: 0.9471, val MSE: 0.9190
Gen 772/2000 ▶ train MSE: 0.9471, val MSE: 0.9189
Gen 773/2000 ▶ train MSE: 0.9471, val MSE: 0.9194
Gen 774/2000 ▶ train MSE: 0.9471, val MSE: 0.9194
Gen 775/2000 ▶ train MSE: 0.9471, val MSE: 0.9194
Gen 776/2000 ▶ train MSE: 0.9471, val MSE: 0.9194
Gen 777/2000 ▶ train MSE: 0.9470, val MSE: 0.9183
Gen 778/2000 ▶ train MSE: 0.9470, val MSE: 0.9177
Gen 779/2000 ▶ train MSE: 0.9470, val MSE: 0.9190
Gen 780/2000 ▶ train MSE: 0.9468, val MSE: 0.9183
Gen 781/2000 ▶ train MSE: 0.9468, val MSE: 0.9183
Gen 782/2000 ▶ train MSE: 0.9468, val MSE: 0.9183
Gen 783/2000 ▶ train MSE: 0.9468, val MSE: 0.9183
Gen 784/2000 ▶ train MSE: 0.9468, val MSE: 0.9173
Gen 785/2000 ▶ train MSE: 0.9467, val MSE: 0.9175
Gen 786/2000 ▶ train MSE: 0.9465, val MSE: 0.9176
Gen 787/2000 ▶ train MSE: 0.9465, val MSE: 0.9176
Gen 788/2000 ▶ train MSE: 0.9465, val MSE: 0.9176
Gen 789/2000 ▶ train MSE: 0.9465, val MSE: 0.9176
Gen 790/2000 ▶ train MSE: 0.9465, val MSE: 0.9176
Gen 791/2000 ▶ train MSE: 0.9464, val MSE: 0.9183
Gen 792/2000 ▶ train MSE: 0.9464, val MSE: 0.9183
Gen 793/2000 ▶ train MSE: 0.9464, val MSE: 0.9183
Gen 794/2000 ▶ train MSE: 0.9464, val MSE: 0.9183
Gen 795/2000 ▶ train MSE: 0.9463, val MSE: 0.9174
Gen 796/2000 ▶ train MSE: 0.9463, val MSE: 0.9174
Gen 797/2000 ▶ train MSE: 0.9463, val MSE: 0.9174
Gen 798/2000 ▶ train MSE: 0.9463, val MSE: 0.9174
Gen 799/2000 ▶ train MSE: 0.9463, val MSE: 0.9174
Gen 800/2000 ▶ train MSE: 0.9463, val MSE: 0.9174
Gen 800/2000 ▶ train MSE: 0.9462, val MSE: 0.9182
Gen 801/2000 ▶ train MSE: 0.9462, val MSE: 0.9182
Gen 802/2000 ▶ train MSE: 0.9462, val MSE: 0.9188
Gen 803/2000 ▶ train MSE: 0.9462, val MSE: 0.9169
Gen 804/2000 ▶ train MSE: 0.9462, val MSE: 0.9167
Gen 805/2000 ▶ train MSE: 0.9461, val MSE: 0.9161
Gen 806/2000 ▶ train MSE: 0.9461, val MSE: 0.9180
Gen 807/2000 ▶ train MSE: 0.9461, val MSE: 0.9180
Gen 808/2000 ▶ train MSE: 0.9460, val MSE: 0.9168
Gen 809/2000 ▶ train MSE: 0.9460, val MSE: 0.9169
Gen 810/2000 ▶ train MSE: 0.9458, val MSE: 0.9162
Gen 811/2000 ▶ train MSE: 0.9458, val MSE: 0.9162
Gen 812/2000 ▶ train MSE: 0.9458, val MSE: 0.9162
Gen 813/2000 ▶ train MSE: 0.9458, val MSE: 0.9162
Gen 814/2000 ▶ train MSE: 0.9458, val MSE: 0.9168
Gen 815/2000 ▶ train MSE: 0.9458, val MSE: 0.9168
Gen 816/2000 ▶ train MSE: 0.9458, val MSE: 0.9168
Gen 817/2000 ▶ train MSE: 0.9458, val MSE: 0.9168
Gen 818/2000 ▶ train MSE: 0.9458, val MSE: 0.9165
Gen 819/2000 ▶ train MSE: 0.9458, val MSE: 0.9165
Gen 820/2000 ▶ train MSE: 0.9458, val MSE: 0.9174
Gen 821/2000 ▶ train MSE: 0.9458, val MSE: 0.9174
Gen 822/2000 ▶ train MSE: 0.9458, val MSE: 0.9174
Gen 823/2000 ▶ train MSE: 0.9458, val MSE: 0.9174
Gen 824/2000 ▶ train MSE: 0.9457, val MSE: 0.9162
Gen 825/2000 ▶ train MSE: 0.9457, val MSE: 0.9164
Gen 826/2000 ▶ train MSE: 0.9456, val MSE: 0.9174
Gen 827/2000 ▶ train MSE: 0.9456, val MSE: 0.9174
Gen 828/2000 ▶ train MSE: 0.9456, val MSE: 0.9174
Gen 829/2000 ▶ train MSE: 0.9456, val MSE: 0.9174
Gen 830/2000 ▶ train MSE: 0.9455, val MSE: 0.9172
Gen 831/2000 ▶ train MSE: 0.9455, val MSE: 0.9172
Gen 832/2000 ▶ train MSE: 0.9455, val MSE: 0.9167
Gen 833/2000 ▶ train MSE: 0.9453, val MSE: 0.9164
Gen 834/2000 ▶ train MSE: 0.9453, val MSE: 0.9164
Gen 835/2000 ▶ train MSE: 0.9453, val MSE: 0.9164
Gen 836/2000 ▶ train MSE: 0.9452, val MSE: 0.9151
Gen 837/2000 ▶ train MSE: 0.9452, val MSE: 0.9151
Gen 838/2000 ▶ train MSE: 0.9452, val MSE: 0.9164
Gen 839/2000 ▶ train MSE: 0.9452, val MSE: 0.9164
Gen 840/2000 ▶ train MSE: 0.9450, val MSE: 0.9159
Gen 841/2000 ▶ train MSE: 0.9450, val MSE: 0.9159
Gen 842/2000 ▶ train MSE: 0.9449, val MSE: 0.9153
Gen 843/2000 ▶ train MSE: 0.9449, val MSE: 0.9153
Gen 844/2000 ▶ train MSE: 0.9448, val MSE: 0.9157
Gen 845/2000 ▶ train MSE: 0.9448, val MSE: 0.9149
Gen 846/2000 ▶ train MSE: 0.9446, val MSE: 0.9145
Gen 847/2000 ▶ train MSE: 0.9446, val MSE: 0.9145
Gen 848/2000 ▶ train MSE: 0.9446, val MSE: 0.9145
Gen 849/2000 ▶ train MSE: 0.9446, val MSE: 0.9142
Gen 850/2000 ▶ train MSE: 0.9446, val MSE: 0.9149
Gen 851/2000 ▶ train MSE: 0.9446, val MSE: 0.9149
Gen 852/2000 ▶ train MSE: 0.9444, val MSE: 0.9138
Gen 853/2000 ▶ train MSE: 0.9443, val MSE: 0.9134
Gen 854/2000 ▶ train MSE: 0.9443, val MSE: 0.9134
Gen 855/2000 ▶ train MSE: 0.9443, val MSE: 0.9134
Gen 856/2000 ▶ train MSE: 0.9443, val MSE: 0.9134
Gen 857/2000 ▶ train MSE: 0.9443, val MSE: 0.9134
Gen 858/2000 ▶ train MSE: 0.9443, val MSE: 0.9134
Gen 859/2000 ▶ train MSE: 0.9443, val MSE: 0.9134
Gen 860/2000 ▶ train MSE: 0.9442, val MSE: 0.9137
Gen 861/2000 ▶ train MSE: 0.9440, val MSE: 0.9135
Gen 862/2000 ▶ train MSE: 0.9440, val MSE: 0.9135
Gen 863/2000 ▶ train MSE: 0.9440, val MSE: 0.9135
Gen 864/2000 ▶ train MSE: 0.9440, val MSE: 0.9135
Gen 865/2000 ▶ train MSE: 0.9440, val MSE: 0.9135
Gen 866/2000 ▶ train MSE: 0.9440, val MSE: 0.9135
Gen 867/2000 ▶ train MSE: 0.9438, val MSE: 0.9138
Gen 868/2000 ▶ train MSE: 0.9438, val MSE: 0.9138
Gen 869/2000 ▶ train MSE: 0.9437, val MSE: 0.9123
Gen 870/2000 ▶ train MSE: 0.9437, val MSE: 0.9123
Gen 871/2000 ▶ train MSE: 0.9437, val MSE: 0.9130
Gen 872/2000 ▶ train MSE: 0.9437, val MSE: 0.9127
Gen 873/2000 ▶ train MSE: 0.9437, val MSE: 0.9127
Gen 874/2000 ▶ train MSE: 0.9437, val MSE: 0.9127
Gen 875/2000 ▶ train MSE: 0.9436, val MSE: 0.9125
Gen 876/2000 ▶ train MSE: 0.9436, val MSE: 0.9125
Gen 877/2000 ▶ train MSE: 0.9435, val MSE: 0.9124
Gen 878/2000 ▶ train MSE: 0.9434, val MSE: 0.9130
Gen 879/2000 ▶ train MSE: 0.9434, val MSE: 0.9132
Gen 880/2000 ▶ train MSE: 0.9434, val MSE: 0.9132
Gen 881/2000 ▶ train MSE: 0.9434, val MSE: 0.9132
Gen 882/2000 ▶ train MSE: 0.9434, val MSE: 0.9132
Gen 883/2000 ▶ train MSE: 0.9434, val MSE: 0.9132
Gen 884/2000 ▶ train MSE: 0.9434, val MSE: 0.9132
Gen 885/2000 ▶ train MSE: 0.9434, val MSE: 0.9132
Gen 886/2000 ▶ train MSE: 0.9432, val MSE: 0.9131
Gen 887/2000 ▶ train MSE: 0.9432, val MSE: 0.9124
Gen 888/2000 ▶ train MSE: 0.9430, val MSE: 0.9118
Gen 889/2000 ▶ train MSE: 0.9430, val MSE: 0.9118
Gen 890/2000 ▶ train MSE: 0.9430, val MSE: 0.9118
Gen 891/2000 ▶ train MSE: 0.9429, val MSE: 0.9116
Gen 892/2000 ▶ train MSE: 0.9428, val MSE: 0.9108
Gen 893/2000 ▶ train MSE: 0.9428, val MSE: 0.9108
Gen 894/2000 ▶ train MSE: 0.9428, val MSE: 0.9108
Gen 895/2000 ▶ train MSE: 0.9427, val MSE: 0.9116
Gen 896/2000 ▶ train MSE: 0.9427, val MSE: 0.9116
Gen 897/2000 ▶ train MSE: 0.9427, val MSE: 0.9116
Gen 898/2000 ▶ train MSE: 0.9427, val MSE: 0.9116
Gen 899/2000 ▶ train MSE: 0.9427, val MSE: 0.9116
Gen 900/2000 ▶ train MSE: 0.9427, val MSE: 0.9116
Gen 900/2000 ▶ train MSE: 0.9427, val MSE: 0.9116
Gen 901/2000 ▶ train MSE: 0.9426, val MSE: 0.9110
Gen 902/2000 ▶ train MSE: 0.9426, val MSE: 0.9110
Gen 903/2000 ▶ train MSE: 0.9426, val MSE: 0.9107
Gen 904/2000 ▶ train MSE: 0.9426, val MSE: 0.9107
Gen 905/2000 ▶ train MSE: 0.9425, val MSE: 0.9109
Gen 906/2000 ▶ train MSE: 0.9423, val MSE: 0.9097
Gen 907/2000 ▶ train MSE: 0.9423, val MSE: 0.9097
Gen 908/2000 ▶ train MSE: 0.9423, val MSE: 0.9097
Gen 909/2000 ▶ train MSE: 0.9423, val MSE: 0.9097
Gen 910/2000 ▶ train MSE: 0.9423, val MSE: 0.9097
Gen 911/2000 ▶ train MSE: 0.9422, val MSE: 0.9107
Gen 912/2000 ▶ train MSE: 0.9422, val MSE: 0.9107
Gen 913/2000 ▶ train MSE: 0.9422, val MSE: 0.9107
Gen 914/2000 ▶ train MSE: 0.9422, val MSE: 0.9107
Gen 915/2000 ▶ train MSE: 0.9422, val MSE: 0.9107
Gen 916/2000 ▶ train MSE: 0.9422, val MSE: 0.9107
Gen 917/2000 ▶ train MSE: 0.9422, val MSE: 0.9107
Gen 918/2000 ▶ train MSE: 0.9422, val MSE: 0.9107
Gen 919/2000 ▶ train MSE: 0.9422, val MSE: 0.9107
Gen 920/2000 ▶ train MSE: 0.9420, val MSE: 0.9093
Gen 921/2000 ▶ train MSE: 0.9420, val MSE: 0.9093
Gen 922/2000 ▶ train MSE: 0.9420, val MSE: 0.9093
Gen 923/2000 ▶ train MSE: 0.9420, val MSE: 0.9093
Gen 924/2000 ▶ train MSE: 0.9418, val MSE: 0.9097
Gen 925/2000 ▶ train MSE: 0.9418, val MSE: 0.9097
Gen 926/2000 ▶ train MSE: 0.9418, val MSE: 0.9097
Gen 927/2000 ▶ train MSE: 0.9418, val MSE: 0.9097
Gen 928/2000 ▶ train MSE: 0.9418, val MSE: 0.9097
Gen 929/2000 ▶ train MSE: 0.9418, val MSE: 0.9087
Gen 930/2000 ▶ train MSE: 0.9418, val MSE: 0.9087
Gen 931/2000 ▶ train MSE: 0.9416, val MSE: 0.9095
Gen 932/2000 ▶ train MSE: 0.9416, val MSE: 0.9095
Gen 933/2000 ▶ train MSE: 0.9416, val MSE: 0.9095
Gen 934/2000 ▶ train MSE: 0.9415, val MSE: 0.9093
Gen 935/2000 ▶ train MSE: 0.9415, val MSE: 0.9092
Gen 936/2000 ▶ train MSE: 0.9415, val MSE: 0.9092
Gen 937/2000 ▶ train MSE: 0.9415, val MSE: 0.9092
Gen 938/2000 ▶ train MSE: 0.9413, val MSE: 0.9077
Gen 939/2000 ▶ train MSE: 0.9413, val MSE: 0.9077
Gen 940/2000 ▶ train MSE: 0.9413, val MSE: 0.9077
Gen 941/2000 ▶ train MSE: 0.9413, val MSE: 0.9077
Gen 942/2000 ▶ train MSE: 0.9413, val MSE: 0.9077
Gen 943/2000 ▶ train MSE: 0.9413, val MSE: 0.9077
Gen 944/2000 ▶ train MSE: 0.9413, val MSE: 0.9077
Gen 945/2000 ▶ train MSE: 0.9413, val MSE: 0.9077
Gen 946/2000 ▶ train MSE: 0.9413, val MSE: 0.9067
Gen 947/2000 ▶ train MSE: 0.9411, val MSE: 0.9068
Gen 948/2000 ▶ train MSE: 0.9411, val MSE: 0.9068
Gen 949/2000 ▶ train MSE: 0.9411, val MSE: 0.9068
Gen 950/2000 ▶ train MSE: 0.9411, val MSE: 0.9078
Gen 951/2000 ▶ train MSE: 0.9408, val MSE: 0.9080
Gen 952/2000 ▶ train MSE: 0.9408, val MSE: 0.9080
Gen 953/2000 ▶ train MSE: 0.9408, val MSE: 0.9080
Gen 954/2000 ▶ train MSE: 0.9408, val MSE: 0.9080
Gen 955/2000 ▶ train MSE: 0.9408, val MSE: 0.9080
Gen 956/2000 ▶ train MSE: 0.9408, val MSE: 0.9080
Gen 957/2000 ▶ train MSE: 0.9408, val MSE: 0.9080
Gen 958/2000 ▶ train MSE: 0.9408, val MSE: 0.9067
Gen 959/2000 ▶ train MSE: 0.9408, val MSE: 0.9067
Gen 960/2000 ▶ train MSE: 0.9408, val MSE: 0.9067
Gen 961/2000 ▶ train MSE: 0.9408, val MSE: 0.9067
Gen 962/2000 ▶ train MSE: 0.9408, val MSE: 0.9067
Gen 963/2000 ▶ train MSE: 0.9406, val MSE: 0.9063
Gen 964/2000 ▶ train MSE: 0.9406, val MSE: 0.9063
Gen 965/2000 ▶ train MSE: 0.9406, val MSE: 0.9063
Gen 966/2000 ▶ train MSE: 0.9406, val MSE: 0.9063
Gen 967/2000 ▶ train MSE: 0.9406, val MSE: 0.9063
Gen 968/2000 ▶ train MSE: 0.9406, val MSE: 0.9063
Gen 969/2000 ▶ train MSE: 0.9405, val MSE: 0.9069
Gen 970/2000 ▶ train MSE: 0.9405, val MSE: 0.9069
Gen 971/2000 ▶ train MSE: 0.9405, val MSE: 0.9061
Gen 972/2000 ▶ train MSE: 0.9405, val MSE: 0.9061
Gen 973/2000 ▶ train MSE: 0.9405, val MSE: 0.9061
Gen 974/2000 ▶ train MSE: 0.9405, val MSE: 0.9061
Gen 975/2000 ▶ train MSE: 0.9404, val MSE: 0.9061
Gen 976/2000 ▶ train MSE: 0.9404, val MSE: 0.9061
Gen 977/2000 ▶ train MSE: 0.9403, val MSE: 0.9070
Gen 978/2000 ▶ train MSE: 0.9402, val MSE: 0.9061
Gen 979/2000 ▶ train MSE: 0.9401, val MSE: 0.9052
Gen 980/2000 ▶ train MSE: 0.9401, val MSE: 0.9052
Gen 981/2000 ▶ train MSE: 0.9401, val MSE: 0.9052
Gen 982/2000 ▶ train MSE: 0.9401, val MSE: 0.9052
Gen 983/2000 ▶ train MSE: 0.9401, val MSE: 0.9052
Gen 984/2000 ▶ train MSE: 0.9401, val MSE: 0.9052
Gen 985/2000 ▶ train MSE: 0.9399, val MSE: 0.9058
Gen 986/2000 ▶ train MSE: 0.9399, val MSE: 0.9058
Gen 987/2000 ▶ train MSE: 0.9399, val MSE: 0.9058
Gen 988/2000 ▶ train MSE: 0.9399, val MSE: 0.9058
Gen 989/2000 ▶ train MSE: 0.9399, val MSE: 0.9058
Gen 990/2000 ▶ train MSE: 0.9399, val MSE: 0.9058
Gen 991/2000 ▶ train MSE: 0.9399, val MSE: 0.9063
Gen 992/2000 ▶ train MSE: 0.9399, val MSE: 0.9050
Gen 993/2000 ▶ train MSE: 0.9399, val MSE: 0.9050
Gen 994/2000 ▶ train MSE: 0.9399, val MSE: 0.9050
Gen 995/2000 ▶ train MSE: 0.9398, val MSE: 0.9070
Gen 996/2000 ▶ train MSE: 0.9398, val MSE: 0.9070
Gen 997/2000 ▶ train MSE: 0.9398, val MSE: 0.9070
Gen 998/2000 ▶ train MSE: 0.9398, val MSE: 0.9070
Gen 999/2000 ▶ train MSE: 0.9398, val MSE: 0.9059
Gen 1000/2000 ▶ train MSE: 0.9398, val MSE: 0.9059
Gen 1000/2000 ▶ train MSE: 0.9396, val MSE: 0.9058
Gen 1001/2000 ▶ train MSE: 0.9396, val MSE: 0.9058
Gen 1002/2000 ▶ train MSE: 0.9396, val MSE: 0.9058
Gen 1003/2000 ▶ train MSE: 0.9396, val MSE: 0.9058
Gen 1004/2000 ▶ train MSE: 0.9396, val MSE: 0.9058
Gen 1005/2000 ▶ train MSE: 0.9396, val MSE: 0.9058
Gen 1006/2000 ▶ train MSE: 0.9396, val MSE: 0.9058
Gen 1007/2000 ▶ train MSE: 0.9395, val MSE: 0.9055
Gen 1008/2000 ▶ train MSE: 0.9395, val MSE: 0.9055
Gen 1009/2000 ▶ train MSE: 0.9395, val MSE: 0.9055
Gen 1010/2000 ▶ train MSE: 0.9395, val MSE: 0.9055
Gen 1011/2000 ▶ train MSE: 0.9395, val MSE: 0.9055
Gen 1012/2000 ▶ train MSE: 0.9395, val MSE: 0.9055
Gen 1013/2000 ▶ train MSE: 0.9395, val MSE: 0.9055
Gen 1014/2000 ▶ train MSE: 0.9394, val MSE: 0.9048
Gen 1015/2000 ▶ train MSE: 0.9394, val MSE: 0.9048
Gen 1016/2000 ▶ train MSE: 0.9394, val MSE: 0.9048
Gen 1017/2000 ▶ train MSE: 0.9394, val MSE: 0.9052
Gen 1018/2000 ▶ train MSE: 0.9394, val MSE: 0.9052
Gen 1019/2000 ▶ train MSE: 0.9393, val MSE: 0.9042
Gen 1020/2000 ▶ train MSE: 0.9393, val MSE: 0.9041
Gen 1021/2000 ▶ train MSE: 0.9393, val MSE: 0.9041
Gen 1022/2000 ▶ train MSE: 0.9393, val MSE: 0.9041
Gen 1023/2000 ▶ train MSE: 0.9393, val MSE: 0.9041
Gen 1024/2000 ▶ train MSE: 0.9393, val MSE: 0.9041
Gen 1025/2000 ▶ train MSE: 0.9393, val MSE: 0.9041
Gen 1026/2000 ▶ train MSE: 0.9390, val MSE: 0.9059
Gen 1027/2000 ▶ train MSE: 0.9390, val MSE: 0.9059
Gen 1028/2000 ▶ train MSE: 0.9390, val MSE: 0.9059
Gen 1029/2000 ▶ train MSE: 0.9390, val MSE: 0.9047
Gen 1030/2000 ▶ train MSE: 0.9390, val MSE: 0.9047
Gen 1031/2000 ▶ train MSE: 0.9390, val MSE: 0.9047
Gen 1032/2000 ▶ train MSE: 0.9390, val MSE: 0.9056
Gen 1033/2000 ▶ train MSE: 0.9390, val MSE: 0.9056
Gen 1034/2000 ▶ train MSE: 0.9390, val MSE: 0.9056
Gen 1035/2000 ▶ train MSE: 0.9390, val MSE: 0.9057
Gen 1036/2000 ▶ train MSE: 0.9389, val MSE: 0.9057
Gen 1037/2000 ▶ train MSE: 0.9389, val MSE: 0.9057
Gen 1038/2000 ▶ train MSE: 0.9388, val MSE: 0.9052
Gen 1039/2000 ▶ train MSE: 0.9388, val MSE: 0.9053
Gen 1040/2000 ▶ train MSE: 0.9388, val MSE: 0.9053
Gen 1041/2000 ▶ train MSE: 0.9388, val MSE: 0.9053
Gen 1042/2000 ▶ train MSE: 0.9386, val MSE: 0.9042
Gen 1043/2000 ▶ train MSE: 0.9386, val MSE: 0.9042
Gen 1044/2000 ▶ train MSE: 0.9386, val MSE: 0.9049
Gen 1045/2000 ▶ train MSE: 0.9384, val MSE: 0.9050
Gen 1046/2000 ▶ train MSE: 0.9383, val MSE: 0.9041
Gen 1047/2000 ▶ train MSE: 0.9383, val MSE: 0.9041
Gen 1048/2000 ▶ train MSE: 0.9383, val MSE: 0.9041
Gen 1049/2000 ▶ train MSE: 0.9383, val MSE: 0.9041
Gen 1050/2000 ▶ train MSE: 0.9383, val MSE: 0.9041
Gen 1051/2000 ▶ train MSE: 0.9383, val MSE: 0.9046
Gen 1052/2000 ▶ train MSE: 0.9383, val MSE: 0.9046
Gen 1053/2000 ▶ train MSE: 0.9383, val MSE: 0.9046
Gen 1054/2000 ▶ train MSE: 0.9382, val MSE: 0.9042
Gen 1055/2000 ▶ train MSE: 0.9381, val MSE: 0.9042
Gen 1056/2000 ▶ train MSE: 0.9381, val MSE: 0.9042
Gen 1057/2000 ▶ train MSE: 0.9381, val MSE: 0.9042
Gen 1058/2000 ▶ train MSE: 0.9381, val MSE: 0.9042
Gen 1059/2000 ▶ train MSE: 0.9381, val MSE: 0.9042
Gen 1060/2000 ▶ train MSE: 0.9381, val MSE: 0.9042
Gen 1061/2000 ▶ train MSE: 0.9381, val MSE: 0.9042
Gen 1062/2000 ▶ train MSE: 0.9381, val MSE: 0.9042
Gen 1063/2000 ▶ train MSE: 0.9381, val MSE: 0.9042
Gen 1064/2000 ▶ train MSE: 0.9380, val MSE: 0.9033
Gen 1065/2000 ▶ train MSE: 0.9380, val MSE: 0.9033
Gen 1066/2000 ▶ train MSE: 0.9380, val MSE: 0.9033
Gen 1067/2000 ▶ train MSE: 0.9379, val MSE: 0.9034
Gen 1068/2000 ▶ train MSE: 0.9379, val MSE: 0.9034
Gen 1069/2000 ▶ train MSE: 0.9379, val MSE: 0.9034
Gen 1070/2000 ▶ train MSE: 0.9378, val MSE: 0.9027
Gen 1071/2000 ▶ train MSE: 0.9378, val MSE: 0.9027
Gen 1072/2000 ▶ train MSE: 0.9378, val MSE: 0.9027
Gen 1073/2000 ▶ train MSE: 0.9378, val MSE: 0.9032
Gen 1074/2000 ▶ train MSE: 0.9375, val MSE: 0.9025
Gen 1075/2000 ▶ train MSE: 0.9375, val MSE: 0.9025
Gen 1076/2000 ▶ train MSE: 0.9375, val MSE: 0.9025
Gen 1077/2000 ▶ train MSE: 0.9375, val MSE: 0.9025
Gen 1078/2000 ▶ train MSE: 0.9375, val MSE: 0.9025
Gen 1079/2000 ▶ train MSE: 0.9375, val MSE: 0.9025
Gen 1080/2000 ▶ train MSE: 0.9375, val MSE: 0.9025
Gen 1081/2000 ▶ train MSE: 0.9374, val MSE: 0.9021
Gen 1082/2000 ▶ train MSE: 0.9374, val MSE: 0.9021
Gen 1083/2000 ▶ train MSE: 0.9374, val MSE: 0.9021
Gen 1084/2000 ▶ train MSE: 0.9374, val MSE: 0.9024
Gen 1085/2000 ▶ train MSE: 0.9374, val MSE: 0.9024
Gen 1086/2000 ▶ train MSE: 0.9374, val MSE: 0.9024
Gen 1087/2000 ▶ train MSE: 0.9374, val MSE: 0.9024
Gen 1088/2000 ▶ train MSE: 0.9374, val MSE: 0.9024
Gen 1089/2000 ▶ train MSE: 0.9374, val MSE: 0.9024
Gen 1090/2000 ▶ train MSE: 0.9372, val MSE: 0.9015
Gen 1091/2000 ▶ train MSE: 0.9372, val MSE: 0.9015
Gen 1092/2000 ▶ train MSE: 0.9372, val MSE: 0.9015
Gen 1093/2000 ▶ train MSE: 0.9372, val MSE: 0.9017
Gen 1094/2000 ▶ train MSE: 0.9372, val MSE: 0.9018
Gen 1095/2000 ▶ train MSE: 0.9371, val MSE: 0.9018
Gen 1096/2000 ▶ train MSE: 0.9371, val MSE: 0.9023
Gen 1097/2000 ▶ train MSE: 0.9371, val MSE: 0.9023
Gen 1098/2000 ▶ train MSE: 0.9371, val MSE: 0.9023
Gen 1099/2000 ▶ train MSE: 0.9370, val MSE: 0.9020
Gen 1100/2000 ▶ train MSE: 0.9370, val MSE: 0.9020
Gen 1100/2000 ▶ train MSE: 0.9370, val MSE: 0.9025
Gen 1101/2000 ▶ train MSE: 0.9370, val MSE: 0.9025
Gen 1102/2000 ▶ train MSE: 0.9369, val MSE: 0.9010
Gen 1103/2000 ▶ train MSE: 0.9369, val MSE: 0.9010
Gen 1104/2000 ▶ train MSE: 0.9368, val MSE: 0.9018
Gen 1105/2000 ▶ train MSE: 0.9368, val MSE: 0.9018
Gen 1106/2000 ▶ train MSE: 0.9368, val MSE: 0.9018
Gen 1107/2000 ▶ train MSE: 0.9368, val MSE: 0.9015
Gen 1108/2000 ▶ train MSE: 0.9368, val MSE: 0.9015
Gen 1109/2000 ▶ train MSE: 0.9368, val MSE: 0.9015
Gen 1110/2000 ▶ train MSE: 0.9367, val MSE: 0.9008
Gen 1111/2000 ▶ train MSE: 0.9367, val MSE: 0.9008
Gen 1112/2000 ▶ train MSE: 0.9367, val MSE: 0.9008
Gen 1113/2000 ▶ train MSE: 0.9366, val MSE: 0.9008
Gen 1114/2000 ▶ train MSE: 0.9366, val MSE: 0.9008
Gen 1115/2000 ▶ train MSE: 0.9366, val MSE: 0.9007
Gen 1116/2000 ▶ train MSE: 0.9366, val MSE: 0.9007
Gen 1117/2000 ▶ train MSE: 0.9365, val MSE: 0.9011
Gen 1118/2000 ▶ train MSE: 0.9365, val MSE: 0.9011
Gen 1119/2000 ▶ train MSE: 0.9365, val MSE: 0.9011
Gen 1120/2000 ▶ train MSE: 0.9365, val MSE: 0.8998
Gen 1121/2000 ▶ train MSE: 0.9364, val MSE: 0.9007
Gen 1122/2000 ▶ train MSE: 0.9363, val MSE: 0.9012
Gen 1123/2000 ▶ train MSE: 0.9363, val MSE: 0.9012
Gen 1124/2000 ▶ train MSE: 0.9363, val MSE: 0.9012
Gen 1125/2000 ▶ train MSE: 0.9363, val MSE: 0.9012
Gen 1126/2000 ▶ train MSE: 0.9363, val MSE: 0.9012
Gen 1127/2000 ▶ train MSE: 0.9363, val MSE: 0.9004
Gen 1128/2000 ▶ train MSE: 0.9362, val MSE: 0.9004
Gen 1129/2000 ▶ train MSE: 0.9362, val MSE: 0.9006
Gen 1130/2000 ▶ train MSE: 0.9362, val MSE: 0.9006
Gen 1131/2000 ▶ train MSE: 0.9360, val MSE: 0.9011
Gen 1132/2000 ▶ train MSE: 0.9360, val MSE: 0.9002
Gen 1133/2000 ▶ train MSE: 0.9359, val MSE: 0.9013
Gen 1134/2000 ▶ train MSE: 0.9359, val MSE: 0.9013
Gen 1135/2000 ▶ train MSE: 0.9359, val MSE: 0.9013
Gen 1136/2000 ▶ train MSE: 0.9359, val MSE: 0.9013
Gen 1137/2000 ▶ train MSE: 0.9359, val MSE: 0.9013
Gen 1138/2000 ▶ train MSE: 0.9359, val MSE: 0.9013
Gen 1139/2000 ▶ train MSE: 0.9359, val MSE: 0.8999
Gen 1140/2000 ▶ train MSE: 0.9358, val MSE: 0.9010
Gen 1141/2000 ▶ train MSE: 0.9358, val MSE: 0.9010
Gen 1142/2000 ▶ train MSE: 0.9357, val MSE: 0.8998
Gen 1143/2000 ▶ train MSE: 0.9357, val MSE: 0.8998
Gen 1144/2000 ▶ train MSE: 0.9357, val MSE: 0.8998
Gen 1145/2000 ▶ train MSE: 0.9357, val MSE: 0.8998
Gen 1146/2000 ▶ train MSE: 0.9357, val MSE: 0.8998
Gen 1147/2000 ▶ train MSE: 0.9356, val MSE: 0.8985
Gen 1148/2000 ▶ train MSE: 0.9354, val MSE: 0.8997
Gen 1149/2000 ▶ train MSE: 0.9354, val MSE: 0.8997
Gen 1150/2000 ▶ train MSE: 0.9354, val MSE: 0.8997
Gen 1151/2000 ▶ train MSE: 0.9354, val MSE: 0.8997
Gen 1152/2000 ▶ train MSE: 0.9354, val MSE: 0.8997
Gen 1153/2000 ▶ train MSE: 0.9354, val MSE: 0.8997
Gen 1154/2000 ▶ train MSE: 0.9354, val MSE: 0.8997
Gen 1155/2000 ▶ train MSE: 0.9354, val MSE: 0.8997
Gen 1156/2000 ▶ train MSE: 0.9354, val MSE: 0.8997
Gen 1157/2000 ▶ train MSE: 0.9354, val MSE: 0.9001
Gen 1158/2000 ▶ train MSE: 0.9354, val MSE: 0.9001
Gen 1159/2000 ▶ train MSE: 0.9352, val MSE: 0.8989
Gen 1160/2000 ▶ train MSE: 0.9352, val MSE: 0.8989
Gen 1161/2000 ▶ train MSE: 0.9352, val MSE: 0.8989
Gen 1162/2000 ▶ train MSE: 0.9352, val MSE: 0.8989
Gen 1163/2000 ▶ train MSE: 0.9348, val MSE: 0.8983
Gen 1164/2000 ▶ train MSE: 0.9348, val MSE: 0.8983
Gen 1165/2000 ▶ train MSE: 0.9348, val MSE: 0.8983
Gen 1166/2000 ▶ train MSE: 0.9348, val MSE: 0.8983
Gen 1167/2000 ▶ train MSE: 0.9348, val MSE: 0.8983
Gen 1168/2000 ▶ train MSE: 0.9348, val MSE: 0.8983
Gen 1169/2000 ▶ train MSE: 0.9348, val MSE: 0.8983
Gen 1170/2000 ▶ train MSE: 0.9347, val MSE: 0.8992
Gen 1171/2000 ▶ train MSE: 0.9347, val MSE: 0.8992
Gen 1172/2000 ▶ train MSE: 0.9347, val MSE: 0.8992
Gen 1173/2000 ▶ train MSE: 0.9345, val MSE: 0.8979
Gen 1174/2000 ▶ train MSE: 0.9345, val MSE: 0.8979
Gen 1175/2000 ▶ train MSE: 0.9345, val MSE: 0.8979
Gen 1176/2000 ▶ train MSE: 0.9345, val MSE: 0.8979
Gen 1177/2000 ▶ train MSE: 0.9343, val MSE: 0.8988
Gen 1178/2000 ▶ train MSE: 0.9343, val MSE: 0.8988
Gen 1179/2000 ▶ train MSE: 0.9343, val MSE: 0.8988
Gen 1180/2000 ▶ train MSE: 0.9343, val MSE: 0.8988
Gen 1181/2000 ▶ train MSE: 0.9341, val MSE: 0.8978
Gen 1182/2000 ▶ train MSE: 0.9341, val MSE: 0.8978
Gen 1183/2000 ▶ train MSE: 0.9341, val MSE: 0.8978
Gen 1184/2000 ▶ train MSE: 0.9341, val MSE: 0.8978
Gen 1185/2000 ▶ train MSE: 0.9341, val MSE: 0.8978
Gen 1186/2000 ▶ train MSE: 0.9341, val MSE: 0.8978
Gen 1187/2000 ▶ train MSE: 0.9341, val MSE: 0.8978
Gen 1188/2000 ▶ train MSE: 0.9341, val MSE: 0.8980
Gen 1189/2000 ▶ train MSE: 0.9341, val MSE: 0.8980
Gen 1190/2000 ▶ train MSE: 0.9341, val MSE: 0.8980
Gen 1191/2000 ▶ train MSE: 0.9341, val MSE: 0.8980
Gen 1192/2000 ▶ train MSE: 0.9340, val MSE: 0.8963
Gen 1193/2000 ▶ train MSE: 0.9340, val MSE: 0.8980
Gen 1194/2000 ▶ train MSE: 0.9340, val MSE: 0.8980
Gen 1195/2000 ▶ train MSE: 0.9338, val MSE: 0.8981
Gen 1196/2000 ▶ train MSE: 0.9338, val MSE: 0.8981
Gen 1197/2000 ▶ train MSE: 0.9338, val MSE: 0.8981
Gen 1198/2000 ▶ train MSE: 0.9338, val MSE: 0.8981
Gen 1199/2000 ▶ train MSE: 0.9338, val MSE: 0.8981
Gen 1200/2000 ▶ train MSE: 0.9338, val MSE: 0.8981
Gen 1200/2000 ▶ train MSE: 0.9337, val MSE: 0.8975
Gen 1201/2000 ▶ train MSE: 0.9335, val MSE: 0.8974
Gen 1202/2000 ▶ train MSE: 0.9335, val MSE: 0.8974
Gen 1203/2000 ▶ train MSE: 0.9335, val MSE: 0.8974
Gen 1204/2000 ▶ train MSE: 0.9335, val MSE: 0.8974
Gen 1205/2000 ▶ train MSE: 0.9335, val MSE: 0.8974
Gen 1206/2000 ▶ train MSE: 0.9335, val MSE: 0.8970
Gen 1207/2000 ▶ train MSE: 0.9335, val MSE: 0.8970
Gen 1208/2000 ▶ train MSE: 0.9335, val MSE: 0.8970
Gen 1209/2000 ▶ train MSE: 0.9335, val MSE: 0.8972
Gen 1210/2000 ▶ train MSE: 0.9332, val MSE: 0.8965
Gen 1211/2000 ▶ train MSE: 0.9332, val MSE: 0.8965
Gen 1212/2000 ▶ train MSE: 0.9332, val MSE: 0.8965
Gen 1213/2000 ▶ train MSE: 0.9331, val MSE: 0.8965
Gen 1214/2000 ▶ train MSE: 0.9331, val MSE: 0.8965
Gen 1215/2000 ▶ train MSE: 0.9330, val MSE: 0.8958
Gen 1216/2000 ▶ train MSE: 0.9330, val MSE: 0.8958
Gen 1217/2000 ▶ train MSE: 0.9329, val MSE: 0.8963
Gen 1218/2000 ▶ train MSE: 0.9328, val MSE: 0.8963
Gen 1219/2000 ▶ train MSE: 0.9328, val MSE: 0.8963
Gen 1220/2000 ▶ train MSE: 0.9328, val MSE: 0.8963
Gen 1221/2000 ▶ train MSE: 0.9325, val MSE: 0.8949
Gen 1222/2000 ▶ train MSE: 0.9325, val MSE: 0.8949
Gen 1223/2000 ▶ train MSE: 0.9324, val MSE: 0.8955
Gen 1224/2000 ▶ train MSE: 0.9324, val MSE: 0.8952
Gen 1225/2000 ▶ train MSE: 0.9323, val MSE: 0.8953
Gen 1226/2000 ▶ train MSE: 0.9323, val MSE: 0.8953
Gen 1227/2000 ▶ train MSE: 0.9323, val MSE: 0.8953
Gen 1228/2000 ▶ train MSE: 0.9322, val MSE: 0.8945
Gen 1229/2000 ▶ train MSE: 0.9322, val MSE: 0.8945
Gen 1230/2000 ▶ train MSE: 0.9322, val MSE: 0.8946
Gen 1231/2000 ▶ train MSE: 0.9322, val MSE: 0.8946
Gen 1232/2000 ▶ train MSE: 0.9322, val MSE: 0.8946
Gen 1233/2000 ▶ train MSE: 0.9320, val MSE: 0.8934
Gen 1234/2000 ▶ train MSE: 0.9320, val MSE: 0.8934
Gen 1235/2000 ▶ train MSE: 0.9320, val MSE: 0.8934
Gen 1236/2000 ▶ train MSE: 0.9320, val MSE: 0.8934
Gen 1237/2000 ▶ train MSE: 0.9320, val MSE: 0.8934
Gen 1238/2000 ▶ train MSE: 0.9319, val MSE: 0.8938
Gen 1239/2000 ▶ train MSE: 0.9317, val MSE: 0.8933
Gen 1240/2000 ▶ train MSE: 0.9317, val MSE: 0.8933
Gen 1241/2000 ▶ train MSE: 0.9317, val MSE: 0.8933
Gen 1242/2000 ▶ train MSE: 0.9317, val MSE: 0.8933
Gen 1243/2000 ▶ train MSE: 0.9317, val MSE: 0.8933
Gen 1244/2000 ▶ train MSE: 0.9317, val MSE: 0.8933
Gen 1245/2000 ▶ train MSE: 0.9317, val MSE: 0.8933
Gen 1246/2000 ▶ train MSE: 0.9317, val MSE: 0.8933
Gen 1247/2000 ▶ train MSE: 0.9316, val MSE: 0.8932
Gen 1248/2000 ▶ train MSE: 0.9316, val MSE: 0.8932
Gen 1249/2000 ▶ train MSE: 0.9315, val MSE: 0.8934
Gen 1250/2000 ▶ train MSE: 0.9315, val MSE: 0.8934
Gen 1251/2000 ▶ train MSE: 0.9315, val MSE: 0.8934
Gen 1252/2000 ▶ train MSE: 0.9315, val MSE: 0.8934
Gen 1253/2000 ▶ train MSE: 0.9315, val MSE: 0.8934
Gen 1254/2000 ▶ train MSE: 0.9314, val MSE: 0.8930
Gen 1255/2000 ▶ train MSE: 0.9314, val MSE: 0.8930
Gen 1256/2000 ▶ train MSE: 0.9314, val MSE: 0.8930
Gen 1257/2000 ▶ train MSE: 0.9314, val MSE: 0.8930
Gen 1258/2000 ▶ train MSE: 0.9314, val MSE: 0.8930
Gen 1259/2000 ▶ train MSE: 0.9314, val MSE: 0.8927
Gen 1260/2000 ▶ train MSE: 0.9314, val MSE: 0.8929
Gen 1261/2000 ▶ train MSE: 0.9314, val MSE: 0.8929
Gen 1262/2000 ▶ train MSE: 0.9313, val MSE: 0.8922
Gen 1263/2000 ▶ train MSE: 0.9313, val MSE: 0.8922
Gen 1264/2000 ▶ train MSE: 0.9312, val MSE: 0.8926
Gen 1265/2000 ▶ train MSE: 0.9312, val MSE: 0.8926
Gen 1266/2000 ▶ train MSE: 0.9311, val MSE: 0.8928
Gen 1267/2000 ▶ train MSE: 0.9311, val MSE: 0.8928
Gen 1268/2000 ▶ train MSE: 0.9311, val MSE: 0.8928
Gen 1269/2000 ▶ train MSE: 0.9311, val MSE: 0.8928
Gen 1270/2000 ▶ train MSE: 0.9309, val MSE: 0.8927
Gen 1271/2000 ▶ train MSE: 0.9309, val MSE: 0.8918
Gen 1272/2000 ▶ train MSE: 0.9309, val MSE: 0.8918
Gen 1273/2000 ▶ train MSE: 0.9308, val MSE: 0.8919
Gen 1274/2000 ▶ train MSE: 0.9308, val MSE: 0.8919
Gen 1275/2000 ▶ train MSE: 0.9308, val MSE: 0.8915
Gen 1276/2000 ▶ train MSE: 0.9308, val MSE: 0.8915
Gen 1277/2000 ▶ train MSE: 0.9308, val MSE: 0.8915
Gen 1278/2000 ▶ train MSE: 0.9308, val MSE: 0.8915
Gen 1279/2000 ▶ train MSE: 0.9307, val MSE: 0.8922
Gen 1280/2000 ▶ train MSE: 0.9307, val MSE: 0.8908
Gen 1281/2000 ▶ train MSE: 0.9306, val MSE: 0.8913
Gen 1282/2000 ▶ train MSE: 0.9306, val MSE: 0.8913
Gen 1283/2000 ▶ train MSE: 0.9306, val MSE: 0.8913
Gen 1284/2000 ▶ train MSE: 0.9306, val MSE: 0.8910
Gen 1285/2000 ▶ train MSE: 0.9306, val MSE: 0.8910
Gen 1286/2000 ▶ train MSE: 0.9305, val MSE: 0.8920
Gen 1287/2000 ▶ train MSE: 0.9305, val MSE: 0.8920
Gen 1288/2000 ▶ train MSE: 0.9305, val MSE: 0.8920
Gen 1289/2000 ▶ train MSE: 0.9304, val MSE: 0.8915
Gen 1290/2000 ▶ train MSE: 0.9304, val MSE: 0.8915
Gen 1291/2000 ▶ train MSE: 0.9304, val MSE: 0.8915
Gen 1292/2000 ▶ train MSE: 0.9303, val MSE: 0.8904
Gen 1293/2000 ▶ train MSE: 0.9303, val MSE: 0.8904
Gen 1294/2000 ▶ train MSE: 0.9303, val MSE: 0.8904
Gen 1295/2000 ▶ train MSE: 0.9302, val MSE: 0.8904
Gen 1296/2000 ▶ train MSE: 0.9302, val MSE: 0.8904
Gen 1297/2000 ▶ train MSE: 0.9301, val MSE: 0.8902
Gen 1298/2000 ▶ train MSE: 0.9301, val MSE: 0.8902
Gen 1299/2000 ▶ train MSE: 0.9301, val MSE: 0.8902
Gen 1300/2000 ▶ train MSE: 0.9301, val MSE: 0.8902
Gen 1300/2000 ▶ train MSE: 0.9301, val MSE: 0.8902
Gen 1301/2000 ▶ train MSE: 0.9300, val MSE: 0.8909
Gen 1302/2000 ▶ train MSE: 0.9300, val MSE: 0.8909
Gen 1303/2000 ▶ train MSE: 0.9300, val MSE: 0.8909
Gen 1304/2000 ▶ train MSE: 0.9300, val MSE: 0.8909
Gen 1305/2000 ▶ train MSE: 0.9300, val MSE: 0.8909
Gen 1306/2000 ▶ train MSE: 0.9300, val MSE: 0.8909
Gen 1307/2000 ▶ train MSE: 0.9300, val MSE: 0.8903
Gen 1308/2000 ▶ train MSE: 0.9300, val MSE: 0.8903
Gen 1309/2000 ▶ train MSE: 0.9300, val MSE: 0.8903
Gen 1310/2000 ▶ train MSE: 0.9300, val MSE: 0.8903
Gen 1311/2000 ▶ train MSE: 0.9300, val MSE: 0.8907
Gen 1312/2000 ▶ train MSE: 0.9300, val MSE: 0.8907
Gen 1313/2000 ▶ train MSE: 0.9300, val MSE: 0.8907
Gen 1314/2000 ▶ train MSE: 0.9299, val MSE: 0.8899
Gen 1315/2000 ▶ train MSE: 0.9298, val MSE: 0.8901
Gen 1316/2000 ▶ train MSE: 0.9298, val MSE: 0.8901
Gen 1317/2000 ▶ train MSE: 0.9298, val MSE: 0.8901
Gen 1318/2000 ▶ train MSE: 0.9298, val MSE: 0.8911
Gen 1319/2000 ▶ train MSE: 0.9298, val MSE: 0.8911
Gen 1320/2000 ▶ train MSE: 0.9298, val MSE: 0.8911
Gen 1321/2000 ▶ train MSE: 0.9298, val MSE: 0.8911
Gen 1322/2000 ▶ train MSE: 0.9298, val MSE: 0.8895
Gen 1323/2000 ▶ train MSE: 0.9297, val MSE: 0.8891
Gen 1324/2000 ▶ train MSE: 0.9297, val MSE: 0.8891
Gen 1325/2000 ▶ train MSE: 0.9297, val MSE: 0.8891
Gen 1326/2000 ▶ train MSE: 0.9297, val MSE: 0.8891
Gen 1327/2000 ▶ train MSE: 0.9297, val MSE: 0.8891
Gen 1328/2000 ▶ train MSE: 0.9297, val MSE: 0.8891
Gen 1329/2000 ▶ train MSE: 0.9296, val MSE: 0.8906
Gen 1330/2000 ▶ train MSE: 0.9296, val MSE: 0.8900
Gen 1331/2000 ▶ train MSE: 0.9294, val MSE: 0.8891
Gen 1332/2000 ▶ train MSE: 0.9294, val MSE: 0.8891
Gen 1333/2000 ▶ train MSE: 0.9294, val MSE: 0.8891
Gen 1334/2000 ▶ train MSE: 0.9294, val MSE: 0.8891
Gen 1335/2000 ▶ train MSE: 0.9294, val MSE: 0.8891
Gen 1336/2000 ▶ train MSE: 0.9294, val MSE: 0.8891
Gen 1337/2000 ▶ train MSE: 0.9294, val MSE: 0.8891
Gen 1338/2000 ▶ train MSE: 0.9293, val MSE: 0.8889
Gen 1339/2000 ▶ train MSE: 0.9293, val MSE: 0.8889
Gen 1340/2000 ▶ train MSE: 0.9293, val MSE: 0.8889
Gen 1341/2000 ▶ train MSE: 0.9293, val MSE: 0.8889
Gen 1342/2000 ▶ train MSE: 0.9293, val MSE: 0.8889
Gen 1343/2000 ▶ train MSE: 0.9293, val MSE: 0.8889
Gen 1344/2000 ▶ train MSE: 0.9293, val MSE: 0.8899
Gen 1345/2000 ▶ train MSE: 0.9293, val MSE: 0.8899
Gen 1346/2000 ▶ train MSE: 0.9293, val MSE: 0.8899
Gen 1347/2000 ▶ train MSE: 0.9293, val MSE: 0.8899
Gen 1348/2000 ▶ train MSE: 0.9293, val MSE: 0.8899
Gen 1349/2000 ▶ train MSE: 0.9291, val MSE: 0.8892
Gen 1350/2000 ▶ train MSE: 0.9291, val MSE: 0.8892
Gen 1351/2000 ▶ train MSE: 0.9291, val MSE: 0.8892
Gen 1352/2000 ▶ train MSE: 0.9291, val MSE: 0.8892
Gen 1353/2000 ▶ train MSE: 0.9291, val MSE: 0.8897
Gen 1354/2000 ▶ train MSE: 0.9291, val MSE: 0.8897
Gen 1355/2000 ▶ train MSE: 0.9291, val MSE: 0.8897
Gen 1356/2000 ▶ train MSE: 0.9291, val MSE: 0.8897
Gen 1357/2000 ▶ train MSE: 0.9291, val MSE: 0.8897
Gen 1358/2000 ▶ train MSE: 0.9290, val MSE: 0.8884
Gen 1359/2000 ▶ train MSE: 0.9290, val MSE: 0.8884
Gen 1360/2000 ▶ train MSE: 0.9290, val MSE: 0.8874
Gen 1361/2000 ▶ train MSE: 0.9290, val MSE: 0.8874
Gen 1362/2000 ▶ train MSE: 0.9290, val MSE: 0.8874
Gen 1363/2000 ▶ train MSE: 0.9290, val MSE: 0.8874
Gen 1364/2000 ▶ train MSE: 0.9288, val MSE: 0.8890
Gen 1365/2000 ▶ train MSE: 0.9287, val MSE: 0.8891
Gen 1366/2000 ▶ train MSE: 0.9287, val MSE: 0.8891
Gen 1367/2000 ▶ train MSE: 0.9287, val MSE: 0.8891
Gen 1368/2000 ▶ train MSE: 0.9287, val MSE: 0.8891
Gen 1369/2000 ▶ train MSE: 0.9287, val MSE: 0.8884
Gen 1370/2000 ▶ train MSE: 0.9286, val MSE: 0.8882
Gen 1371/2000 ▶ train MSE: 0.9286, val MSE: 0.8882
Gen 1372/2000 ▶ train MSE: 0.9286, val MSE: 0.8885
Gen 1373/2000 ▶ train MSE: 0.9284, val MSE: 0.8880
Gen 1374/2000 ▶ train MSE: 0.9284, val MSE: 0.8880
Gen 1375/2000 ▶ train MSE: 0.9284, val MSE: 0.8880
Gen 1376/2000 ▶ train MSE: 0.9284, val MSE: 0.8880
Gen 1377/2000 ▶ train MSE: 0.9284, val MSE: 0.8880
Gen 1378/2000 ▶ train MSE: 0.9284, val MSE: 0.8880
Gen 1379/2000 ▶ train MSE: 0.9284, val MSE: 0.8880
Gen 1380/2000 ▶ train MSE: 0.9283, val MSE: 0.8879
Gen 1381/2000 ▶ train MSE: 0.9283, val MSE: 0.8879
Gen 1382/2000 ▶ train MSE: 0.9283, val MSE: 0.8879
Gen 1383/2000 ▶ train MSE: 0.9283, val MSE: 0.8879
Gen 1384/2000 ▶ train MSE: 0.9283, val MSE: 0.8879
Gen 1385/2000 ▶ train MSE: 0.9283, val MSE: 0.8879
Gen 1386/2000 ▶ train MSE: 0.9282, val MSE: 0.8884
Gen 1387/2000 ▶ train MSE: 0.9280, val MSE: 0.8878
Gen 1388/2000 ▶ train MSE: 0.9280, val MSE: 0.8878
Gen 1389/2000 ▶ train MSE: 0.9280, val MSE: 0.8878
Gen 1390/2000 ▶ train MSE: 0.9280, val MSE: 0.8878
Gen 1391/2000 ▶ train MSE: 0.9280, val MSE: 0.8878
Gen 1392/2000 ▶ train MSE: 0.9277, val MSE: 0.8870
Gen 1393/2000 ▶ train MSE: 0.9277, val MSE: 0.8870
Gen 1394/2000 ▶ train MSE: 0.9277, val MSE: 0.8870
Gen 1395/2000 ▶ train MSE: 0.9277, val MSE: 0.8870
Gen 1396/2000 ▶ train MSE: 0.9277, val MSE: 0.8870
Gen 1397/2000 ▶ train MSE: 0.9277, val MSE: 0.8870
Gen 1398/2000 ▶ train MSE: 0.9277, val MSE: 0.8870
Gen 1399/2000 ▶ train MSE: 0.9277, val MSE: 0.8870
Gen 1400/2000 ▶ train MSE: 0.9277, val MSE: 0.8870
Gen 1400/2000 ▶ train MSE: 0.9276, val MSE: 0.8872
Gen 1401/2000 ▶ train MSE: 0.9275, val MSE: 0.8862
Gen 1402/2000 ▶ train MSE: 0.9275, val MSE: 0.8862
Gen 1403/2000 ▶ train MSE: 0.9275, val MSE: 0.8862
Gen 1404/2000 ▶ train MSE: 0.9275, val MSE: 0.8862
Gen 1405/2000 ▶ train MSE: 0.9275, val MSE: 0.8862
Gen 1406/2000 ▶ train MSE: 0.9275, val MSE: 0.8862
Gen 1407/2000 ▶ train MSE: 0.9275, val MSE: 0.8862
Gen 1408/2000 ▶ train MSE: 0.9275, val MSE: 0.8862
Gen 1409/2000 ▶ train MSE: 0.9275, val MSE: 0.8863
Gen 1410/2000 ▶ train MSE: 0.9275, val MSE: 0.8866
Gen 1411/2000 ▶ train MSE: 0.9274, val MSE: 0.8858
Gen 1412/2000 ▶ train MSE: 0.9274, val MSE: 0.8858
Gen 1413/2000 ▶ train MSE: 0.9274, val MSE: 0.8869
Gen 1414/2000 ▶ train MSE: 0.9274, val MSE: 0.8869
Gen 1415/2000 ▶ train MSE: 0.9274, val MSE: 0.8869
Gen 1416/2000 ▶ train MSE: 0.9273, val MSE: 0.8862
Gen 1417/2000 ▶ train MSE: 0.9273, val MSE: 0.8862
Gen 1418/2000 ▶ train MSE: 0.9273, val MSE: 0.8862
Gen 1419/2000 ▶ train MSE: 0.9273, val MSE: 0.8863
Gen 1420/2000 ▶ train MSE: 0.9273, val MSE: 0.8862
Gen 1421/2000 ▶ train MSE: 0.9273, val MSE: 0.8862
Gen 1422/2000 ▶ train MSE: 0.9273, val MSE: 0.8862
Gen 1423/2000 ▶ train MSE: 0.9273, val MSE: 0.8862
Gen 1424/2000 ▶ train MSE: 0.9273, val MSE: 0.8862
Gen 1425/2000 ▶ train MSE: 0.9273, val MSE: 0.8862
Gen 1426/2000 ▶ train MSE: 0.9272, val MSE: 0.8858
Gen 1427/2000 ▶ train MSE: 0.9272, val MSE: 0.8858
Gen 1428/2000 ▶ train MSE: 0.9272, val MSE: 0.8858
Gen 1429/2000 ▶ train MSE: 0.9272, val MSE: 0.8858
Gen 1430/2000 ▶ train MSE: 0.9272, val MSE: 0.8858
Gen 1431/2000 ▶ train MSE: 0.9270, val MSE: 0.8857
Gen 1432/2000 ▶ train MSE: 0.9270, val MSE: 0.8857
Gen 1433/2000 ▶ train MSE: 0.9270, val MSE: 0.8857
Gen 1434/2000 ▶ train MSE: 0.9270, val MSE: 0.8857
Gen 1435/2000 ▶ train MSE: 0.9270, val MSE: 0.8857
Gen 1436/2000 ▶ train MSE: 0.9270, val MSE: 0.8857
Gen 1437/2000 ▶ train MSE: 0.9269, val MSE: 0.8859
Gen 1438/2000 ▶ train MSE: 0.9269, val MSE: 0.8859
Gen 1439/2000 ▶ train MSE: 0.9269, val MSE: 0.8859
Gen 1440/2000 ▶ train MSE: 0.9269, val MSE: 0.8859
Gen 1441/2000 ▶ train MSE: 0.9269, val MSE: 0.8859
Gen 1442/2000 ▶ train MSE: 0.9269, val MSE: 0.8859
Gen 1443/2000 ▶ train MSE: 0.9269, val MSE: 0.8859
Gen 1444/2000 ▶ train MSE: 0.9269, val MSE: 0.8859
Gen 1445/2000 ▶ train MSE: 0.9269, val MSE: 0.8861
Gen 1446/2000 ▶ train MSE: 0.9268, val MSE: 0.8851
Gen 1447/2000 ▶ train MSE: 0.9268, val MSE: 0.8851
Gen 1448/2000 ▶ train MSE: 0.9268, val MSE: 0.8851
Gen 1449/2000 ▶ train MSE: 0.9268, val MSE: 0.8851
Gen 1450/2000 ▶ train MSE: 0.9268, val MSE: 0.8851
Gen 1451/2000 ▶ train MSE: 0.9268, val MSE: 0.8851
Gen 1452/2000 ▶ train MSE: 0.9267, val MSE: 0.8859
Gen 1453/2000 ▶ train MSE: 0.9267, val MSE: 0.8859
Gen 1454/2000 ▶ train MSE: 0.9267, val MSE: 0.8854
Gen 1455/2000 ▶ train MSE: 0.9267, val MSE: 0.8854
Gen 1456/2000 ▶ train MSE: 0.9267, val MSE: 0.8854
Gen 1457/2000 ▶ train MSE: 0.9267, val MSE: 0.8854
Gen 1458/2000 ▶ train MSE: 0.9267, val MSE: 0.8854
Gen 1459/2000 ▶ train MSE: 0.9267, val MSE: 0.8854
Gen 1460/2000 ▶ train MSE: 0.9267, val MSE: 0.8854
Gen 1461/2000 ▶ train MSE: 0.9266, val MSE: 0.8857
Gen 1462/2000 ▶ train MSE: 0.9266, val MSE: 0.8857
Gen 1463/2000 ▶ train MSE: 0.9266, val MSE: 0.8851
Gen 1464/2000 ▶ train MSE: 0.9266, val MSE: 0.8851
Gen 1465/2000 ▶ train MSE: 0.9265, val MSE: 0.8845
Gen 1466/2000 ▶ train MSE: 0.9265, val MSE: 0.8845
Gen 1467/2000 ▶ train MSE: 0.9265, val MSE: 0.8845
Gen 1468/2000 ▶ train MSE: 0.9265, val MSE: 0.8845
Gen 1469/2000 ▶ train MSE: 0.9265, val MSE: 0.8845
Gen 1470/2000 ▶ train MSE: 0.9265, val MSE: 0.8845
Gen 1471/2000 ▶ train MSE: 0.9265, val MSE: 0.8845
Gen 1472/2000 ▶ train MSE: 0.9265, val MSE: 0.8845
Gen 1473/2000 ▶ train MSE: 0.9265, val MSE: 0.8845
Gen 1474/2000 ▶ train MSE: 0.9265, val MSE: 0.8845
Gen 1475/2000 ▶ train MSE: 0.9265, val MSE: 0.8856
Gen 1476/2000 ▶ train MSE: 0.9265, val MSE: 0.8856
Gen 1477/2000 ▶ train MSE: 0.9265, val MSE: 0.8856
Gen 1478/2000 ▶ train MSE: 0.9265, val MSE: 0.8856
Gen 1479/2000 ▶ train MSE: 0.9264, val MSE: 0.8848
Gen 1480/2000 ▶ train MSE: 0.9264, val MSE: 0.8848
Gen 1481/2000 ▶ train MSE: 0.9264, val MSE: 0.8848
Gen 1482/2000 ▶ train MSE: 0.9264, val MSE: 0.8848
Gen 1483/2000 ▶ train MSE: 0.9264, val MSE: 0.8855
Gen 1484/2000 ▶ train MSE: 0.9264, val MSE: 0.8855
Gen 1485/2000 ▶ train MSE: 0.9264, val MSE: 0.8855
Gen 1486/2000 ▶ train MSE: 0.9263, val MSE: 0.8846
Gen 1487/2000 ▶ train MSE: 0.9263, val MSE: 0.8846
Gen 1488/2000 ▶ train MSE: 0.9263, val MSE: 0.8846
Gen 1489/2000 ▶ train MSE: 0.9263, val MSE: 0.8846
Gen 1490/2000 ▶ train MSE: 0.9263, val MSE: 0.8846
Gen 1491/2000 ▶ train MSE: 0.9262, val MSE: 0.8849
Gen 1492/2000 ▶ train MSE: 0.9262, val MSE: 0.8849
Gen 1493/2000 ▶ train MSE: 0.9262, val MSE: 0.8849
Gen 1494/2000 ▶ train MSE: 0.9262, val MSE: 0.8849
Gen 1495/2000 ▶ train MSE: 0.9262, val MSE: 0.8849
Gen 1496/2000 ▶ train MSE: 0.9262, val MSE: 0.8845
Gen 1497/2000 ▶ train MSE: 0.9262, val MSE: 0.8845
Gen 1498/2000 ▶ train MSE: 0.9262, val MSE: 0.8849
Gen 1499/2000 ▶ train MSE: 0.9262, val MSE: 0.8849
Gen 1500/2000 ▶ train MSE: 0.9262, val MSE: 0.8849
Gen 1500/2000 ▶ train MSE: 0.9261, val MSE: 0.8840
Gen 1501/2000 ▶ train MSE: 0.9261, val MSE: 0.8840
Gen 1502/2000 ▶ train MSE: 0.9261, val MSE: 0.8840
Gen 1503/2000 ▶ train MSE: 0.9261, val MSE: 0.8851
Gen 1504/2000 ▶ train MSE: 0.9261, val MSE: 0.8851
Gen 1505/2000 ▶ train MSE: 0.9261, val MSE: 0.8847
Gen 1506/2000 ▶ train MSE: 0.9260, val MSE: 0.8846
Gen 1507/2000 ▶ train MSE: 0.9260, val MSE: 0.8846
Gen 1508/2000 ▶ train MSE: 0.9260, val MSE: 0.8846
Gen 1509/2000 ▶ train MSE: 0.9260, val MSE: 0.8846
Gen 1510/2000 ▶ train MSE: 0.9260, val MSE: 0.8846
Gen 1511/2000 ▶ train MSE: 0.9260, val MSE: 0.8841
Gen 1512/2000 ▶ train MSE: 0.9260, val MSE: 0.8841
Gen 1513/2000 ▶ train MSE: 0.9259, val MSE: 0.8834
Gen 1514/2000 ▶ train MSE: 0.9259, val MSE: 0.8834
Gen 1515/2000 ▶ train MSE: 0.9259, val MSE: 0.8834
Gen 1516/2000 ▶ train MSE: 0.9259, val MSE: 0.8834
Gen 1517/2000 ▶ train MSE: 0.9259, val MSE: 0.8834
Gen 1518/2000 ▶ train MSE: 0.9259, val MSE: 0.8837
Gen 1519/2000 ▶ train MSE: 0.9259, val MSE: 0.8845
Gen 1520/2000 ▶ train MSE: 0.9258, val MSE: 0.8843
Gen 1521/2000 ▶ train MSE: 0.9258, val MSE: 0.8843
Gen 1522/2000 ▶ train MSE: 0.9258, val MSE: 0.8843
Gen 1523/2000 ▶ train MSE: 0.9258, val MSE: 0.8843
Gen 1524/2000 ▶ train MSE: 0.9258, val MSE: 0.8843
Gen 1525/2000 ▶ train MSE: 0.9258, val MSE: 0.8843
Gen 1526/2000 ▶ train MSE: 0.9257, val MSE: 0.8839
Gen 1527/2000 ▶ train MSE: 0.9257, val MSE: 0.8839
Gen 1528/2000 ▶ train MSE: 0.9257, val MSE: 0.8839
Gen 1529/2000 ▶ train MSE: 0.9257, val MSE: 0.8839
Gen 1530/2000 ▶ train MSE: 0.9257, val MSE: 0.8843
Gen 1531/2000 ▶ train MSE: 0.9257, val MSE: 0.8843
Gen 1532/2000 ▶ train MSE: 0.9257, val MSE: 0.8843
Gen 1533/2000 ▶ train MSE: 0.9257, val MSE: 0.8842
Gen 1534/2000 ▶ train MSE: 0.9256, val MSE: 0.8841
Gen 1535/2000 ▶ train MSE: 0.9256, val MSE: 0.8841
Gen 1536/2000 ▶ train MSE: 0.9256, val MSE: 0.8841
Gen 1537/2000 ▶ train MSE: 0.9256, val MSE: 0.8841
Gen 1538/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1539/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1540/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1541/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1542/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1543/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1544/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1545/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1546/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1547/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1548/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1549/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1550/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1551/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1552/2000 ▶ train MSE: 0.9254, val MSE: 0.8840
Gen 1553/2000 ▶ train MSE: 0.9254, val MSE: 0.8830
Gen 1554/2000 ▶ train MSE: 0.9254, val MSE: 0.8830
Gen 1555/2000 ▶ train MSE: 0.9253, val MSE: 0.8838
Gen 1556/2000 ▶ train MSE: 0.9253, val MSE: 0.8838
Gen 1557/2000 ▶ train MSE: 0.9253, val MSE: 0.8838
Gen 1558/2000 ▶ train MSE: 0.9253, val MSE: 0.8838
Gen 1559/2000 ▶ train MSE: 0.9253, val MSE: 0.8838
Gen 1560/2000 ▶ train MSE: 0.9253, val MSE: 0.8844
Gen 1561/2000 ▶ train MSE: 0.9253, val MSE: 0.8834
Gen 1562/2000 ▶ train MSE: 0.9253, val MSE: 0.8834
Gen 1563/2000 ▶ train MSE: 0.9253, val MSE: 0.8834
Gen 1564/2000 ▶ train MSE: 0.9253, val MSE: 0.8834
Gen 1565/2000 ▶ train MSE: 0.9253, val MSE: 0.8834
Gen 1566/2000 ▶ train MSE: 0.9253, val MSE: 0.8834
Gen 1567/2000 ▶ train MSE: 0.9251, val MSE: 0.8835
Gen 1568/2000 ▶ train MSE: 0.9251, val MSE: 0.8835
Gen 1569/2000 ▶ train MSE: 0.9251, val MSE: 0.8835
Gen 1570/2000 ▶ train MSE: 0.9251, val MSE: 0.8835
Gen 1571/2000 ▶ train MSE: 0.9251, val MSE: 0.8829
Gen 1572/2000 ▶ train MSE: 0.9251, val MSE: 0.8829
Gen 1573/2000 ▶ train MSE: 0.9251, val MSE: 0.8829
Gen 1574/2000 ▶ train MSE: 0.9251, val MSE: 0.8829
Gen 1575/2000 ▶ train MSE: 0.9251, val MSE: 0.8829
Gen 1576/2000 ▶ train MSE: 0.9251, val MSE: 0.8829
Gen 1577/2000 ▶ train MSE: 0.9250, val MSE: 0.8826
Gen 1578/2000 ▶ train MSE: 0.9250, val MSE: 0.8832
Gen 1579/2000 ▶ train MSE: 0.9249, val MSE: 0.8827
Gen 1580/2000 ▶ train MSE: 0.9249, val MSE: 0.8827
Gen 1581/2000 ▶ train MSE: 0.9249, val MSE: 0.8827
Gen 1582/2000 ▶ train MSE: 0.9249, val MSE: 0.8827
Gen 1583/2000 ▶ train MSE: 0.9249, val MSE: 0.8827
Gen 1584/2000 ▶ train MSE: 0.9248, val MSE: 0.8826
Gen 1585/2000 ▶ train MSE: 0.9248, val MSE: 0.8826
Gen 1586/2000 ▶ train MSE: 0.9248, val MSE: 0.8826
Gen 1587/2000 ▶ train MSE: 0.9247, val MSE: 0.8826
Gen 1588/2000 ▶ train MSE: 0.9247, val MSE: 0.8821
Gen 1589/2000 ▶ train MSE: 0.9247, val MSE: 0.8821
Gen 1590/2000 ▶ train MSE: 0.9247, val MSE: 0.8817
Gen 1591/2000 ▶ train MSE: 0.9247, val MSE: 0.8817
Gen 1592/2000 ▶ train MSE: 0.9247, val MSE: 0.8817
Gen 1593/2000 ▶ train MSE: 0.9246, val MSE: 0.8822
Gen 1594/2000 ▶ train MSE: 0.9246, val MSE: 0.8822
Gen 1595/2000 ▶ train MSE: 0.9246, val MSE: 0.8822
Gen 1596/2000 ▶ train MSE: 0.9246, val MSE: 0.8822
Gen 1597/2000 ▶ train MSE: 0.9246, val MSE: 0.8819
Gen 1598/2000 ▶ train MSE: 0.9246, val MSE: 0.8819
Gen 1599/2000 ▶ train MSE: 0.9245, val MSE: 0.8819
Gen 1600/2000 ▶ train MSE: 0.9245, val MSE: 0.8819
Gen 1600/2000 ▶ train MSE: 0.9243, val MSE: 0.8824
Gen 1601/2000 ▶ train MSE: 0.9243, val MSE: 0.8824
Gen 1602/2000 ▶ train MSE: 0.9243, val MSE: 0.8824
Gen 1603/2000 ▶ train MSE: 0.9243, val MSE: 0.8824
Gen 1604/2000 ▶ train MSE: 0.9243, val MSE: 0.8824
Gen 1605/2000 ▶ train MSE: 0.9243, val MSE: 0.8825
Gen 1606/2000 ▶ train MSE: 0.9243, val MSE: 0.8825
Gen 1607/2000 ▶ train MSE: 0.9243, val MSE: 0.8825
Gen 1608/2000 ▶ train MSE: 0.9243, val MSE: 0.8825
Gen 1609/2000 ▶ train MSE: 0.9243, val MSE: 0.8825
Gen 1610/2000 ▶ train MSE: 0.9243, val MSE: 0.8825
Gen 1611/2000 ▶ train MSE: 0.9242, val MSE: 0.8821
Gen 1612/2000 ▶ train MSE: 0.9242, val MSE: 0.8815
Gen 1613/2000 ▶ train MSE: 0.9242, val MSE: 0.8815
Gen 1614/2000 ▶ train MSE: 0.9240, val MSE: 0.8822
Gen 1615/2000 ▶ train MSE: 0.9240, val MSE: 0.8822
Gen 1616/2000 ▶ train MSE: 0.9240, val MSE: 0.8822
Gen 1617/2000 ▶ train MSE: 0.9240, val MSE: 0.8822
Gen 1618/2000 ▶ train MSE: 0.9240, val MSE: 0.8820
Gen 1619/2000 ▶ train MSE: 0.9240, val MSE: 0.8820
Gen 1620/2000 ▶ train MSE: 0.9240, val MSE: 0.8820
Gen 1621/2000 ▶ train MSE: 0.9240, val MSE: 0.8820
Gen 1622/2000 ▶ train MSE: 0.9240, val MSE: 0.8816
Gen 1623/2000 ▶ train MSE: 0.9240, val MSE: 0.8816
Gen 1624/2000 ▶ train MSE: 0.9238, val MSE: 0.8817
Gen 1625/2000 ▶ train MSE: 0.9238, val MSE: 0.8817
Gen 1626/2000 ▶ train MSE: 0.9238, val MSE: 0.8817
Gen 1627/2000 ▶ train MSE: 0.9236, val MSE: 0.8816
Gen 1628/2000 ▶ train MSE: 0.9236, val MSE: 0.8816
Gen 1629/2000 ▶ train MSE: 0.9236, val MSE: 0.8816
Gen 1630/2000 ▶ train MSE: 0.9236, val MSE: 0.8816
Gen 1631/2000 ▶ train MSE: 0.9236, val MSE: 0.8816
Gen 1632/2000 ▶ train MSE: 0.9236, val MSE: 0.8816
Gen 1633/2000 ▶ train MSE: 0.9236, val MSE: 0.8816
Gen 1634/2000 ▶ train MSE: 0.9236, val MSE: 0.8799
Gen 1635/2000 ▶ train MSE: 0.9236, val MSE: 0.8799
Gen 1636/2000 ▶ train MSE: 0.9236, val MSE: 0.8800
Gen 1637/2000 ▶ train MSE: 0.9236, val MSE: 0.8800
Gen 1638/2000 ▶ train MSE: 0.9236, val MSE: 0.8800
Gen 1639/2000 ▶ train MSE: 0.9235, val MSE: 0.8813
Gen 1640/2000 ▶ train MSE: 0.9235, val MSE: 0.8813
Gen 1641/2000 ▶ train MSE: 0.9235, val MSE: 0.8808
Gen 1642/2000 ▶ train MSE: 0.9235, val MSE: 0.8808
Gen 1643/2000 ▶ train MSE: 0.9234, val MSE: 0.8804
Gen 1644/2000 ▶ train MSE: 0.9234, val MSE: 0.8804
Gen 1645/2000 ▶ train MSE: 0.9234, val MSE: 0.8804
Gen 1646/2000 ▶ train MSE: 0.9233, val MSE: 0.8806
Gen 1647/2000 ▶ train MSE: 0.9233, val MSE: 0.8806
Gen 1648/2000 ▶ train MSE: 0.9233, val MSE: 0.8806
Gen 1649/2000 ▶ train MSE: 0.9233, val MSE: 0.8806
Gen 1650/2000 ▶ train MSE: 0.9233, val MSE: 0.8801
Gen 1651/2000 ▶ train MSE: 0.9232, val MSE: 0.8809
Gen 1652/2000 ▶ train MSE: 0.9232, val MSE: 0.8809
Gen 1653/2000 ▶ train MSE: 0.9231, val MSE: 0.8803
Gen 1654/2000 ▶ train MSE: 0.9231, val MSE: 0.8803
Gen 1655/2000 ▶ train MSE: 0.9231, val MSE: 0.8803
Gen 1656/2000 ▶ train MSE: 0.9231, val MSE: 0.8803
Gen 1657/2000 ▶ train MSE: 0.9231, val MSE: 0.8798
Gen 1658/2000 ▶ train MSE: 0.9231, val MSE: 0.8798
Gen 1659/2000 ▶ train MSE: 0.9231, val MSE: 0.8802
Gen 1660/2000 ▶ train MSE: 0.9230, val MSE: 0.8799
Gen 1661/2000 ▶ train MSE: 0.9230, val MSE: 0.8799
Gen 1662/2000 ▶ train MSE: 0.9229, val MSE: 0.8804
Gen 1663/2000 ▶ train MSE: 0.9229, val MSE: 0.8804
Gen 1664/2000 ▶ train MSE: 0.9229, val MSE: 0.8804
Gen 1665/2000 ▶ train MSE: 0.9229, val MSE: 0.8791
Gen 1666/2000 ▶ train MSE: 0.9229, val MSE: 0.8791
Gen 1667/2000 ▶ train MSE: 0.9229, val MSE: 0.8791
Gen 1668/2000 ▶ train MSE: 0.9229, val MSE: 0.8789
Gen 1669/2000 ▶ train MSE: 0.9228, val MSE: 0.8798
Gen 1670/2000 ▶ train MSE: 0.9228, val MSE: 0.8798
Gen 1671/2000 ▶ train MSE: 0.9228, val MSE: 0.8798
Gen 1672/2000 ▶ train MSE: 0.9228, val MSE: 0.8798
Gen 1673/2000 ▶ train MSE: 0.9228, val MSE: 0.8798
Gen 1674/2000 ▶ train MSE: 0.9228, val MSE: 0.8782
Gen 1675/2000 ▶ train MSE: 0.9228, val MSE: 0.8782
Gen 1676/2000 ▶ train MSE: 0.9228, val MSE: 0.8782
Gen 1677/2000 ▶ train MSE: 0.9226, val MSE: 0.8788
Gen 1678/2000 ▶ train MSE: 0.9226, val MSE: 0.8788
Gen 1679/2000 ▶ train MSE: 0.9225, val MSE: 0.8786
Gen 1680/2000 ▶ train MSE: 0.9225, val MSE: 0.8786
Gen 1681/2000 ▶ train MSE: 0.9225, val MSE: 0.8786
Gen 1682/2000 ▶ train MSE: 0.9225, val MSE: 0.8786
Gen 1683/2000 ▶ train MSE: 0.9225, val MSE: 0.8786
Gen 1684/2000 ▶ train MSE: 0.9225, val MSE: 0.8786
Gen 1685/2000 ▶ train MSE: 0.9225, val MSE: 0.8792
Gen 1686/2000 ▶ train MSE: 0.9225, val MSE: 0.8792
Gen 1687/2000 ▶ train MSE: 0.9225, val MSE: 0.8792
Gen 1688/2000 ▶ train MSE: 0.9225, val MSE: 0.8792
Gen 1689/2000 ▶ train MSE: 0.9225, val MSE: 0.8792
Gen 1690/2000 ▶ train MSE: 0.9225, val MSE: 0.8784
Gen 1691/2000 ▶ train MSE: 0.9225, val MSE: 0.8784
Gen 1692/2000 ▶ train MSE: 0.9224, val MSE: 0.8785
Gen 1693/2000 ▶ train MSE: 0.9224, val MSE: 0.8785
Gen 1694/2000 ▶ train MSE: 0.9224, val MSE: 0.8784
Gen 1695/2000 ▶ train MSE: 0.9223, val MSE: 0.8788
Gen 1696/2000 ▶ train MSE: 0.9223, val MSE: 0.8788
Gen 1697/2000 ▶ train MSE: 0.9223, val MSE: 0.8788
Gen 1698/2000 ▶ train MSE: 0.9223, val MSE: 0.8788
Gen 1699/2000 ▶ train MSE: 0.9223, val MSE: 0.8788
Gen 1700/2000 ▶ train MSE: 0.9223, val MSE: 0.8788
Gen 1700/2000 ▶ train MSE: 0.9223, val MSE: 0.8788
Gen 1701/2000 ▶ train MSE: 0.9223, val MSE: 0.8789
Gen 1702/2000 ▶ train MSE: 0.9223, val MSE: 0.8789
Gen 1703/2000 ▶ train MSE: 0.9222, val MSE: 0.8786
Gen 1704/2000 ▶ train MSE: 0.9222, val MSE: 0.8786
Gen 1705/2000 ▶ train MSE: 0.9222, val MSE: 0.8786
Gen 1706/2000 ▶ train MSE: 0.9222, val MSE: 0.8786
Gen 1707/2000 ▶ train MSE: 0.9222, val MSE: 0.8786
Gen 1708/2000 ▶ train MSE: 0.9222, val MSE: 0.8786
Gen 1709/2000 ▶ train MSE: 0.9222, val MSE: 0.8785
Gen 1710/2000 ▶ train MSE: 0.9222, val MSE: 0.8785
Gen 1711/2000 ▶ train MSE: 0.9222, val MSE: 0.8785
Gen 1712/2000 ▶ train MSE: 0.9222, val MSE: 0.8785
Gen 1713/2000 ▶ train MSE: 0.9222, val MSE: 0.8785
Gen 1714/2000 ▶ train MSE: 0.9220, val MSE: 0.8783
Gen 1715/2000 ▶ train MSE: 0.9220, val MSE: 0.8783
Gen 1716/2000 ▶ train MSE: 0.9220, val MSE: 0.8786
Gen 1717/2000 ▶ train MSE: 0.9220, val MSE: 0.8786
Gen 1718/2000 ▶ train MSE: 0.9220, val MSE: 0.8786
Gen 1719/2000 ▶ train MSE: 0.9220, val MSE: 0.8786
Gen 1720/2000 ▶ train MSE: 0.9220, val MSE: 0.8786
Gen 1721/2000 ▶ train MSE: 0.9220, val MSE: 0.8786
Gen 1722/2000 ▶ train MSE: 0.9220, val MSE: 0.8786
Gen 1723/2000 ▶ train MSE: 0.9220, val MSE: 0.8786
Gen 1724/2000 ▶ train MSE: 0.9220, val MSE: 0.8782
Gen 1725/2000 ▶ train MSE: 0.9220, val MSE: 0.8781
Gen 1726/2000 ▶ train MSE: 0.9220, val MSE: 0.8781
Gen 1727/2000 ▶ train MSE: 0.9220, val MSE: 0.8781
Gen 1728/2000 ▶ train MSE: 0.9220, val MSE: 0.8781
Gen 1729/2000 ▶ train MSE: 0.9220, val MSE: 0.8781
Gen 1730/2000 ▶ train MSE: 0.9220, val MSE: 0.8781
Gen 1731/2000 ▶ train MSE: 0.9220, val MSE: 0.8781
Gen 1732/2000 ▶ train MSE: 0.9220, val MSE: 0.8781
Gen 1733/2000 ▶ train MSE: 0.9219, val MSE: 0.8790
Gen 1734/2000 ▶ train MSE: 0.9219, val MSE: 0.8790
Gen 1735/2000 ▶ train MSE: 0.9219, val MSE: 0.8790
Gen 1736/2000 ▶ train MSE: 0.9219, val MSE: 0.8790
Gen 1737/2000 ▶ train MSE: 0.9219, val MSE: 0.8790
Gen 1738/2000 ▶ train MSE: 0.9219, val MSE: 0.8777
Gen 1739/2000 ▶ train MSE: 0.9218, val MSE: 0.8779
Gen 1740/2000 ▶ train MSE: 0.9218, val MSE: 0.8779
Gen 1741/2000 ▶ train MSE: 0.9218, val MSE: 0.8779
Gen 1742/2000 ▶ train MSE: 0.9218, val MSE: 0.8779
Gen 1743/2000 ▶ train MSE: 0.9218, val MSE: 0.8789
Gen 1744/2000 ▶ train MSE: 0.9217, val MSE: 0.8775
Gen 1745/2000 ▶ train MSE: 0.9216, val MSE: 0.8776
Gen 1746/2000 ▶ train MSE: 0.9216, val MSE: 0.8776
Gen 1747/2000 ▶ train MSE: 0.9216, val MSE: 0.8782
Gen 1748/2000 ▶ train MSE: 0.9216, val MSE: 0.8782
Gen 1749/2000 ▶ train MSE: 0.9216, val MSE: 0.8782
Gen 1750/2000 ▶ train MSE: 0.9216, val MSE: 0.8782
Gen 1751/2000 ▶ train MSE: 0.9216, val MSE: 0.8782
Gen 1752/2000 ▶ train MSE: 0.9216, val MSE: 0.8778
Gen 1753/2000 ▶ train MSE: 0.9215, val MSE: 0.8779
Gen 1754/2000 ▶ train MSE: 0.9215, val MSE: 0.8779
Gen 1755/2000 ▶ train MSE: 0.9215, val MSE: 0.8779
Gen 1756/2000 ▶ train MSE: 0.9215, val MSE: 0.8769
Gen 1757/2000 ▶ train MSE: 0.9214, val MSE: 0.8777
Gen 1758/2000 ▶ train MSE: 0.9214, val MSE: 0.8778
Gen 1759/2000 ▶ train MSE: 0.9214, val MSE: 0.8778
Gen 1760/2000 ▶ train MSE: 0.9214, val MSE: 0.8778
Gen 1761/2000 ▶ train MSE: 0.9213, val MSE: 0.8775
Gen 1762/2000 ▶ train MSE: 0.9213, val MSE: 0.8775
Gen 1763/2000 ▶ train MSE: 0.9213, val MSE: 0.8775
Gen 1764/2000 ▶ train MSE: 0.9213, val MSE: 0.8775
Gen 1765/2000 ▶ train MSE: 0.9211, val MSE: 0.8769
Gen 1766/2000 ▶ train MSE: 0.9211, val MSE: 0.8769
Gen 1767/2000 ▶ train MSE: 0.9211, val MSE: 0.8769
Gen 1768/2000 ▶ train MSE: 0.9211, val MSE: 0.8765
Gen 1769/2000 ▶ train MSE: 0.9210, val MSE: 0.8763
Gen 1770/2000 ▶ train MSE: 0.9210, val MSE: 0.8763
Gen 1771/2000 ▶ train MSE: 0.9210, val MSE: 0.8763
Gen 1772/2000 ▶ train MSE: 0.9210, val MSE: 0.8763
Gen 1773/2000 ▶ train MSE: 0.9210, val MSE: 0.8763
Gen 1774/2000 ▶ train MSE: 0.9210, val MSE: 0.8763
Gen 1775/2000 ▶ train MSE: 0.9209, val MSE: 0.8763
Gen 1776/2000 ▶ train MSE: 0.9209, val MSE: 0.8763
Gen 1777/2000 ▶ train MSE: 0.9209, val MSE: 0.8763
Gen 1778/2000 ▶ train MSE: 0.9209, val MSE: 0.8763
Gen 1779/2000 ▶ train MSE: 0.9209, val MSE: 0.8760
Gen 1780/2000 ▶ train MSE: 0.9209, val MSE: 0.8760
Gen 1781/2000 ▶ train MSE: 0.9209, val MSE: 0.8767
Gen 1782/2000 ▶ train MSE: 0.9209, val MSE: 0.8767
Gen 1783/2000 ▶ train MSE: 0.9209, val MSE: 0.8767
Gen 1784/2000 ▶ train MSE: 0.9209, val MSE: 0.8767
Gen 1785/2000 ▶ train MSE: 0.9209, val MSE: 0.8767
Gen 1786/2000 ▶ train MSE: 0.9209, val MSE: 0.8767
Gen 1787/2000 ▶ train MSE: 0.9209, val MSE: 0.8762
Gen 1788/2000 ▶ train MSE: 0.9208, val MSE: 0.8762
Gen 1789/2000 ▶ train MSE: 0.9208, val MSE: 0.8762
Gen 1790/2000 ▶ train MSE: 0.9208, val MSE: 0.8759
Gen 1791/2000 ▶ train MSE: 0.9208, val MSE: 0.8766
Gen 1792/2000 ▶ train MSE: 0.9208, val MSE: 0.8766
Gen 1793/2000 ▶ train MSE: 0.9208, val MSE: 0.8766
Gen 1794/2000 ▶ train MSE: 0.9208, val MSE: 0.8761
Gen 1795/2000 ▶ train MSE: 0.9208, val MSE: 0.8761
Gen 1796/2000 ▶ train MSE: 0.9207, val MSE: 0.8762
Gen 1797/2000 ▶ train MSE: 0.9207, val MSE: 0.8762
Gen 1798/2000 ▶ train MSE: 0.9207, val MSE: 0.8762
Gen 1799/2000 ▶ train MSE: 0.9207, val MSE: 0.8762
Gen 1800/2000 ▶ train MSE: 0.9207, val MSE: 0.8762
Gen 1800/2000 ▶ train MSE: 0.9206, val MSE: 0.8757
Gen 1801/2000 ▶ train MSE: 0.9205, val MSE: 0.8752
Gen 1802/2000 ▶ train MSE: 0.9205, val MSE: 0.8752
Gen 1803/2000 ▶ train MSE: 0.9205, val MSE: 0.8758
Gen 1804/2000 ▶ train MSE: 0.9205, val MSE: 0.8756
Gen 1805/2000 ▶ train MSE: 0.9205, val MSE: 0.8756
Gen 1806/2000 ▶ train MSE: 0.9204, val MSE: 0.8752
Gen 1807/2000 ▶ train MSE: 0.9204, val MSE: 0.8752
Gen 1808/2000 ▶ train MSE: 0.9204, val MSE: 0.8752
Gen 1809/2000 ▶ train MSE: 0.9203, val MSE: 0.8753
Gen 1810/2000 ▶ train MSE: 0.9203, val MSE: 0.8753
Gen 1811/2000 ▶ train MSE: 0.9203, val MSE: 0.8758
Gen 1812/2000 ▶ train MSE: 0.9202, val MSE: 0.8757
Gen 1813/2000 ▶ train MSE: 0.9202, val MSE: 0.8757
Gen 1814/2000 ▶ train MSE: 0.9202, val MSE: 0.8757
Gen 1815/2000 ▶ train MSE: 0.9202, val MSE: 0.8757
Gen 1816/2000 ▶ train MSE: 0.9202, val MSE: 0.8757
Gen 1817/2000 ▶ train MSE: 0.9202, val MSE: 0.8757
Gen 1818/2000 ▶ train MSE: 0.9202, val MSE: 0.8757
Gen 1819/2000 ▶ train MSE: 0.9202, val MSE: 0.8749
Gen 1820/2000 ▶ train MSE: 0.9202, val MSE: 0.8751
Gen 1821/2000 ▶ train MSE: 0.9202, val MSE: 0.8751
Gen 1822/2000 ▶ train MSE: 0.9202, val MSE: 0.8751
Gen 1823/2000 ▶ train MSE: 0.9201, val MSE: 0.8753
Gen 1824/2000 ▶ train MSE: 0.9201, val MSE: 0.8753
Gen 1825/2000 ▶ train MSE: 0.9201, val MSE: 0.8753
Gen 1826/2000 ▶ train MSE: 0.9201, val MSE: 0.8741
Gen 1827/2000 ▶ train MSE: 0.9201, val MSE: 0.8741
Gen 1828/2000 ▶ train MSE: 0.9201, val MSE: 0.8741
Gen 1829/2000 ▶ train MSE: 0.9201, val MSE: 0.8748
Gen 1830/2000 ▶ train MSE: 0.9201, val MSE: 0.8748
Gen 1831/2000 ▶ train MSE: 0.9200, val MSE: 0.8746
Gen 1832/2000 ▶ train MSE: 0.9200, val MSE: 0.8746
Gen 1833/2000 ▶ train MSE: 0.9200, val MSE: 0.8742
Gen 1834/2000 ▶ train MSE: 0.9199, val MSE: 0.8741
Gen 1835/2000 ▶ train MSE: 0.9199, val MSE: 0.8741
Gen 1836/2000 ▶ train MSE: 0.9199, val MSE: 0.8741
Gen 1837/2000 ▶ train MSE: 0.9199, val MSE: 0.8751
Gen 1838/2000 ▶ train MSE: 0.9199, val MSE: 0.8749
Gen 1839/2000 ▶ train MSE: 0.9199, val MSE: 0.8749
Gen 1840/2000 ▶ train MSE: 0.9199, val MSE: 0.8735
Gen 1841/2000 ▶ train MSE: 0.9198, val MSE: 0.8744
Gen 1842/2000 ▶ train MSE: 0.9198, val MSE: 0.8744
Gen 1843/2000 ▶ train MSE: 0.9198, val MSE: 0.8744
Gen 1844/2000 ▶ train MSE: 0.9198, val MSE: 0.8741
Gen 1845/2000 ▶ train MSE: 0.9197, val MSE: 0.8744
Gen 1846/2000 ▶ train MSE: 0.9197, val MSE: 0.8744
Gen 1847/2000 ▶ train MSE: 0.9197, val MSE: 0.8744
Gen 1848/2000 ▶ train MSE: 0.9197, val MSE: 0.8744
Gen 1849/2000 ▶ train MSE: 0.9197, val MSE: 0.8735
Gen 1850/2000 ▶ train MSE: 0.9197, val MSE: 0.8735
Gen 1851/2000 ▶ train MSE: 0.9197, val MSE: 0.8735
Gen 1852/2000 ▶ train MSE: 0.9197, val MSE: 0.8735
Gen 1853/2000 ▶ train MSE: 0.9197, val MSE: 0.8735
Gen 1854/2000 ▶ train MSE: 0.9197, val MSE: 0.8735
Gen 1855/2000 ▶ train MSE: 0.9197, val MSE: 0.8735
Gen 1856/2000 ▶ train MSE: 0.9197, val MSE: 0.8735
Gen 1857/2000 ▶ train MSE: 0.9197, val MSE: 0.8735
Gen 1858/2000 ▶ train MSE: 0.9196, val MSE: 0.8738
Gen 1859/2000 ▶ train MSE: 0.9196, val MSE: 0.8738
Gen 1860/2000 ▶ train MSE: 0.9196, val MSE: 0.8738
Gen 1861/2000 ▶ train MSE: 0.9196, val MSE: 0.8738
Gen 1862/2000 ▶ train MSE: 0.9196, val MSE: 0.8737
Gen 1863/2000 ▶ train MSE: 0.9196, val MSE: 0.8737
Gen 1864/2000 ▶ train MSE: 0.9196, val MSE: 0.8737
Gen 1865/2000 ▶ train MSE: 0.9196, val MSE: 0.8737
Gen 1866/2000 ▶ train MSE: 0.9196, val MSE: 0.8737
Gen 1867/2000 ▶ train MSE: 0.9196, val MSE: 0.8737
Gen 1868/2000 ▶ train MSE: 0.9196, val MSE: 0.8737
Gen 1869/2000 ▶ train MSE: 0.9194, val MSE: 0.8738
Gen 1870/2000 ▶ train MSE: 0.9194, val MSE: 0.8738
Gen 1871/2000 ▶ train MSE: 0.9194, val MSE: 0.8738
Gen 1872/2000 ▶ train MSE: 0.9194, val MSE: 0.8738
Gen 1873/2000 ▶ train MSE: 0.9194, val MSE: 0.8738
Gen 1874/2000 ▶ train MSE: 0.9194, val MSE: 0.8738
Gen 1875/2000 ▶ train MSE: 0.9193, val MSE: 0.8723
Gen 1876/2000 ▶ train MSE: 0.9193, val MSE: 0.8723
Gen 1877/2000 ▶ train MSE: 0.9193, val MSE: 0.8723
Gen 1878/2000 ▶ train MSE: 0.9193, val MSE: 0.8723
Gen 1879/2000 ▶ train MSE: 0.9193, val MSE: 0.8723
Gen 1880/2000 ▶ train MSE: 0.9193, val MSE: 0.8723
Gen 1881/2000 ▶ train MSE: 0.9193, val MSE: 0.8723
Gen 1882/2000 ▶ train MSE: 0.9192, val MSE: 0.8720
Gen 1883/2000 ▶ train MSE: 0.9192, val MSE: 0.8720
Gen 1884/2000 ▶ train MSE: 0.9192, val MSE: 0.8720
Gen 1885/2000 ▶ train MSE: 0.9192, val MSE: 0.8727
Gen 1886/2000 ▶ train MSE: 0.9192, val MSE: 0.8727
Gen 1887/2000 ▶ train MSE: 0.9192, val MSE: 0.8727
Gen 1888/2000 ▶ train MSE: 0.9190, val MSE: 0.8714
Gen 1889/2000 ▶ train MSE: 0.9190, val MSE: 0.8714
Gen 1890/2000 ▶ train MSE: 0.9190, val MSE: 0.8714
Gen 1891/2000 ▶ train MSE: 0.9190, val MSE: 0.8714
Gen 1892/2000 ▶ train MSE: 0.9190, val MSE: 0.8714
Gen 1893/2000 ▶ train MSE: 0.9188, val MSE: 0.8730
Gen 1894/2000 ▶ train MSE: 0.9188, val MSE: 0.8730
Gen 1895/2000 ▶ train MSE: 0.9188, val MSE: 0.8730
Gen 1896/2000 ▶ train MSE: 0.9187, val MSE: 0.8727
Gen 1897/2000 ▶ train MSE: 0.9187, val MSE: 0.8727
Gen 1898/2000 ▶ train MSE: 0.9187, val MSE: 0.8727
Gen 1899/2000 ▶ train MSE: 0.9187, val MSE: 0.8729
Gen 1900/2000 ▶ train MSE: 0.9187, val MSE: 0.8729
Gen 1900/2000 ▶ train MSE: 0.9187, val MSE: 0.8729
Gen 1901/2000 ▶ train MSE: 0.9187, val MSE: 0.8729
Gen 1902/2000 ▶ train MSE: 0.9187, val MSE: 0.8729
Gen 1903/2000 ▶ train MSE: 0.9186, val MSE: 0.8727
Gen 1904/2000 ▶ train MSE: 0.9186, val MSE: 0.8724
Gen 1905/2000 ▶ train MSE: 0.9186, val MSE: 0.8724
Gen 1906/2000 ▶ train MSE: 0.9186, val MSE: 0.8724
Gen 1907/2000 ▶ train MSE: 0.9186, val MSE: 0.8724
Gen 1908/2000 ▶ train MSE: 0.9186, val MSE: 0.8724
Gen 1909/2000 ▶ train MSE: 0.9185, val MSE: 0.8718
Gen 1910/2000 ▶ train MSE: 0.9185, val MSE: 0.8718
Gen 1911/2000 ▶ train MSE: 0.9185, val MSE: 0.8718
Gen 1912/2000 ▶ train MSE: 0.9183, val MSE: 0.8719
Gen 1913/2000 ▶ train MSE: 0.9183, val MSE: 0.8719
Gen 1914/2000 ▶ train MSE: 0.9183, val MSE: 0.8719
Gen 1915/2000 ▶ train MSE: 0.9183, val MSE: 0.8719
Gen 1916/2000 ▶ train MSE: 0.9183, val MSE: 0.8719
Gen 1917/2000 ▶ train MSE: 0.9182, val MSE: 0.8719
Gen 1918/2000 ▶ train MSE: 0.9182, val MSE: 0.8719
Gen 1919/2000 ▶ train MSE: 0.9181, val MSE: 0.8715
Gen 1920/2000 ▶ train MSE: 0.9181, val MSE: 0.8715
Gen 1921/2000 ▶ train MSE: 0.9181, val MSE: 0.8715
Gen 1922/2000 ▶ train MSE: 0.9181, val MSE: 0.8715
Gen 1923/2000 ▶ train MSE: 0.9181, val MSE: 0.8715
Gen 1924/2000 ▶ train MSE: 0.9181, val MSE: 0.8709
Gen 1925/2000 ▶ train MSE: 0.9180, val MSE: 0.8714
Gen 1926/2000 ▶ train MSE: 0.9180, val MSE: 0.8714
Gen 1927/2000 ▶ train MSE: 0.9180, val MSE: 0.8714
Gen 1928/2000 ▶ train MSE: 0.9180, val MSE: 0.8714
Gen 1929/2000 ▶ train MSE: 0.9180, val MSE: 0.8717
Gen 1930/2000 ▶ train MSE: 0.9179, val MSE: 0.8713
Gen 1931/2000 ▶ train MSE: 0.9178, val MSE: 0.8718
Gen 1932/2000 ▶ train MSE: 0.9178, val MSE: 0.8712
Gen 1933/2000 ▶ train MSE: 0.9178, val MSE: 0.8712
Gen 1934/2000 ▶ train MSE: 0.9178, val MSE: 0.8712
Gen 1935/2000 ▶ train MSE: 0.9178, val MSE: 0.8712
Gen 1936/2000 ▶ train MSE: 0.9178, val MSE: 0.8712
Gen 1937/2000 ▶ train MSE: 0.9178, val MSE: 0.8715
Gen 1938/2000 ▶ train MSE: 0.9177, val MSE: 0.8712
Gen 1939/2000 ▶ train MSE: 0.9176, val MSE: 0.8712
Gen 1940/2000 ▶ train MSE: 0.9176, val MSE: 0.8711
Gen 1941/2000 ▶ train MSE: 0.9176, val MSE: 0.8711
Gen 1942/2000 ▶ train MSE: 0.9176, val MSE: 0.8711
Gen 1943/2000 ▶ train MSE: 0.9176, val MSE: 0.8711
Gen 1944/2000 ▶ train MSE: 0.9176, val MSE: 0.8715
Gen 1945/2000 ▶ train MSE: 0.9176, val MSE: 0.8715
Gen 1946/2000 ▶ train MSE: 0.9175, val MSE: 0.8706
Gen 1947/2000 ▶ train MSE: 0.9175, val MSE: 0.8706
Gen 1948/2000 ▶ train MSE: 0.9175, val MSE: 0.8706
Gen 1949/2000 ▶ train MSE: 0.9175, val MSE: 0.8707
Gen 1950/2000 ▶ train MSE: 0.9175, val MSE: 0.8707
Gen 1951/2000 ▶ train MSE: 0.9175, val MSE: 0.8718
Gen 1952/2000 ▶ train MSE: 0.9175, val MSE: 0.8718
Gen 1953/2000 ▶ train MSE: 0.9174, val MSE: 0.8711
Gen 1954/2000 ▶ train MSE: 0.9174, val MSE: 0.8711
Gen 1955/2000 ▶ train MSE: 0.9174, val MSE: 0.8711
Gen 1956/2000 ▶ train MSE: 0.9174, val MSE: 0.8711
Gen 1957/2000 ▶ train MSE: 0.9174, val MSE: 0.8711
Gen 1958/2000 ▶ train MSE: 0.9174, val MSE: 0.8711
Gen 1959/2000 ▶ train MSE: 0.9173, val MSE: 0.8713
Gen 1960/2000 ▶ train MSE: 0.9173, val MSE: 0.8713
Gen 1961/2000 ▶ train MSE: 0.9173, val MSE: 0.8713
Gen 1962/2000 ▶ train MSE: 0.9173, val MSE: 0.8713
Gen 1963/2000 ▶ train MSE: 0.9173, val MSE: 0.8713
Gen 1964/2000 ▶ train MSE: 0.9173, val MSE: 0.8713
Gen 1965/2000 ▶ train MSE: 0.9173, val MSE: 0.8713
Gen 1966/2000 ▶ train MSE: 0.9173, val MSE: 0.8713
Gen 1967/2000 ▶ train MSE: 0.9173, val MSE: 0.8713
Gen 1968/2000 ▶ train MSE: 0.9172, val MSE: 0.8709
Gen 1969/2000 ▶ train MSE: 0.9172, val MSE: 0.8709
Gen 1970/2000 ▶ train MSE: 0.9172, val MSE: 0.8709
Gen 1971/2000 ▶ train MSE: 0.9172, val MSE: 0.8709
Gen 1972/2000 ▶ train MSE: 0.9172, val MSE: 0.8709
Gen 1973/2000 ▶ train MSE: 0.9172, val MSE: 0.8716
Gen 1974/2000 ▶ train MSE: 0.9171, val MSE: 0.8708
Gen 1975/2000 ▶ train MSE: 0.9171, val MSE: 0.8709
Gen 1976/2000 ▶ train MSE: 0.9171, val MSE: 0.8709
Gen 1977/2000 ▶ train MSE: 0.9171, val MSE: 0.8709
Gen 1978/2000 ▶ train MSE: 0.9171, val MSE: 0.8709
Gen 1979/2000 ▶ train MSE: 0.9171, val MSE: 0.8709
Gen 1980/2000 ▶ train MSE: 0.9171, val MSE: 0.8709
Gen 1981/2000 ▶ train MSE: 0.9170, val MSE: 0.8713
Gen 1982/2000 ▶ train MSE: 0.9170, val MSE: 0.8713
Gen 1983/2000 ▶ train MSE: 0.9170, val MSE: 0.8713
Gen 1984/2000 ▶ train MSE: 0.9170, val MSE: 0.8711
Gen 1985/2000 ▶ train MSE: 0.9170, val MSE: 0.8711
Gen 1986/2000 ▶ train MSE: 0.9169, val MSE: 0.8704
Gen 1987/2000 ▶ train MSE: 0.9169, val MSE: 0.8704
Gen 1988/2000 ▶ train MSE: 0.9169, val MSE: 0.8704
Gen 1989/2000 ▶ train MSE: 0.9169, val MSE: 0.8704
Gen 1990/2000 ▶ train MSE: 0.9169, val MSE: 0.8704
Gen 1991/2000 ▶ train MSE: 0.9169, val MSE: 0.8704
Gen 1992/2000 ▶ train MSE: 0.9169, val MSE: 0.8704
Gen 1993/2000 ▶ train MSE: 0.9169, val MSE: 0.8704
Gen 1994/2000 ▶ train MSE: 0.9169, val MSE: 0.8704
Gen 1995/2000 ▶ train MSE: 0.9169, val MSE: 0.8714
Gen 1996/2000 ▶ train MSE: 0.9169, val MSE: 0.8714
Gen 1997/2000 ▶ train MSE: 0.9168, val MSE: 0.8702
Gen 1998/2000 ▶ train MSE: 0.9168, val MSE: 0.8704
Gen 1999/2000 ▶ train MSE: 0.9166, val MSE: 0.8696
Gen 2000/2000 ▶ train MSE: 0.9166, val MSE: 0.8696
Gen 2000/2000 ▶ train MSE: 0.9166, val MSE: 0.8696

✅ GA done!  Final Train MSE: 0.9166, Val MSE: 0.8696
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=35cc6e16">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Same-implementation,-increasing-blx-alpha-to-0.6:">Same implementation, increasing blx alpha to 0.6:<a class="anchor-link" href="#Same-implementation,-increasing-blx-alpha-to-0.6:">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=7c527a0d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Genetic Algorithm (GA) for Optimising Feed-Forward Neural Network Weights</span>
<span class="sd">==========================================================================</span>

<span class="sd">This script implements a real-valued Genetic Algorithm (GA) using Blend Crossover (BLX-α)</span>
<span class="sd">and Gaussian mutation to optimise the parameters of a fixed-architecture feed-forward</span>
<span class="sd">neural network (FFN). The algorithm is applied to a regression task with MSE loss, and</span>
<span class="sd">performance is tracked across generations.</span>

<span class="sd">Key Features:</span>
<span class="sd">-------------</span>
<span class="sd">- Fixed FFN architecture using PyTorch with Xavier weight initialisation</span>
<span class="sd">- Real-valued genome representation using PyTorch parameter vectors</span>
<span class="sd">- Tournament selection (size = 3) with elitism (top 10% retained each generation)</span>
<span class="sd">- BLX-α crossover with α = 0.6 for diversity-preserving recombination</span>
<span class="sd">- Gaussian mutation applied per gene with probability `mutation_p` and std dev `mutation_sd`</span>
<span class="sd">- Reproducible results via fixed seeds for NumPy and PyTorch</span>
<span class="sd">- Final best model reconstructed and evaluated</span>
<span class="sd">- MSE loss curves plotted across generations</span>

<span class="sd">Instructions:</span>
<span class="sd">-------------</span>
<span class="sd">1. Ensure that `X_train`, `y_train`, `X_val`, and `y_val` are defined as PyTorch tensors.</span>
<span class="sd">2. Adjust architecture via the `arch` dictionary.</span>
<span class="sd">3. Modify genetic algorithm hyperparameters (e.g., `pop_size`, `generations`, `mutation_p`) as needed.</span>
<span class="sd">4. Run the script to execute the full evolutionary cycle and view results.</span>

<span class="sd">Hyperparameters:</span>
<span class="sd">----------------</span>
<span class="sd">- `pop_size`: Number of individuals in the population</span>
<span class="sd">- `generations`: Number of generations to evolve</span>
<span class="sd">- `elite_frac`: Proportion of top individuals carried over each generation</span>
<span class="sd">- `tourn_size`: Tournament size for selection</span>
<span class="sd">- `mutation_p`: Probability of mutating each gene</span>
<span class="sd">- `mutation_sd`: Standard deviation of mutation noise</span>
<span class="sd">- `blx_alpha`: BLX-α parameter controlling crossover range</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Printed train/validation MSE every 100 generations</span>
<span class="sd">- Final best model with evaluation on training and validation sets</span>
<span class="sd">- Plot of training and validation MSE vs. generation</span>
<span class="sd">- Internal genome evolution stored in memory only (can be extended to save)</span>

<span class="sd">"""</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">skPCA</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>

<span class="c1">#  0) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1"># ─── 1) Data to device </span>
<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># ─── 2) Fixed Architecture + Xavier init </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"ReLU"</span><span class="p">)</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="n">criterion</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span><span class="p">,</span> <span class="n">in_f</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
<span class="c1">#  3) GA Hyperparams </span>
<span class="n">pop_size</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">generations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">elite_frac</span>  <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tourn_size</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">mutation_p</span>  <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">mutation_sd</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">blx_alpha</span>   <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1">#  4) Init Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pop_size</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="c1">#  visualize initial population </span>
<span class="n">pop_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>           
<span class="n">genome_len</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#  5) Tournament Selection </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pop_size</span><span class="p">,</span> <span class="n">tourn_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>
<span class="c1">#  6) Evolution </span>
<span class="n">train_curve</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_curve</span>   <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1">#  Fitness eval</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gen</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">gen</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2"> ▶ train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">tr_mse</span>   <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">m_best</span>   <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">va_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tr_mse</span><span class="p">)</span>
    <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">va_mse</span><span class="p">)</span>
    <span class="c1"># b) Elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elites</span>  <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]]</span>
    <span class="n">pop_size</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
    <span class="n">elite_frac</span> <span class="o">=</span> <span class="mf">0.2</span>           
    <span class="n">elite_n</span>    <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
    <span class="n">elites</span>     <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">]</span>
    <span class="c1"># selection probabilities</span>
    <span class="n">p_elite</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_tourn</span>   <span class="o">=</span> <span class="mi">1</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># pure tournament selection</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="c1"># pure elitism</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span><span class="p">:</span>
            <span class="c1"># exploit: uniform from elites</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="k">elif</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span> <span class="o">+</span> <span class="n">p_tourn</span><span class="p">:</span>
            <span class="c1"># competition: standard tournament over full pop</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># pure exploration: uniform from entire pop</span>
            <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">pop_size</span><span class="p">)]</span>
    <span class="c1"># c) Reproduce via BLX-α + mutation</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">pop_size</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="c1"># BLX-α crossover</span>
        <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># mutation</span>
        <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mutation_p</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mutation_sd</span>
        <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1"># ── 7) Final Best Model </span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))]</span>
<span class="n">best_model_ga</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                     <span class="n">best_model_ga</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">best_model_ga</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ GA done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1">#  8) Plot </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA (w/ BLX-α) Optimization of FFN Weights"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen  1/2000 ▶ train MSE: 0.9968, val MSE: 0.9961
Gen 100/2000 ▶ train MSE: 0.9953, val MSE: 0.9951
Gen 200/2000 ▶ train MSE: 0.9900, val MSE: 0.9884
Gen 300/2000 ▶ train MSE: 0.9811, val MSE: 0.9770
Gen 400/2000 ▶ train MSE: 0.9740, val MSE: 0.9669
Gen 500/2000 ▶ train MSE: 0.9679, val MSE: 0.9572
Gen 600/2000 ▶ train MSE: 0.9633, val MSE: 0.9498
Gen 700/2000 ▶ train MSE: 0.9552, val MSE: 0.9367
Gen 800/2000 ▶ train MSE: 0.9498, val MSE: 0.9307
Gen 900/2000 ▶ train MSE: 0.9447, val MSE: 0.9225
Gen 1000/2000 ▶ train MSE: 0.9416, val MSE: 0.9181
Gen 1100/2000 ▶ train MSE: 0.9356, val MSE: 0.9080
Gen 1200/2000 ▶ train MSE: 0.9291, val MSE: 0.8976
Gen 1300/2000 ▶ train MSE: 0.9222, val MSE: 0.8861
Gen 1400/2000 ▶ train MSE: 0.9191, val MSE: 0.8826
Gen 1500/2000 ▶ train MSE: 0.9120, val MSE: 0.8672
Gen 1600/2000 ▶ train MSE: 0.9067, val MSE: 0.8601
Gen 1700/2000 ▶ train MSE: 0.9005, val MSE: 0.8491
Gen 1800/2000 ▶ train MSE: 0.8963, val MSE: 0.8416
Gen 1900/2000 ▶ train MSE: 0.8935, val MSE: 0.8368
Gen 2000/2000 ▶ train MSE: 0.8908, val MSE: 0.8316

✅ GA done!  Final Train MSE: 0.8915, Val MSE: 0.8320
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=c8a713b1">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Same-implementation,-with-0.6-blx-alpha,-now-increasing-elitism-percentage:">Same implementation, with 0.6 blx alpha, now increasing elitism percentage:<a class="anchor-link" href="#Same-implementation,-with-0.6-blx-alpha,-now-increasing-elitism-percentage:">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bf2f8472">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Genetic Algorithm (GA) with Increased BLX-α for Feed-Forward Neural Network Optimisation</span>
<span class="sd">=========================================================================================</span>

<span class="sd">This script implements a real-valued Genetic Algorithm (GA) to optimise the weights of a</span>
<span class="sd">Feed-Forward Neural Network (FFN) using BLX-α crossover and Gaussian mutation. The key</span>
<span class="sd">modification in this version is an increased elitism percentage.</span>

<span class="sd">Core Features:</span>
<span class="sd">--------------</span>
<span class="sd">- FFN with fixed architecture: 2 hidden layers, 24 units each, ReLU activations</span>
<span class="sd">- Xavier normal initialisation</span>
<span class="sd">- Real-valued GA using:</span>
<span class="sd">    * Tournament selection (size 3)</span>
<span class="sd">    * Elitism (top 20% retained)</span>
<span class="sd">    * BLX-α crossover with α = 0.6</span>
<span class="sd">    * Gaussian mutation (μ=0, σ=0.01) with mutation rate 1%</span>
<span class="sd">- Reproducibility via fixed seeds</span>
<span class="sd">- Evaluation on both training and validation sets using MSE loss</span>
<span class="sd">- Plot of MSE over generations</span>

<span class="sd">Key Hyperparameters:</span>
<span class="sd">--------------------</span>
<span class="sd">- `pop_size`: 200 (population size)</span>
<span class="sd">- `generations`: 2000 (evolution length)</span>
<span class="sd">- `elite_frac`: 0.2 (elitism retention)</span>
<span class="sd">- `tourn_size`: 3 (selection pressure)</span>
<span class="sd">- `blx_alpha`: 0.6 (exploration during crossover ↑)</span>
<span class="sd">- `mutation_p`: 0.01 (probability of mutation per gene)</span>
<span class="sd">- `mutation_sd`: 0.01 (standard deviation of Gaussian mutation)</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Prints MSE every 100 generations</span>
<span class="sd">- Final training and validation MSE of the best individual</span>
<span class="sd">- Matplotlib plot showing training and validation MSE curves</span>

<span class="sd">Usage Instructions:</span>
<span class="sd">-------------------</span>
<span class="sd">- Ensure that `X_train`, `y_train`, `X_val`, and `y_val` are defined as PyTorch tensors.</span>
<span class="sd">- Adjust the architecture or GA hyperparameters if needed.</span>
<span class="sd">- Run the script to evolve the FFN weights and observe performance.</span>

<span class="sd">Remarks:</span>
<span class="sd">--------</span>
<span class="sd">This version increases the BLX-α crossover range, allowing offspring to explore further</span>
<span class="sd">beyond the bounds of the parent genes. This may help escape local minima in multimodal</span>
<span class="sd">landscapes but could also introduce more variance in early training.</span>

<span class="sd">"""</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">skPCA</span>
<span class="c1">#  0) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  1) Data to device </span>
<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">#  2) Fixed Architecture + Xavier init </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"ReLU"</span><span class="p">)</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="n">criterion</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span><span class="p">,</span> <span class="n">in_f</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
<span class="c1">#  3) GA Hyperparams </span>
<span class="n">pop_size</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">generations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">elite_frac</span>  <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">tourn_size</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">mutation_p</span>  <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">mutation_sd</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">blx_alpha</span>   <span class="o">=</span> <span class="mf">0.6</span>
<span class="c1">#  4) Init Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pop_size</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="c1">#  visualize initial population </span>
<span class="n">pop_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>           
<span class="n">genome_len</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#  5) Tournament Selection </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pop_size</span><span class="p">,</span> <span class="n">tourn_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>
<span class="c1">#  6) Evolution </span>
<span class="n">train_curve</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_curve</span>   <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># a) Fitness eval</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gen</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">gen</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2"> ▶ train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">tr_mse</span>   <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">m_best</span>   <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">va_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tr_mse</span><span class="p">)</span>
    <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">va_mse</span><span class="p">)</span>
    <span class="c1"># b) Elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elites</span>  <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]]</span>
    <span class="n">pop_size</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
    <span class="c1"># selection probabilities</span>
    <span class="n">p_elite</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_tourn</span>   <span class="o">=</span> <span class="mi">1</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># pure tournament selection</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        
    <span class="c1"># c) Reproduce via BLX-α + mutation</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">pop_size</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="c1"># BLX-α crossover</span>
        <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># mutation</span>
        <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mutation_p</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mutation_sd</span>
        <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>

    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1">#  7) Final Best Model </span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))]</span>
<span class="n">best_model_ga</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                     <span class="n">best_model_ga</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">best_model_ga</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ GA done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  8) Plot </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA (w/ BLX-α) Optimization of FFN Weights"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen  1/2000 ▶ train MSE: 0.9953, val MSE: 0.9951
Gen 100/2000 ▶ train MSE: 0.9953, val MSE: 0.9951
Gen 200/2000 ▶ train MSE: 0.9900, val MSE: 0.9884
Gen 300/2000 ▶ train MSE: 0.9811, val MSE: 0.9770
Gen 400/2000 ▶ train MSE: 0.9740, val MSE: 0.9669
Gen 500/2000 ▶ train MSE: 0.9679, val MSE: 0.9572
Gen 600/2000 ▶ train MSE: 0.9633, val MSE: 0.9498
Gen 700/2000 ▶ train MSE: 0.9552, val MSE: 0.9367
Gen 800/2000 ▶ train MSE: 0.9498, val MSE: 0.9307
Gen 900/2000 ▶ train MSE: 0.9447, val MSE: 0.9225
Gen 1000/2000 ▶ train MSE: 0.9416, val MSE: 0.9181
Gen 1100/2000 ▶ train MSE: 0.9356, val MSE: 0.9080
Gen 1200/2000 ▶ train MSE: 0.9291, val MSE: 0.8976
Gen 1300/2000 ▶ train MSE: 0.9222, val MSE: 0.8861
Gen 1400/2000 ▶ train MSE: 0.9191, val MSE: 0.8826
Gen 1500/2000 ▶ train MSE: 0.9120, val MSE: 0.8672
Gen 1600/2000 ▶ train MSE: 0.9067, val MSE: 0.8601
Gen 1700/2000 ▶ train MSE: 0.9005, val MSE: 0.8491
Gen 1800/2000 ▶ train MSE: 0.8963, val MSE: 0.8416
Gen 1900/2000 ▶ train MSE: 0.8935, val MSE: 0.8368
Gen 2000/2000 ▶ train MSE: 0.8908, val MSE: 0.8316

✅ GA done!  Final Train MSE: 0.8915, Val MSE: 0.8320
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=c162711b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Best-implementation-from-previous-experiments,-now-increasing-tournament-size:">Best implementation from previous experiments, now increasing tournament size:<a class="anchor-link" href="#Best-implementation-from-previous-experiments,-now-increasing-tournament-size:">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8ff057af">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Genetic Algorithm (GA) with Larger Tournament Size for FFN Weight Optimisation</span>
<span class="sd">===============================================================================</span>

<span class="sd">This script evolves the weights of a fixed-architecture feed-forward neural network (FFN)</span>
<span class="sd">using a real-valued Genetic Algorithm. The crossover operator is BLX-α with α = 0.6,</span>
<span class="sd">and mutation is Gaussian. The key difference in this version is an **increased tournament</span>
<span class="sd">size (tourn_size = 5)** to apply stronger selective pressure during parent selection.</span>

<span class="sd">Key Features:</span>
<span class="sd">-------------</span>
<span class="sd">- Fixed FFN: 2 layers × 24 units, ReLU activations</span>
<span class="sd">- Xavier normal initialisation for consistency</span>
<span class="sd">- Real-valued genome (vector of FFN weights)</span>
<span class="sd">- BLX-α crossover (α = 0.6) for diversity</span>
<span class="sd">- Gaussian mutation (1% mutation rate, σ = 0.01)</span>
<span class="sd">- Tournament selection with size = 5 (↑ selective pressure)</span>
<span class="sd">- Elitism: top 20% of population preserved each generation</span>

<span class="sd">Hyperparameters:</span>
<span class="sd">----------------</span>
<span class="sd">- `pop_size`: 200</span>
<span class="sd">- `generations`: 2000</span>
<span class="sd">- `elite_frac`: 0.2</span>
<span class="sd">- `tourn_size`: 5  ← Increased from earlier runs</span>
<span class="sd">- `blx_alpha`: 0.6</span>
<span class="sd">- `mutation_p`: 0.01</span>
<span class="sd">- `mutation_sd`: 0.01</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Printed MSE every 100 generations</span>
<span class="sd">- Final train/val MSE for best individual</span>
<span class="sd">- Plot of training and validation loss curves</span>

<span class="sd">"""</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>

<span class="c1">#  0) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  1) Data to device </span>
<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">#  2) Fixed Architecture + Xavier init </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"ReLU"</span><span class="p">)</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="n">criterion</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span><span class="p">,</span> <span class="n">in_f</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>

<span class="c1">#  3) GA Hyperparams </span>
<span class="n">pop_size</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">generations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">elite_frac</span>  <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">tourn_size</span>  <span class="o">=</span> <span class="mi">5</span>
<span class="n">mutation_p</span>  <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">mutation_sd</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">blx_alpha</span>   <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1">#  4) Init Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pop_size</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">skPCA</span>
<span class="c1">#  visualize initial population </span>
<span class="n">pop_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>         
<span class="n">genome_len</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#  5) Tournament Selection </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pop_size</span><span class="p">,</span> <span class="n">tourn_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>
<span class="c1">#  6) Evolution </span>
<span class="n">train_curve</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_curve</span>   <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># a) Fitness eval</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gen</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">gen</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2"> ▶ train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">tr_mse</span>   <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">m_best</span>   <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">va_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tr_mse</span><span class="p">)</span>
    <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">va_mse</span><span class="p">)</span>
    <span class="c1"># b) Elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elites</span>  <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]]</span>
    <span class="n">pop_size</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
    <span class="c1"># selection probabilities</span>
    <span class="n">p_elite</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_tourn</span>   <span class="o">=</span> <span class="mi">1</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># pure tournament selection</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
    <span class="c1"># c) Reproduce via BLX-α + mutation</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">pop_size</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="c1"># BLX-α crossover</span>
        <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># mutation</span>
        <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mutation_p</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mutation_sd</span>
        <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1">#  7) Final Best Model </span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))]</span>
<span class="n">best_model_ga</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                     <span class="n">best_model_ga</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">best_model_ga</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ GA done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  8) Plot </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA (w/ BLX-α) Optimization of FFN Weights"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen  1/2000 ▶ train MSE: 0.8906, val MSE: 0.8294
Gen 100/2000 ▶ train MSE: 0.9938, val MSE: 0.9944
Gen 200/2000 ▶ train MSE: 0.9901, val MSE: 0.9904
Gen 300/2000 ▶ train MSE: 0.9875, val MSE: 0.9864
Gen 400/2000 ▶ train MSE: 0.9853, val MSE: 0.9842
Gen 500/2000 ▶ train MSE: 0.9835, val MSE: 0.9820
Gen 600/2000 ▶ train MSE: 0.9820, val MSE: 0.9805
Gen 700/2000 ▶ train MSE: 0.9806, val MSE: 0.9781
Gen 800/2000 ▶ train MSE: 0.9789, val MSE: 0.9753
Gen 900/2000 ▶ train MSE: 0.9770, val MSE: 0.9723
Gen 1000/2000 ▶ train MSE: 0.9755, val MSE: 0.9698
Gen 1100/2000 ▶ train MSE: 0.9742, val MSE: 0.9687
Gen 1200/2000 ▶ train MSE: 0.9731, val MSE: 0.9669
Gen 1300/2000 ▶ train MSE: 0.9710, val MSE: 0.9626
Gen 1400/2000 ▶ train MSE: 0.9698, val MSE: 0.9609
Gen 1500/2000 ▶ train MSE: 0.9687, val MSE: 0.9593
Gen 1600/2000 ▶ train MSE: 0.9671, val MSE: 0.9569
Gen 1700/2000 ▶ train MSE: 0.9659, val MSE: 0.9542
Gen 1800/2000 ▶ train MSE: 0.9647, val MSE: 0.9531
Gen 1900/2000 ▶ train MSE: 0.9637, val MSE: 0.9516
Gen 2000/2000 ▶ train MSE: 0.9623, val MSE: 0.9498

✅ GA done!  Final Train MSE: 0.9623, Val MSE: 0.9498
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e3f12bed">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Hybrid-GA:-First-implementation.-Refine-elites-every-100-generations,-for-10-epochs-using-gradient-descent.-Only-run-for-2000-generations.">Hybrid GA: First implementation. Refine elites every 100 generations, for 10 epochs using gradient descent. Only run for 2000 generations.<a class="anchor-link" href="#Hybrid-GA:-First-implementation.-Refine-elites-every-100-generations,-for-10-epochs-using-gradient-descent.-Only-run-for-2000-generations.">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=df3f3b78">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Hybrid Genetic Algorithm with Periodic SGD Refinement for FFN Training</span>
<span class="sd">=======================================================================</span>

<span class="sd">This script implements a hybrid optimisation strategy that combines a real-valued</span>
<span class="sd">Genetic Algorithm (GA) with periodic Stochastic Gradient Descent (SGD) refinement</span>
<span class="sd">for training a Feed-Forward Neural Network (FFN). The GA uses BLX-α crossover,</span>
<span class="sd">Gaussian mutation, and elitism. Every 100 generations, the top-performing elite</span>
<span class="sd">individuals are further fine-tuned using a fixed number of SGD epochs.</span>

<span class="sd">Key Innovations:</span>
<span class="sd">----------------</span>
<span class="sd">- Hybrid Strategy: Combines global GA exploration with local gradient-based exploitation</span>
<span class="sd">- Periodic Refinement: Every 100 generations, elite individuals undergo 10 epochs of SGD</span>
<span class="sd">- Logging: Logs elite MSEs before and after refinement to a CSV file for ablation and analysis</span>

<span class="sd">Core GA Configuration:</span>
<span class="sd">----------------------</span>
<span class="sd">- Population Size: 200</span>
<span class="sd">- Generations: 2000</span>
<span class="sd">- Elitism: top 20% retained per generation</span>
<span class="sd">- Tournament Selection: size 3</span>
<span class="sd">- Crossover: BLX-α with α = 0.6</span>
<span class="sd">- Mutation: Gaussian with probability 1%, std 0.01</span>

<span class="sd">SGD Refinement:</span>
<span class="sd">---------------</span>
<span class="sd">- Performed every 100 generations (`REFINE_EVERY`)</span>
<span class="sd">- Runs for 10 epochs (`REFINE_EPOCHS`) on each elite</span>
<span class="sd">- Uses momentum SGD with hyperparameters obtained via Optuna</span>

<span class="sd">Feedforward Network Architecture:</span>
<span class="sd">---------------------------------</span>
<span class="sd">- 2 hidden layers with 24 ReLU units each</span>
<span class="sd">- Xavier weight initialisation</span>
<span class="sd">- Output layer: linear (for regression)</span>

<span class="sd">Output:</span>
<span class="sd">-------</span>
<span class="sd">- Console logs for generation-wise MSE and refinement notifications</span>
<span class="sd">- `elite_mse_log.csv` file containing per-elite MSE before and after refinement</span>
<span class="sd">- Final training and validation MSE</span>
<span class="sd">- Matplotlib plot of training and validation MSE over time</span>

<span class="sd">Usage Notes:</span>
<span class="sd">------------</span>
<span class="sd">- Assumes `X_train`, `y_train`, `X_val`, `y_val` are pre-defined as `torch.Tensor`</span>
<span class="sd">- Designed for regression tasks (uses `nn.MSELoss`)</span>
<span class="sd">- Can be adapted to run refinement more/less frequently or to use other optimisers</span>

<span class="sd">"""</span>


<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1">#  1) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  2) Hyperparameters </span>
<span class="c1"># GA settings</span>
<span class="n">POP_SIZE</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">GENERATIONS</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">ELITE_FRAC</span>  <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">TOURN_SIZE</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">MUT_P</span>       <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">MUT_SD</span>      <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">BLX_ALPHA</span>   <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1"># Local SGD refinement (every REFINE_EVERY gens, on top ELITE_FRAC)</span>
<span class="n">REFINE_EVERY</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">REFINE_EPOCHS</span>  <span class="o">=</span> <span class="mi">10</span> 
<span class="c1"># FFN / SGD settings (from Optuna)</span>
<span class="n">best_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"lr"</span><span class="p">:</span>         <span class="mf">0.013988</span><span class="p">,</span>
    <span class="s2">"momentum"</span><span class="p">:</span>   <span class="mf">0.6346</span><span class="p">,</span>
    <span class="s2">"batch_size"</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"n_units"</span><span class="p">:</span>    <span class="mi">24</span><span class="p">,</span>
    <span class="s2">"activation"</span><span class="p">:</span> <span class="s2">"ReLU"</span>
<span class="p">}</span>

<span class="c1">#  3) DataPrep </span>
<span class="c1"># assume X_train, y_train, X_val, y_val are already in scope as torch.Tensor</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">],</span>
                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1">#  4) Model Builder </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">n_layers</span>   <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">],</span>
    <span class="n">n_units</span>    <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">],</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Constructs a feed-forward neural network (FFN) using the architecture specified in `arch`.</span>
<span class="sd">    </span>
<span class="sd">    The model includes:</span>
<span class="sd">    - `n_layers` hidden layers with `n_units` neurons each</span>
<span class="sd">    - Activation function as defined in `arch["activation"]`</span>
<span class="sd">    - Xavier normal weight initialization and zero-initialized biases</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: A PyTorch sequential model moved to the appropriate device.</span>
<span class="sd">    """</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_f</span> <span class="o">=</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Xavier init</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1">#  5) GA Helpers </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Selects one individual from the population using tournament selection.</span>

<span class="sd">    Args:</span>
<span class="sd">        pop (list of np.ndarray): Population of genome vectors.</span>
<span class="sd">        fitness (list of float): Corresponding fitness values (lower is better).</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Genome of the selected individual.</span>
<span class="sd">    """</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">),</span> <span class="n">TOURN_SIZE</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">crossover_and_mutate</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Produces a new child genome via BLX-α crossover and Gaussian mutation.</span>

<span class="sd">    - BLX-α expands the gene-wise search space around two parents.</span>
<span class="sd">    - Gaussian noise is added to a subset of genes based on mutation probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        p1 (np.ndarray): Parent 1 genome.</span>
<span class="sd">        p2 (np.ndarray): Parent 2 genome.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Mutated child genome.</span>
<span class="sd">    """</span>
    <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">BLX_ALPHA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
    <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">BLX_ALPHA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
    <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># mutation</span>
    <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">MUT_P</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">MUT_SD</span>
    <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">child</span>

<span class="k">def</span><span class="w"> </span><span class="nf">refine_with_sgd</span><span class="p">(</span><span class="n">genome</span><span class="p">):</span>

<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Produces a new child genome via BLX-α crossover and Gaussian mutation.</span>

<span class="sd">    - BLX-α expands the gene-wise search space around two parents.</span>
<span class="sd">    - Gaussian noise is added to a subset of genes based on mutation probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        p1 (np.ndarray): Parent 1 genome.</span>
<span class="sd">        p2 (np.ndarray): Parent 2 genome.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Mutated child genome.</span>
<span class="sd">    """</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                         <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="n">optim_sgd</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                          <span class="n">lr</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span>
                          <span class="n">momentum</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"momentum"</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">REFINE_EPOCHS</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"refine step : "</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optim_sgd</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optim_sgd</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># write back</span>
    <span class="k">return</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1">#  6) Initialize Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">POP_SIZE</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="c1">#  7) GA Main Loop </span>
<span class="n">train_curve</span><span class="p">,</span> <span class="n">val_curve</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="c1">###logging of mse values before and after sgd</span>
<span class="n">LOG_FN</span> <span class="o">=</span> <span class="s2">"elite_mse_log.csv"</span>
<span class="c1"># write header: gen,phase,elite0,elite1,…</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="c1"># we'll fill in the number of elites after we know ELITE_FRAC and POP_SIZE</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"gen"</span><span class="p">,</span><span class="s2">"phase"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"elite</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">GENERATIONS</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># a) evaluate all fitness</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="c1"># b) stats &amp; record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span>
    <span class="c1"># validation mse of best</span>
    <span class="n">m_best</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="c1">#if gen % 100 == 0 or gen == 1:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">GENERATIONS</span><span class="si">}</span><span class="s2"> ▶ "</span>
        <span class="sa">f</span><span class="s2">"train MSE: </span><span class="si">{</span><span class="n">fitness</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">val_curve</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># c) elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
    <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
    <span class="n">elites</span> <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">]</span>
    <span class="n">elite_tuned</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">((</span><span class="n">ELITE_FRAC</span><span class="p">)</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
    <span class="n">elite_tuned_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">gen</span> <span class="o">%</span> <span class="n">REFINE_EVERY</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># (re)compute your elite indices</span>
        <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
        <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
        <span class="c1"># 1) capture “pre” MSEs</span>
        <span class="n">pre_vals</span> <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">elite_idxs</span><span class="p">]</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">line</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">gen</span><span class="p">),</span> <span class="s2">"pre"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">pre_vals</span><span class="p">]</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
        <span class="c1"># 2) do your SGD refine exactly as before</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Refinement @ gen </span><span class="si">{</span><span class="n">gen</span><span class="si">}</span><span class="s2"> ---"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">:</span>
            <span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">refine_with_sgd</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="c1"># 3) capture “post” MSEs (re‐evaluate on train set)</span>
        <span class="n">post_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
            <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">post_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">line</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">gen</span><span class="p">),</span> <span class="s2">"post"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">post_vals</span><span class="p">]</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># e) selection + reproduction</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="p">[</span> <span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span> <span class="p">]</span> 
    <span class="n">new_popa</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="c1"># show that new_pop still points at the old arrays A, B, C… not your refined ones</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">POP_SIZE</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">crossover_and_mutate</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">)</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1">#  8) Final Eval &amp; Plot </span>
<span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
<span class="n">best_model</span>  <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">best_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ GA+SGD done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA + Periodic SGD Refinement"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen 1/2000 ▶ train MSE: 1.0075, val MSE: 1.0054
Gen 2/2000 ▶ train MSE: 1.0075, val MSE: 1.0054
Gen 3/2000 ▶ train MSE: 1.0075, val MSE: 1.0054
Gen 4/2000 ▶ train MSE: 1.0073, val MSE: 1.0051
Gen 5/2000 ▶ train MSE: 1.0073, val MSE: 1.0051
Gen 6/2000 ▶ train MSE: 1.0073, val MSE: 1.0051
Gen 7/2000 ▶ train MSE: 1.0073, val MSE: 1.0051
Gen 8/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 9/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 10/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 11/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 12/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 13/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 14/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 15/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 16/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 17/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 18/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 19/2000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 20/2000 ▶ train MSE: 1.0023, val MSE: 1.0025
Gen 21/2000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 22/2000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 23/2000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 24/2000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 25/2000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 26/2000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 27/2000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 28/2000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 29/2000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 30/2000 ▶ train MSE: 1.0009, val MSE: 1.0004
Gen 31/2000 ▶ train MSE: 1.0001, val MSE: 1.0015
Gen 32/2000 ▶ train MSE: 1.0001, val MSE: 1.0015
Gen 33/2000 ▶ train MSE: 1.0001, val MSE: 1.0015
Gen 34/2000 ▶ train MSE: 1.0001, val MSE: 1.0015
Gen 35/2000 ▶ train MSE: 1.0001, val MSE: 1.0015
Gen 36/2000 ▶ train MSE: 1.0001, val MSE: 1.0013
Gen 37/2000 ▶ train MSE: 1.0001, val MSE: 1.0013
Gen 38/2000 ▶ train MSE: 1.0001, val MSE: 1.0013
Gen 39/2000 ▶ train MSE: 0.9995, val MSE: 1.0004
Gen 40/2000 ▶ train MSE: 0.9995, val MSE: 1.0004
Gen 41/2000 ▶ train MSE: 0.9995, val MSE: 1.0004
Gen 42/2000 ▶ train MSE: 0.9995, val MSE: 1.0004
Gen 43/2000 ▶ train MSE: 0.9995, val MSE: 1.0004
Gen 44/2000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 45/2000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 46/2000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 47/2000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 48/2000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 49/2000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 50/2000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 51/2000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 52/2000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 53/2000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 54/2000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 55/2000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 56/2000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 57/2000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 58/2000 ▶ train MSE: 0.9986, val MSE: 0.9985
Gen 59/2000 ▶ train MSE: 0.9981, val MSE: 0.9981
Gen 60/2000 ▶ train MSE: 0.9981, val MSE: 0.9981
Gen 61/2000 ▶ train MSE: 0.9981, val MSE: 0.9981
Gen 62/2000 ▶ train MSE: 0.9981, val MSE: 0.9981
Gen 63/2000 ▶ train MSE: 0.9981, val MSE: 0.9981
Gen 64/2000 ▶ train MSE: 0.9981, val MSE: 0.9996
Gen 65/2000 ▶ train MSE: 0.9981, val MSE: 0.9996
Gen 66/2000 ▶ train MSE: 0.9981, val MSE: 0.9996
Gen 67/2000 ▶ train MSE: 0.9980, val MSE: 0.9987
Gen 68/2000 ▶ train MSE: 0.9980, val MSE: 0.9987
Gen 69/2000 ▶ train MSE: 0.9979, val MSE: 0.9974
Gen 70/2000 ▶ train MSE: 0.9979, val MSE: 0.9974
Gen 71/2000 ▶ train MSE: 0.9977, val MSE: 0.9981
Gen 72/2000 ▶ train MSE: 0.9977, val MSE: 0.9981
Gen 73/2000 ▶ train MSE: 0.9977, val MSE: 0.9981
Gen 74/2000 ▶ train MSE: 0.9977, val MSE: 0.9981
Gen 75/2000 ▶ train MSE: 0.9977, val MSE: 0.9981
Gen 76/2000 ▶ train MSE: 0.9975, val MSE: 0.9986
Gen 77/2000 ▶ train MSE: 0.9975, val MSE: 0.9986
Gen 78/2000 ▶ train MSE: 0.9975, val MSE: 0.9986
Gen 79/2000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 80/2000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 81/2000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 82/2000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 83/2000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 84/2000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 85/2000 ▶ train MSE: 0.9975, val MSE: 0.9979
Gen 86/2000 ▶ train MSE: 0.9975, val MSE: 0.9979
Gen 87/2000 ▶ train MSE: 0.9975, val MSE: 0.9979
Gen 88/2000 ▶ train MSE: 0.9974, val MSE: 0.9980
Gen 89/2000 ▶ train MSE: 0.9973, val MSE: 0.9972
Gen 90/2000 ▶ train MSE: 0.9973, val MSE: 0.9972
Gen 91/2000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 92/2000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 93/2000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 94/2000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 95/2000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 96/2000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 97/2000 ▶ train MSE: 0.9968, val MSE: 0.9974
Gen 98/2000 ▶ train MSE: 0.9968, val MSE: 0.9974
Gen 99/2000 ▶ train MSE: 0.9968, val MSE: 0.9974
Gen 100/2000 ▶ train MSE: 0.9968, val MSE: 0.9974

--- Refinement @ gen 100 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 101/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 102/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 103/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 104/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 105/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 106/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 107/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 108/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 109/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 110/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 111/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 112/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 113/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 114/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 115/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 116/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 117/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 118/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 119/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 120/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 121/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 122/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 123/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 124/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 125/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 126/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 127/2000 ▶ train MSE: 0.9738, val MSE: 0.9631
Gen 128/2000 ▶ train MSE: 0.9734, val MSE: 0.9631
Gen 129/2000 ▶ train MSE: 0.9734, val MSE: 0.9631
Gen 130/2000 ▶ train MSE: 0.9733, val MSE: 0.9629
Gen 131/2000 ▶ train MSE: 0.9732, val MSE: 0.9627
Gen 132/2000 ▶ train MSE: 0.9729, val MSE: 0.9622
Gen 133/2000 ▶ train MSE: 0.9724, val MSE: 0.9623
Gen 134/2000 ▶ train MSE: 0.9724, val MSE: 0.9623
Gen 135/2000 ▶ train MSE: 0.9724, val MSE: 0.9625
Gen 136/2000 ▶ train MSE: 0.9722, val MSE: 0.9625
Gen 137/2000 ▶ train MSE: 0.9722, val MSE: 0.9626
Gen 138/2000 ▶ train MSE: 0.9721, val MSE: 0.9623
Gen 139/2000 ▶ train MSE: 0.9719, val MSE: 0.9620
Gen 140/2000 ▶ train MSE: 0.9717, val MSE: 0.9618
Gen 141/2000 ▶ train MSE: 0.9716, val MSE: 0.9620
Gen 142/2000 ▶ train MSE: 0.9716, val MSE: 0.9624
Gen 143/2000 ▶ train MSE: 0.9716, val MSE: 0.9624
Gen 144/2000 ▶ train MSE: 0.9715, val MSE: 0.9621
Gen 145/2000 ▶ train MSE: 0.9714, val MSE: 0.9623
Gen 146/2000 ▶ train MSE: 0.9712, val MSE: 0.9621
Gen 147/2000 ▶ train MSE: 0.9712, val MSE: 0.9622
Gen 148/2000 ▶ train MSE: 0.9711, val MSE: 0.9622
Gen 149/2000 ▶ train MSE: 0.9709, val MSE: 0.9614
Gen 150/2000 ▶ train MSE: 0.9709, val MSE: 0.9614
Gen 151/2000 ▶ train MSE: 0.9709, val MSE: 0.9614
Gen 152/2000 ▶ train MSE: 0.9705, val MSE: 0.9612
Gen 153/2000 ▶ train MSE: 0.9705, val MSE: 0.9612
Gen 154/2000 ▶ train MSE: 0.9705, val MSE: 0.9612
Gen 155/2000 ▶ train MSE: 0.9705, val MSE: 0.9612
Gen 156/2000 ▶ train MSE: 0.9705, val MSE: 0.9612
Gen 157/2000 ▶ train MSE: 0.9705, val MSE: 0.9612
Gen 158/2000 ▶ train MSE: 0.9705, val MSE: 0.9612
Gen 159/2000 ▶ train MSE: 0.9705, val MSE: 0.9612
Gen 160/2000 ▶ train MSE: 0.9705, val MSE: 0.9612
Gen 161/2000 ▶ train MSE: 0.9705, val MSE: 0.9612
Gen 162/2000 ▶ train MSE: 0.9705, val MSE: 0.9615
Gen 163/2000 ▶ train MSE: 0.9705, val MSE: 0.9615
Gen 164/2000 ▶ train MSE: 0.9705, val MSE: 0.9615
Gen 165/2000 ▶ train MSE: 0.9705, val MSE: 0.9615
Gen 166/2000 ▶ train MSE: 0.9705, val MSE: 0.9615
Gen 167/2000 ▶ train MSE: 0.9704, val MSE: 0.9611
Gen 168/2000 ▶ train MSE: 0.9704, val MSE: 0.9611
Gen 169/2000 ▶ train MSE: 0.9704, val MSE: 0.9611
Gen 170/2000 ▶ train MSE: 0.9704, val MSE: 0.9611
Gen 171/2000 ▶ train MSE: 0.9704, val MSE: 0.9609
Gen 172/2000 ▶ train MSE: 0.9703, val MSE: 0.9614
Gen 173/2000 ▶ train MSE: 0.9703, val MSE: 0.9614
Gen 174/2000 ▶ train MSE: 0.9703, val MSE: 0.9614
Gen 175/2000 ▶ train MSE: 0.9703, val MSE: 0.9614
Gen 176/2000 ▶ train MSE: 0.9703, val MSE: 0.9615
Gen 177/2000 ▶ train MSE: 0.9702, val MSE: 0.9611
Gen 178/2000 ▶ train MSE: 0.9702, val MSE: 0.9611
Gen 179/2000 ▶ train MSE: 0.9702, val MSE: 0.9611
Gen 180/2000 ▶ train MSE: 0.9702, val MSE: 0.9611
Gen 181/2000 ▶ train MSE: 0.9702, val MSE: 0.9611
Gen 182/2000 ▶ train MSE: 0.9702, val MSE: 0.9611
Gen 183/2000 ▶ train MSE: 0.9700, val MSE: 0.9605
Gen 184/2000 ▶ train MSE: 0.9700, val MSE: 0.9605
Gen 185/2000 ▶ train MSE: 0.9700, val MSE: 0.9605
Gen 186/2000 ▶ train MSE: 0.9700, val MSE: 0.9605
Gen 187/2000 ▶ train MSE: 0.9700, val MSE: 0.9605
Gen 188/2000 ▶ train MSE: 0.9700, val MSE: 0.9605
Gen 189/2000 ▶ train MSE: 0.9700, val MSE: 0.9605
Gen 190/2000 ▶ train MSE: 0.9700, val MSE: 0.9605
Gen 191/2000 ▶ train MSE: 0.9699, val MSE: 0.9605
Gen 192/2000 ▶ train MSE: 0.9699, val MSE: 0.9605
Gen 193/2000 ▶ train MSE: 0.9699, val MSE: 0.9605
Gen 194/2000 ▶ train MSE: 0.9699, val MSE: 0.9605
Gen 195/2000 ▶ train MSE: 0.9698, val MSE: 0.9601
Gen 196/2000 ▶ train MSE: 0.9698, val MSE: 0.9601
Gen 197/2000 ▶ train MSE: 0.9698, val MSE: 0.9601
Gen 198/2000 ▶ train MSE: 0.9698, val MSE: 0.9601
Gen 199/2000 ▶ train MSE: 0.9698, val MSE: 0.9601
Gen 200/2000 ▶ train MSE: 0.9698, val MSE: 0.9601

--- Refinement @ gen 200 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 201/2000 ▶ train MSE: 0.9361, val MSE: 0.9021
Gen 202/2000 ▶ train MSE: 0.9361, val MSE: 0.9033
Gen 203/2000 ▶ train MSE: 0.9361, val MSE: 0.9033
Gen 204/2000 ▶ train MSE: 0.9361, val MSE: 0.9033
Gen 205/2000 ▶ train MSE: 0.9361, val MSE: 0.9033
Gen 206/2000 ▶ train MSE: 0.9361, val MSE: 0.9033
Gen 207/2000 ▶ train MSE: 0.9361, val MSE: 0.9033
Gen 208/2000 ▶ train MSE: 0.9361, val MSE: 0.9033
Gen 209/2000 ▶ train MSE: 0.9359, val MSE: 0.9019
Gen 210/2000 ▶ train MSE: 0.9359, val MSE: 0.9019
Gen 211/2000 ▶ train MSE: 0.9359, val MSE: 0.9019
Gen 212/2000 ▶ train MSE: 0.9359, val MSE: 0.9019
Gen 213/2000 ▶ train MSE: 0.9359, val MSE: 0.9019
Gen 214/2000 ▶ train MSE: 0.9359, val MSE: 0.9019
Gen 215/2000 ▶ train MSE: 0.9359, val MSE: 0.9019
Gen 216/2000 ▶ train MSE: 0.9349, val MSE: 0.9036
Gen 217/2000 ▶ train MSE: 0.9349, val MSE: 0.9036
Gen 218/2000 ▶ train MSE: 0.9349, val MSE: 0.9036
Gen 219/2000 ▶ train MSE: 0.9349, val MSE: 0.9036
Gen 220/2000 ▶ train MSE: 0.9349, val MSE: 0.9036
Gen 221/2000 ▶ train MSE: 0.9333, val MSE: 0.9006
Gen 222/2000 ▶ train MSE: 0.9328, val MSE: 0.8989
Gen 223/2000 ▶ train MSE: 0.9320, val MSE: 0.8979
Gen 224/2000 ▶ train MSE: 0.9315, val MSE: 0.8979
Gen 225/2000 ▶ train MSE: 0.9313, val MSE: 0.8991
Gen 226/2000 ▶ train MSE: 0.9299, val MSE: 0.8970
Gen 227/2000 ▶ train MSE: 0.9299, val MSE: 0.8970
Gen 228/2000 ▶ train MSE: 0.9299, val MSE: 0.8970
Gen 229/2000 ▶ train MSE: 0.9281, val MSE: 0.8947
Gen 230/2000 ▶ train MSE: 0.9281, val MSE: 0.8947
Gen 231/2000 ▶ train MSE: 0.9281, val MSE: 0.8947
Gen 232/2000 ▶ train MSE: 0.9280, val MSE: 0.8951
Gen 233/2000 ▶ train MSE: 0.9278, val MSE: 0.8946
Gen 234/2000 ▶ train MSE: 0.9271, val MSE: 0.8923
Gen 235/2000 ▶ train MSE: 0.9271, val MSE: 0.8923
Gen 236/2000 ▶ train MSE: 0.9271, val MSE: 0.8923
Gen 237/2000 ▶ train MSE: 0.9271, val MSE: 0.8923
Gen 238/2000 ▶ train MSE: 0.9271, val MSE: 0.8923
Gen 239/2000 ▶ train MSE: 0.9271, val MSE: 0.8923
Gen 240/2000 ▶ train MSE: 0.9270, val MSE: 0.8923
Gen 241/2000 ▶ train MSE: 0.9270, val MSE: 0.8923
Gen 242/2000 ▶ train MSE: 0.9266, val MSE: 0.8931
Gen 243/2000 ▶ train MSE: 0.9266, val MSE: 0.8931
Gen 244/2000 ▶ train MSE: 0.9266, val MSE: 0.8931
Gen 245/2000 ▶ train MSE: 0.9266, val MSE: 0.8931
Gen 246/2000 ▶ train MSE: 0.9261, val MSE: 0.8897
Gen 247/2000 ▶ train MSE: 0.9261, val MSE: 0.8897
Gen 248/2000 ▶ train MSE: 0.9261, val MSE: 0.8897
Gen 249/2000 ▶ train MSE: 0.9261, val MSE: 0.8897
Gen 250/2000 ▶ train MSE: 0.9259, val MSE: 0.8911
Gen 251/2000 ▶ train MSE: 0.9259, val MSE: 0.8911
Gen 252/2000 ▶ train MSE: 0.9256, val MSE: 0.8896
Gen 253/2000 ▶ train MSE: 0.9256, val MSE: 0.8896
Gen 254/2000 ▶ train MSE: 0.9256, val MSE: 0.8896
Gen 255/2000 ▶ train MSE: 0.9256, val MSE: 0.8896
Gen 256/2000 ▶ train MSE: 0.9256, val MSE: 0.8896
Gen 257/2000 ▶ train MSE: 0.9253, val MSE: 0.8893
Gen 258/2000 ▶ train MSE: 0.9253, val MSE: 0.8893
Gen 259/2000 ▶ train MSE: 0.9253, val MSE: 0.8893
Gen 260/2000 ▶ train MSE: 0.9252, val MSE: 0.8902
Gen 261/2000 ▶ train MSE: 0.9252, val MSE: 0.8902
Gen 262/2000 ▶ train MSE: 0.9252, val MSE: 0.8902
Gen 263/2000 ▶ train MSE: 0.9246, val MSE: 0.8900
Gen 264/2000 ▶ train MSE: 0.9246, val MSE: 0.8900
Gen 265/2000 ▶ train MSE: 0.9246, val MSE: 0.8900
Gen 266/2000 ▶ train MSE: 0.9246, val MSE: 0.8900
Gen 267/2000 ▶ train MSE: 0.9246, val MSE: 0.8900
Gen 268/2000 ▶ train MSE: 0.9246, val MSE: 0.8900
Gen 269/2000 ▶ train MSE: 0.9244, val MSE: 0.8896
Gen 270/2000 ▶ train MSE: 0.9244, val MSE: 0.8896
Gen 271/2000 ▶ train MSE: 0.9244, val MSE: 0.8896
Gen 272/2000 ▶ train MSE: 0.9243, val MSE: 0.8905
Gen 273/2000 ▶ train MSE: 0.9243, val MSE: 0.8905
Gen 274/2000 ▶ train MSE: 0.9243, val MSE: 0.8891
Gen 275/2000 ▶ train MSE: 0.9243, val MSE: 0.8891
Gen 276/2000 ▶ train MSE: 0.9243, val MSE: 0.8891
Gen 277/2000 ▶ train MSE: 0.9242, val MSE: 0.8889
Gen 278/2000 ▶ train MSE: 0.9242, val MSE: 0.8882
Gen 279/2000 ▶ train MSE: 0.9242, val MSE: 0.8882
Gen 280/2000 ▶ train MSE: 0.9242, val MSE: 0.8882
Gen 281/2000 ▶ train MSE: 0.9242, val MSE: 0.8882
Gen 282/2000 ▶ train MSE: 0.9239, val MSE: 0.8883
Gen 283/2000 ▶ train MSE: 0.9239, val MSE: 0.8883
Gen 284/2000 ▶ train MSE: 0.9238, val MSE: 0.8885
Gen 285/2000 ▶ train MSE: 0.9238, val MSE: 0.8885
Gen 286/2000 ▶ train MSE: 0.9238, val MSE: 0.8885
Gen 287/2000 ▶ train MSE: 0.9238, val MSE: 0.8885
Gen 288/2000 ▶ train MSE: 0.9238, val MSE: 0.8885
Gen 289/2000 ▶ train MSE: 0.9238, val MSE: 0.8885
Gen 290/2000 ▶ train MSE: 0.9238, val MSE: 0.8886
Gen 291/2000 ▶ train MSE: 0.9237, val MSE: 0.8886
Gen 292/2000 ▶ train MSE: 0.9233, val MSE: 0.8884
Gen 293/2000 ▶ train MSE: 0.9233, val MSE: 0.8884
Gen 294/2000 ▶ train MSE: 0.9233, val MSE: 0.8884
Gen 295/2000 ▶ train MSE: 0.9233, val MSE: 0.8884
Gen 296/2000 ▶ train MSE: 0.9233, val MSE: 0.8884
Gen 297/2000 ▶ train MSE: 0.9233, val MSE: 0.8884
Gen 298/2000 ▶ train MSE: 0.9233, val MSE: 0.8884
Gen 299/2000 ▶ train MSE: 0.9233, val MSE: 0.8884
Gen 300/2000 ▶ train MSE: 0.9233, val MSE: 0.8884

--- Refinement @ gen 300 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 301/2000 ▶ train MSE: 0.7639, val MSE: 0.6133
Gen 302/2000 ▶ train MSE: 0.7639, val MSE: 0.6133
Gen 303/2000 ▶ train MSE: 0.7639, val MSE: 0.6133
Gen 304/2000 ▶ train MSE: 0.7639, val MSE: 0.6133
Gen 305/2000 ▶ train MSE: 0.7639, val MSE: 0.6133
Gen 306/2000 ▶ train MSE: 0.7639, val MSE: 0.6133
Gen 307/2000 ▶ train MSE: 0.7639, val MSE: 0.6133
Gen 308/2000 ▶ train MSE: 0.7635, val MSE: 0.6129
Gen 309/2000 ▶ train MSE: 0.7635, val MSE: 0.6129
Gen 310/2000 ▶ train MSE: 0.7635, val MSE: 0.6129
Gen 311/2000 ▶ train MSE: 0.7635, val MSE: 0.6129
Gen 312/2000 ▶ train MSE: 0.7635, val MSE: 0.6129
Gen 313/2000 ▶ train MSE: 0.7632, val MSE: 0.6129
Gen 314/2000 ▶ train MSE: 0.7632, val MSE: 0.6129
Gen 315/2000 ▶ train MSE: 0.7632, val MSE: 0.6129
Gen 316/2000 ▶ train MSE: 0.7632, val MSE: 0.6129
Gen 317/2000 ▶ train MSE: 0.7632, val MSE: 0.6129
Gen 318/2000 ▶ train MSE: 0.7632, val MSE: 0.6129
Gen 319/2000 ▶ train MSE: 0.7632, val MSE: 0.6129
Gen 320/2000 ▶ train MSE: 0.7629, val MSE: 0.6126
Gen 321/2000 ▶ train MSE: 0.7629, val MSE: 0.6126
Gen 322/2000 ▶ train MSE: 0.7629, val MSE: 0.6126
Gen 323/2000 ▶ train MSE: 0.7629, val MSE: 0.6126
Gen 324/2000 ▶ train MSE: 0.7626, val MSE: 0.6120
Gen 325/2000 ▶ train MSE: 0.7611, val MSE: 0.6102
Gen 326/2000 ▶ train MSE: 0.7611, val MSE: 0.6102
Gen 327/2000 ▶ train MSE: 0.7605, val MSE: 0.6106
Gen 328/2000 ▶ train MSE: 0.7590, val MSE: 0.6095
Gen 329/2000 ▶ train MSE: 0.7579, val MSE: 0.6074
Gen 330/2000 ▶ train MSE: 0.7561, val MSE: 0.6075
Gen 331/2000 ▶ train MSE: 0.7556, val MSE: 0.6059
Gen 332/2000 ▶ train MSE: 0.7512, val MSE: 0.6034
Gen 333/2000 ▶ train MSE: 0.7500, val MSE: 0.6040
Gen 334/2000 ▶ train MSE: 0.7451, val MSE: 0.5962
Gen 335/2000 ▶ train MSE: 0.7420, val MSE: 0.5975
Gen 336/2000 ▶ train MSE: 0.7420, val MSE: 0.5975
Gen 337/2000 ▶ train MSE: 0.7399, val MSE: 0.5936
Gen 338/2000 ▶ train MSE: 0.7397, val MSE: 0.5938
Gen 339/2000 ▶ train MSE: 0.7359, val MSE: 0.5941
Gen 340/2000 ▶ train MSE: 0.7359, val MSE: 0.5941
Gen 341/2000 ▶ train MSE: 0.7344, val MSE: 0.5918
Gen 342/2000 ▶ train MSE: 0.7332, val MSE: 0.5886
Gen 343/2000 ▶ train MSE: 0.7328, val MSE: 0.5890
Gen 344/2000 ▶ train MSE: 0.7328, val MSE: 0.5890
Gen 345/2000 ▶ train MSE: 0.7328, val MSE: 0.5890
Gen 346/2000 ▶ train MSE: 0.7313, val MSE: 0.5908
Gen 347/2000 ▶ train MSE: 0.7311, val MSE: 0.5898
Gen 348/2000 ▶ train MSE: 0.7311, val MSE: 0.5898
Gen 349/2000 ▶ train MSE: 0.7305, val MSE: 0.5872
Gen 350/2000 ▶ train MSE: 0.7305, val MSE: 0.5872
Gen 351/2000 ▶ train MSE: 0.7293, val MSE: 0.5833
Gen 352/2000 ▶ train MSE: 0.7287, val MSE: 0.5843
Gen 353/2000 ▶ train MSE: 0.7287, val MSE: 0.5843
Gen 354/2000 ▶ train MSE: 0.7281, val MSE: 0.5812
Gen 355/2000 ▶ train MSE: 0.7281, val MSE: 0.5812
Gen 356/2000 ▶ train MSE: 0.7279, val MSE: 0.5858
Gen 357/2000 ▶ train MSE: 0.7277, val MSE: 0.5857
Gen 358/2000 ▶ train MSE: 0.7272, val MSE: 0.5837
Gen 359/2000 ▶ train MSE: 0.7260, val MSE: 0.5841
Gen 360/2000 ▶ train MSE: 0.7260, val MSE: 0.5841
Gen 361/2000 ▶ train MSE: 0.7256, val MSE: 0.5825
Gen 362/2000 ▶ train MSE: 0.7253, val MSE: 0.5822
Gen 363/2000 ▶ train MSE: 0.7253, val MSE: 0.5822
Gen 364/2000 ▶ train MSE: 0.7253, val MSE: 0.5822
Gen 365/2000 ▶ train MSE: 0.7253, val MSE: 0.5822
Gen 366/2000 ▶ train MSE: 0.7253, val MSE: 0.5822
Gen 367/2000 ▶ train MSE: 0.7253, val MSE: 0.5822
Gen 368/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 369/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 370/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 371/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 372/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 373/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 374/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 375/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 376/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 377/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 378/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 379/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 380/2000 ▶ train MSE: 0.7209, val MSE: 0.5766
Gen 381/2000 ▶ train MSE: 0.7209, val MSE: 0.5773
Gen 382/2000 ▶ train MSE: 0.7209, val MSE: 0.5773
Gen 383/2000 ▶ train MSE: 0.7195, val MSE: 0.5727
Gen 384/2000 ▶ train MSE: 0.7195, val MSE: 0.5727
Gen 385/2000 ▶ train MSE: 0.7195, val MSE: 0.5727
Gen 386/2000 ▶ train MSE: 0.7195, val MSE: 0.5727
Gen 387/2000 ▶ train MSE: 0.7189, val MSE: 0.5724
Gen 388/2000 ▶ train MSE: 0.7189, val MSE: 0.5724
Gen 389/2000 ▶ train MSE: 0.7189, val MSE: 0.5724
Gen 390/2000 ▶ train MSE: 0.7189, val MSE: 0.5724
Gen 391/2000 ▶ train MSE: 0.7189, val MSE: 0.5724
Gen 392/2000 ▶ train MSE: 0.7181, val MSE: 0.5717
Gen 393/2000 ▶ train MSE: 0.7181, val MSE: 0.5717
Gen 394/2000 ▶ train MSE: 0.7178, val MSE: 0.5739
Gen 395/2000 ▶ train MSE: 0.7178, val MSE: 0.5739
Gen 396/2000 ▶ train MSE: 0.7178, val MSE: 0.5739
Gen 397/2000 ▶ train MSE: 0.7176, val MSE: 0.5729
Gen 398/2000 ▶ train MSE: 0.7176, val MSE: 0.5729
Gen 399/2000 ▶ train MSE: 0.7176, val MSE: 0.5729
Gen 400/2000 ▶ train MSE: 0.7176, val MSE: 0.5729

--- Refinement @ gen 400 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 401/2000 ▶ train MSE: 0.6965, val MSE: 0.5159
Gen 402/2000 ▶ train MSE: 0.6965, val MSE: 0.5159
Gen 403/2000 ▶ train MSE: 0.6937, val MSE: 0.5026
Gen 404/2000 ▶ train MSE: 0.6915, val MSE: 0.5080
Gen 405/2000 ▶ train MSE: 0.6915, val MSE: 0.5080
Gen 406/2000 ▶ train MSE: 0.6915, val MSE: 0.5080
Gen 407/2000 ▶ train MSE: 0.6915, val MSE: 0.5080
Gen 408/2000 ▶ train MSE: 0.6915, val MSE: 0.5080
Gen 409/2000 ▶ train MSE: 0.6915, val MSE: 0.5080
Gen 410/2000 ▶ train MSE: 0.6877, val MSE: 0.5014
Gen 411/2000 ▶ train MSE: 0.6877, val MSE: 0.5014
Gen 412/2000 ▶ train MSE: 0.6845, val MSE: 0.4985
Gen 413/2000 ▶ train MSE: 0.6845, val MSE: 0.4985
Gen 414/2000 ▶ train MSE: 0.6838, val MSE: 0.4990
Gen 415/2000 ▶ train MSE: 0.6838, val MSE: 0.4990
Gen 416/2000 ▶ train MSE: 0.6826, val MSE: 0.4936
Gen 417/2000 ▶ train MSE: 0.6826, val MSE: 0.4936
Gen 418/2000 ▶ train MSE: 0.6824, val MSE: 0.4930
Gen 419/2000 ▶ train MSE: 0.6808, val MSE: 0.4890
Gen 420/2000 ▶ train MSE: 0.6796, val MSE: 0.4948
Gen 421/2000 ▶ train MSE: 0.6796, val MSE: 0.4948
Gen 422/2000 ▶ train MSE: 0.6784, val MSE: 0.4867
Gen 423/2000 ▶ train MSE: 0.6784, val MSE: 0.4867
Gen 424/2000 ▶ train MSE: 0.6761, val MSE: 0.4887
Gen 425/2000 ▶ train MSE: 0.6761, val MSE: 0.4887
Gen 426/2000 ▶ train MSE: 0.6761, val MSE: 0.4887
Gen 427/2000 ▶ train MSE: 0.6761, val MSE: 0.4887
Gen 428/2000 ▶ train MSE: 0.6760, val MSE: 0.4845
Gen 429/2000 ▶ train MSE: 0.6747, val MSE: 0.4842
Gen 430/2000 ▶ train MSE: 0.6747, val MSE: 0.4842
Gen 431/2000 ▶ train MSE: 0.6736, val MSE: 0.4902
Gen 432/2000 ▶ train MSE: 0.6736, val MSE: 0.4902
Gen 433/2000 ▶ train MSE: 0.6736, val MSE: 0.4902
Gen 434/2000 ▶ train MSE: 0.6736, val MSE: 0.4902
Gen 435/2000 ▶ train MSE: 0.6736, val MSE: 0.4902
Gen 436/2000 ▶ train MSE: 0.6716, val MSE: 0.4791
Gen 437/2000 ▶ train MSE: 0.6716, val MSE: 0.4791
Gen 438/2000 ▶ train MSE: 0.6716, val MSE: 0.4791
Gen 439/2000 ▶ train MSE: 0.6716, val MSE: 0.4791
Gen 440/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 441/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 442/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 443/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 444/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 445/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 446/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 447/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 448/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 449/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 450/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 451/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 452/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 453/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 454/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 455/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 456/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 457/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 458/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 459/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 460/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 461/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 462/2000 ▶ train MSE: 0.6668, val MSE: 0.4770
Gen 463/2000 ▶ train MSE: 0.6668, val MSE: 0.4816
Gen 464/2000 ▶ train MSE: 0.6661, val MSE: 0.4780
Gen 465/2000 ▶ train MSE: 0.6650, val MSE: 0.4797
Gen 466/2000 ▶ train MSE: 0.6647, val MSE: 0.4781
Gen 467/2000 ▶ train MSE: 0.6645, val MSE: 0.4773
Gen 468/2000 ▶ train MSE: 0.6645, val MSE: 0.4773
Gen 469/2000 ▶ train MSE: 0.6645, val MSE: 0.4773
Gen 470/2000 ▶ train MSE: 0.6642, val MSE: 0.4768
Gen 471/2000 ▶ train MSE: 0.6632, val MSE: 0.4755
Gen 472/2000 ▶ train MSE: 0.6628, val MSE: 0.4745
Gen 473/2000 ▶ train MSE: 0.6628, val MSE: 0.4745
Gen 474/2000 ▶ train MSE: 0.6627, val MSE: 0.4755
Gen 475/2000 ▶ train MSE: 0.6622, val MSE: 0.4749
Gen 476/2000 ▶ train MSE: 0.6622, val MSE: 0.4743
Gen 477/2000 ▶ train MSE: 0.6622, val MSE: 0.4743
Gen 478/2000 ▶ train MSE: 0.6622, val MSE: 0.4743
Gen 479/2000 ▶ train MSE: 0.6616, val MSE: 0.4753
Gen 480/2000 ▶ train MSE: 0.6609, val MSE: 0.4724
Gen 481/2000 ▶ train MSE: 0.6609, val MSE: 0.4724
Gen 482/2000 ▶ train MSE: 0.6609, val MSE: 0.4724
Gen 483/2000 ▶ train MSE: 0.6609, val MSE: 0.4724
Gen 484/2000 ▶ train MSE: 0.6609, val MSE: 0.4724
Gen 485/2000 ▶ train MSE: 0.6605, val MSE: 0.4710
Gen 486/2000 ▶ train MSE: 0.6605, val MSE: 0.4710
Gen 487/2000 ▶ train MSE: 0.6605, val MSE: 0.4710
Gen 488/2000 ▶ train MSE: 0.6605, val MSE: 0.4710
Gen 489/2000 ▶ train MSE: 0.6600, val MSE: 0.4724
Gen 490/2000 ▶ train MSE: 0.6587, val MSE: 0.4677
Gen 491/2000 ▶ train MSE: 0.6587, val MSE: 0.4677
Gen 492/2000 ▶ train MSE: 0.6587, val MSE: 0.4677
Gen 493/2000 ▶ train MSE: 0.6587, val MSE: 0.4677
Gen 494/2000 ▶ train MSE: 0.6587, val MSE: 0.4677
Gen 495/2000 ▶ train MSE: 0.6582, val MSE: 0.4705
Gen 496/2000 ▶ train MSE: 0.6582, val MSE: 0.4705
Gen 497/2000 ▶ train MSE: 0.6582, val MSE: 0.4705
Gen 498/2000 ▶ train MSE: 0.6582, val MSE: 0.4705
Gen 499/2000 ▶ train MSE: 0.6582, val MSE: 0.4705
Gen 500/2000 ▶ train MSE: 0.6582, val MSE: 0.4705

--- Refinement @ gen 500 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 501/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 502/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 503/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 504/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 505/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 506/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 507/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 508/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 509/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 510/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 511/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 512/2000 ▶ train MSE: 0.6578, val MSE: 0.4677
Gen 513/2000 ▶ train MSE: 0.6576, val MSE: 0.4679
Gen 514/2000 ▶ train MSE: 0.6576, val MSE: 0.4679
Gen 515/2000 ▶ train MSE: 0.6576, val MSE: 0.4679
Gen 516/2000 ▶ train MSE: 0.6575, val MSE: 0.4679
Gen 517/2000 ▶ train MSE: 0.6574, val MSE: 0.4675
Gen 518/2000 ▶ train MSE: 0.6572, val MSE: 0.4658
Gen 519/2000 ▶ train MSE: 0.6571, val MSE: 0.4651
Gen 520/2000 ▶ train MSE: 0.6571, val MSE: 0.4651
Gen 521/2000 ▶ train MSE: 0.6568, val MSE: 0.4677
Gen 522/2000 ▶ train MSE: 0.6563, val MSE: 0.4673
Gen 523/2000 ▶ train MSE: 0.6560, val MSE: 0.4652
Gen 524/2000 ▶ train MSE: 0.6557, val MSE: 0.4665
Gen 525/2000 ▶ train MSE: 0.6553, val MSE: 0.4639
Gen 526/2000 ▶ train MSE: 0.6548, val MSE: 0.4630
Gen 527/2000 ▶ train MSE: 0.6548, val MSE: 0.4630
Gen 528/2000 ▶ train MSE: 0.6548, val MSE: 0.4630
Gen 529/2000 ▶ train MSE: 0.6548, val MSE: 0.4630
Gen 530/2000 ▶ train MSE: 0.6544, val MSE: 0.4647
Gen 531/2000 ▶ train MSE: 0.6542, val MSE: 0.4636
Gen 532/2000 ▶ train MSE: 0.6534, val MSE: 0.4624
Gen 533/2000 ▶ train MSE: 0.6534, val MSE: 0.4624
Gen 534/2000 ▶ train MSE: 0.6534, val MSE: 0.4624
Gen 535/2000 ▶ train MSE: 0.6528, val MSE: 0.4613
Gen 536/2000 ▶ train MSE: 0.6528, val MSE: 0.4613
Gen 537/2000 ▶ train MSE: 0.6528, val MSE: 0.4613
Gen 538/2000 ▶ train MSE: 0.6528, val MSE: 0.4613
Gen 539/2000 ▶ train MSE: 0.6524, val MSE: 0.4631
Gen 540/2000 ▶ train MSE: 0.6524, val MSE: 0.4631
Gen 541/2000 ▶ train MSE: 0.6522, val MSE: 0.4589
Gen 542/2000 ▶ train MSE: 0.6522, val MSE: 0.4609
Gen 543/2000 ▶ train MSE: 0.6517, val MSE: 0.4583
Gen 544/2000 ▶ train MSE: 0.6511, val MSE: 0.4596
Gen 545/2000 ▶ train MSE: 0.6511, val MSE: 0.4596
Gen 546/2000 ▶ train MSE: 0.6511, val MSE: 0.4596
Gen 547/2000 ▶ train MSE: 0.6505, val MSE: 0.4580
Gen 548/2000 ▶ train MSE: 0.6505, val MSE: 0.4580
Gen 549/2000 ▶ train MSE: 0.6504, val MSE: 0.4566
Gen 550/2000 ▶ train MSE: 0.6504, val MSE: 0.4566
Gen 551/2000 ▶ train MSE: 0.6504, val MSE: 0.4566
Gen 552/2000 ▶ train MSE: 0.6498, val MSE: 0.4538
Gen 553/2000 ▶ train MSE: 0.6498, val MSE: 0.4538
Gen 554/2000 ▶ train MSE: 0.6498, val MSE: 0.4538
Gen 555/2000 ▶ train MSE: 0.6498, val MSE: 0.4538
Gen 556/2000 ▶ train MSE: 0.6498, val MSE: 0.4538
Gen 557/2000 ▶ train MSE: 0.6498, val MSE: 0.4538
Gen 558/2000 ▶ train MSE: 0.6498, val MSE: 0.4538
Gen 559/2000 ▶ train MSE: 0.6498, val MSE: 0.4538
Gen 560/2000 ▶ train MSE: 0.6493, val MSE: 0.4531
Gen 561/2000 ▶ train MSE: 0.6493, val MSE: 0.4531
Gen 562/2000 ▶ train MSE: 0.6493, val MSE: 0.4531
Gen 563/2000 ▶ train MSE: 0.6493, val MSE: 0.4531
Gen 564/2000 ▶ train MSE: 0.6493, val MSE: 0.4531
Gen 565/2000 ▶ train MSE: 0.6493, val MSE: 0.4531
Gen 566/2000 ▶ train MSE: 0.6492, val MSE: 0.4575
Gen 567/2000 ▶ train MSE: 0.6492, val MSE: 0.4575
Gen 568/2000 ▶ train MSE: 0.6492, val MSE: 0.4575
Gen 569/2000 ▶ train MSE: 0.6492, val MSE: 0.4575
Gen 570/2000 ▶ train MSE: 0.6492, val MSE: 0.4570
Gen 571/2000 ▶ train MSE: 0.6487, val MSE: 0.4568
Gen 572/2000 ▶ train MSE: 0.6487, val MSE: 0.4568
Gen 573/2000 ▶ train MSE: 0.6481, val MSE: 0.4568
Gen 574/2000 ▶ train MSE: 0.6480, val MSE: 0.4557
Gen 575/2000 ▶ train MSE: 0.6476, val MSE: 0.4545
Gen 576/2000 ▶ train MSE: 0.6476, val MSE: 0.4545
Gen 577/2000 ▶ train MSE: 0.6475, val MSE: 0.4547
Gen 578/2000 ▶ train MSE: 0.6474, val MSE: 0.4541
Gen 579/2000 ▶ train MSE: 0.6474, val MSE: 0.4541
Gen 580/2000 ▶ train MSE: 0.6467, val MSE: 0.4534
Gen 581/2000 ▶ train MSE: 0.6467, val MSE: 0.4534
Gen 582/2000 ▶ train MSE: 0.6467, val MSE: 0.4534
Gen 583/2000 ▶ train MSE: 0.6467, val MSE: 0.4534
Gen 584/2000 ▶ train MSE: 0.6467, val MSE: 0.4534
Gen 585/2000 ▶ train MSE: 0.6467, val MSE: 0.4534
Gen 586/2000 ▶ train MSE: 0.6467, val MSE: 0.4534
Gen 587/2000 ▶ train MSE: 0.6464, val MSE: 0.4543
Gen 588/2000 ▶ train MSE: 0.6464, val MSE: 0.4543
Gen 589/2000 ▶ train MSE: 0.6464, val MSE: 0.4543
Gen 590/2000 ▶ train MSE: 0.6464, val MSE: 0.4543
Gen 591/2000 ▶ train MSE: 0.6459, val MSE: 0.4532
Gen 592/2000 ▶ train MSE: 0.6458, val MSE: 0.4536
Gen 593/2000 ▶ train MSE: 0.6458, val MSE: 0.4536
Gen 594/2000 ▶ train MSE: 0.6458, val MSE: 0.4536
Gen 595/2000 ▶ train MSE: 0.6458, val MSE: 0.4537
Gen 596/2000 ▶ train MSE: 0.6451, val MSE: 0.4542
Gen 597/2000 ▶ train MSE: 0.6451, val MSE: 0.4542
Gen 598/2000 ▶ train MSE: 0.6451, val MSE: 0.4542
Gen 599/2000 ▶ train MSE: 0.6451, val MSE: 0.4542
Gen 600/2000 ▶ train MSE: 0.6451, val MSE: 0.4542

--- Refinement @ gen 600 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 601/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 602/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 603/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 604/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 605/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 606/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 607/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 608/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 609/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 610/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 611/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 612/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 613/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 614/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 615/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 616/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 617/2000 ▶ train MSE: 0.6453, val MSE: 0.4521
Gen 618/2000 ▶ train MSE: 0.6452, val MSE: 0.4497
Gen 619/2000 ▶ train MSE: 0.6449, val MSE: 0.4477
Gen 620/2000 ▶ train MSE: 0.6449, val MSE: 0.4477
Gen 621/2000 ▶ train MSE: 0.6449, val MSE: 0.4477
Gen 622/2000 ▶ train MSE: 0.6449, val MSE: 0.4477
Gen 623/2000 ▶ train MSE: 0.6449, val MSE: 0.4477
Gen 624/2000 ▶ train MSE: 0.6448, val MSE: 0.4475
Gen 625/2000 ▶ train MSE: 0.6448, val MSE: 0.4475
Gen 626/2000 ▶ train MSE: 0.6448, val MSE: 0.4475
Gen 627/2000 ▶ train MSE: 0.6448, val MSE: 0.4475
Gen 628/2000 ▶ train MSE: 0.6448, val MSE: 0.4475
Gen 629/2000 ▶ train MSE: 0.6448, val MSE: 0.4475
Gen 630/2000 ▶ train MSE: 0.6448, val MSE: 0.4473
Gen 631/2000 ▶ train MSE: 0.6447, val MSE: 0.4477
Gen 632/2000 ▶ train MSE: 0.6446, val MSE: 0.4467
Gen 633/2000 ▶ train MSE: 0.6446, val MSE: 0.4467
Gen 634/2000 ▶ train MSE: 0.6446, val MSE: 0.4467
Gen 635/2000 ▶ train MSE: 0.6446, val MSE: 0.4467
Gen 636/2000 ▶ train MSE: 0.6445, val MSE: 0.4476
Gen 637/2000 ▶ train MSE: 0.6444, val MSE: 0.4475
Gen 638/2000 ▶ train MSE: 0.6436, val MSE: 0.4472
Gen 639/2000 ▶ train MSE: 0.6436, val MSE: 0.4472
Gen 640/2000 ▶ train MSE: 0.6436, val MSE: 0.4472
Gen 641/2000 ▶ train MSE: 0.6429, val MSE: 0.4449
Gen 642/2000 ▶ train MSE: 0.6429, val MSE: 0.4449
Gen 643/2000 ▶ train MSE: 0.6429, val MSE: 0.4449
Gen 644/2000 ▶ train MSE: 0.6429, val MSE: 0.4449
Gen 645/2000 ▶ train MSE: 0.6428, val MSE: 0.4467
Gen 646/2000 ▶ train MSE: 0.6428, val MSE: 0.4467
Gen 647/2000 ▶ train MSE: 0.6428, val MSE: 0.4467
Gen 648/2000 ▶ train MSE: 0.6427, val MSE: 0.4456
Gen 649/2000 ▶ train MSE: 0.6423, val MSE: 0.4471
Gen 650/2000 ▶ train MSE: 0.6422, val MSE: 0.4449
Gen 651/2000 ▶ train MSE: 0.6422, val MSE: 0.4449
Gen 652/2000 ▶ train MSE: 0.6422, val MSE: 0.4449
Gen 653/2000 ▶ train MSE: 0.6422, val MSE: 0.4449
Gen 654/2000 ▶ train MSE: 0.6419, val MSE: 0.4441
Gen 655/2000 ▶ train MSE: 0.6419, val MSE: 0.4441
Gen 656/2000 ▶ train MSE: 0.6419, val MSE: 0.4441
Gen 657/2000 ▶ train MSE: 0.6414, val MSE: 0.4408
Gen 658/2000 ▶ train MSE: 0.6414, val MSE: 0.4408
Gen 659/2000 ▶ train MSE: 0.6414, val MSE: 0.4408
Gen 660/2000 ▶ train MSE: 0.6414, val MSE: 0.4408
Gen 661/2000 ▶ train MSE: 0.6414, val MSE: 0.4408
Gen 662/2000 ▶ train MSE: 0.6414, val MSE: 0.4408
Gen 663/2000 ▶ train MSE: 0.6414, val MSE: 0.4408
Gen 664/2000 ▶ train MSE: 0.6414, val MSE: 0.4408
Gen 665/2000 ▶ train MSE: 0.6414, val MSE: 0.4413
Gen 666/2000 ▶ train MSE: 0.6414, val MSE: 0.4413
Gen 667/2000 ▶ train MSE: 0.6413, val MSE: 0.4416
Gen 668/2000 ▶ train MSE: 0.6413, val MSE: 0.4416
Gen 669/2000 ▶ train MSE: 0.6413, val MSE: 0.4416
Gen 670/2000 ▶ train MSE: 0.6411, val MSE: 0.4397
Gen 671/2000 ▶ train MSE: 0.6408, val MSE: 0.4434
Gen 672/2000 ▶ train MSE: 0.6408, val MSE: 0.4434
Gen 673/2000 ▶ train MSE: 0.6408, val MSE: 0.4434
Gen 674/2000 ▶ train MSE: 0.6402, val MSE: 0.4398
Gen 675/2000 ▶ train MSE: 0.6402, val MSE: 0.4398
Gen 676/2000 ▶ train MSE: 0.6402, val MSE: 0.4398
Gen 677/2000 ▶ train MSE: 0.6402, val MSE: 0.4398
Gen 678/2000 ▶ train MSE: 0.6402, val MSE: 0.4398
Gen 679/2000 ▶ train MSE: 0.6402, val MSE: 0.4398
Gen 680/2000 ▶ train MSE: 0.6402, val MSE: 0.4399
Gen 681/2000 ▶ train MSE: 0.6402, val MSE: 0.4399
Gen 682/2000 ▶ train MSE: 0.6402, val MSE: 0.4399
Gen 683/2000 ▶ train MSE: 0.6402, val MSE: 0.4384
Gen 684/2000 ▶ train MSE: 0.6396, val MSE: 0.4385
Gen 685/2000 ▶ train MSE: 0.6395, val MSE: 0.4383
Gen 686/2000 ▶ train MSE: 0.6395, val MSE: 0.4383
Gen 687/2000 ▶ train MSE: 0.6395, val MSE: 0.4383
Gen 688/2000 ▶ train MSE: 0.6395, val MSE: 0.4383
Gen 689/2000 ▶ train MSE: 0.6395, val MSE: 0.4383
Gen 690/2000 ▶ train MSE: 0.6395, val MSE: 0.4383
Gen 691/2000 ▶ train MSE: 0.6395, val MSE: 0.4383
Gen 692/2000 ▶ train MSE: 0.6395, val MSE: 0.4383
Gen 693/2000 ▶ train MSE: 0.6395, val MSE: 0.4373
Gen 694/2000 ▶ train MSE: 0.6391, val MSE: 0.4352
Gen 695/2000 ▶ train MSE: 0.6391, val MSE: 0.4352
Gen 696/2000 ▶ train MSE: 0.6391, val MSE: 0.4352
Gen 697/2000 ▶ train MSE: 0.6385, val MSE: 0.4364
Gen 698/2000 ▶ train MSE: 0.6385, val MSE: 0.4364
Gen 699/2000 ▶ train MSE: 0.6385, val MSE: 0.4364
Gen 700/2000 ▶ train MSE: 0.6385, val MSE: 0.4364

--- Refinement @ gen 700 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 701/2000 ▶ train MSE: 0.6397, val MSE: 0.4369
Gen 702/2000 ▶ train MSE: 0.6397, val MSE: 0.4369
Gen 703/2000 ▶ train MSE: 0.6397, val MSE: 0.4369
Gen 704/2000 ▶ train MSE: 0.6397, val MSE: 0.4369
Gen 705/2000 ▶ train MSE: 0.6397, val MSE: 0.4369
Gen 706/2000 ▶ train MSE: 0.6397, val MSE: 0.4369
Gen 707/2000 ▶ train MSE: 0.6397, val MSE: 0.4369
Gen 708/2000 ▶ train MSE: 0.6397, val MSE: 0.4369
Gen 709/2000 ▶ train MSE: 0.6396, val MSE: 0.4361
Gen 710/2000 ▶ train MSE: 0.6396, val MSE: 0.4361
Gen 711/2000 ▶ train MSE: 0.6393, val MSE: 0.4383
Gen 712/2000 ▶ train MSE: 0.6383, val MSE: 0.4364
Gen 713/2000 ▶ train MSE: 0.6383, val MSE: 0.4364
Gen 714/2000 ▶ train MSE: 0.6383, val MSE: 0.4364
Gen 715/2000 ▶ train MSE: 0.6383, val MSE: 0.4364
Gen 716/2000 ▶ train MSE: 0.6383, val MSE: 0.4364
Gen 717/2000 ▶ train MSE: 0.6379, val MSE: 0.4363
Gen 718/2000 ▶ train MSE: 0.6379, val MSE: 0.4363
Gen 719/2000 ▶ train MSE: 0.6379, val MSE: 0.4363
Gen 720/2000 ▶ train MSE: 0.6379, val MSE: 0.4363
Gen 721/2000 ▶ train MSE: 0.6379, val MSE: 0.4363
Gen 722/2000 ▶ train MSE: 0.6379, val MSE: 0.4363
Gen 723/2000 ▶ train MSE: 0.6379, val MSE: 0.4363
Gen 724/2000 ▶ train MSE: 0.6379, val MSE: 0.4363
Gen 725/2000 ▶ train MSE: 0.6375, val MSE: 0.4348
Gen 726/2000 ▶ train MSE: 0.6368, val MSE: 0.4331
Gen 727/2000 ▶ train MSE: 0.6368, val MSE: 0.4331
Gen 728/2000 ▶ train MSE: 0.6368, val MSE: 0.4331
Gen 729/2000 ▶ train MSE: 0.6367, val MSE: 0.4332
Gen 730/2000 ▶ train MSE: 0.6367, val MSE: 0.4332
Gen 731/2000 ▶ train MSE: 0.6367, val MSE: 0.4332
Gen 732/2000 ▶ train MSE: 0.6367, val MSE: 0.4332
Gen 733/2000 ▶ train MSE: 0.6367, val MSE: 0.4332
Gen 734/2000 ▶ train MSE: 0.6365, val MSE: 0.4326
Gen 735/2000 ▶ train MSE: 0.6365, val MSE: 0.4326
Gen 736/2000 ▶ train MSE: 0.6362, val MSE: 0.4327
Gen 737/2000 ▶ train MSE: 0.6362, val MSE: 0.4327
Gen 738/2000 ▶ train MSE: 0.6362, val MSE: 0.4327
Gen 739/2000 ▶ train MSE: 0.6362, val MSE: 0.4327
Gen 740/2000 ▶ train MSE: 0.6362, val MSE: 0.4327
Gen 741/2000 ▶ train MSE: 0.6353, val MSE: 0.4296
Gen 742/2000 ▶ train MSE: 0.6353, val MSE: 0.4296
Gen 743/2000 ▶ train MSE: 0.6353, val MSE: 0.4296
Gen 744/2000 ▶ train MSE: 0.6353, val MSE: 0.4296
Gen 745/2000 ▶ train MSE: 0.6353, val MSE: 0.4296
Gen 746/2000 ▶ train MSE: 0.6353, val MSE: 0.4296
Gen 747/2000 ▶ train MSE: 0.6353, val MSE: 0.4296
Gen 748/2000 ▶ train MSE: 0.6353, val MSE: 0.4296
Gen 749/2000 ▶ train MSE: 0.6353, val MSE: 0.4296
Gen 750/2000 ▶ train MSE: 0.6351, val MSE: 0.4293
Gen 751/2000 ▶ train MSE: 0.6349, val MSE: 0.4310
Gen 752/2000 ▶ train MSE: 0.6349, val MSE: 0.4310
Gen 753/2000 ▶ train MSE: 0.6349, val MSE: 0.4310
Gen 754/2000 ▶ train MSE: 0.6349, val MSE: 0.4310
Gen 755/2000 ▶ train MSE: 0.6349, val MSE: 0.4310
Gen 756/2000 ▶ train MSE: 0.6349, val MSE: 0.4310
Gen 757/2000 ▶ train MSE: 0.6349, val MSE: 0.4310
Gen 758/2000 ▶ train MSE: 0.6349, val MSE: 0.4310
Gen 759/2000 ▶ train MSE: 0.6345, val MSE: 0.4305
Gen 760/2000 ▶ train MSE: 0.6343, val MSE: 0.4315
Gen 761/2000 ▶ train MSE: 0.6340, val MSE: 0.4290
Gen 762/2000 ▶ train MSE: 0.6340, val MSE: 0.4301
Gen 763/2000 ▶ train MSE: 0.6340, val MSE: 0.4301
Gen 764/2000 ▶ train MSE: 0.6340, val MSE: 0.4301
Gen 765/2000 ▶ train MSE: 0.6339, val MSE: 0.4306
Gen 766/2000 ▶ train MSE: 0.6338, val MSE: 0.4289
Gen 767/2000 ▶ train MSE: 0.6336, val MSE: 0.4301
Gen 768/2000 ▶ train MSE: 0.6332, val MSE: 0.4292
Gen 769/2000 ▶ train MSE: 0.6332, val MSE: 0.4292
Gen 770/2000 ▶ train MSE: 0.6332, val MSE: 0.4292
Gen 771/2000 ▶ train MSE: 0.6332, val MSE: 0.4292
Gen 772/2000 ▶ train MSE: 0.6332, val MSE: 0.4292
Gen 773/2000 ▶ train MSE: 0.6328, val MSE: 0.4284
Gen 774/2000 ▶ train MSE: 0.6328, val MSE: 0.4284
Gen 775/2000 ▶ train MSE: 0.6328, val MSE: 0.4284
Gen 776/2000 ▶ train MSE: 0.6328, val MSE: 0.4284
Gen 777/2000 ▶ train MSE: 0.6328, val MSE: 0.4284
Gen 778/2000 ▶ train MSE: 0.6328, val MSE: 0.4284
Gen 779/2000 ▶ train MSE: 0.6327, val MSE: 0.4270
Gen 780/2000 ▶ train MSE: 0.6327, val MSE: 0.4270
Gen 781/2000 ▶ train MSE: 0.6327, val MSE: 0.4270
Gen 782/2000 ▶ train MSE: 0.6327, val MSE: 0.4270
Gen 783/2000 ▶ train MSE: 0.6327, val MSE: 0.4270
Gen 784/2000 ▶ train MSE: 0.6327, val MSE: 0.4270
Gen 785/2000 ▶ train MSE: 0.6325, val MSE: 0.4274
Gen 786/2000 ▶ train MSE: 0.6325, val MSE: 0.4274
Gen 787/2000 ▶ train MSE: 0.6323, val MSE: 0.4264
Gen 788/2000 ▶ train MSE: 0.6323, val MSE: 0.4264
Gen 789/2000 ▶ train MSE: 0.6323, val MSE: 0.4264
Gen 790/2000 ▶ train MSE: 0.6323, val MSE: 0.4264
Gen 791/2000 ▶ train MSE: 0.6323, val MSE: 0.4264
Gen 792/2000 ▶ train MSE: 0.6323, val MSE: 0.4264
Gen 793/2000 ▶ train MSE: 0.6323, val MSE: 0.4264
Gen 794/2000 ▶ train MSE: 0.6323, val MSE: 0.4264
Gen 795/2000 ▶ train MSE: 0.6323, val MSE: 0.4264
Gen 796/2000 ▶ train MSE: 0.6323, val MSE: 0.4264
Gen 797/2000 ▶ train MSE: 0.6323, val MSE: 0.4264
Gen 798/2000 ▶ train MSE: 0.6321, val MSE: 0.4276
Gen 799/2000 ▶ train MSE: 0.6321, val MSE: 0.4276
Gen 800/2000 ▶ train MSE: 0.6321, val MSE: 0.4276

--- Refinement @ gen 800 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 801/2000 ▶ train MSE: 0.6339, val MSE: 0.4275
Gen 802/2000 ▶ train MSE: 0.6330, val MSE: 0.4298
Gen 803/2000 ▶ train MSE: 0.6330, val MSE: 0.4298
Gen 804/2000 ▶ train MSE: 0.6330, val MSE: 0.4298
Gen 805/2000 ▶ train MSE: 0.6330, val MSE: 0.4298
Gen 806/2000 ▶ train MSE: 0.6330, val MSE: 0.4298
Gen 807/2000 ▶ train MSE: 0.6330, val MSE: 0.4298
Gen 808/2000 ▶ train MSE: 0.6330, val MSE: 0.4298
Gen 809/2000 ▶ train MSE: 0.6330, val MSE: 0.4298
Gen 810/2000 ▶ train MSE: 0.6330, val MSE: 0.4298
Gen 811/2000 ▶ train MSE: 0.6330, val MSE: 0.4298
Gen 812/2000 ▶ train MSE: 0.6330, val MSE: 0.4269
Gen 813/2000 ▶ train MSE: 0.6329, val MSE: 0.4285
Gen 814/2000 ▶ train MSE: 0.6329, val MSE: 0.4285
Gen 815/2000 ▶ train MSE: 0.6329, val MSE: 0.4285
Gen 816/2000 ▶ train MSE: 0.6329, val MSE: 0.4285
Gen 817/2000 ▶ train MSE: 0.6329, val MSE: 0.4285
Gen 818/2000 ▶ train MSE: 0.6328, val MSE: 0.4273
Gen 819/2000 ▶ train MSE: 0.6328, val MSE: 0.4273
Gen 820/2000 ▶ train MSE: 0.6328, val MSE: 0.4273
Gen 821/2000 ▶ train MSE: 0.6326, val MSE: 0.4282
Gen 822/2000 ▶ train MSE: 0.6326, val MSE: 0.4282
Gen 823/2000 ▶ train MSE: 0.6326, val MSE: 0.4282
Gen 824/2000 ▶ train MSE: 0.6326, val MSE: 0.4282
Gen 825/2000 ▶ train MSE: 0.6326, val MSE: 0.4282
Gen 826/2000 ▶ train MSE: 0.6317, val MSE: 0.4284
Gen 827/2000 ▶ train MSE: 0.6317, val MSE: 0.4284
Gen 828/2000 ▶ train MSE: 0.6317, val MSE: 0.4284
Gen 829/2000 ▶ train MSE: 0.6317, val MSE: 0.4284
Gen 830/2000 ▶ train MSE: 0.6317, val MSE: 0.4284
Gen 831/2000 ▶ train MSE: 0.6317, val MSE: 0.4284
Gen 832/2000 ▶ train MSE: 0.6317, val MSE: 0.4265
Gen 833/2000 ▶ train MSE: 0.6312, val MSE: 0.4274
Gen 834/2000 ▶ train MSE: 0.6312, val MSE: 0.4274
Gen 835/2000 ▶ train MSE: 0.6312, val MSE: 0.4269
Gen 836/2000 ▶ train MSE: 0.6310, val MSE: 0.4254
Gen 837/2000 ▶ train MSE: 0.6310, val MSE: 0.4254
Gen 838/2000 ▶ train MSE: 0.6310, val MSE: 0.4254
Gen 839/2000 ▶ train MSE: 0.6310, val MSE: 0.4254
Gen 840/2000 ▶ train MSE: 0.6307, val MSE: 0.4256
Gen 841/2000 ▶ train MSE: 0.6306, val MSE: 0.4241
Gen 842/2000 ▶ train MSE: 0.6304, val MSE: 0.4269
Gen 843/2000 ▶ train MSE: 0.6304, val MSE: 0.4269
Gen 844/2000 ▶ train MSE: 0.6304, val MSE: 0.4269
Gen 845/2000 ▶ train MSE: 0.6304, val MSE: 0.4269
Gen 846/2000 ▶ train MSE: 0.6304, val MSE: 0.4269
Gen 847/2000 ▶ train MSE: 0.6304, val MSE: 0.4269
Gen 848/2000 ▶ train MSE: 0.6304, val MSE: 0.4269
Gen 849/2000 ▶ train MSE: 0.6304, val MSE: 0.4269
Gen 850/2000 ▶ train MSE: 0.6304, val MSE: 0.4269
Gen 851/2000 ▶ train MSE: 0.6304, val MSE: 0.4269
Gen 852/2000 ▶ train MSE: 0.6303, val MSE: 0.4249
Gen 853/2000 ▶ train MSE: 0.6303, val MSE: 0.4249
Gen 854/2000 ▶ train MSE: 0.6303, val MSE: 0.4249
Gen 855/2000 ▶ train MSE: 0.6302, val MSE: 0.4231
Gen 856/2000 ▶ train MSE: 0.6302, val MSE: 0.4231
Gen 857/2000 ▶ train MSE: 0.6302, val MSE: 0.4244
Gen 858/2000 ▶ train MSE: 0.6298, val MSE: 0.4251
Gen 859/2000 ▶ train MSE: 0.6298, val MSE: 0.4251
Gen 860/2000 ▶ train MSE: 0.6298, val MSE: 0.4251
Gen 861/2000 ▶ train MSE: 0.6298, val MSE: 0.4251
Gen 862/2000 ▶ train MSE: 0.6298, val MSE: 0.4251
Gen 863/2000 ▶ train MSE: 0.6298, val MSE: 0.4251
Gen 864/2000 ▶ train MSE: 0.6298, val MSE: 0.4251
Gen 865/2000 ▶ train MSE: 0.6296, val MSE: 0.4240
Gen 866/2000 ▶ train MSE: 0.6296, val MSE: 0.4240
Gen 867/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 868/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 869/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 870/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 871/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 872/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 873/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 874/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 875/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 876/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 877/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 878/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 879/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 880/2000 ▶ train MSE: 0.6291, val MSE: 0.4223
Gen 881/2000 ▶ train MSE: 0.6290, val MSE: 0.4241
Gen 882/2000 ▶ train MSE: 0.6290, val MSE: 0.4241
Gen 883/2000 ▶ train MSE: 0.6290, val MSE: 0.4233
Gen 884/2000 ▶ train MSE: 0.6290, val MSE: 0.4233
Gen 885/2000 ▶ train MSE: 0.6289, val MSE: 0.4243
Gen 886/2000 ▶ train MSE: 0.6286, val MSE: 0.4227
Gen 887/2000 ▶ train MSE: 0.6286, val MSE: 0.4215
Gen 888/2000 ▶ train MSE: 0.6285, val MSE: 0.4227
Gen 889/2000 ▶ train MSE: 0.6283, val MSE: 0.4224
Gen 890/2000 ▶ train MSE: 0.6283, val MSE: 0.4234
Gen 891/2000 ▶ train MSE: 0.6280, val MSE: 0.4217
Gen 892/2000 ▶ train MSE: 0.6280, val MSE: 0.4219
Gen 893/2000 ▶ train MSE: 0.6280, val MSE: 0.4219
Gen 894/2000 ▶ train MSE: 0.6280, val MSE: 0.4219
Gen 895/2000 ▶ train MSE: 0.6277, val MSE: 0.4208
Gen 896/2000 ▶ train MSE: 0.6277, val MSE: 0.4208
Gen 897/2000 ▶ train MSE: 0.6277, val MSE: 0.4208
Gen 898/2000 ▶ train MSE: 0.6276, val MSE: 0.4192
Gen 899/2000 ▶ train MSE: 0.6276, val MSE: 0.4192
Gen 900/2000 ▶ train MSE: 0.6275, val MSE: 0.4203

--- Refinement @ gen 900 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 901/2000 ▶ train MSE: 0.6283, val MSE: 0.4216
Gen 902/2000 ▶ train MSE: 0.6283, val MSE: 0.4216
Gen 903/2000 ▶ train MSE: 0.6283, val MSE: 0.4216
Gen 904/2000 ▶ train MSE: 0.6283, val MSE: 0.4210
Gen 905/2000 ▶ train MSE: 0.6281, val MSE: 0.4208
Gen 906/2000 ▶ train MSE: 0.6281, val MSE: 0.4208
Gen 907/2000 ▶ train MSE: 0.6280, val MSE: 0.4192
Gen 908/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 909/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 910/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 911/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 912/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 913/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 914/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 915/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 916/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 917/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 918/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 919/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 920/2000 ▶ train MSE: 0.6274, val MSE: 0.4200
Gen 921/2000 ▶ train MSE: 0.6274, val MSE: 0.4201
Gen 922/2000 ▶ train MSE: 0.6274, val MSE: 0.4201
Gen 923/2000 ▶ train MSE: 0.6274, val MSE: 0.4201
Gen 924/2000 ▶ train MSE: 0.6274, val MSE: 0.4201
Gen 925/2000 ▶ train MSE: 0.6274, val MSE: 0.4201
Gen 926/2000 ▶ train MSE: 0.6268, val MSE: 0.4174
Gen 927/2000 ▶ train MSE: 0.6268, val MSE: 0.4174
Gen 928/2000 ▶ train MSE: 0.6268, val MSE: 0.4174
Gen 929/2000 ▶ train MSE: 0.6268, val MSE: 0.4174
Gen 930/2000 ▶ train MSE: 0.6268, val MSE: 0.4174
Gen 931/2000 ▶ train MSE: 0.6268, val MSE: 0.4174
Gen 932/2000 ▶ train MSE: 0.6268, val MSE: 0.4174
Gen 933/2000 ▶ train MSE: 0.6268, val MSE: 0.4174
Gen 934/2000 ▶ train MSE: 0.6266, val MSE: 0.4178
Gen 935/2000 ▶ train MSE: 0.6266, val MSE: 0.4178
Gen 936/2000 ▶ train MSE: 0.6265, val MSE: 0.4169
Gen 937/2000 ▶ train MSE: 0.6265, val MSE: 0.4175
Gen 938/2000 ▶ train MSE: 0.6265, val MSE: 0.4175
Gen 939/2000 ▶ train MSE: 0.6265, val MSE: 0.4175
Gen 940/2000 ▶ train MSE: 0.6265, val MSE: 0.4175
Gen 941/2000 ▶ train MSE: 0.6259, val MSE: 0.4169
Gen 942/2000 ▶ train MSE: 0.6259, val MSE: 0.4169
Gen 943/2000 ▶ train MSE: 0.6259, val MSE: 0.4169
Gen 944/2000 ▶ train MSE: 0.6259, val MSE: 0.4169
Gen 945/2000 ▶ train MSE: 0.6259, val MSE: 0.4169
Gen 946/2000 ▶ train MSE: 0.6259, val MSE: 0.4169
Gen 947/2000 ▶ train MSE: 0.6259, val MSE: 0.4169
Gen 948/2000 ▶ train MSE: 0.6259, val MSE: 0.4169
Gen 949/2000 ▶ train MSE: 0.6259, val MSE: 0.4169
Gen 950/2000 ▶ train MSE: 0.6259, val MSE: 0.4169
Gen 951/2000 ▶ train MSE: 0.6259, val MSE: 0.4169
Gen 952/2000 ▶ train MSE: 0.6256, val MSE: 0.4160
Gen 953/2000 ▶ train MSE: 0.6256, val MSE: 0.4160
Gen 954/2000 ▶ train MSE: 0.6256, val MSE: 0.4160
Gen 955/2000 ▶ train MSE: 0.6256, val MSE: 0.4160
Gen 956/2000 ▶ train MSE: 0.6256, val MSE: 0.4160
Gen 957/2000 ▶ train MSE: 0.6256, val MSE: 0.4160
Gen 958/2000 ▶ train MSE: 0.6256, val MSE: 0.4160
Gen 959/2000 ▶ train MSE: 0.6256, val MSE: 0.4160
Gen 960/2000 ▶ train MSE: 0.6256, val MSE: 0.4160
Gen 961/2000 ▶ train MSE: 0.6252, val MSE: 0.4155
Gen 962/2000 ▶ train MSE: 0.6252, val MSE: 0.4155
Gen 963/2000 ▶ train MSE: 0.6252, val MSE: 0.4155
Gen 964/2000 ▶ train MSE: 0.6252, val MSE: 0.4155
Gen 965/2000 ▶ train MSE: 0.6252, val MSE: 0.4155
Gen 966/2000 ▶ train MSE: 0.6252, val MSE: 0.4155
Gen 967/2000 ▶ train MSE: 0.6252, val MSE: 0.4155
Gen 968/2000 ▶ train MSE: 0.6252, val MSE: 0.4155
Gen 969/2000 ▶ train MSE: 0.6252, val MSE: 0.4155
Gen 970/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 971/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 972/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 973/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 974/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 975/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 976/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 977/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 978/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 979/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 980/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 981/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 982/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 983/2000 ▶ train MSE: 0.6250, val MSE: 0.4145
Gen 984/2000 ▶ train MSE: 0.6250, val MSE: 0.4136
Gen 985/2000 ▶ train MSE: 0.6250, val MSE: 0.4136
Gen 986/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 987/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 988/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 989/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 990/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 991/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 992/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 993/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 994/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 995/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 996/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 997/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 998/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 999/2000 ▶ train MSE: 0.6249, val MSE: 0.4141
Gen 1000/2000 ▶ train MSE: 0.6249, val MSE: 0.4131

--- Refinement @ gen 1000 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1001/2000 ▶ train MSE: 0.6260, val MSE: 0.4131
Gen 1002/2000 ▶ train MSE: 0.6260, val MSE: 0.4131
Gen 1003/2000 ▶ train MSE: 0.6260, val MSE: 0.4131
Gen 1004/2000 ▶ train MSE: 0.6260, val MSE: 0.4131
Gen 1005/2000 ▶ train MSE: 0.6260, val MSE: 0.4131
Gen 1006/2000 ▶ train MSE: 0.6260, val MSE: 0.4131
Gen 1007/2000 ▶ train MSE: 0.6260, val MSE: 0.4131
Gen 1008/2000 ▶ train MSE: 0.6260, val MSE: 0.4131
Gen 1009/2000 ▶ train MSE: 0.6256, val MSE: 0.4119
Gen 1010/2000 ▶ train MSE: 0.6256, val MSE: 0.4119
Gen 1011/2000 ▶ train MSE: 0.6256, val MSE: 0.4119
Gen 1012/2000 ▶ train MSE: 0.6256, val MSE: 0.4119
Gen 1013/2000 ▶ train MSE: 0.6256, val MSE: 0.4119
Gen 1014/2000 ▶ train MSE: 0.6256, val MSE: 0.4119
Gen 1015/2000 ▶ train MSE: 0.6256, val MSE: 0.4119
Gen 1016/2000 ▶ train MSE: 0.6256, val MSE: 0.4119
Gen 1017/2000 ▶ train MSE: 0.6256, val MSE: 0.4119
Gen 1018/2000 ▶ train MSE: 0.6255, val MSE: 0.4121
Gen 1019/2000 ▶ train MSE: 0.6255, val MSE: 0.4121
Gen 1020/2000 ▶ train MSE: 0.6255, val MSE: 0.4121
Gen 1021/2000 ▶ train MSE: 0.6254, val MSE: 0.4124
Gen 1022/2000 ▶ train MSE: 0.6254, val MSE: 0.4124
Gen 1023/2000 ▶ train MSE: 0.6254, val MSE: 0.4124
Gen 1024/2000 ▶ train MSE: 0.6254, val MSE: 0.4124
Gen 1025/2000 ▶ train MSE: 0.6254, val MSE: 0.4124
Gen 1026/2000 ▶ train MSE: 0.6254, val MSE: 0.4124
Gen 1027/2000 ▶ train MSE: 0.6253, val MSE: 0.4117
Gen 1028/2000 ▶ train MSE: 0.6251, val MSE: 0.4133
Gen 1029/2000 ▶ train MSE: 0.6251, val MSE: 0.4133
Gen 1030/2000 ▶ train MSE: 0.6251, val MSE: 0.4133
Gen 1031/2000 ▶ train MSE: 0.6251, val MSE: 0.4133
Gen 1032/2000 ▶ train MSE: 0.6251, val MSE: 0.4133
Gen 1033/2000 ▶ train MSE: 0.6251, val MSE: 0.4133
Gen 1034/2000 ▶ train MSE: 0.6250, val MSE: 0.4124
Gen 1035/2000 ▶ train MSE: 0.6250, val MSE: 0.4130
Gen 1036/2000 ▶ train MSE: 0.6250, val MSE: 0.4130
Gen 1037/2000 ▶ train MSE: 0.6250, val MSE: 0.4130
Gen 1038/2000 ▶ train MSE: 0.6250, val MSE: 0.4130
Gen 1039/2000 ▶ train MSE: 0.6248, val MSE: 0.4112
Gen 1040/2000 ▶ train MSE: 0.6247, val MSE: 0.4108
Gen 1041/2000 ▶ train MSE: 0.6247, val MSE: 0.4108
Gen 1042/2000 ▶ train MSE: 0.6247, val MSE: 0.4115
Gen 1043/2000 ▶ train MSE: 0.6243, val MSE: 0.4112
Gen 1044/2000 ▶ train MSE: 0.6242, val MSE: 0.4107
Gen 1045/2000 ▶ train MSE: 0.6240, val MSE: 0.4100
Gen 1046/2000 ▶ train MSE: 0.6238, val MSE: 0.4101
Gen 1047/2000 ▶ train MSE: 0.6238, val MSE: 0.4101
Gen 1048/2000 ▶ train MSE: 0.6237, val MSE: 0.4100
Gen 1049/2000 ▶ train MSE: 0.6235, val MSE: 0.4103
Gen 1050/2000 ▶ train MSE: 0.6234, val MSE: 0.4085
Gen 1051/2000 ▶ train MSE: 0.6234, val MSE: 0.4085
Gen 1052/2000 ▶ train MSE: 0.6234, val MSE: 0.4085
Gen 1053/2000 ▶ train MSE: 0.6234, val MSE: 0.4088
Gen 1054/2000 ▶ train MSE: 0.6233, val MSE: 0.4084
Gen 1055/2000 ▶ train MSE: 0.6230, val MSE: 0.4085
Gen 1056/2000 ▶ train MSE: 0.6229, val MSE: 0.4072
Gen 1057/2000 ▶ train MSE: 0.6229, val MSE: 0.4072
Gen 1058/2000 ▶ train MSE: 0.6228, val MSE: 0.4087
Gen 1059/2000 ▶ train MSE: 0.6228, val MSE: 0.4087
Gen 1060/2000 ▶ train MSE: 0.6228, val MSE: 0.4087
Gen 1061/2000 ▶ train MSE: 0.6227, val MSE: 0.4087
Gen 1062/2000 ▶ train MSE: 0.6227, val MSE: 0.4087
Gen 1063/2000 ▶ train MSE: 0.6227, val MSE: 0.4087
Gen 1064/2000 ▶ train MSE: 0.6227, val MSE: 0.4087
Gen 1065/2000 ▶ train MSE: 0.6227, val MSE: 0.4087
Gen 1066/2000 ▶ train MSE: 0.6226, val MSE: 0.4069
Gen 1067/2000 ▶ train MSE: 0.6226, val MSE: 0.4069
Gen 1068/2000 ▶ train MSE: 0.6226, val MSE: 0.4069
Gen 1069/2000 ▶ train MSE: 0.6226, val MSE: 0.4069
Gen 1070/2000 ▶ train MSE: 0.6222, val MSE: 0.4069
Gen 1071/2000 ▶ train MSE: 0.6222, val MSE: 0.4069
Gen 1072/2000 ▶ train MSE: 0.6222, val MSE: 0.4069
Gen 1073/2000 ▶ train MSE: 0.6222, val MSE: 0.4069
Gen 1074/2000 ▶ train MSE: 0.6222, val MSE: 0.4069
Gen 1075/2000 ▶ train MSE: 0.6222, val MSE: 0.4069
Gen 1076/2000 ▶ train MSE: 0.6222, val MSE: 0.4069
Gen 1077/2000 ▶ train MSE: 0.6222, val MSE: 0.4069
Gen 1078/2000 ▶ train MSE: 0.6222, val MSE: 0.4069
Gen 1079/2000 ▶ train MSE: 0.6222, val MSE: 0.4069
Gen 1080/2000 ▶ train MSE: 0.6222, val MSE: 0.4077
Gen 1081/2000 ▶ train MSE: 0.6222, val MSE: 0.4078
Gen 1082/2000 ▶ train MSE: 0.6222, val MSE: 0.4078
Gen 1083/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1084/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1085/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1086/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1087/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1088/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1089/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1090/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1091/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1092/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1093/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1094/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1095/2000 ▶ train MSE: 0.6218, val MSE: 0.4074
Gen 1096/2000 ▶ train MSE: 0.6217, val MSE: 0.4062
Gen 1097/2000 ▶ train MSE: 0.6217, val MSE: 0.4062
Gen 1098/2000 ▶ train MSE: 0.6217, val MSE: 0.4062
Gen 1099/2000 ▶ train MSE: 0.6217, val MSE: 0.4062
Gen 1100/2000 ▶ train MSE: 0.6217, val MSE: 0.4062

--- Refinement @ gen 1100 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1101/2000 ▶ train MSE: 0.6221, val MSE: 0.4066
Gen 1102/2000 ▶ train MSE: 0.6221, val MSE: 0.4066
Gen 1103/2000 ▶ train MSE: 0.6221, val MSE: 0.4066
Gen 1104/2000 ▶ train MSE: 0.6220, val MSE: 0.4069
Gen 1105/2000 ▶ train MSE: 0.6220, val MSE: 0.4069
Gen 1106/2000 ▶ train MSE: 0.6220, val MSE: 0.4069
Gen 1107/2000 ▶ train MSE: 0.6220, val MSE: 0.4069
Gen 1108/2000 ▶ train MSE: 0.6220, val MSE: 0.4069
Gen 1109/2000 ▶ train MSE: 0.6219, val MSE: 0.4067
Gen 1110/2000 ▶ train MSE: 0.6219, val MSE: 0.4067
Gen 1111/2000 ▶ train MSE: 0.6219, val MSE: 0.4067
Gen 1112/2000 ▶ train MSE: 0.6219, val MSE: 0.4067
Gen 1113/2000 ▶ train MSE: 0.6219, val MSE: 0.4067
Gen 1114/2000 ▶ train MSE: 0.6219, val MSE: 0.4067
Gen 1115/2000 ▶ train MSE: 0.6219, val MSE: 0.4067
Gen 1116/2000 ▶ train MSE: 0.6219, val MSE: 0.4067
Gen 1117/2000 ▶ train MSE: 0.6219, val MSE: 0.4075
Gen 1118/2000 ▶ train MSE: 0.6219, val MSE: 0.4075
Gen 1119/2000 ▶ train MSE: 0.6219, val MSE: 0.4075
Gen 1120/2000 ▶ train MSE: 0.6219, val MSE: 0.4075
Gen 1121/2000 ▶ train MSE: 0.6219, val MSE: 0.4075
Gen 1122/2000 ▶ train MSE: 0.6219, val MSE: 0.4075
Gen 1123/2000 ▶ train MSE: 0.6217, val MSE: 0.4068
Gen 1124/2000 ▶ train MSE: 0.6217, val MSE: 0.4068
Gen 1125/2000 ▶ train MSE: 0.6217, val MSE: 0.4068
Gen 1126/2000 ▶ train MSE: 0.6217, val MSE: 0.4068
Gen 1127/2000 ▶ train MSE: 0.6217, val MSE: 0.4068
Gen 1128/2000 ▶ train MSE: 0.6214, val MSE: 0.4074
Gen 1129/2000 ▶ train MSE: 0.6214, val MSE: 0.4074
Gen 1130/2000 ▶ train MSE: 0.6214, val MSE: 0.4074
Gen 1131/2000 ▶ train MSE: 0.6214, val MSE: 0.4074
Gen 1132/2000 ▶ train MSE: 0.6214, val MSE: 0.4074
Gen 1133/2000 ▶ train MSE: 0.6214, val MSE: 0.4074
Gen 1134/2000 ▶ train MSE: 0.6214, val MSE: 0.4074
Gen 1135/2000 ▶ train MSE: 0.6214, val MSE: 0.4074
Gen 1136/2000 ▶ train MSE: 0.6214, val MSE: 0.4074
Gen 1137/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1138/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1139/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1140/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1141/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1142/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1143/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1144/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1145/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1146/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1147/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1148/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1149/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1150/2000 ▶ train MSE: 0.6212, val MSE: 0.4061
Gen 1151/2000 ▶ train MSE: 0.6212, val MSE: 0.4073
Gen 1152/2000 ▶ train MSE: 0.6211, val MSE: 0.4066
Gen 1153/2000 ▶ train MSE: 0.6211, val MSE: 0.4066
Gen 1154/2000 ▶ train MSE: 0.6211, val MSE: 0.4066
Gen 1155/2000 ▶ train MSE: 0.6211, val MSE: 0.4066
Gen 1156/2000 ▶ train MSE: 0.6211, val MSE: 0.4068
Gen 1157/2000 ▶ train MSE: 0.6211, val MSE: 0.4068
Gen 1158/2000 ▶ train MSE: 0.6211, val MSE: 0.4068
Gen 1159/2000 ▶ train MSE: 0.6211, val MSE: 0.4068
Gen 1160/2000 ▶ train MSE: 0.6210, val MSE: 0.4052
Gen 1161/2000 ▶ train MSE: 0.6210, val MSE: 0.4052
Gen 1162/2000 ▶ train MSE: 0.6209, val MSE: 0.4061
Gen 1163/2000 ▶ train MSE: 0.6209, val MSE: 0.4061
Gen 1164/2000 ▶ train MSE: 0.6208, val MSE: 0.4067
Gen 1165/2000 ▶ train MSE: 0.6208, val MSE: 0.4067
Gen 1166/2000 ▶ train MSE: 0.6208, val MSE: 0.4070
Gen 1167/2000 ▶ train MSE: 0.6208, val MSE: 0.4070
Gen 1168/2000 ▶ train MSE: 0.6208, val MSE: 0.4070
Gen 1169/2000 ▶ train MSE: 0.6208, val MSE: 0.4070
Gen 1170/2000 ▶ train MSE: 0.6208, val MSE: 0.4062
Gen 1171/2000 ▶ train MSE: 0.6208, val MSE: 0.4062
Gen 1172/2000 ▶ train MSE: 0.6203, val MSE: 0.4066
Gen 1173/2000 ▶ train MSE: 0.6203, val MSE: 0.4066
Gen 1174/2000 ▶ train MSE: 0.6203, val MSE: 0.4066
Gen 1175/2000 ▶ train MSE: 0.6203, val MSE: 0.4066
Gen 1176/2000 ▶ train MSE: 0.6203, val MSE: 0.4066
Gen 1177/2000 ▶ train MSE: 0.6203, val MSE: 0.4066
Gen 1178/2000 ▶ train MSE: 0.6203, val MSE: 0.4066
Gen 1179/2000 ▶ train MSE: 0.6203, val MSE: 0.4052
Gen 1180/2000 ▶ train MSE: 0.6203, val MSE: 0.4052
Gen 1181/2000 ▶ train MSE: 0.6203, val MSE: 0.4052
Gen 1182/2000 ▶ train MSE: 0.6203, val MSE: 0.4052
Gen 1183/2000 ▶ train MSE: 0.6203, val MSE: 0.4052
Gen 1184/2000 ▶ train MSE: 0.6203, val MSE: 0.4052
Gen 1185/2000 ▶ train MSE: 0.6201, val MSE: 0.4041
Gen 1186/2000 ▶ train MSE: 0.6201, val MSE: 0.4041
Gen 1187/2000 ▶ train MSE: 0.6200, val MSE: 0.4051
Gen 1188/2000 ▶ train MSE: 0.6200, val MSE: 0.4051
Gen 1189/2000 ▶ train MSE: 0.6200, val MSE: 0.4051
Gen 1190/2000 ▶ train MSE: 0.6196, val MSE: 0.4040
Gen 1191/2000 ▶ train MSE: 0.6196, val MSE: 0.4040
Gen 1192/2000 ▶ train MSE: 0.6196, val MSE: 0.4040
Gen 1193/2000 ▶ train MSE: 0.6196, val MSE: 0.4040
Gen 1194/2000 ▶ train MSE: 0.6196, val MSE: 0.4040
Gen 1195/2000 ▶ train MSE: 0.6196, val MSE: 0.4040
Gen 1196/2000 ▶ train MSE: 0.6196, val MSE: 0.4040
Gen 1197/2000 ▶ train MSE: 0.6196, val MSE: 0.4040
Gen 1198/2000 ▶ train MSE: 0.6196, val MSE: 0.4040
Gen 1199/2000 ▶ train MSE: 0.6196, val MSE: 0.4040
Gen 1200/2000 ▶ train MSE: 0.6196, val MSE: 0.4040

--- Refinement @ gen 1200 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1201/2000 ▶ train MSE: 0.6206, val MSE: 0.4047
Gen 1202/2000 ▶ train MSE: 0.6206, val MSE: 0.4047
Gen 1203/2000 ▶ train MSE: 0.6205, val MSE: 0.4060
Gen 1204/2000 ▶ train MSE: 0.6205, val MSE: 0.4060
Gen 1205/2000 ▶ train MSE: 0.6205, val MSE: 0.4060
Gen 1206/2000 ▶ train MSE: 0.6205, val MSE: 0.4060
Gen 1207/2000 ▶ train MSE: 0.6205, val MSE: 0.4060
Gen 1208/2000 ▶ train MSE: 0.6205, val MSE: 0.4060
Gen 1209/2000 ▶ train MSE: 0.6205, val MSE: 0.4060
Gen 1210/2000 ▶ train MSE: 0.6205, val MSE: 0.4060
Gen 1211/2000 ▶ train MSE: 0.6205, val MSE: 0.4060
Gen 1212/2000 ▶ train MSE: 0.6205, val MSE: 0.4044
Gen 1213/2000 ▶ train MSE: 0.6201, val MSE: 0.4059
Gen 1214/2000 ▶ train MSE: 0.6201, val MSE: 0.4059
Gen 1215/2000 ▶ train MSE: 0.6201, val MSE: 0.4059
Gen 1216/2000 ▶ train MSE: 0.6201, val MSE: 0.4059
Gen 1217/2000 ▶ train MSE: 0.6201, val MSE: 0.4059
Gen 1218/2000 ▶ train MSE: 0.6201, val MSE: 0.4059
Gen 1219/2000 ▶ train MSE: 0.6201, val MSE: 0.4059
Gen 1220/2000 ▶ train MSE: 0.6201, val MSE: 0.4059
Gen 1221/2000 ▶ train MSE: 0.6201, val MSE: 0.4059
Gen 1222/2000 ▶ train MSE: 0.6201, val MSE: 0.4059
Gen 1223/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1224/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1225/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1226/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1227/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1228/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1229/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1230/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1231/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1232/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1233/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1234/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1235/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1236/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1237/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1238/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1239/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1240/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1241/2000 ▶ train MSE: 0.6194, val MSE: 0.4031
Gen 1242/2000 ▶ train MSE: 0.6192, val MSE: 0.4032
Gen 1243/2000 ▶ train MSE: 0.6192, val MSE: 0.4032
Gen 1244/2000 ▶ train MSE: 0.6192, val MSE: 0.4032
Gen 1245/2000 ▶ train MSE: 0.6188, val MSE: 0.4010
Gen 1246/2000 ▶ train MSE: 0.6188, val MSE: 0.4010
Gen 1247/2000 ▶ train MSE: 0.6188, val MSE: 0.4010
Gen 1248/2000 ▶ train MSE: 0.6186, val MSE: 0.3995
Gen 1249/2000 ▶ train MSE: 0.6186, val MSE: 0.3995
Gen 1250/2000 ▶ train MSE: 0.6186, val MSE: 0.3995
Gen 1251/2000 ▶ train MSE: 0.6186, val MSE: 0.3995
Gen 1252/2000 ▶ train MSE: 0.6186, val MSE: 0.3995
Gen 1253/2000 ▶ train MSE: 0.6186, val MSE: 0.3995
Gen 1254/2000 ▶ train MSE: 0.6186, val MSE: 0.3995
Gen 1255/2000 ▶ train MSE: 0.6186, val MSE: 0.3995
Gen 1256/2000 ▶ train MSE: 0.6186, val MSE: 0.3995
Gen 1257/2000 ▶ train MSE: 0.6186, val MSE: 0.3995
Gen 1258/2000 ▶ train MSE: 0.6186, val MSE: 0.3995
Gen 1259/2000 ▶ train MSE: 0.6186, val MSE: 0.3993
Gen 1260/2000 ▶ train MSE: 0.6186, val MSE: 0.3993
Gen 1261/2000 ▶ train MSE: 0.6186, val MSE: 0.3993
Gen 1262/2000 ▶ train MSE: 0.6186, val MSE: 0.3993
Gen 1263/2000 ▶ train MSE: 0.6186, val MSE: 0.3993
Gen 1264/2000 ▶ train MSE: 0.6186, val MSE: 0.3993
Gen 1265/2000 ▶ train MSE: 0.6186, val MSE: 0.3993
Gen 1266/2000 ▶ train MSE: 0.6186, val MSE: 0.3993
Gen 1267/2000 ▶ train MSE: 0.6186, val MSE: 0.3999
Gen 1268/2000 ▶ train MSE: 0.6184, val MSE: 0.3999
Gen 1269/2000 ▶ train MSE: 0.6184, val MSE: 0.3996
Gen 1270/2000 ▶ train MSE: 0.6184, val MSE: 0.3996
Gen 1271/2000 ▶ train MSE: 0.6184, val MSE: 0.3996
Gen 1272/2000 ▶ train MSE: 0.6184, val MSE: 0.3996
Gen 1273/2000 ▶ train MSE: 0.6184, val MSE: 0.3996
Gen 1274/2000 ▶ train MSE: 0.6184, val MSE: 0.3996
Gen 1275/2000 ▶ train MSE: 0.6184, val MSE: 0.3996
Gen 1276/2000 ▶ train MSE: 0.6184, val MSE: 0.3996
Gen 1277/2000 ▶ train MSE: 0.6184, val MSE: 0.3996
Gen 1278/2000 ▶ train MSE: 0.6183, val MSE: 0.3997
Gen 1279/2000 ▶ train MSE: 0.6183, val MSE: 0.3989
Gen 1280/2000 ▶ train MSE: 0.6182, val MSE: 0.3994
Gen 1281/2000 ▶ train MSE: 0.6182, val MSE: 0.3986
Gen 1282/2000 ▶ train MSE: 0.6181, val MSE: 0.3993
Gen 1283/2000 ▶ train MSE: 0.6180, val MSE: 0.3990
Gen 1284/2000 ▶ train MSE: 0.6180, val MSE: 0.3990
Gen 1285/2000 ▶ train MSE: 0.6179, val MSE: 0.3991
Gen 1286/2000 ▶ train MSE: 0.6179, val MSE: 0.3991
Gen 1287/2000 ▶ train MSE: 0.6179, val MSE: 0.3991
Gen 1288/2000 ▶ train MSE: 0.6179, val MSE: 0.3995
Gen 1289/2000 ▶ train MSE: 0.6178, val MSE: 0.3989
Gen 1290/2000 ▶ train MSE: 0.6178, val MSE: 0.3989
Gen 1291/2000 ▶ train MSE: 0.6178, val MSE: 0.3994
Gen 1292/2000 ▶ train MSE: 0.6178, val MSE: 0.3994
Gen 1293/2000 ▶ train MSE: 0.6178, val MSE: 0.3994
Gen 1294/2000 ▶ train MSE: 0.6176, val MSE: 0.3985
Gen 1295/2000 ▶ train MSE: 0.6176, val MSE: 0.3985
Gen 1296/2000 ▶ train MSE: 0.6175, val MSE: 0.3985
Gen 1297/2000 ▶ train MSE: 0.6175, val MSE: 0.3985
Gen 1298/2000 ▶ train MSE: 0.6175, val MSE: 0.3985
Gen 1299/2000 ▶ train MSE: 0.6175, val MSE: 0.3985
Gen 1300/2000 ▶ train MSE: 0.6174, val MSE: 0.3987

--- Refinement @ gen 1300 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1301/2000 ▶ train MSE: 0.6178, val MSE: 0.4001
Gen 1302/2000 ▶ train MSE: 0.6178, val MSE: 0.4001
Gen 1303/2000 ▶ train MSE: 0.6178, val MSE: 0.4001
Gen 1304/2000 ▶ train MSE: 0.6178, val MSE: 0.4001
Gen 1305/2000 ▶ train MSE: 0.6176, val MSE: 0.3989
Gen 1306/2000 ▶ train MSE: 0.6176, val MSE: 0.3989
Gen 1307/2000 ▶ train MSE: 0.6176, val MSE: 0.3989
Gen 1308/2000 ▶ train MSE: 0.6176, val MSE: 0.3989
Gen 1309/2000 ▶ train MSE: 0.6176, val MSE: 0.3989
Gen 1310/2000 ▶ train MSE: 0.6176, val MSE: 0.3989
Gen 1311/2000 ▶ train MSE: 0.6176, val MSE: 0.3989
Gen 1312/2000 ▶ train MSE: 0.6176, val MSE: 0.3989
Gen 1313/2000 ▶ train MSE: 0.6176, val MSE: 0.3989
Gen 1314/2000 ▶ train MSE: 0.6176, val MSE: 0.3989
Gen 1315/2000 ▶ train MSE: 0.6175, val MSE: 0.3995
Gen 1316/2000 ▶ train MSE: 0.6175, val MSE: 0.3979
Gen 1317/2000 ▶ train MSE: 0.6174, val MSE: 0.3984
Gen 1318/2000 ▶ train MSE: 0.6174, val MSE: 0.3984
Gen 1319/2000 ▶ train MSE: 0.6174, val MSE: 0.3984
Gen 1320/2000 ▶ train MSE: 0.6172, val MSE: 0.3977
Gen 1321/2000 ▶ train MSE: 0.6172, val MSE: 0.3977
Gen 1322/2000 ▶ train MSE: 0.6172, val MSE: 0.3977
Gen 1323/2000 ▶ train MSE: 0.6172, val MSE: 0.3977
Gen 1324/2000 ▶ train MSE: 0.6172, val MSE: 0.3977
Gen 1325/2000 ▶ train MSE: 0.6172, val MSE: 0.3977
Gen 1326/2000 ▶ train MSE: 0.6172, val MSE: 0.3977
Gen 1327/2000 ▶ train MSE: 0.6172, val MSE: 0.3977
Gen 1328/2000 ▶ train MSE: 0.6172, val MSE: 0.3988
Gen 1329/2000 ▶ train MSE: 0.6172, val MSE: 0.3988
Gen 1330/2000 ▶ train MSE: 0.6172, val MSE: 0.3988
Gen 1331/2000 ▶ train MSE: 0.6172, val MSE: 0.3988
Gen 1332/2000 ▶ train MSE: 0.6172, val MSE: 0.3988
Gen 1333/2000 ▶ train MSE: 0.6172, val MSE: 0.3988
Gen 1334/2000 ▶ train MSE: 0.6172, val MSE: 0.3988
Gen 1335/2000 ▶ train MSE: 0.6171, val MSE: 0.3984
Gen 1336/2000 ▶ train MSE: 0.6170, val MSE: 0.3972
Gen 1337/2000 ▶ train MSE: 0.6170, val MSE: 0.3972
Gen 1338/2000 ▶ train MSE: 0.6170, val MSE: 0.3972
Gen 1339/2000 ▶ train MSE: 0.6170, val MSE: 0.3972
Gen 1340/2000 ▶ train MSE: 0.6170, val MSE: 0.3972
Gen 1341/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1342/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1343/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1344/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1345/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1346/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1347/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1348/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1349/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1350/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1351/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1352/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1353/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1354/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1355/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1356/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1357/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1358/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1359/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1360/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1361/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1362/2000 ▶ train MSE: 0.6166, val MSE: 0.3968
Gen 1363/2000 ▶ train MSE: 0.6165, val MSE: 0.3969
Gen 1364/2000 ▶ train MSE: 0.6165, val MSE: 0.3969
Gen 1365/2000 ▶ train MSE: 0.6165, val MSE: 0.3981
Gen 1366/2000 ▶ train MSE: 0.6165, val MSE: 0.3981
Gen 1367/2000 ▶ train MSE: 0.6165, val MSE: 0.3981
Gen 1368/2000 ▶ train MSE: 0.6165, val MSE: 0.3981
Gen 1369/2000 ▶ train MSE: 0.6165, val MSE: 0.3981
Gen 1370/2000 ▶ train MSE: 0.6165, val MSE: 0.3981
Gen 1371/2000 ▶ train MSE: 0.6165, val MSE: 0.3981
Gen 1372/2000 ▶ train MSE: 0.6164, val MSE: 0.3960
Gen 1373/2000 ▶ train MSE: 0.6164, val MSE: 0.3960
Gen 1374/2000 ▶ train MSE: 0.6164, val MSE: 0.3960
Gen 1375/2000 ▶ train MSE: 0.6164, val MSE: 0.3960
Gen 1376/2000 ▶ train MSE: 0.6163, val MSE: 0.3957
Gen 1377/2000 ▶ train MSE: 0.6163, val MSE: 0.3957
Gen 1378/2000 ▶ train MSE: 0.6163, val MSE: 0.3957
Gen 1379/2000 ▶ train MSE: 0.6163, val MSE: 0.3957
Gen 1380/2000 ▶ train MSE: 0.6162, val MSE: 0.3959
Gen 1381/2000 ▶ train MSE: 0.6162, val MSE: 0.3959
Gen 1382/2000 ▶ train MSE: 0.6162, val MSE: 0.3964
Gen 1383/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1384/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1385/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1386/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1387/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1388/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1389/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1390/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1391/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1392/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1393/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1394/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1395/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1396/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1397/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1398/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1399/2000 ▶ train MSE: 0.6157, val MSE: 0.3950
Gen 1400/2000 ▶ train MSE: 0.6157, val MSE: 0.3950

--- Refinement @ gen 1400 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1401/2000 ▶ train MSE: 0.6166, val MSE: 0.3974
Gen 1402/2000 ▶ train MSE: 0.6166, val MSE: 0.3974
Gen 1403/2000 ▶ train MSE: 0.6166, val MSE: 0.3974
Gen 1404/2000 ▶ train MSE: 0.6166, val MSE: 0.3974
Gen 1405/2000 ▶ train MSE: 0.6166, val MSE: 0.3974
Gen 1406/2000 ▶ train MSE: 0.6165, val MSE: 0.3969
Gen 1407/2000 ▶ train MSE: 0.6165, val MSE: 0.3969
Gen 1408/2000 ▶ train MSE: 0.6165, val MSE: 0.3962
Gen 1409/2000 ▶ train MSE: 0.6165, val MSE: 0.3962
Gen 1410/2000 ▶ train MSE: 0.6165, val MSE: 0.3962
Gen 1411/2000 ▶ train MSE: 0.6165, val MSE: 0.3962
Gen 1412/2000 ▶ train MSE: 0.6165, val MSE: 0.3962
Gen 1413/2000 ▶ train MSE: 0.6165, val MSE: 0.3962
Gen 1414/2000 ▶ train MSE: 0.6165, val MSE: 0.3962
Gen 1415/2000 ▶ train MSE: 0.6163, val MSE: 0.3938
Gen 1416/2000 ▶ train MSE: 0.6163, val MSE: 0.3938
Gen 1417/2000 ▶ train MSE: 0.6163, val MSE: 0.3938
Gen 1418/2000 ▶ train MSE: 0.6163, val MSE: 0.3938
Gen 1419/2000 ▶ train MSE: 0.6163, val MSE: 0.3938
Gen 1420/2000 ▶ train MSE: 0.6163, val MSE: 0.3944
Gen 1421/2000 ▶ train MSE: 0.6163, val MSE: 0.3944
Gen 1422/2000 ▶ train MSE: 0.6163, val MSE: 0.3944
Gen 1423/2000 ▶ train MSE: 0.6163, val MSE: 0.3944
Gen 1424/2000 ▶ train MSE: 0.6163, val MSE: 0.3944
Gen 1425/2000 ▶ train MSE: 0.6163, val MSE: 0.3956
Gen 1426/2000 ▶ train MSE: 0.6163, val MSE: 0.3956
Gen 1427/2000 ▶ train MSE: 0.6163, val MSE: 0.3956
Gen 1428/2000 ▶ train MSE: 0.6163, val MSE: 0.3956
Gen 1429/2000 ▶ train MSE: 0.6163, val MSE: 0.3956
Gen 1430/2000 ▶ train MSE: 0.6163, val MSE: 0.3956
Gen 1431/2000 ▶ train MSE: 0.6163, val MSE: 0.3956
Gen 1432/2000 ▶ train MSE: 0.6162, val MSE: 0.3951
Gen 1433/2000 ▶ train MSE: 0.6162, val MSE: 0.3951
Gen 1434/2000 ▶ train MSE: 0.6162, val MSE: 0.3951
Gen 1435/2000 ▶ train MSE: 0.6162, val MSE: 0.3951
Gen 1436/2000 ▶ train MSE: 0.6161, val MSE: 0.3958
Gen 1437/2000 ▶ train MSE: 0.6161, val MSE: 0.3958
Gen 1438/2000 ▶ train MSE: 0.6159, val MSE: 0.3973
Gen 1439/2000 ▶ train MSE: 0.6159, val MSE: 0.3973
Gen 1440/2000 ▶ train MSE: 0.6159, val MSE: 0.3973
Gen 1441/2000 ▶ train MSE: 0.6159, val MSE: 0.3973
Gen 1442/2000 ▶ train MSE: 0.6159, val MSE: 0.3973
Gen 1443/2000 ▶ train MSE: 0.6157, val MSE: 0.3951
Gen 1444/2000 ▶ train MSE: 0.6157, val MSE: 0.3951
Gen 1445/2000 ▶ train MSE: 0.6157, val MSE: 0.3951
Gen 1446/2000 ▶ train MSE: 0.6157, val MSE: 0.3951
Gen 1447/2000 ▶ train MSE: 0.6157, val MSE: 0.3951
Gen 1448/2000 ▶ train MSE: 0.6157, val MSE: 0.3951
Gen 1449/2000 ▶ train MSE: 0.6157, val MSE: 0.3951
Gen 1450/2000 ▶ train MSE: 0.6157, val MSE: 0.3940
Gen 1451/2000 ▶ train MSE: 0.6155, val MSE: 0.3949
Gen 1452/2000 ▶ train MSE: 0.6155, val MSE: 0.3949
Gen 1453/2000 ▶ train MSE: 0.6155, val MSE: 0.3951
Gen 1454/2000 ▶ train MSE: 0.6152, val MSE: 0.3939
Gen 1455/2000 ▶ train MSE: 0.6152, val MSE: 0.3939
Gen 1456/2000 ▶ train MSE: 0.6152, val MSE: 0.3939
Gen 1457/2000 ▶ train MSE: 0.6152, val MSE: 0.3939
Gen 1458/2000 ▶ train MSE: 0.6152, val MSE: 0.3958
Gen 1459/2000 ▶ train MSE: 0.6152, val MSE: 0.3945
Gen 1460/2000 ▶ train MSE: 0.6152, val MSE: 0.3945
Gen 1461/2000 ▶ train MSE: 0.6152, val MSE: 0.3945
Gen 1462/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1463/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1464/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1465/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1466/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1467/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1468/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1469/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1470/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1471/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1472/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1473/2000 ▶ train MSE: 0.6148, val MSE: 0.3942
Gen 1474/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1475/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1476/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1477/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1478/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1479/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1480/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1481/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1482/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1483/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1484/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1485/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1486/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1487/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1488/2000 ▶ train MSE: 0.6145, val MSE: 0.3949
Gen 1489/2000 ▶ train MSE: 0.6144, val MSE: 0.3939
Gen 1490/2000 ▶ train MSE: 0.6144, val MSE: 0.3939
Gen 1491/2000 ▶ train MSE: 0.6144, val MSE: 0.3939
Gen 1492/2000 ▶ train MSE: 0.6144, val MSE: 0.3939
Gen 1493/2000 ▶ train MSE: 0.6144, val MSE: 0.3939
Gen 1494/2000 ▶ train MSE: 0.6144, val MSE: 0.3939
Gen 1495/2000 ▶ train MSE: 0.6144, val MSE: 0.3939
Gen 1496/2000 ▶ train MSE: 0.6143, val MSE: 0.3954
Gen 1497/2000 ▶ train MSE: 0.6143, val MSE: 0.3943
Gen 1498/2000 ▶ train MSE: 0.6143, val MSE: 0.3943
Gen 1499/2000 ▶ train MSE: 0.6141, val MSE: 0.3938
Gen 1500/2000 ▶ train MSE: 0.6141, val MSE: 0.3938

--- Refinement @ gen 1500 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1501/2000 ▶ train MSE: 0.6144, val MSE: 0.3947
Gen 1502/2000 ▶ train MSE: 0.6144, val MSE: 0.3947
Gen 1503/2000 ▶ train MSE: 0.6144, val MSE: 0.3947
Gen 1504/2000 ▶ train MSE: 0.6141, val MSE: 0.3941
Gen 1505/2000 ▶ train MSE: 0.6141, val MSE: 0.3941
Gen 1506/2000 ▶ train MSE: 0.6141, val MSE: 0.3941
Gen 1507/2000 ▶ train MSE: 0.6141, val MSE: 0.3941
Gen 1508/2000 ▶ train MSE: 0.6141, val MSE: 0.3941
Gen 1509/2000 ▶ train MSE: 0.6141, val MSE: 0.3941
Gen 1510/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1511/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1512/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1513/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1514/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1515/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1516/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1517/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1518/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1519/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1520/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1521/2000 ▶ train MSE: 0.6140, val MSE: 0.3937
Gen 1522/2000 ▶ train MSE: 0.6140, val MSE: 0.3941
Gen 1523/2000 ▶ train MSE: 0.6140, val MSE: 0.3941
Gen 1524/2000 ▶ train MSE: 0.6140, val MSE: 0.3948
Gen 1525/2000 ▶ train MSE: 0.6140, val MSE: 0.3939
Gen 1526/2000 ▶ train MSE: 0.6140, val MSE: 0.3939
Gen 1527/2000 ▶ train MSE: 0.6140, val MSE: 0.3939
Gen 1528/2000 ▶ train MSE: 0.6139, val MSE: 0.3936
Gen 1529/2000 ▶ train MSE: 0.6139, val MSE: 0.3936
Gen 1530/2000 ▶ train MSE: 0.6139, val MSE: 0.3936
Gen 1531/2000 ▶ train MSE: 0.6139, val MSE: 0.3936
Gen 1532/2000 ▶ train MSE: 0.6139, val MSE: 0.3936
Gen 1533/2000 ▶ train MSE: 0.6139, val MSE: 0.3936
Gen 1534/2000 ▶ train MSE: 0.6139, val MSE: 0.3936
Gen 1535/2000 ▶ train MSE: 0.6138, val MSE: 0.3933
Gen 1536/2000 ▶ train MSE: 0.6138, val MSE: 0.3933
Gen 1537/2000 ▶ train MSE: 0.6138, val MSE: 0.3932
Gen 1538/2000 ▶ train MSE: 0.6137, val MSE: 0.3935
Gen 1539/2000 ▶ train MSE: 0.6137, val MSE: 0.3935
Gen 1540/2000 ▶ train MSE: 0.6137, val MSE: 0.3935
Gen 1541/2000 ▶ train MSE: 0.6137, val MSE: 0.3935
Gen 1542/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1543/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1544/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1545/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1546/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1547/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1548/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1549/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1550/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1551/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1552/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1553/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1554/2000 ▶ train MSE: 0.6137, val MSE: 0.3930
Gen 1555/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1556/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1557/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1558/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1559/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1560/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1561/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1562/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1563/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1564/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1565/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1566/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1567/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1568/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1569/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1570/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1571/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1572/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1573/2000 ▶ train MSE: 0.6136, val MSE: 0.3925
Gen 1574/2000 ▶ train MSE: 0.6135, val MSE: 0.3920
Gen 1575/2000 ▶ train MSE: 0.6135, val MSE: 0.3920
Gen 1576/2000 ▶ train MSE: 0.6135, val MSE: 0.3920
Gen 1577/2000 ▶ train MSE: 0.6135, val MSE: 0.3920
Gen 1578/2000 ▶ train MSE: 0.6135, val MSE: 0.3920
Gen 1579/2000 ▶ train MSE: 0.6135, val MSE: 0.3920
Gen 1580/2000 ▶ train MSE: 0.6135, val MSE: 0.3920
Gen 1581/2000 ▶ train MSE: 0.6135, val MSE: 0.3920
Gen 1582/2000 ▶ train MSE: 0.6135, val MSE: 0.3920
Gen 1583/2000 ▶ train MSE: 0.6135, val MSE: 0.3920
Gen 1584/2000 ▶ train MSE: 0.6134, val MSE: 0.3935
Gen 1585/2000 ▶ train MSE: 0.6134, val MSE: 0.3935
Gen 1586/2000 ▶ train MSE: 0.6134, val MSE: 0.3935
Gen 1587/2000 ▶ train MSE: 0.6134, val MSE: 0.3935
Gen 1588/2000 ▶ train MSE: 0.6134, val MSE: 0.3936
Gen 1589/2000 ▶ train MSE: 0.6134, val MSE: 0.3936
Gen 1590/2000 ▶ train MSE: 0.6134, val MSE: 0.3936
Gen 1591/2000 ▶ train MSE: 0.6134, val MSE: 0.3936
Gen 1592/2000 ▶ train MSE: 0.6134, val MSE: 0.3936
Gen 1593/2000 ▶ train MSE: 0.6134, val MSE: 0.3936
Gen 1594/2000 ▶ train MSE: 0.6134, val MSE: 0.3936
Gen 1595/2000 ▶ train MSE: 0.6134, val MSE: 0.3936
Gen 1596/2000 ▶ train MSE: 0.6134, val MSE: 0.3936
Gen 1597/2000 ▶ train MSE: 0.6134, val MSE: 0.3936
Gen 1598/2000 ▶ train MSE: 0.6133, val MSE: 0.3928
Gen 1599/2000 ▶ train MSE: 0.6133, val MSE: 0.3928
Gen 1600/2000 ▶ train MSE: 0.6132, val MSE: 0.3926

--- Refinement @ gen 1600 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1601/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1602/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1603/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1604/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1605/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1606/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1607/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1608/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1609/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1610/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1611/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1612/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1613/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1614/2000 ▶ train MSE: 0.6136, val MSE: 0.3946
Gen 1615/2000 ▶ train MSE: 0.6136, val MSE: 0.3928
Gen 1616/2000 ▶ train MSE: 0.6136, val MSE: 0.3928
Gen 1617/2000 ▶ train MSE: 0.6136, val MSE: 0.3919
Gen 1618/2000 ▶ train MSE: 0.6136, val MSE: 0.3919
Gen 1619/2000 ▶ train MSE: 0.6136, val MSE: 0.3919
Gen 1620/2000 ▶ train MSE: 0.6134, val MSE: 0.3919
Gen 1621/2000 ▶ train MSE: 0.6134, val MSE: 0.3919
Gen 1622/2000 ▶ train MSE: 0.6134, val MSE: 0.3919
Gen 1623/2000 ▶ train MSE: 0.6134, val MSE: 0.3919
Gen 1624/2000 ▶ train MSE: 0.6133, val MSE: 0.3918
Gen 1625/2000 ▶ train MSE: 0.6133, val MSE: 0.3918
Gen 1626/2000 ▶ train MSE: 0.6132, val MSE: 0.3917
Gen 1627/2000 ▶ train MSE: 0.6132, val MSE: 0.3917
Gen 1628/2000 ▶ train MSE: 0.6132, val MSE: 0.3917
Gen 1629/2000 ▶ train MSE: 0.6132, val MSE: 0.3917
Gen 1630/2000 ▶ train MSE: 0.6132, val MSE: 0.3917
Gen 1631/2000 ▶ train MSE: 0.6132, val MSE: 0.3917
Gen 1632/2000 ▶ train MSE: 0.6132, val MSE: 0.3917
Gen 1633/2000 ▶ train MSE: 0.6132, val MSE: 0.3917
Gen 1634/2000 ▶ train MSE: 0.6132, val MSE: 0.3917
Gen 1635/2000 ▶ train MSE: 0.6132, val MSE: 0.3917
Gen 1636/2000 ▶ train MSE: 0.6132, val MSE: 0.3923
Gen 1637/2000 ▶ train MSE: 0.6131, val MSE: 0.3914
Gen 1638/2000 ▶ train MSE: 0.6131, val MSE: 0.3914
Gen 1639/2000 ▶ train MSE: 0.6131, val MSE: 0.3930
Gen 1640/2000 ▶ train MSE: 0.6128, val MSE: 0.3919
Gen 1641/2000 ▶ train MSE: 0.6128, val MSE: 0.3919
Gen 1642/2000 ▶ train MSE: 0.6128, val MSE: 0.3919
Gen 1643/2000 ▶ train MSE: 0.6128, val MSE: 0.3919
Gen 1644/2000 ▶ train MSE: 0.6128, val MSE: 0.3919
Gen 1645/2000 ▶ train MSE: 0.6128, val MSE: 0.3919
Gen 1646/2000 ▶ train MSE: 0.6128, val MSE: 0.3919
Gen 1647/2000 ▶ train MSE: 0.6128, val MSE: 0.3916
Gen 1648/2000 ▶ train MSE: 0.6128, val MSE: 0.3913
Gen 1649/2000 ▶ train MSE: 0.6128, val MSE: 0.3913
Gen 1650/2000 ▶ train MSE: 0.6127, val MSE: 0.3906
Gen 1651/2000 ▶ train MSE: 0.6127, val MSE: 0.3906
Gen 1652/2000 ▶ train MSE: 0.6127, val MSE: 0.3906
Gen 1653/2000 ▶ train MSE: 0.6127, val MSE: 0.3906
Gen 1654/2000 ▶ train MSE: 0.6127, val MSE: 0.3906
Gen 1655/2000 ▶ train MSE: 0.6127, val MSE: 0.3906
Gen 1656/2000 ▶ train MSE: 0.6127, val MSE: 0.3906
Gen 1657/2000 ▶ train MSE: 0.6127, val MSE: 0.3906
Gen 1658/2000 ▶ train MSE: 0.6127, val MSE: 0.3907
Gen 1659/2000 ▶ train MSE: 0.6127, val MSE: 0.3907
Gen 1660/2000 ▶ train MSE: 0.6125, val MSE: 0.3908
Gen 1661/2000 ▶ train MSE: 0.6125, val MSE: 0.3908
Gen 1662/2000 ▶ train MSE: 0.6125, val MSE: 0.3908
Gen 1663/2000 ▶ train MSE: 0.6125, val MSE: 0.3908
Gen 1664/2000 ▶ train MSE: 0.6125, val MSE: 0.3908
Gen 1665/2000 ▶ train MSE: 0.6125, val MSE: 0.3908
Gen 1666/2000 ▶ train MSE: 0.6125, val MSE: 0.3908
Gen 1667/2000 ▶ train MSE: 0.6125, val MSE: 0.3908
Gen 1668/2000 ▶ train MSE: 0.6125, val MSE: 0.3908
Gen 1669/2000 ▶ train MSE: 0.6125, val MSE: 0.3908
Gen 1670/2000 ▶ train MSE: 0.6125, val MSE: 0.3908
Gen 1671/2000 ▶ train MSE: 0.6124, val MSE: 0.3906
Gen 1672/2000 ▶ train MSE: 0.6124, val MSE: 0.3906
Gen 1673/2000 ▶ train MSE: 0.6124, val MSE: 0.3906
Gen 1674/2000 ▶ train MSE: 0.6124, val MSE: 0.3906
Gen 1675/2000 ▶ train MSE: 0.6124, val MSE: 0.3906
Gen 1676/2000 ▶ train MSE: 0.6124, val MSE: 0.3906
Gen 1677/2000 ▶ train MSE: 0.6124, val MSE: 0.3906
Gen 1678/2000 ▶ train MSE: 0.6123, val MSE: 0.3906
Gen 1679/2000 ▶ train MSE: 0.6123, val MSE: 0.3906
Gen 1680/2000 ▶ train MSE: 0.6123, val MSE: 0.3906
Gen 1681/2000 ▶ train MSE: 0.6123, val MSE: 0.3906
Gen 1682/2000 ▶ train MSE: 0.6123, val MSE: 0.3906
Gen 1683/2000 ▶ train MSE: 0.6123, val MSE: 0.3906
Gen 1684/2000 ▶ train MSE: 0.6123, val MSE: 0.3906
Gen 1685/2000 ▶ train MSE: 0.6123, val MSE: 0.3906
Gen 1686/2000 ▶ train MSE: 0.6123, val MSE: 0.3906
Gen 1687/2000 ▶ train MSE: 0.6123, val MSE: 0.3904
Gen 1688/2000 ▶ train MSE: 0.6123, val MSE: 0.3904
Gen 1689/2000 ▶ train MSE: 0.6123, val MSE: 0.3904
Gen 1690/2000 ▶ train MSE: 0.6123, val MSE: 0.3908
Gen 1691/2000 ▶ train MSE: 0.6123, val MSE: 0.3908
Gen 1692/2000 ▶ train MSE: 0.6123, val MSE: 0.3908
Gen 1693/2000 ▶ train MSE: 0.6123, val MSE: 0.3904
Gen 1694/2000 ▶ train MSE: 0.6122, val MSE: 0.3890
Gen 1695/2000 ▶ train MSE: 0.6122, val MSE: 0.3890
Gen 1696/2000 ▶ train MSE: 0.6121, val MSE: 0.3891
Gen 1697/2000 ▶ train MSE: 0.6121, val MSE: 0.3891
Gen 1698/2000 ▶ train MSE: 0.6120, val MSE: 0.3888
Gen 1699/2000 ▶ train MSE: 0.6120, val MSE: 0.3888
Gen 1700/2000 ▶ train MSE: 0.6120, val MSE: 0.3888

--- Refinement @ gen 1700 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1701/2000 ▶ train MSE: 0.6124, val MSE: 0.3893
Gen 1702/2000 ▶ train MSE: 0.6124, val MSE: 0.3893
Gen 1703/2000 ▶ train MSE: 0.6124, val MSE: 0.3893
Gen 1704/2000 ▶ train MSE: 0.6124, val MSE: 0.3893
Gen 1705/2000 ▶ train MSE: 0.6124, val MSE: 0.3893
Gen 1706/2000 ▶ train MSE: 0.6124, val MSE: 0.3893
Gen 1707/2000 ▶ train MSE: 0.6124, val MSE: 0.3907
Gen 1708/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1709/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1710/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1711/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1712/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1713/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1714/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1715/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1716/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1717/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1718/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1719/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1720/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1721/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1722/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1723/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1724/2000 ▶ train MSE: 0.6121, val MSE: 0.3894
Gen 1725/2000 ▶ train MSE: 0.6119, val MSE: 0.3896
Gen 1726/2000 ▶ train MSE: 0.6119, val MSE: 0.3896
Gen 1727/2000 ▶ train MSE: 0.6119, val MSE: 0.3896
Gen 1728/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1729/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1730/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1731/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1732/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1733/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1734/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1735/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1736/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1737/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1738/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1739/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1740/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1741/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1742/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1743/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1744/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1745/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1746/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1747/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1748/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1749/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1750/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1751/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1752/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1753/2000 ▶ train MSE: 0.6117, val MSE: 0.3883
Gen 1754/2000 ▶ train MSE: 0.6117, val MSE: 0.3888
Gen 1755/2000 ▶ train MSE: 0.6117, val MSE: 0.3888
Gen 1756/2000 ▶ train MSE: 0.6117, val MSE: 0.3888
Gen 1757/2000 ▶ train MSE: 0.6117, val MSE: 0.3888
Gen 1758/2000 ▶ train MSE: 0.6117, val MSE: 0.3888
Gen 1759/2000 ▶ train MSE: 0.6117, val MSE: 0.3893
Gen 1760/2000 ▶ train MSE: 0.6117, val MSE: 0.3893
Gen 1761/2000 ▶ train MSE: 0.6116, val MSE: 0.3879
Gen 1762/2000 ▶ train MSE: 0.6116, val MSE: 0.3879
Gen 1763/2000 ▶ train MSE: 0.6116, val MSE: 0.3879
Gen 1764/2000 ▶ train MSE: 0.6116, val MSE: 0.3879
Gen 1765/2000 ▶ train MSE: 0.6116, val MSE: 0.3879
Gen 1766/2000 ▶ train MSE: 0.6116, val MSE: 0.3879
Gen 1767/2000 ▶ train MSE: 0.6115, val MSE: 0.3889
Gen 1768/2000 ▶ train MSE: 0.6115, val MSE: 0.3889
Gen 1769/2000 ▶ train MSE: 0.6115, val MSE: 0.3889
Gen 1770/2000 ▶ train MSE: 0.6115, val MSE: 0.3889
Gen 1771/2000 ▶ train MSE: 0.6115, val MSE: 0.3889
Gen 1772/2000 ▶ train MSE: 0.6115, val MSE: 0.3889
Gen 1773/2000 ▶ train MSE: 0.6115, val MSE: 0.3889
Gen 1774/2000 ▶ train MSE: 0.6115, val MSE: 0.3889
Gen 1775/2000 ▶ train MSE: 0.6115, val MSE: 0.3889
Gen 1776/2000 ▶ train MSE: 0.6115, val MSE: 0.3889
Gen 1777/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1778/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1779/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1780/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1781/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1782/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1783/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1784/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1785/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1786/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1787/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1788/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1789/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1790/2000 ▶ train MSE: 0.6113, val MSE: 0.3875
Gen 1791/2000 ▶ train MSE: 0.6110, val MSE: 0.3881
Gen 1792/2000 ▶ train MSE: 0.6110, val MSE: 0.3881
Gen 1793/2000 ▶ train MSE: 0.6110, val MSE: 0.3881
Gen 1794/2000 ▶ train MSE: 0.6110, val MSE: 0.3881
Gen 1795/2000 ▶ train MSE: 0.6110, val MSE: 0.3881
Gen 1796/2000 ▶ train MSE: 0.6110, val MSE: 0.3881
Gen 1797/2000 ▶ train MSE: 0.6110, val MSE: 0.3881
Gen 1798/2000 ▶ train MSE: 0.6110, val MSE: 0.3881
Gen 1799/2000 ▶ train MSE: 0.6110, val MSE: 0.3881
Gen 1800/2000 ▶ train MSE: 0.6110, val MSE: 0.3881

--- Refinement @ gen 1800 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1801/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1802/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1803/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1804/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1805/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1806/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1807/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1808/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1809/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1810/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1811/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1812/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1813/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1814/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1815/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1816/2000 ▶ train MSE: 0.6118, val MSE: 0.3866
Gen 1817/2000 ▶ train MSE: 0.6117, val MSE: 0.3865
Gen 1818/2000 ▶ train MSE: 0.6117, val MSE: 0.3865
Gen 1819/2000 ▶ train MSE: 0.6117, val MSE: 0.3865
Gen 1820/2000 ▶ train MSE: 0.6117, val MSE: 0.3865
Gen 1821/2000 ▶ train MSE: 0.6117, val MSE: 0.3865
Gen 1822/2000 ▶ train MSE: 0.6117, val MSE: 0.3864
Gen 1823/2000 ▶ train MSE: 0.6117, val MSE: 0.3864
Gen 1824/2000 ▶ train MSE: 0.6113, val MSE: 0.3866
Gen 1825/2000 ▶ train MSE: 0.6113, val MSE: 0.3866
Gen 1826/2000 ▶ train MSE: 0.6113, val MSE: 0.3866
Gen 1827/2000 ▶ train MSE: 0.6112, val MSE: 0.3858
Gen 1828/2000 ▶ train MSE: 0.6112, val MSE: 0.3858
Gen 1829/2000 ▶ train MSE: 0.6110, val MSE: 0.3859
Gen 1830/2000 ▶ train MSE: 0.6110, val MSE: 0.3859
Gen 1831/2000 ▶ train MSE: 0.6109, val MSE: 0.3864
Gen 1832/2000 ▶ train MSE: 0.6109, val MSE: 0.3864
Gen 1833/2000 ▶ train MSE: 0.6109, val MSE: 0.3864
Gen 1834/2000 ▶ train MSE: 0.6109, val MSE: 0.3850
Gen 1835/2000 ▶ train MSE: 0.6109, val MSE: 0.3850
Gen 1836/2000 ▶ train MSE: 0.6109, val MSE: 0.3850
Gen 1837/2000 ▶ train MSE: 0.6109, val MSE: 0.3850
Gen 1838/2000 ▶ train MSE: 0.6109, val MSE: 0.3850
Gen 1839/2000 ▶ train MSE: 0.6108, val MSE: 0.3852
Gen 1840/2000 ▶ train MSE: 0.6108, val MSE: 0.3852
Gen 1841/2000 ▶ train MSE: 0.6108, val MSE: 0.3852
Gen 1842/2000 ▶ train MSE: 0.6108, val MSE: 0.3852
Gen 1843/2000 ▶ train MSE: 0.6108, val MSE: 0.3852
Gen 1844/2000 ▶ train MSE: 0.6108, val MSE: 0.3852
Gen 1845/2000 ▶ train MSE: 0.6107, val MSE: 0.3857
Gen 1846/2000 ▶ train MSE: 0.6107, val MSE: 0.3857
Gen 1847/2000 ▶ train MSE: 0.6107, val MSE: 0.3857
Gen 1848/2000 ▶ train MSE: 0.6107, val MSE: 0.3857
Gen 1849/2000 ▶ train MSE: 0.6105, val MSE: 0.3857
Gen 1850/2000 ▶ train MSE: 0.6105, val MSE: 0.3857
Gen 1851/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1852/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1853/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1854/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1855/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1856/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1857/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1858/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1859/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1860/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1861/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1862/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1863/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1864/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1865/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1866/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1867/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1868/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1869/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1870/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1871/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1872/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1873/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1874/2000 ▶ train MSE: 0.6105, val MSE: 0.3859
Gen 1875/2000 ▶ train MSE: 0.6104, val MSE: 0.3857
Gen 1876/2000 ▶ train MSE: 0.6104, val MSE: 0.3857
Gen 1877/2000 ▶ train MSE: 0.6104, val MSE: 0.3857
Gen 1878/2000 ▶ train MSE: 0.6104, val MSE: 0.3857
Gen 1879/2000 ▶ train MSE: 0.6104, val MSE: 0.3851
Gen 1880/2000 ▶ train MSE: 0.6104, val MSE: 0.3851
Gen 1881/2000 ▶ train MSE: 0.6104, val MSE: 0.3851
Gen 1882/2000 ▶ train MSE: 0.6104, val MSE: 0.3851
Gen 1883/2000 ▶ train MSE: 0.6104, val MSE: 0.3851
Gen 1884/2000 ▶ train MSE: 0.6103, val MSE: 0.3856
Gen 1885/2000 ▶ train MSE: 0.6103, val MSE: 0.3856
Gen 1886/2000 ▶ train MSE: 0.6103, val MSE: 0.3856
Gen 1887/2000 ▶ train MSE: 0.6103, val MSE: 0.3856
Gen 1888/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1889/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1890/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1891/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1892/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1893/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1894/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1895/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1896/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1897/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1898/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1899/2000 ▶ train MSE: 0.6100, val MSE: 0.3855
Gen 1900/2000 ▶ train MSE: 0.6100, val MSE: 0.3855

--- Refinement @ gen 1900 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1901/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1902/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1903/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1904/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1905/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1906/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1907/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1908/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1909/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1910/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1911/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1912/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1913/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1914/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1915/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1916/2000 ▶ train MSE: 0.6103, val MSE: 0.3866
Gen 1917/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1918/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1919/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1920/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1921/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1922/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1923/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1924/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1925/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1926/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1927/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1928/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1929/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1930/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1931/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1932/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1933/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1934/2000 ▶ train MSE: 0.6101, val MSE: 0.3864
Gen 1935/2000 ▶ train MSE: 0.6099, val MSE: 0.3860
Gen 1936/2000 ▶ train MSE: 0.6099, val MSE: 0.3860
Gen 1937/2000 ▶ train MSE: 0.6099, val MSE: 0.3860
Gen 1938/2000 ▶ train MSE: 0.6099, val MSE: 0.3860
Gen 1939/2000 ▶ train MSE: 0.6099, val MSE: 0.3860
Gen 1940/2000 ▶ train MSE: 0.6099, val MSE: 0.3860
Gen 1941/2000 ▶ train MSE: 0.6099, val MSE: 0.3860
Gen 1942/2000 ▶ train MSE: 0.6099, val MSE: 0.3860
Gen 1943/2000 ▶ train MSE: 0.6099, val MSE: 0.3860
Gen 1944/2000 ▶ train MSE: 0.6099, val MSE: 0.3860
Gen 1945/2000 ▶ train MSE: 0.6099, val MSE: 0.3843
Gen 1946/2000 ▶ train MSE: 0.6099, val MSE: 0.3845
Gen 1947/2000 ▶ train MSE: 0.6099, val MSE: 0.3845
Gen 1948/2000 ▶ train MSE: 0.6099, val MSE: 0.3845
Gen 1949/2000 ▶ train MSE: 0.6099, val MSE: 0.3845
Gen 1950/2000 ▶ train MSE: 0.6099, val MSE: 0.3845
Gen 1951/2000 ▶ train MSE: 0.6098, val MSE: 0.3844
Gen 1952/2000 ▶ train MSE: 0.6098, val MSE: 0.3844
Gen 1953/2000 ▶ train MSE: 0.6098, val MSE: 0.3844
Gen 1954/2000 ▶ train MSE: 0.6098, val MSE: 0.3844
Gen 1955/2000 ▶ train MSE: 0.6098, val MSE: 0.3844
Gen 1956/2000 ▶ train MSE: 0.6097, val MSE: 0.3844
Gen 1957/2000 ▶ train MSE: 0.6097, val MSE: 0.3844
Gen 1958/2000 ▶ train MSE: 0.6096, val MSE: 0.3834
Gen 1959/2000 ▶ train MSE: 0.6096, val MSE: 0.3834
Gen 1960/2000 ▶ train MSE: 0.6096, val MSE: 0.3834
Gen 1961/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1962/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1963/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1964/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1965/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1966/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1967/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1968/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1969/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1970/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1971/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1972/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1973/2000 ▶ train MSE: 0.6096, val MSE: 0.3839
Gen 1974/2000 ▶ train MSE: 0.6094, val MSE: 0.3828
Gen 1975/2000 ▶ train MSE: 0.6094, val MSE: 0.3828
Gen 1976/2000 ▶ train MSE: 0.6094, val MSE: 0.3828
Gen 1977/2000 ▶ train MSE: 0.6094, val MSE: 0.3828
Gen 1978/2000 ▶ train MSE: 0.6094, val MSE: 0.3828
Gen 1979/2000 ▶ train MSE: 0.6094, val MSE: 0.3828
Gen 1980/2000 ▶ train MSE: 0.6094, val MSE: 0.3828
Gen 1981/2000 ▶ train MSE: 0.6094, val MSE: 0.3828
Gen 1982/2000 ▶ train MSE: 0.6094, val MSE: 0.3828
Gen 1983/2000 ▶ train MSE: 0.6094, val MSE: 0.3828
Gen 1984/2000 ▶ train MSE: 0.6094, val MSE: 0.3828
Gen 1985/2000 ▶ train MSE: 0.6094, val MSE: 0.3826
Gen 1986/2000 ▶ train MSE: 0.6094, val MSE: 0.3826
Gen 1987/2000 ▶ train MSE: 0.6094, val MSE: 0.3826
Gen 1988/2000 ▶ train MSE: 0.6094, val MSE: 0.3826
Gen 1989/2000 ▶ train MSE: 0.6094, val MSE: 0.3813
Gen 1990/2000 ▶ train MSE: 0.6094, val MSE: 0.3813
Gen 1991/2000 ▶ train MSE: 0.6094, val MSE: 0.3813
Gen 1992/2000 ▶ train MSE: 0.6093, val MSE: 0.3830
Gen 1993/2000 ▶ train MSE: 0.6093, val MSE: 0.3830
Gen 1994/2000 ▶ train MSE: 0.6091, val MSE: 0.3819
Gen 1995/2000 ▶ train MSE: 0.6091, val MSE: 0.3819
Gen 1996/2000 ▶ train MSE: 0.6091, val MSE: 0.3819
Gen 1997/2000 ▶ train MSE: 0.6091, val MSE: 0.3819
Gen 1998/2000 ▶ train MSE: 0.6091, val MSE: 0.3819
Gen 1999/2000 ▶ train MSE: 0.6091, val MSE: 0.3819
Gen 2000/2000 ▶ train MSE: 0.6090, val MSE: 0.3815

--- Refinement @ gen 2000 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9

✅ GA+SGD done!  Final Train MSE: 1.2190, Val MSE: 0.9835
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=23890bde">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Final-hybrid-GA-implementation:-Final-run:-Stop-SGD-refining-after-600-generations-due-to-divergence-of-solutions.">Final hybrid GA implementation: Final run: Stop SGD refining after 600 generations due to divergence of solutions.<a class="anchor-link" href="#Final-hybrid-GA-implementation:-Final-run:-Stop-SGD-refining-after-600-generations-due-to-divergence-of-solutions.">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=db239bc9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Hybrid Genetic Algorithm with Periodic SGD Refinement for FFN Training</span>
<span class="sd">=======================================================================</span>

<span class="sd">This script implements a hybrid optimisation strategy that combines a real-valued</span>
<span class="sd">Genetic Algorithm (GA) with periodic Stochastic Gradient Descent (SGD) refinement</span>
<span class="sd">for training a Feed-Forward Neural Network (FFN). The GA uses BLX-α crossover,</span>
<span class="sd">Gaussian mutation, and elitism. Every 100 generations, the top-performing elite</span>
<span class="sd">individuals are further fine-tuned using a fixed number of SGD epochs.</span>

<span class="sd">Key Innovations:</span>
<span class="sd">----------------</span>
<span class="sd">- Hybrid Strategy: Combines global GA exploration with local gradient-based exploitation</span>
<span class="sd">- Periodic Refinement: Every 100 generations, elite individuals undergo 10 epochs of SGD</span>
<span class="sd">- Logging: Logs elite MSEs before and after refinement to a CSV file for ablation and analysis</span>
<span class="sd">- Now stops refining after generation 600</span>

<span class="sd">Core GA Configuration:</span>
<span class="sd">----------------------</span>
<span class="sd">- Population Size: 200</span>
<span class="sd">- Generations: 2000</span>
<span class="sd">- Elitism: top 20% retained per generation</span>
<span class="sd">- Tournament Selection: size 3</span>
<span class="sd">- Crossover: BLX-α with α = 0.6</span>
<span class="sd">- Mutation: Gaussian with probability 1%, std 0.01</span>

<span class="sd">SGD Refinement:</span>
<span class="sd">---------------</span>
<span class="sd">- Performed every 100 generations (`REFINE_EVERY`)</span>
<span class="sd">- Runs for 10 epochs (`REFINE_EPOCHS`) on each elite</span>
<span class="sd">- Uses momentum SGD with hyperparameters obtained via Optuna</span>

<span class="sd">Feedforward Network Architecture:</span>
<span class="sd">---------------------------------</span>
<span class="sd">- 2 hidden layers with 24 ReLU units each</span>
<span class="sd">- Xavier weight initialisation</span>
<span class="sd">- Output layer: linear (for regression)</span>

<span class="sd">Output:</span>
<span class="sd">-------</span>
<span class="sd">- Console logs for generation-wise MSE and refinement notifications</span>
<span class="sd">- `elite_mse_log.csv` file containing per-elite MSE before and after refinement</span>
<span class="sd">- Final training and validation MSE</span>
<span class="sd">- Matplotlib plot of training and validation MSE over time</span>

<span class="sd">Usage Notes:</span>
<span class="sd">------------</span>
<span class="sd">- Assumes `X_train`, `y_train`, `X_val`, `y_val` are pre-defined as `torch.Tensor`</span>
<span class="sd">- Designed for regression tasks (uses `nn.MSELoss`)</span>
<span class="sd">- Can be adapted to run refinement more/less frequently or to use other optimisers</span>

<span class="sd">"""</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="c1">#  1) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  2) Hyperparameters </span>
<span class="c1"># GA settings</span>
<span class="n">POP_SIZE</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">GENERATIONS</span> <span class="o">=</span> <span class="mi">8000</span>
<span class="n">ELITE_FRAC</span>  <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">TOURN_SIZE</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">MUT_P</span>       <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">MUT_SD</span>      <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">BLX_ALPHA</span>   <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1"># Local SGD refinement (every REFINE_EVERY gens, on top ELITE_FRAC)</span>
<span class="n">REFINE_EVERY</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">REFINE_EPOCHS</span>  <span class="o">=</span> <span class="mi">10</span> 
<span class="c1"># FFN / SGD settings (from Optuna)</span>
<span class="n">best_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"lr"</span><span class="p">:</span>         <span class="mf">0.013988</span><span class="p">,</span>
    <span class="s2">"momentum"</span><span class="p">:</span>   <span class="mf">0.6346</span><span class="p">,</span>
    <span class="s2">"batch_size"</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"n_units"</span><span class="p">:</span>    <span class="mi">24</span><span class="p">,</span>
    <span class="s2">"activation"</span><span class="p">:</span> <span class="s2">"ReLU"</span>
<span class="p">}</span>

<span class="c1">#  3) DataPrep </span>
<span class="c1"># assume X_train, y_train, X_val, y_val are already in scope as torch.Tensor</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">],</span>
                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># ─── 4) Model Builder ─────────────────────────────────────────────────────────</span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">n_layers</span>   <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">],</span>
    <span class="n">n_units</span>    <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">],</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Constructs a feed-forward neural network (FFN) using the architecture specified in `arch`.</span>
<span class="sd">    </span>
<span class="sd">    The model includes:</span>
<span class="sd">    - `n_layers` hidden layers with `n_units` neurons each</span>
<span class="sd">    - Activation function as defined in `arch["activation"]`</span>
<span class="sd">    - Xavier normal weight initialization and zero-initialized biases</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: A PyTorch sequential model moved to the appropriate device.</span>
<span class="sd">    """</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_f</span> <span class="o">=</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Xavier init</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="c1">#  5) GA Helpers </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Selects one individual from the population using tournament selection.</span>

<span class="sd">    Args:</span>
<span class="sd">        pop (list of np.ndarray): Population of genome vectors.</span>
<span class="sd">        fitness (list of float): Corresponding fitness values (lower is better).</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Genome of the selected individual.</span>
<span class="sd">    """</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">),</span> <span class="n">TOURN_SIZE</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">crossover_and_mutate</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Produces a new child genome via BLX-α crossover and Gaussian mutation.</span>

<span class="sd">    - BLX-α expands the gene-wise search space around two parents.</span>
<span class="sd">    - Gaussian noise is added to a subset of genes based on mutation probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        p1 (np.ndarray): Parent 1 genome.</span>
<span class="sd">        p2 (np.ndarray): Parent 2 genome.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Mutated child genome.</span>
<span class="sd">    """</span>
    <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">BLX_ALPHA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
    <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">BLX_ALPHA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
    <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># mutation</span>
    <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">MUT_P</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">MUT_SD</span>
    <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">child</span>

<span class="k">def</span><span class="w"> </span><span class="nf">refine_with_sgd</span><span class="p">(</span><span class="n">genome</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Produces a new child genome via BLX-α crossover and Gaussian mutation.</span>

<span class="sd">    - BLX-α expands the gene-wise search space around two parents.</span>
<span class="sd">    - Gaussian noise is added to a subset of genes based on mutation probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        p1 (np.ndarray): Parent 1 genome.</span>
<span class="sd">        p2 (np.ndarray): Parent 2 genome.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Mutated child genome.</span>
<span class="sd">    """</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                         <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="n">optim_sgd</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                          <span class="n">lr</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span>
                          <span class="n">momentum</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"momentum"</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">REFINE_EPOCHS</span><span class="p">):</span>
        
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optim_sgd</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optim_sgd</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># write back</span>
    <span class="k">return</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1">#  6) Initialize Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">POP_SIZE</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="c1">#  7) GA Main Loop </span>
<span class="n">train_curve</span><span class="p">,</span> <span class="n">val_curve</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="c1">###logging of mse values before and after sgd</span>
<span class="n">LOG_FN</span> <span class="o">=</span> <span class="s2">"elite_mse_log.csv"</span>
<span class="c1"># write header: gen,phase,elite0,elite1,…</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="c1"># we'll fill in the number of elites after we know ELITE_FRAC and POP_SIZE</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"gen"</span><span class="p">,</span><span class="s2">"phase"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"elite</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="n">GRAPH_LOG_FN</span> <span class="o">=</span> <span class="s2">"graph_logs.csv"</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">GRAPH_LOG_FN</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">gf</span><span class="p">:</span>
    <span class="n">gf</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">"gen,mean_pop,mean_elite_pre,mean_elite_post,best_new,mean_children,gen_time</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1">#----------------------------------------------------------------------#</span>
<span class="c1">#Start of generations:</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">GENERATIONS</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># a) evaluate all fitness</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="c1">#calculate mean fitness of pop</span>
    <span class="n">mean_pop</span> <span class="o">=</span> <span class="n">fitness</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span>
    <span class="c1"># validation mse of best</span>
    <span class="n">m_best</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">GENERATIONS</span><span class="si">}</span><span class="s2"> ▶ "</span>
        <span class="sa">f</span><span class="s2">"train MSE: </span><span class="si">{</span><span class="n">fitness</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">val_curve</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># c) elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
    <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
    <span class="c1">#Elite fitness meaan pre </span>
    <span class="n">mean_elite_pre</span> <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">elite_idxs</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">elites</span> <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">]</span>
    <span class="c1">#Mean elite post, intiialised the same</span>
    <span class="n">mean_elite_post</span> <span class="o">=</span> <span class="n">mean_elite_pre</span>
    <span class="c1">#Fitness of elites by indexing into the fitness array:</span>
    <span class="n">elites_fitness</span> <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">elite_idxs</span><span class="p">]</span>

    <span class="n">mean_of_prerefined_elites</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">elites_fitness</span><span class="p">)</span>
   
    <span class="k">if</span> <span class="n">gen</span> <span class="o">%</span> <span class="n">REFINE_EVERY</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">gen</span> <span class="o">&lt;</span> <span class="mi">699</span><span class="p">:</span>
        <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
        <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
        <span class="n">pre_vals</span> <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">elite_idxs</span><span class="p">]</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">line</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">gen</span><span class="p">),</span> <span class="s2">"pre"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">pre_vals</span><span class="p">]</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Refinement @ gen </span><span class="si">{</span><span class="n">gen</span><span class="si">}</span><span class="s2"> ---"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">:</span>
            <span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">refine_with_sgd</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">post_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
            <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">post_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">mean_elite_post</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">post_vals</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">line</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">gen</span><span class="p">),</span> <span class="s2">"post"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">post_vals</span><span class="p">]</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># e) selection + reproduction</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="p">[</span> <span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span> <span class="p">]</span> 
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">POP_SIZE</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">crossover_and_mutate</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">)</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
    <span class="c1"># 5) Recompute fitness on the replaced population</span>
    <span class="n">new_fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">new_fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">new_fitness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">new_fitness</span><span class="p">)</span>
    <span class="c1"># best MSE in the new population</span>
    <span class="n">best_new</span> <span class="o">=</span> <span class="n">new_fitness</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="c1"># mean MSE of _just_ the children (everything after the first `elite_n` slots)</span>
    <span class="k">if</span> <span class="n">POP_SIZE</span> <span class="o">&gt;</span> <span class="n">elite_n</span><span class="p">:</span>
        <span class="n">mean_children</span> <span class="o">=</span> <span class="n">new_fitness</span><span class="p">[</span><span class="n">elite_n</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mean_children</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"nan"</span><span class="p">)</span>
    <span class="n">gen_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">GRAPH_LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">gf</span><span class="p">:</span>
        <span class="n">gf</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">gen</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">mean_pop</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">mean_elite_pre</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">mean_elite_post</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">best_new</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">mean_children</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">gen_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span>
        <span class="p">)</span>

    <span class="c1"># ─── very final: recompute fitness on the updated pop ─────────────────</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>

<span class="c1">#  8) Final Eval &amp; Plot </span>
<span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
<span class="n">best_model</span>  <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">best_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">✅ GA+SGD done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">"checkpoints"</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 1) Save the PyTorch state_dict</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">best_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
           <span class="s2">"checkpoints/best_model_weights.pth"</span><span class="p">)</span>
<span class="c1"># 2) Save the flat genome vector (numpy array)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"checkpoints/best_genome.npy"</span><span class="p">,</span> <span class="n">best_genome</span><span class="p">)</span>
<span class="c1"># 3) (Optional) Save arch + Optuna params so you can rebuild the same model</span>
<span class="n">meta</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"arch"</span><span class="p">:</span> <span class="n">arch</span><span class="p">,</span>
    <span class="s2">"best_params"</span><span class="p">:</span> <span class="n">best_params</span>
<span class="p">}</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"checkpoints/best_model_meta.json"</span><span class="p">,</span><span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">meta</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"✅ Best model weights, genome and metadata saved to ./checkpoints/"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">refine_gens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">600</span><span class="p">]</span>
<span class="n">refine_gens</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">refine_gens</span> <span class="k">if</span> <span class="n">g</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_curve</span><span class="p">)]</span>
<span class="n">refine_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">train_curve</span><span class="p">[</span><span class="n">g</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">refine_gens</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">refine_gens</span><span class="p">,</span> <span class="n">refine_vals</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'x'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Refinement with SGD'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA + Periodic SGD Refinement"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen 1/8000 ▶ train MSE: 1.0075, val MSE: 1.0054
Gen 2/8000 ▶ train MSE: 1.0075, val MSE: 1.0054
Gen 3/8000 ▶ train MSE: 1.0075, val MSE: 1.0054
Gen 4/8000 ▶ train MSE: 1.0073, val MSE: 1.0051
Gen 5/8000 ▶ train MSE: 1.0073, val MSE: 1.0051
Gen 6/8000 ▶ train MSE: 1.0073, val MSE: 1.0051
Gen 7/8000 ▶ train MSE: 1.0073, val MSE: 1.0051
Gen 8/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 9/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 10/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 11/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 12/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 13/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 14/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 15/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 16/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 17/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 18/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 19/8000 ▶ train MSE: 1.0072, val MSE: 1.0051
Gen 20/8000 ▶ train MSE: 1.0023, val MSE: 1.0025
Gen 21/8000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 22/8000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 23/8000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 24/8000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 25/8000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 26/8000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 27/8000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 28/8000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 29/8000 ▶ train MSE: 1.0011, val MSE: 1.0026
Gen 30/8000 ▶ train MSE: 1.0009, val MSE: 1.0004
Gen 31/8000 ▶ train MSE: 1.0001, val MSE: 1.0015
Gen 32/8000 ▶ train MSE: 1.0001, val MSE: 1.0015
Gen 33/8000 ▶ train MSE: 1.0001, val MSE: 1.0015
Gen 34/8000 ▶ train MSE: 1.0001, val MSE: 1.0015
Gen 35/8000 ▶ train MSE: 1.0001, val MSE: 1.0015
Gen 36/8000 ▶ train MSE: 1.0001, val MSE: 1.0013
Gen 37/8000 ▶ train MSE: 1.0001, val MSE: 1.0013
Gen 38/8000 ▶ train MSE: 1.0001, val MSE: 1.0013
Gen 39/8000 ▶ train MSE: 0.9995, val MSE: 1.0004
Gen 40/8000 ▶ train MSE: 0.9995, val MSE: 1.0004
Gen 41/8000 ▶ train MSE: 0.9995, val MSE: 1.0004
Gen 42/8000 ▶ train MSE: 0.9995, val MSE: 1.0004
Gen 43/8000 ▶ train MSE: 0.9995, val MSE: 1.0004
Gen 44/8000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 45/8000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 46/8000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 47/8000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 48/8000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 49/8000 ▶ train MSE: 0.9991, val MSE: 1.0000
Gen 50/8000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 51/8000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 52/8000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 53/8000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 54/8000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 55/8000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 56/8000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 57/8000 ▶ train MSE: 0.9987, val MSE: 1.0000
Gen 58/8000 ▶ train MSE: 0.9986, val MSE: 0.9985
Gen 59/8000 ▶ train MSE: 0.9981, val MSE: 0.9981
Gen 60/8000 ▶ train MSE: 0.9981, val MSE: 0.9981
Gen 61/8000 ▶ train MSE: 0.9981, val MSE: 0.9981
Gen 62/8000 ▶ train MSE: 0.9981, val MSE: 0.9981
Gen 63/8000 ▶ train MSE: 0.9981, val MSE: 0.9981
Gen 64/8000 ▶ train MSE: 0.9981, val MSE: 0.9996
Gen 65/8000 ▶ train MSE: 0.9981, val MSE: 0.9996
Gen 66/8000 ▶ train MSE: 0.9981, val MSE: 0.9996
Gen 67/8000 ▶ train MSE: 0.9980, val MSE: 0.9987
Gen 68/8000 ▶ train MSE: 0.9980, val MSE: 0.9987
Gen 69/8000 ▶ train MSE: 0.9979, val MSE: 0.9974
Gen 70/8000 ▶ train MSE: 0.9979, val MSE: 0.9974
Gen 71/8000 ▶ train MSE: 0.9977, val MSE: 0.9981
Gen 72/8000 ▶ train MSE: 0.9977, val MSE: 0.9981
Gen 73/8000 ▶ train MSE: 0.9977, val MSE: 0.9981
Gen 74/8000 ▶ train MSE: 0.9977, val MSE: 0.9981
Gen 75/8000 ▶ train MSE: 0.9977, val MSE: 0.9981
Gen 76/8000 ▶ train MSE: 0.9975, val MSE: 0.9986
Gen 77/8000 ▶ train MSE: 0.9975, val MSE: 0.9986
Gen 78/8000 ▶ train MSE: 0.9975, val MSE: 0.9986
Gen 79/8000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 80/8000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 81/8000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 82/8000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 83/8000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 84/8000 ▶ train MSE: 0.9975, val MSE: 0.9978
Gen 85/8000 ▶ train MSE: 0.9975, val MSE: 0.9979
Gen 86/8000 ▶ train MSE: 0.9975, val MSE: 0.9979
Gen 87/8000 ▶ train MSE: 0.9975, val MSE: 0.9979
Gen 88/8000 ▶ train MSE: 0.9974, val MSE: 0.9980
Gen 89/8000 ▶ train MSE: 0.9973, val MSE: 0.9972
Gen 90/8000 ▶ train MSE: 0.9973, val MSE: 0.9972
Gen 91/8000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 92/8000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 93/8000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 94/8000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 95/8000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 96/8000 ▶ train MSE: 0.9970, val MSE: 0.9978
Gen 97/8000 ▶ train MSE: 0.9968, val MSE: 0.9974
Gen 98/8000 ▶ train MSE: 0.9968, val MSE: 0.9974
Gen 99/8000 ▶ train MSE: 0.9968, val MSE: 0.9974
Gen 100/8000 ▶ train MSE: 0.9968, val MSE: 0.9974

--- Refinement @ gen 100 ---
Gen 101/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 102/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 103/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 104/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 105/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 106/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 107/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 108/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 109/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 110/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 111/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 112/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 113/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 114/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 115/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 116/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 117/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 118/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 119/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 120/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 121/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 122/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 123/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 124/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 125/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 126/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 127/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 128/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 129/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 130/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 131/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 132/8000 ▶ train MSE: 0.9709, val MSE: 0.9603
Gen 133/8000 ▶ train MSE: 0.9708, val MSE: 0.9584
Gen 134/8000 ▶ train MSE: 0.9704, val MSE: 0.9586
Gen 135/8000 ▶ train MSE: 0.9701, val MSE: 0.9581
Gen 136/8000 ▶ train MSE: 0.9699, val MSE: 0.9586
Gen 137/8000 ▶ train MSE: 0.9695, val MSE: 0.9579
Gen 138/8000 ▶ train MSE: 0.9689, val MSE: 0.9583
Gen 139/8000 ▶ train MSE: 0.9689, val MSE: 0.9575
Gen 140/8000 ▶ train MSE: 0.9687, val MSE: 0.9577
Gen 141/8000 ▶ train MSE: 0.9685, val MSE: 0.9571
Gen 142/8000 ▶ train MSE: 0.9682, val MSE: 0.9578
Gen 143/8000 ▶ train MSE: 0.9682, val MSE: 0.9573
Gen 144/8000 ▶ train MSE: 0.9681, val MSE: 0.9577
Gen 145/8000 ▶ train MSE: 0.9681, val MSE: 0.9577
Gen 146/8000 ▶ train MSE: 0.9679, val MSE: 0.9573
Gen 147/8000 ▶ train MSE: 0.9677, val MSE: 0.9563
Gen 148/8000 ▶ train MSE: 0.9675, val MSE: 0.9561
Gen 149/8000 ▶ train MSE: 0.9675, val MSE: 0.9561
Gen 150/8000 ▶ train MSE: 0.9675, val MSE: 0.9560
Gen 151/8000 ▶ train MSE: 0.9673, val MSE: 0.9552
Gen 152/8000 ▶ train MSE: 0.9673, val MSE: 0.9552
Gen 153/8000 ▶ train MSE: 0.9673, val MSE: 0.9552
Gen 154/8000 ▶ train MSE: 0.9673, val MSE: 0.9552
Gen 155/8000 ▶ train MSE: 0.9672, val MSE: 0.9560
Gen 156/8000 ▶ train MSE: 0.9670, val MSE: 0.9558
Gen 157/8000 ▶ train MSE: 0.9670, val MSE: 0.9558
Gen 158/8000 ▶ train MSE: 0.9670, val MSE: 0.9558
Gen 159/8000 ▶ train MSE: 0.9670, val MSE: 0.9550
Gen 160/8000 ▶ train MSE: 0.9670, val MSE: 0.9550
Gen 161/8000 ▶ train MSE: 0.9670, val MSE: 0.9550
Gen 162/8000 ▶ train MSE: 0.9670, val MSE: 0.9557
Gen 163/8000 ▶ train MSE: 0.9670, val MSE: 0.9557
Gen 164/8000 ▶ train MSE: 0.9669, val MSE: 0.9554
Gen 165/8000 ▶ train MSE: 0.9668, val MSE: 0.9555
Gen 166/8000 ▶ train MSE: 0.9668, val MSE: 0.9555
Gen 167/8000 ▶ train MSE: 0.9667, val MSE: 0.9546
Gen 168/8000 ▶ train MSE: 0.9667, val MSE: 0.9546
Gen 169/8000 ▶ train MSE: 0.9667, val MSE: 0.9545
Gen 170/8000 ▶ train MSE: 0.9667, val MSE: 0.9545
Gen 171/8000 ▶ train MSE: 0.9667, val MSE: 0.9545
Gen 172/8000 ▶ train MSE: 0.9667, val MSE: 0.9546
Gen 173/8000 ▶ train MSE: 0.9667, val MSE: 0.9546
Gen 174/8000 ▶ train MSE: 0.9667, val MSE: 0.9546
Gen 175/8000 ▶ train MSE: 0.9666, val MSE: 0.9549
Gen 176/8000 ▶ train MSE: 0.9666, val MSE: 0.9549
Gen 177/8000 ▶ train MSE: 0.9666, val MSE: 0.9549
Gen 178/8000 ▶ train MSE: 0.9666, val MSE: 0.9549
Gen 179/8000 ▶ train MSE: 0.9666, val MSE: 0.9549
Gen 180/8000 ▶ train MSE: 0.9666, val MSE: 0.9546
Gen 181/8000 ▶ train MSE: 0.9665, val MSE: 0.9549
Gen 182/8000 ▶ train MSE: 0.9665, val MSE: 0.9549
Gen 183/8000 ▶ train MSE: 0.9664, val MSE: 0.9552
Gen 184/8000 ▶ train MSE: 0.9664, val MSE: 0.9552
Gen 185/8000 ▶ train MSE: 0.9664, val MSE: 0.9552
Gen 186/8000 ▶ train MSE: 0.9664, val MSE: 0.9552
Gen 187/8000 ▶ train MSE: 0.9664, val MSE: 0.9552
Gen 188/8000 ▶ train MSE: 0.9664, val MSE: 0.9550
Gen 189/8000 ▶ train MSE: 0.9664, val MSE: 0.9550
Gen 190/8000 ▶ train MSE: 0.9664, val MSE: 0.9550
Gen 191/8000 ▶ train MSE: 0.9663, val MSE: 0.9542
Gen 192/8000 ▶ train MSE: 0.9663, val MSE: 0.9542
Gen 193/8000 ▶ train MSE: 0.9663, val MSE: 0.9542
Gen 194/8000 ▶ train MSE: 0.9662, val MSE: 0.9539
Gen 195/8000 ▶ train MSE: 0.9662, val MSE: 0.9539
Gen 196/8000 ▶ train MSE: 0.9662, val MSE: 0.9539
Gen 197/8000 ▶ train MSE: 0.9662, val MSE: 0.9539
Gen 198/8000 ▶ train MSE: 0.9662, val MSE: 0.9539
Gen 199/8000 ▶ train MSE: 0.9662, val MSE: 0.9539
Gen 200/8000 ▶ train MSE: 0.9661, val MSE: 0.9547

--- Refinement @ gen 200 ---
Gen 201/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 202/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 203/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 204/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 205/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 206/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 207/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 208/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 209/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 210/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 211/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 212/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 213/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 214/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 215/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 216/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 217/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 218/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 219/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 220/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 221/8000 ▶ train MSE: 0.8110, val MSE: 0.7011
Gen 222/8000 ▶ train MSE: 0.8084, val MSE: 0.6936
Gen 223/8000 ▶ train MSE: 0.8078, val MSE: 0.6954
Gen 224/8000 ▶ train MSE: 0.8019, val MSE: 0.6884
Gen 225/8000 ▶ train MSE: 0.8019, val MSE: 0.6884
Gen 226/8000 ▶ train MSE: 0.8012, val MSE: 0.6882
Gen 227/8000 ▶ train MSE: 0.8000, val MSE: 0.6857
Gen 228/8000 ▶ train MSE: 0.7983, val MSE: 0.6873
Gen 229/8000 ▶ train MSE: 0.7976, val MSE: 0.6855
Gen 230/8000 ▶ train MSE: 0.7971, val MSE: 0.6854
Gen 231/8000 ▶ train MSE: 0.7956, val MSE: 0.6823
Gen 232/8000 ▶ train MSE: 0.7948, val MSE: 0.6851
Gen 233/8000 ▶ train MSE: 0.7941, val MSE: 0.6769
Gen 234/8000 ▶ train MSE: 0.7941, val MSE: 0.6769
Gen 235/8000 ▶ train MSE: 0.7938, val MSE: 0.6790
Gen 236/8000 ▶ train MSE: 0.7931, val MSE: 0.6818
Gen 237/8000 ▶ train MSE: 0.7931, val MSE: 0.6818
Gen 238/8000 ▶ train MSE: 0.7894, val MSE: 0.6754
Gen 239/8000 ▶ train MSE: 0.7894, val MSE: 0.6754
Gen 240/8000 ▶ train MSE: 0.7894, val MSE: 0.6754
Gen 241/8000 ▶ train MSE: 0.7894, val MSE: 0.6754
Gen 242/8000 ▶ train MSE: 0.7893, val MSE: 0.6741
Gen 243/8000 ▶ train MSE: 0.7893, val MSE: 0.6741
Gen 244/8000 ▶ train MSE: 0.7893, val MSE: 0.6741
Gen 245/8000 ▶ train MSE: 0.7881, val MSE: 0.6749
Gen 246/8000 ▶ train MSE: 0.7881, val MSE: 0.6749
Gen 247/8000 ▶ train MSE: 0.7881, val MSE: 0.6749
Gen 248/8000 ▶ train MSE: 0.7881, val MSE: 0.6749
Gen 249/8000 ▶ train MSE: 0.7881, val MSE: 0.6749
Gen 250/8000 ▶ train MSE: 0.7881, val MSE: 0.6749
Gen 251/8000 ▶ train MSE: 0.7873, val MSE: 0.6751
Gen 252/8000 ▶ train MSE: 0.7873, val MSE: 0.6751
Gen 253/8000 ▶ train MSE: 0.7873, val MSE: 0.6751
Gen 254/8000 ▶ train MSE: 0.7863, val MSE: 0.6746
Gen 255/8000 ▶ train MSE: 0.7858, val MSE: 0.6738
Gen 256/8000 ▶ train MSE: 0.7853, val MSE: 0.6715
Gen 257/8000 ▶ train MSE: 0.7842, val MSE: 0.6696
Gen 258/8000 ▶ train MSE: 0.7837, val MSE: 0.6705
Gen 259/8000 ▶ train MSE: 0.7833, val MSE: 0.6706
Gen 260/8000 ▶ train MSE: 0.7833, val MSE: 0.6699
Gen 261/8000 ▶ train MSE: 0.7820, val MSE: 0.6680
Gen 262/8000 ▶ train MSE: 0.7820, val MSE: 0.6680
Gen 263/8000 ▶ train MSE: 0.7820, val MSE: 0.6680
Gen 264/8000 ▶ train MSE: 0.7816, val MSE: 0.6653
Gen 265/8000 ▶ train MSE: 0.7816, val MSE: 0.6653
Gen 266/8000 ▶ train MSE: 0.7813, val MSE: 0.6665
Gen 267/8000 ▶ train MSE: 0.7807, val MSE: 0.6681
Gen 268/8000 ▶ train MSE: 0.7807, val MSE: 0.6681
Gen 269/8000 ▶ train MSE: 0.7803, val MSE: 0.6675
Gen 270/8000 ▶ train MSE: 0.7794, val MSE: 0.6621
Gen 271/8000 ▶ train MSE: 0.7794, val MSE: 0.6621
Gen 272/8000 ▶ train MSE: 0.7794, val MSE: 0.6621
Gen 273/8000 ▶ train MSE: 0.7786, val MSE: 0.6659
Gen 274/8000 ▶ train MSE: 0.7786, val MSE: 0.6659
Gen 275/8000 ▶ train MSE: 0.7786, val MSE: 0.6659
Gen 276/8000 ▶ train MSE: 0.7786, val MSE: 0.6659
Gen 277/8000 ▶ train MSE: 0.7786, val MSE: 0.6659
Gen 278/8000 ▶ train MSE: 0.7786, val MSE: 0.6659
Gen 279/8000 ▶ train MSE: 0.7785, val MSE: 0.6664
Gen 280/8000 ▶ train MSE: 0.7785, val MSE: 0.6668
Gen 281/8000 ▶ train MSE: 0.7785, val MSE: 0.6668
Gen 282/8000 ▶ train MSE: 0.7785, val MSE: 0.6668
Gen 283/8000 ▶ train MSE: 0.7785, val MSE: 0.6668
Gen 284/8000 ▶ train MSE: 0.7780, val MSE: 0.6615
Gen 285/8000 ▶ train MSE: 0.7780, val MSE: 0.6615
Gen 286/8000 ▶ train MSE: 0.7780, val MSE: 0.6615
Gen 287/8000 ▶ train MSE: 0.7780, val MSE: 0.6615
Gen 288/8000 ▶ train MSE: 0.7772, val MSE: 0.6657
Gen 289/8000 ▶ train MSE: 0.7772, val MSE: 0.6657
Gen 290/8000 ▶ train MSE: 0.7771, val MSE: 0.6657
Gen 291/8000 ▶ train MSE: 0.7770, val MSE: 0.6654
Gen 292/8000 ▶ train MSE: 0.7768, val MSE: 0.6612
Gen 293/8000 ▶ train MSE: 0.7768, val MSE: 0.6612
Gen 294/8000 ▶ train MSE: 0.7764, val MSE: 0.6657
Gen 295/8000 ▶ train MSE: 0.7764, val MSE: 0.6657
Gen 296/8000 ▶ train MSE: 0.7764, val MSE: 0.6657
Gen 297/8000 ▶ train MSE: 0.7764, val MSE: 0.6657
Gen 298/8000 ▶ train MSE: 0.7764, val MSE: 0.6657
Gen 299/8000 ▶ train MSE: 0.7764, val MSE: 0.6657
Gen 300/8000 ▶ train MSE: 0.7764, val MSE: 0.6657

--- Refinement @ gen 300 ---
Gen 301/8000 ▶ train MSE: 0.7087, val MSE: 0.5411
Gen 302/8000 ▶ train MSE: 0.7085, val MSE: 0.5411
Gen 303/8000 ▶ train MSE: 0.7085, val MSE: 0.5411
Gen 304/8000 ▶ train MSE: 0.7085, val MSE: 0.5411
Gen 305/8000 ▶ train MSE: 0.7085, val MSE: 0.5411
Gen 306/8000 ▶ train MSE: 0.7085, val MSE: 0.5411
Gen 307/8000 ▶ train MSE: 0.7085, val MSE: 0.5411
Gen 308/8000 ▶ train MSE: 0.7085, val MSE: 0.5411
Gen 309/8000 ▶ train MSE: 0.7085, val MSE: 0.5411
Gen 310/8000 ▶ train MSE: 0.7085, val MSE: 0.5411
Gen 311/8000 ▶ train MSE: 0.7085, val MSE: 0.5411
Gen 312/8000 ▶ train MSE: 0.7083, val MSE: 0.5407
Gen 313/8000 ▶ train MSE: 0.7083, val MSE: 0.5407
Gen 314/8000 ▶ train MSE: 0.7083, val MSE: 0.5407
Gen 315/8000 ▶ train MSE: 0.7075, val MSE: 0.5402
Gen 316/8000 ▶ train MSE: 0.7072, val MSE: 0.5400
Gen 317/8000 ▶ train MSE: 0.7068, val MSE: 0.5392
Gen 318/8000 ▶ train MSE: 0.7058, val MSE: 0.5403
Gen 319/8000 ▶ train MSE: 0.7053, val MSE: 0.5366
Gen 320/8000 ▶ train MSE: 0.7034, val MSE: 0.5359
Gen 321/8000 ▶ train MSE: 0.7015, val MSE: 0.5323
Gen 322/8000 ▶ train MSE: 0.7009, val MSE: 0.5303
Gen 323/8000 ▶ train MSE: 0.6999, val MSE: 0.5315
Gen 324/8000 ▶ train MSE: 0.6973, val MSE: 0.5308
Gen 325/8000 ▶ train MSE: 0.6969, val MSE: 0.5316
Gen 326/8000 ▶ train MSE: 0.6949, val MSE: 0.5280
Gen 327/8000 ▶ train MSE: 0.6939, val MSE: 0.5224
Gen 328/8000 ▶ train MSE: 0.6926, val MSE: 0.5259
Gen 329/8000 ▶ train MSE: 0.6915, val MSE: 0.5251
Gen 330/8000 ▶ train MSE: 0.6910, val MSE: 0.5258
Gen 331/8000 ▶ train MSE: 0.6903, val MSE: 0.5233
Gen 332/8000 ▶ train MSE: 0.6896, val MSE: 0.5194
Gen 333/8000 ▶ train MSE: 0.6885, val MSE: 0.5204
Gen 334/8000 ▶ train MSE: 0.6885, val MSE: 0.5212
Gen 335/8000 ▶ train MSE: 0.6885, val MSE: 0.5212
Gen 336/8000 ▶ train MSE: 0.6877, val MSE: 0.5189
Gen 337/8000 ▶ train MSE: 0.6869, val MSE: 0.5183
Gen 338/8000 ▶ train MSE: 0.6868, val MSE: 0.5166
Gen 339/8000 ▶ train MSE: 0.6856, val MSE: 0.5160
Gen 340/8000 ▶ train MSE: 0.6856, val MSE: 0.5160
Gen 341/8000 ▶ train MSE: 0.6856, val MSE: 0.5160
Gen 342/8000 ▶ train MSE: 0.6856, val MSE: 0.5160
Gen 343/8000 ▶ train MSE: 0.6852, val MSE: 0.5133
Gen 344/8000 ▶ train MSE: 0.6852, val MSE: 0.5133
Gen 345/8000 ▶ train MSE: 0.6843, val MSE: 0.5136
Gen 346/8000 ▶ train MSE: 0.6843, val MSE: 0.5136
Gen 347/8000 ▶ train MSE: 0.6837, val MSE: 0.5140
Gen 348/8000 ▶ train MSE: 0.6837, val MSE: 0.5140
Gen 349/8000 ▶ train MSE: 0.6837, val MSE: 0.5140
Gen 350/8000 ▶ train MSE: 0.6837, val MSE: 0.5140
Gen 351/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 352/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 353/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 354/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 355/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 356/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 357/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 358/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 359/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 360/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 361/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 362/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 363/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 364/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 365/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 366/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 367/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 368/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 369/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 370/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 371/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 372/8000 ▶ train MSE: 0.6811, val MSE: 0.5108
Gen 373/8000 ▶ train MSE: 0.6808, val MSE: 0.5110
Gen 374/8000 ▶ train MSE: 0.6808, val MSE: 0.5127
Gen 375/8000 ▶ train MSE: 0.6802, val MSE: 0.5114
Gen 376/8000 ▶ train MSE: 0.6802, val MSE: 0.5114
Gen 377/8000 ▶ train MSE: 0.6796, val MSE: 0.5079
Gen 378/8000 ▶ train MSE: 0.6791, val MSE: 0.5100
Gen 379/8000 ▶ train MSE: 0.6791, val MSE: 0.5100
Gen 380/8000 ▶ train MSE: 0.6787, val MSE: 0.5079
Gen 381/8000 ▶ train MSE: 0.6787, val MSE: 0.5079
Gen 382/8000 ▶ train MSE: 0.6784, val MSE: 0.5052
Gen 383/8000 ▶ train MSE: 0.6784, val MSE: 0.5072
Gen 384/8000 ▶ train MSE: 0.6784, val MSE: 0.5072
Gen 385/8000 ▶ train MSE: 0.6784, val MSE: 0.5072
Gen 386/8000 ▶ train MSE: 0.6784, val MSE: 0.5072
Gen 387/8000 ▶ train MSE: 0.6780, val MSE: 0.5064
Gen 388/8000 ▶ train MSE: 0.6771, val MSE: 0.5059
Gen 389/8000 ▶ train MSE: 0.6771, val MSE: 0.5059
Gen 390/8000 ▶ train MSE: 0.6771, val MSE: 0.5059
Gen 391/8000 ▶ train MSE: 0.6771, val MSE: 0.5059
Gen 392/8000 ▶ train MSE: 0.6771, val MSE: 0.5059
Gen 393/8000 ▶ train MSE: 0.6771, val MSE: 0.5059
Gen 394/8000 ▶ train MSE: 0.6771, val MSE: 0.5023
Gen 395/8000 ▶ train MSE: 0.6771, val MSE: 0.5023
Gen 396/8000 ▶ train MSE: 0.6771, val MSE: 0.5023
Gen 397/8000 ▶ train MSE: 0.6771, val MSE: 0.5023
Gen 398/8000 ▶ train MSE: 0.6771, val MSE: 0.5023
Gen 399/8000 ▶ train MSE: 0.6771, val MSE: 0.5023
Gen 400/8000 ▶ train MSE: 0.6771, val MSE: 0.5023

--- Refinement @ gen 400 ---
Gen 401/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 402/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 403/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 404/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 405/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 406/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 407/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 408/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 409/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 410/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 411/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 412/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 413/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 414/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 415/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 416/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 417/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 418/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 419/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 420/8000 ▶ train MSE: 0.6512, val MSE: 0.4522
Gen 421/8000 ▶ train MSE: 0.6505, val MSE: 0.4549
Gen 422/8000 ▶ train MSE: 0.6478, val MSE: 0.4498
Gen 423/8000 ▶ train MSE: 0.6465, val MSE: 0.4463
Gen 424/8000 ▶ train MSE: 0.6462, val MSE: 0.4496
Gen 425/8000 ▶ train MSE: 0.6442, val MSE: 0.4471
Gen 426/8000 ▶ train MSE: 0.6442, val MSE: 0.4471
Gen 427/8000 ▶ train MSE: 0.6437, val MSE: 0.4485
Gen 428/8000 ▶ train MSE: 0.6428, val MSE: 0.4422
Gen 429/8000 ▶ train MSE: 0.6422, val MSE: 0.4451
Gen 430/8000 ▶ train MSE: 0.6400, val MSE: 0.4409
Gen 431/8000 ▶ train MSE: 0.6396, val MSE: 0.4399
Gen 432/8000 ▶ train MSE: 0.6395, val MSE: 0.4394
Gen 433/8000 ▶ train MSE: 0.6381, val MSE: 0.4365
Gen 434/8000 ▶ train MSE: 0.6379, val MSE: 0.4376
Gen 435/8000 ▶ train MSE: 0.6359, val MSE: 0.4371
Gen 436/8000 ▶ train MSE: 0.6355, val MSE: 0.4266
Gen 437/8000 ▶ train MSE: 0.6355, val MSE: 0.4266
Gen 438/8000 ▶ train MSE: 0.6355, val MSE: 0.4266
Gen 439/8000 ▶ train MSE: 0.6355, val MSE: 0.4266
Gen 440/8000 ▶ train MSE: 0.6355, val MSE: 0.4266
Gen 441/8000 ▶ train MSE: 0.6332, val MSE: 0.4276
Gen 442/8000 ▶ train MSE: 0.6332, val MSE: 0.4276
Gen 443/8000 ▶ train MSE: 0.6332, val MSE: 0.4276
Gen 444/8000 ▶ train MSE: 0.6332, val MSE: 0.4276
Gen 445/8000 ▶ train MSE: 0.6332, val MSE: 0.4276
Gen 446/8000 ▶ train MSE: 0.6332, val MSE: 0.4276
Gen 447/8000 ▶ train MSE: 0.6310, val MSE: 0.4266
Gen 448/8000 ▶ train MSE: 0.6310, val MSE: 0.4266
Gen 449/8000 ▶ train MSE: 0.6310, val MSE: 0.4266
Gen 450/8000 ▶ train MSE: 0.6310, val MSE: 0.4266
Gen 451/8000 ▶ train MSE: 0.6310, val MSE: 0.4266
Gen 452/8000 ▶ train MSE: 0.6310, val MSE: 0.4266
Gen 453/8000 ▶ train MSE: 0.6305, val MSE: 0.4276
Gen 454/8000 ▶ train MSE: 0.6305, val MSE: 0.4276
Gen 455/8000 ▶ train MSE: 0.6303, val MSE: 0.4251
Gen 456/8000 ▶ train MSE: 0.6303, val MSE: 0.4251
Gen 457/8000 ▶ train MSE: 0.6303, val MSE: 0.4251
Gen 458/8000 ▶ train MSE: 0.6303, val MSE: 0.4251
Gen 459/8000 ▶ train MSE: 0.6303, val MSE: 0.4251
Gen 460/8000 ▶ train MSE: 0.6303, val MSE: 0.4251
Gen 461/8000 ▶ train MSE: 0.6303, val MSE: 0.4251
Gen 462/8000 ▶ train MSE: 0.6294, val MSE: 0.4260
Gen 463/8000 ▶ train MSE: 0.6294, val MSE: 0.4260
Gen 464/8000 ▶ train MSE: 0.6294, val MSE: 0.4260
Gen 465/8000 ▶ train MSE: 0.6294, val MSE: 0.4260
Gen 466/8000 ▶ train MSE: 0.6294, val MSE: 0.4260
Gen 467/8000 ▶ train MSE: 0.6294, val MSE: 0.4260
Gen 468/8000 ▶ train MSE: 0.6292, val MSE: 0.4214
Gen 469/8000 ▶ train MSE: 0.6281, val MSE: 0.4222
Gen 470/8000 ▶ train MSE: 0.6281, val MSE: 0.4222
Gen 471/8000 ▶ train MSE: 0.6281, val MSE: 0.4222
Gen 472/8000 ▶ train MSE: 0.6281, val MSE: 0.4222
Gen 473/8000 ▶ train MSE: 0.6281, val MSE: 0.4222
Gen 474/8000 ▶ train MSE: 0.6281, val MSE: 0.4222
Gen 475/8000 ▶ train MSE: 0.6281, val MSE: 0.4222
Gen 476/8000 ▶ train MSE: 0.6281, val MSE: 0.4222
Gen 477/8000 ▶ train MSE: 0.6281, val MSE: 0.4222
Gen 478/8000 ▶ train MSE: 0.6277, val MSE: 0.4203
Gen 479/8000 ▶ train MSE: 0.6277, val MSE: 0.4203
Gen 480/8000 ▶ train MSE: 0.6277, val MSE: 0.4203
Gen 481/8000 ▶ train MSE: 0.6277, val MSE: 0.4203
Gen 482/8000 ▶ train MSE: 0.6277, val MSE: 0.4203
Gen 483/8000 ▶ train MSE: 0.6277, val MSE: 0.4203
Gen 484/8000 ▶ train MSE: 0.6277, val MSE: 0.4203
Gen 485/8000 ▶ train MSE: 0.6277, val MSE: 0.4203
Gen 486/8000 ▶ train MSE: 0.6275, val MSE: 0.4204
Gen 487/8000 ▶ train MSE: 0.6267, val MSE: 0.4218
Gen 488/8000 ▶ train MSE: 0.6265, val MSE: 0.4218
Gen 489/8000 ▶ train MSE: 0.6263, val MSE: 0.4207
Gen 490/8000 ▶ train MSE: 0.6260, val MSE: 0.4201
Gen 491/8000 ▶ train MSE: 0.6260, val MSE: 0.4201
Gen 492/8000 ▶ train MSE: 0.6260, val MSE: 0.4201
Gen 493/8000 ▶ train MSE: 0.6258, val MSE: 0.4146
Gen 494/8000 ▶ train MSE: 0.6253, val MSE: 0.4195
Gen 495/8000 ▶ train MSE: 0.6245, val MSE: 0.4185
Gen 496/8000 ▶ train MSE: 0.6245, val MSE: 0.4185
Gen 497/8000 ▶ train MSE: 0.6244, val MSE: 0.4184
Gen 498/8000 ▶ train MSE: 0.6244, val MSE: 0.4184
Gen 499/8000 ▶ train MSE: 0.6244, val MSE: 0.4184
Gen 500/8000 ▶ train MSE: 0.6242, val MSE: 0.4172

--- Refinement @ gen 500 ---
Gen 501/8000 ▶ train MSE: 0.6253, val MSE: 0.4188
Gen 502/8000 ▶ train MSE: 0.6252, val MSE: 0.4222
Gen 503/8000 ▶ train MSE: 0.6252, val MSE: 0.4222
Gen 504/8000 ▶ train MSE: 0.6252, val MSE: 0.4222
Gen 505/8000 ▶ train MSE: 0.6249, val MSE: 0.4180
Gen 506/8000 ▶ train MSE: 0.6248, val MSE: 0.4194
Gen 507/8000 ▶ train MSE: 0.6241, val MSE: 0.4203
Gen 508/8000 ▶ train MSE: 0.6241, val MSE: 0.4203
Gen 509/8000 ▶ train MSE: 0.6240, val MSE: 0.4209
Gen 510/8000 ▶ train MSE: 0.6233, val MSE: 0.4152
Gen 511/8000 ▶ train MSE: 0.6232, val MSE: 0.4186
Gen 512/8000 ▶ train MSE: 0.6223, val MSE: 0.4156
Gen 513/8000 ▶ train MSE: 0.6223, val MSE: 0.4156
Gen 514/8000 ▶ train MSE: 0.6221, val MSE: 0.4147
Gen 515/8000 ▶ train MSE: 0.6221, val MSE: 0.4147
Gen 516/8000 ▶ train MSE: 0.6218, val MSE: 0.4150
Gen 517/8000 ▶ train MSE: 0.6218, val MSE: 0.4150
Gen 518/8000 ▶ train MSE: 0.6213, val MSE: 0.4111
Gen 519/8000 ▶ train MSE: 0.6213, val MSE: 0.4111
Gen 520/8000 ▶ train MSE: 0.6211, val MSE: 0.4144
Gen 521/8000 ▶ train MSE: 0.6211, val MSE: 0.4144
Gen 522/8000 ▶ train MSE: 0.6211, val MSE: 0.4144
Gen 523/8000 ▶ train MSE: 0.6211, val MSE: 0.4144
Gen 524/8000 ▶ train MSE: 0.6211, val MSE: 0.4144
Gen 525/8000 ▶ train MSE: 0.6211, val MSE: 0.4144
Gen 526/8000 ▶ train MSE: 0.6208, val MSE: 0.4135
Gen 527/8000 ▶ train MSE: 0.6208, val MSE: 0.4135
Gen 528/8000 ▶ train MSE: 0.6205, val MSE: 0.4083
Gen 529/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 530/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 531/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 532/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 533/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 534/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 535/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 536/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 537/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 538/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 539/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 540/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 541/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 542/8000 ▶ train MSE: 0.6197, val MSE: 0.4134
Gen 543/8000 ▶ train MSE: 0.6195, val MSE: 0.4129
Gen 544/8000 ▶ train MSE: 0.6189, val MSE: 0.4084
Gen 545/8000 ▶ train MSE: 0.6189, val MSE: 0.4084
Gen 546/8000 ▶ train MSE: 0.6189, val MSE: 0.4084
Gen 547/8000 ▶ train MSE: 0.6189, val MSE: 0.4084
Gen 548/8000 ▶ train MSE: 0.6189, val MSE: 0.4084
Gen 549/8000 ▶ train MSE: 0.6189, val MSE: 0.4084
Gen 550/8000 ▶ train MSE: 0.6189, val MSE: 0.4084
Gen 551/8000 ▶ train MSE: 0.6189, val MSE: 0.4084
Gen 552/8000 ▶ train MSE: 0.6189, val MSE: 0.4084
Gen 553/8000 ▶ train MSE: 0.6189, val MSE: 0.4084
Gen 554/8000 ▶ train MSE: 0.6189, val MSE: 0.4084
Gen 555/8000 ▶ train MSE: 0.6186, val MSE: 0.4131
Gen 556/8000 ▶ train MSE: 0.6186, val MSE: 0.4131
Gen 557/8000 ▶ train MSE: 0.6182, val MSE: 0.4125
Gen 558/8000 ▶ train MSE: 0.6182, val MSE: 0.4125
Gen 559/8000 ▶ train MSE: 0.6182, val MSE: 0.4125
Gen 560/8000 ▶ train MSE: 0.6180, val MSE: 0.4106
Gen 561/8000 ▶ train MSE: 0.6180, val MSE: 0.4106
Gen 562/8000 ▶ train MSE: 0.6180, val MSE: 0.4106
Gen 563/8000 ▶ train MSE: 0.6180, val MSE: 0.4106
Gen 564/8000 ▶ train MSE: 0.6180, val MSE: 0.4106
Gen 565/8000 ▶ train MSE: 0.6180, val MSE: 0.4106
Gen 566/8000 ▶ train MSE: 0.6180, val MSE: 0.4106
Gen 567/8000 ▶ train MSE: 0.6180, val MSE: 0.4106
Gen 568/8000 ▶ train MSE: 0.6180, val MSE: 0.4106
Gen 569/8000 ▶ train MSE: 0.6180, val MSE: 0.4106
Gen 570/8000 ▶ train MSE: 0.6178, val MSE: 0.4096
Gen 571/8000 ▶ train MSE: 0.6170, val MSE: 0.4089
Gen 572/8000 ▶ train MSE: 0.6170, val MSE: 0.4089
Gen 573/8000 ▶ train MSE: 0.6170, val MSE: 0.4089
Gen 574/8000 ▶ train MSE: 0.6170, val MSE: 0.4089
Gen 575/8000 ▶ train MSE: 0.6170, val MSE: 0.4089
Gen 576/8000 ▶ train MSE: 0.6170, val MSE: 0.4089
Gen 577/8000 ▶ train MSE: 0.6170, val MSE: 0.4089
Gen 578/8000 ▶ train MSE: 0.6169, val MSE: 0.4073
Gen 579/8000 ▶ train MSE: 0.6167, val MSE: 0.4086
Gen 580/8000 ▶ train MSE: 0.6165, val MSE: 0.4077
Gen 581/8000 ▶ train MSE: 0.6161, val MSE: 0.4086
Gen 582/8000 ▶ train MSE: 0.6161, val MSE: 0.4086
Gen 583/8000 ▶ train MSE: 0.6161, val MSE: 0.4086
Gen 584/8000 ▶ train MSE: 0.6161, val MSE: 0.4086
Gen 585/8000 ▶ train MSE: 0.6161, val MSE: 0.4086
Gen 586/8000 ▶ train MSE: 0.6161, val MSE: 0.4086
Gen 587/8000 ▶ train MSE: 0.6161, val MSE: 0.4086
Gen 588/8000 ▶ train MSE: 0.6160, val MSE: 0.4063
Gen 589/8000 ▶ train MSE: 0.6160, val MSE: 0.4063
Gen 590/8000 ▶ train MSE: 0.6160, val MSE: 0.4063
Gen 591/8000 ▶ train MSE: 0.6157, val MSE: 0.4063
Gen 592/8000 ▶ train MSE: 0.6156, val MSE: 0.4061
Gen 593/8000 ▶ train MSE: 0.6156, val MSE: 0.4061
Gen 594/8000 ▶ train MSE: 0.6156, val MSE: 0.4061
Gen 595/8000 ▶ train MSE: 0.6153, val MSE: 0.4057
Gen 596/8000 ▶ train MSE: 0.6153, val MSE: 0.4057
Gen 597/8000 ▶ train MSE: 0.6153, val MSE: 0.4057
Gen 598/8000 ▶ train MSE: 0.6152, val MSE: 0.4067
Gen 599/8000 ▶ train MSE: 0.6150, val MSE: 0.4050
Gen 600/8000 ▶ train MSE: 0.6150, val MSE: 0.4050

--- Refinement @ gen 600 ---
Gen 601/8000 ▶ train MSE: 0.6158, val MSE: 0.4066
Gen 602/8000 ▶ train MSE: 0.6158, val MSE: 0.4066
Gen 603/8000 ▶ train MSE: 0.6158, val MSE: 0.4066
Gen 604/8000 ▶ train MSE: 0.6158, val MSE: 0.4066
Gen 605/8000 ▶ train MSE: 0.6158, val MSE: 0.4066
Gen 606/8000 ▶ train MSE: 0.6151, val MSE: 0.4061
Gen 607/8000 ▶ train MSE: 0.6151, val MSE: 0.4061
Gen 608/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 609/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 610/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 611/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 612/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 613/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 614/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 615/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 616/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 617/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 618/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 619/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 620/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 621/8000 ▶ train MSE: 0.6150, val MSE: 0.4029
Gen 622/8000 ▶ train MSE: 0.6150, val MSE: 0.4027
Gen 623/8000 ▶ train MSE: 0.6150, val MSE: 0.4027
Gen 624/8000 ▶ train MSE: 0.6150, val MSE: 0.4027
Gen 625/8000 ▶ train MSE: 0.6150, val MSE: 0.4031
Gen 626/8000 ▶ train MSE: 0.6150, val MSE: 0.4031
Gen 627/8000 ▶ train MSE: 0.6145, val MSE: 0.4044
Gen 628/8000 ▶ train MSE: 0.6145, val MSE: 0.4044
Gen 629/8000 ▶ train MSE: 0.6145, val MSE: 0.4044
Gen 630/8000 ▶ train MSE: 0.6141, val MSE: 0.4043
Gen 631/8000 ▶ train MSE: 0.6141, val MSE: 0.4043
Gen 632/8000 ▶ train MSE: 0.6141, val MSE: 0.4028
Gen 633/8000 ▶ train MSE: 0.6138, val MSE: 0.4035
Gen 634/8000 ▶ train MSE: 0.6138, val MSE: 0.4035
Gen 635/8000 ▶ train MSE: 0.6138, val MSE: 0.4035
Gen 636/8000 ▶ train MSE: 0.6138, val MSE: 0.4020
Gen 637/8000 ▶ train MSE: 0.6138, val MSE: 0.4020
Gen 638/8000 ▶ train MSE: 0.6138, val MSE: 0.4020
Gen 639/8000 ▶ train MSE: 0.6136, val MSE: 0.4035
Gen 640/8000 ▶ train MSE: 0.6136, val MSE: 0.4035
Gen 641/8000 ▶ train MSE: 0.6134, val MSE: 0.4021
Gen 642/8000 ▶ train MSE: 0.6134, val MSE: 0.4021
Gen 643/8000 ▶ train MSE: 0.6134, val MSE: 0.4021
Gen 644/8000 ▶ train MSE: 0.6134, val MSE: 0.4004
Gen 645/8000 ▶ train MSE: 0.6134, val MSE: 0.4004
Gen 646/8000 ▶ train MSE: 0.6134, val MSE: 0.4026
Gen 647/8000 ▶ train MSE: 0.6134, val MSE: 0.4026
Gen 648/8000 ▶ train MSE: 0.6134, val MSE: 0.4026
Gen 649/8000 ▶ train MSE: 0.6132, val MSE: 0.4013
Gen 650/8000 ▶ train MSE: 0.6125, val MSE: 0.3988
Gen 651/8000 ▶ train MSE: 0.6125, val MSE: 0.3988
Gen 652/8000 ▶ train MSE: 0.6125, val MSE: 0.3988
Gen 653/8000 ▶ train MSE: 0.6125, val MSE: 0.3988
Gen 654/8000 ▶ train MSE: 0.6125, val MSE: 0.3988
Gen 655/8000 ▶ train MSE: 0.6125, val MSE: 0.3988
Gen 656/8000 ▶ train MSE: 0.6125, val MSE: 0.3988
Gen 657/8000 ▶ train MSE: 0.6125, val MSE: 0.3988
Gen 658/8000 ▶ train MSE: 0.6125, val MSE: 0.3988
Gen 659/8000 ▶ train MSE: 0.6125, val MSE: 0.3988
Gen 660/8000 ▶ train MSE: 0.6123, val MSE: 0.4023
Gen 661/8000 ▶ train MSE: 0.6123, val MSE: 0.4023
Gen 662/8000 ▶ train MSE: 0.6123, val MSE: 0.4023
Gen 663/8000 ▶ train MSE: 0.6123, val MSE: 0.4023
Gen 664/8000 ▶ train MSE: 0.6123, val MSE: 0.4005
Gen 665/8000 ▶ train MSE: 0.6123, val MSE: 0.4005
Gen 666/8000 ▶ train MSE: 0.6123, val MSE: 0.4012
Gen 667/8000 ▶ train MSE: 0.6121, val MSE: 0.3998
Gen 668/8000 ▶ train MSE: 0.6121, val MSE: 0.3998
Gen 669/8000 ▶ train MSE: 0.6121, val MSE: 0.3998
Gen 670/8000 ▶ train MSE: 0.6121, val MSE: 0.3998
Gen 671/8000 ▶ train MSE: 0.6121, val MSE: 0.3998
Gen 672/8000 ▶ train MSE: 0.6119, val MSE: 0.3995
Gen 673/8000 ▶ train MSE: 0.6119, val MSE: 0.3995
Gen 674/8000 ▶ train MSE: 0.6119, val MSE: 0.3995
Gen 675/8000 ▶ train MSE: 0.6119, val MSE: 0.3995
Gen 676/8000 ▶ train MSE: 0.6119, val MSE: 0.3995
Gen 677/8000 ▶ train MSE: 0.6119, val MSE: 0.3995
Gen 678/8000 ▶ train MSE: 0.6118, val MSE: 0.4007
Gen 679/8000 ▶ train MSE: 0.6117, val MSE: 0.3996
Gen 680/8000 ▶ train MSE: 0.6117, val MSE: 0.3996
Gen 681/8000 ▶ train MSE: 0.6115, val MSE: 0.3992
Gen 682/8000 ▶ train MSE: 0.6115, val MSE: 0.3990
Gen 683/8000 ▶ train MSE: 0.6114, val MSE: 0.3996
Gen 684/8000 ▶ train MSE: 0.6114, val MSE: 0.3996
Gen 685/8000 ▶ train MSE: 0.6112, val MSE: 0.3972
Gen 686/8000 ▶ train MSE: 0.6112, val MSE: 0.3972
Gen 687/8000 ▶ train MSE: 0.6112, val MSE: 0.3972
Gen 688/8000 ▶ train MSE: 0.6112, val MSE: 0.3972
Gen 689/8000 ▶ train MSE: 0.6112, val MSE: 0.3972
Gen 690/8000 ▶ train MSE: 0.6112, val MSE: 0.3972
Gen 691/8000 ▶ train MSE: 0.6112, val MSE: 0.3972
Gen 692/8000 ▶ train MSE: 0.6112, val MSE: 0.3972
Gen 693/8000 ▶ train MSE: 0.6112, val MSE: 0.3972
Gen 694/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 695/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 696/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 697/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 698/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 699/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 700/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 701/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 702/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 703/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 704/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 705/8000 ▶ train MSE: 0.6112, val MSE: 0.3984
Gen 706/8000 ▶ train MSE: 0.6111, val MSE: 0.3989
Gen 707/8000 ▶ train MSE: 0.6111, val MSE: 0.3989
Gen 708/8000 ▶ train MSE: 0.6111, val MSE: 0.3989
Gen 709/8000 ▶ train MSE: 0.6111, val MSE: 0.3989
Gen 710/8000 ▶ train MSE: 0.6107, val MSE: 0.3978
Gen 711/8000 ▶ train MSE: 0.6107, val MSE: 0.3978
Gen 712/8000 ▶ train MSE: 0.6107, val MSE: 0.3978
Gen 713/8000 ▶ train MSE: 0.6106, val MSE: 0.3983
Gen 714/8000 ▶ train MSE: 0.6106, val MSE: 0.3983
Gen 715/8000 ▶ train MSE: 0.6106, val MSE: 0.3983
Gen 716/8000 ▶ train MSE: 0.6106, val MSE: 0.3983
Gen 717/8000 ▶ train MSE: 0.6106, val MSE: 0.3983
Gen 718/8000 ▶ train MSE: 0.6106, val MSE: 0.3983
Gen 719/8000 ▶ train MSE: 0.6106, val MSE: 0.3976
Gen 720/8000 ▶ train MSE: 0.6104, val MSE: 0.3972
Gen 721/8000 ▶ train MSE: 0.6103, val MSE: 0.3978
Gen 722/8000 ▶ train MSE: 0.6103, val MSE: 0.3978
Gen 723/8000 ▶ train MSE: 0.6103, val MSE: 0.3978
Gen 724/8000 ▶ train MSE: 0.6103, val MSE: 0.3978
Gen 725/8000 ▶ train MSE: 0.6101, val MSE: 0.3952
Gen 726/8000 ▶ train MSE: 0.6101, val MSE: 0.3952
Gen 727/8000 ▶ train MSE: 0.6100, val MSE: 0.3960
Gen 728/8000 ▶ train MSE: 0.6100, val MSE: 0.3960
Gen 729/8000 ▶ train MSE: 0.6100, val MSE: 0.3960
Gen 730/8000 ▶ train MSE: 0.6100, val MSE: 0.3960
Gen 731/8000 ▶ train MSE: 0.6100, val MSE: 0.3960
Gen 732/8000 ▶ train MSE: 0.6100, val MSE: 0.3960
Gen 733/8000 ▶ train MSE: 0.6100, val MSE: 0.3960
Gen 734/8000 ▶ train MSE: 0.6100, val MSE: 0.3973
Gen 735/8000 ▶ train MSE: 0.6096, val MSE: 0.3948
Gen 736/8000 ▶ train MSE: 0.6096, val MSE: 0.3948
Gen 737/8000 ▶ train MSE: 0.6096, val MSE: 0.3948
Gen 738/8000 ▶ train MSE: 0.6096, val MSE: 0.3948
Gen 739/8000 ▶ train MSE: 0.6096, val MSE: 0.3948
Gen 740/8000 ▶ train MSE: 0.6096, val MSE: 0.3948
Gen 741/8000 ▶ train MSE: 0.6095, val MSE: 0.3948
Gen 742/8000 ▶ train MSE: 0.6095, val MSE: 0.3948
Gen 743/8000 ▶ train MSE: 0.6095, val MSE: 0.3946
Gen 744/8000 ▶ train MSE: 0.6094, val MSE: 0.3944
Gen 745/8000 ▶ train MSE: 0.6094, val MSE: 0.3944
Gen 746/8000 ▶ train MSE: 0.6094, val MSE: 0.3944
Gen 747/8000 ▶ train MSE: 0.6094, val MSE: 0.3944
Gen 748/8000 ▶ train MSE: 0.6092, val MSE: 0.3929
Gen 749/8000 ▶ train MSE: 0.6092, val MSE: 0.3929
Gen 750/8000 ▶ train MSE: 0.6092, val MSE: 0.3929
Gen 751/8000 ▶ train MSE: 0.6092, val MSE: 0.3929
Gen 752/8000 ▶ train MSE: 0.6089, val MSE: 0.3942
Gen 753/8000 ▶ train MSE: 0.6089, val MSE: 0.3942
Gen 754/8000 ▶ train MSE: 0.6089, val MSE: 0.3942
Gen 755/8000 ▶ train MSE: 0.6089, val MSE: 0.3942
Gen 756/8000 ▶ train MSE: 0.6089, val MSE: 0.3942
Gen 757/8000 ▶ train MSE: 0.6089, val MSE: 0.3942
Gen 758/8000 ▶ train MSE: 0.6089, val MSE: 0.3942
Gen 759/8000 ▶ train MSE: 0.6089, val MSE: 0.3942
Gen 760/8000 ▶ train MSE: 0.6089, val MSE: 0.3942
Gen 761/8000 ▶ train MSE: 0.6089, val MSE: 0.3943
Gen 762/8000 ▶ train MSE: 0.6089, val MSE: 0.3940
Gen 763/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 764/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 765/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 766/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 767/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 768/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 769/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 770/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 771/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 772/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 773/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 774/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 775/8000 ▶ train MSE: 0.6085, val MSE: 0.3932
Gen 776/8000 ▶ train MSE: 0.6083, val MSE: 0.3934
Gen 777/8000 ▶ train MSE: 0.6083, val MSE: 0.3934
Gen 778/8000 ▶ train MSE: 0.6083, val MSE: 0.3934
Gen 779/8000 ▶ train MSE: 0.6083, val MSE: 0.3934
Gen 780/8000 ▶ train MSE: 0.6083, val MSE: 0.3934
Gen 781/8000 ▶ train MSE: 0.6083, val MSE: 0.3934
Gen 782/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 783/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 784/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 785/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 786/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 787/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 788/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 789/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 790/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 791/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 792/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 793/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 794/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 795/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 796/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 797/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 798/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 799/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 800/8000 ▶ train MSE: 0.6081, val MSE: 0.3921
Gen 801/8000 ▶ train MSE: 0.6081, val MSE: 0.3924
Gen 802/8000 ▶ train MSE: 0.6081, val MSE: 0.3912
Gen 803/8000 ▶ train MSE: 0.6078, val MSE: 0.3917
Gen 804/8000 ▶ train MSE: 0.6078, val MSE: 0.3917
Gen 805/8000 ▶ train MSE: 0.6078, val MSE: 0.3917
Gen 806/8000 ▶ train MSE: 0.6078, val MSE: 0.3917
Gen 807/8000 ▶ train MSE: 0.6078, val MSE: 0.3917
Gen 808/8000 ▶ train MSE: 0.6078, val MSE: 0.3917
Gen 809/8000 ▶ train MSE: 0.6078, val MSE: 0.3917
Gen 810/8000 ▶ train MSE: 0.6078, val MSE: 0.3917
Gen 811/8000 ▶ train MSE: 0.6078, val MSE: 0.3917
Gen 812/8000 ▶ train MSE: 0.6078, val MSE: 0.3917
Gen 813/8000 ▶ train MSE: 0.6078, val MSE: 0.3917
Gen 814/8000 ▶ train MSE: 0.6077, val MSE: 0.3920
Gen 815/8000 ▶ train MSE: 0.6077, val MSE: 0.3920
Gen 816/8000 ▶ train MSE: 0.6077, val MSE: 0.3920
Gen 817/8000 ▶ train MSE: 0.6077, val MSE: 0.3920
Gen 818/8000 ▶ train MSE: 0.6073, val MSE: 0.3916
Gen 819/8000 ▶ train MSE: 0.6073, val MSE: 0.3916
Gen 820/8000 ▶ train MSE: 0.6073, val MSE: 0.3916
Gen 821/8000 ▶ train MSE: 0.6073, val MSE: 0.3916
Gen 822/8000 ▶ train MSE: 0.6073, val MSE: 0.3916
Gen 823/8000 ▶ train MSE: 0.6073, val MSE: 0.3916
Gen 824/8000 ▶ train MSE: 0.6073, val MSE: 0.3916
Gen 825/8000 ▶ train MSE: 0.6073, val MSE: 0.3916
Gen 826/8000 ▶ train MSE: 0.6072, val MSE: 0.3918
Gen 827/8000 ▶ train MSE: 0.6072, val MSE: 0.3918
Gen 828/8000 ▶ train MSE: 0.6072, val MSE: 0.3916
Gen 829/8000 ▶ train MSE: 0.6068, val MSE: 0.3879
Gen 830/8000 ▶ train MSE: 0.6068, val MSE: 0.3879
Gen 831/8000 ▶ train MSE: 0.6068, val MSE: 0.3879
Gen 832/8000 ▶ train MSE: 0.6068, val MSE: 0.3879
Gen 833/8000 ▶ train MSE: 0.6068, val MSE: 0.3879
Gen 834/8000 ▶ train MSE: 0.6068, val MSE: 0.3879
Gen 835/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 836/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 837/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 838/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 839/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 840/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 841/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 842/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 843/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 844/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 845/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 846/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 847/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 848/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 849/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 850/8000 ▶ train MSE: 0.6066, val MSE: 0.3892
Gen 851/8000 ▶ train MSE: 0.6064, val MSE: 0.3867
Gen 852/8000 ▶ train MSE: 0.6063, val MSE: 0.3870
Gen 853/8000 ▶ train MSE: 0.6063, val MSE: 0.3870
Gen 854/8000 ▶ train MSE: 0.6063, val MSE: 0.3870
Gen 855/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 856/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 857/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 858/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 859/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 860/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 861/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 862/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 863/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 864/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 865/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 866/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 867/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 868/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 869/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 870/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 871/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 872/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 873/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 874/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 875/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 876/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 877/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 878/8000 ▶ train MSE: 0.6062, val MSE: 0.3886
Gen 879/8000 ▶ train MSE: 0.6060, val MSE: 0.3859
Gen 880/8000 ▶ train MSE: 0.6060, val MSE: 0.3866
Gen 881/8000 ▶ train MSE: 0.6060, val MSE: 0.3860
Gen 882/8000 ▶ train MSE: 0.6058, val MSE: 0.3872
Gen 883/8000 ▶ train MSE: 0.6058, val MSE: 0.3872
Gen 884/8000 ▶ train MSE: 0.6058, val MSE: 0.3872
Gen 885/8000 ▶ train MSE: 0.6057, val MSE: 0.3852
Gen 886/8000 ▶ train MSE: 0.6056, val MSE: 0.3853
Gen 887/8000 ▶ train MSE: 0.6056, val MSE: 0.3853
Gen 888/8000 ▶ train MSE: 0.6054, val MSE: 0.3850
Gen 889/8000 ▶ train MSE: 0.6054, val MSE: 0.3850
Gen 890/8000 ▶ train MSE: 0.6054, val MSE: 0.3850
Gen 891/8000 ▶ train MSE: 0.6052, val MSE: 0.3860
Gen 892/8000 ▶ train MSE: 0.6050, val MSE: 0.3841
Gen 893/8000 ▶ train MSE: 0.6050, val MSE: 0.3841
Gen 894/8000 ▶ train MSE: 0.6050, val MSE: 0.3841
Gen 895/8000 ▶ train MSE: 0.6050, val MSE: 0.3841
Gen 896/8000 ▶ train MSE: 0.6050, val MSE: 0.3841
Gen 897/8000 ▶ train MSE: 0.6050, val MSE: 0.3841
Gen 898/8000 ▶ train MSE: 0.6050, val MSE: 0.3841
Gen 899/8000 ▶ train MSE: 0.6050, val MSE: 0.3841
Gen 900/8000 ▶ train MSE: 0.6050, val MSE: 0.3847
Gen 901/8000 ▶ train MSE: 0.6050, val MSE: 0.3847
Gen 902/8000 ▶ train MSE: 0.6050, val MSE: 0.3847
Gen 903/8000 ▶ train MSE: 0.6047, val MSE: 0.3853
Gen 904/8000 ▶ train MSE: 0.6047, val MSE: 0.3853
Gen 905/8000 ▶ train MSE: 0.6047, val MSE: 0.3853
Gen 906/8000 ▶ train MSE: 0.6047, val MSE: 0.3853
Gen 907/8000 ▶ train MSE: 0.6047, val MSE: 0.3853
Gen 908/8000 ▶ train MSE: 0.6047, val MSE: 0.3853
Gen 909/8000 ▶ train MSE: 0.6047, val MSE: 0.3853
Gen 910/8000 ▶ train MSE: 0.6047, val MSE: 0.3853
Gen 911/8000 ▶ train MSE: 0.6047, val MSE: 0.3853
Gen 912/8000 ▶ train MSE: 0.6047, val MSE: 0.3853
Gen 913/8000 ▶ train MSE: 0.6045, val MSE: 0.3858
Gen 914/8000 ▶ train MSE: 0.6045, val MSE: 0.3858
Gen 915/8000 ▶ train MSE: 0.6045, val MSE: 0.3858
Gen 916/8000 ▶ train MSE: 0.6045, val MSE: 0.3858
Gen 917/8000 ▶ train MSE: 0.6045, val MSE: 0.3858
Gen 918/8000 ▶ train MSE: 0.6045, val MSE: 0.3858
Gen 919/8000 ▶ train MSE: 0.6045, val MSE: 0.3858
Gen 920/8000 ▶ train MSE: 0.6045, val MSE: 0.3858
Gen 921/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 922/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 923/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 924/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 925/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 926/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 927/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 928/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 929/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 930/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 931/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 932/8000 ▶ train MSE: 0.6045, val MSE: 0.3838
Gen 933/8000 ▶ train MSE: 0.6043, val MSE: 0.3851
Gen 934/8000 ▶ train MSE: 0.6043, val MSE: 0.3851
Gen 935/8000 ▶ train MSE: 0.6043, val MSE: 0.3851
Gen 936/8000 ▶ train MSE: 0.6042, val MSE: 0.3849
Gen 937/8000 ▶ train MSE: 0.6042, val MSE: 0.3849
Gen 938/8000 ▶ train MSE: 0.6040, val MSE: 0.3846
Gen 939/8000 ▶ train MSE: 0.6040, val MSE: 0.3846
Gen 940/8000 ▶ train MSE: 0.6040, val MSE: 0.3846
Gen 941/8000 ▶ train MSE: 0.6040, val MSE: 0.3846
Gen 942/8000 ▶ train MSE: 0.6040, val MSE: 0.3846
Gen 943/8000 ▶ train MSE: 0.6040, val MSE: 0.3846
Gen 944/8000 ▶ train MSE: 0.6039, val MSE: 0.3840
Gen 945/8000 ▶ train MSE: 0.6039, val MSE: 0.3840
Gen 946/8000 ▶ train MSE: 0.6039, val MSE: 0.3840
Gen 947/8000 ▶ train MSE: 0.6039, val MSE: 0.3840
Gen 948/8000 ▶ train MSE: 0.6038, val MSE: 0.3848
Gen 949/8000 ▶ train MSE: 0.6038, val MSE: 0.3848
Gen 950/8000 ▶ train MSE: 0.6038, val MSE: 0.3848
Gen 951/8000 ▶ train MSE: 0.6038, val MSE: 0.3848
Gen 952/8000 ▶ train MSE: 0.6036, val MSE: 0.3835
Gen 953/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 954/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 955/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 956/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 957/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 958/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 959/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 960/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 961/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 962/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 963/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 964/8000 ▶ train MSE: 0.6036, val MSE: 0.3833
Gen 965/8000 ▶ train MSE: 0.6036, val MSE: 0.3846
Gen 966/8000 ▶ train MSE: 0.6036, val MSE: 0.3846
Gen 967/8000 ▶ train MSE: 0.6036, val MSE: 0.3846
Gen 968/8000 ▶ train MSE: 0.6036, val MSE: 0.3846
Gen 969/8000 ▶ train MSE: 0.6036, val MSE: 0.3846
Gen 970/8000 ▶ train MSE: 0.6036, val MSE: 0.3846
Gen 971/8000 ▶ train MSE: 0.6036, val MSE: 0.3846
Gen 972/8000 ▶ train MSE: 0.6035, val MSE: 0.3837
Gen 973/8000 ▶ train MSE: 0.6034, val MSE: 0.3843
Gen 974/8000 ▶ train MSE: 0.6034, val MSE: 0.3843
Gen 975/8000 ▶ train MSE: 0.6034, val MSE: 0.3843
Gen 976/8000 ▶ train MSE: 0.6034, val MSE: 0.3843
Gen 977/8000 ▶ train MSE: 0.6034, val MSE: 0.3843
Gen 978/8000 ▶ train MSE: 0.6034, val MSE: 0.3843
Gen 979/8000 ▶ train MSE: 0.6033, val MSE: 0.3836
Gen 980/8000 ▶ train MSE: 0.6033, val MSE: 0.3836
Gen 981/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 982/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 983/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 984/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 985/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 986/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 987/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 988/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 989/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 990/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 991/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 992/8000 ▶ train MSE: 0.6032, val MSE: 0.3835
Gen 993/8000 ▶ train MSE: 0.6031, val MSE: 0.3838
Gen 994/8000 ▶ train MSE: 0.6031, val MSE: 0.3830
Gen 995/8000 ▶ train MSE: 0.6031, val MSE: 0.3830
Gen 996/8000 ▶ train MSE: 0.6031, val MSE: 0.3830
Gen 997/8000 ▶ train MSE: 0.6031, val MSE: 0.3830
Gen 998/8000 ▶ train MSE: 0.6030, val MSE: 0.3826
Gen 999/8000 ▶ train MSE: 0.6028, val MSE: 0.3825
Gen 1000/8000 ▶ train MSE: 0.6028, val MSE: 0.3825
Gen 1001/8000 ▶ train MSE: 0.6028, val MSE: 0.3825
Gen 1002/8000 ▶ train MSE: 0.6028, val MSE: 0.3825
Gen 1003/8000 ▶ train MSE: 0.6028, val MSE: 0.3825
Gen 1004/8000 ▶ train MSE: 0.6028, val MSE: 0.3825
Gen 1005/8000 ▶ train MSE: 0.6028, val MSE: 0.3825
Gen 1006/8000 ▶ train MSE: 0.6026, val MSE: 0.3826
Gen 1007/8000 ▶ train MSE: 0.6026, val MSE: 0.3826
Gen 1008/8000 ▶ train MSE: 0.6026, val MSE: 0.3826
Gen 1009/8000 ▶ train MSE: 0.6026, val MSE: 0.3826
Gen 1010/8000 ▶ train MSE: 0.6026, val MSE: 0.3826
Gen 1011/8000 ▶ train MSE: 0.6026, val MSE: 0.3826
Gen 1012/8000 ▶ train MSE: 0.6025, val MSE: 0.3840
Gen 1013/8000 ▶ train MSE: 0.6025, val MSE: 0.3840
Gen 1014/8000 ▶ train MSE: 0.6025, val MSE: 0.3840
Gen 1015/8000 ▶ train MSE: 0.6025, val MSE: 0.3840
Gen 1016/8000 ▶ train MSE: 0.6025, val MSE: 0.3840
Gen 1017/8000 ▶ train MSE: 0.6025, val MSE: 0.3840
Gen 1018/8000 ▶ train MSE: 0.6025, val MSE: 0.3840
Gen 1019/8000 ▶ train MSE: 0.6025, val MSE: 0.3840
Gen 1020/8000 ▶ train MSE: 0.6025, val MSE: 0.3840
Gen 1021/8000 ▶ train MSE: 0.6025, val MSE: 0.3830
Gen 1022/8000 ▶ train MSE: 0.6025, val MSE: 0.3830
Gen 1023/8000 ▶ train MSE: 0.6025, val MSE: 0.3830
Gen 1024/8000 ▶ train MSE: 0.6025, val MSE: 0.3830
Gen 1025/8000 ▶ train MSE: 0.6025, val MSE: 0.3830
Gen 1026/8000 ▶ train MSE: 0.6024, val MSE: 0.3809
Gen 1027/8000 ▶ train MSE: 0.6024, val MSE: 0.3809
Gen 1028/8000 ▶ train MSE: 0.6024, val MSE: 0.3809
Gen 1029/8000 ▶ train MSE: 0.6024, val MSE: 0.3809
Gen 1030/8000 ▶ train MSE: 0.6024, val MSE: 0.3809
Gen 1031/8000 ▶ train MSE: 0.6024, val MSE: 0.3809
Gen 1032/8000 ▶ train MSE: 0.6024, val MSE: 0.3809
Gen 1033/8000 ▶ train MSE: 0.6024, val MSE: 0.3809
Gen 1034/8000 ▶ train MSE: 0.6024, val MSE: 0.3809
Gen 1035/8000 ▶ train MSE: 0.6024, val MSE: 0.3809
Gen 1036/8000 ▶ train MSE: 0.6023, val MSE: 0.3804
Gen 1037/8000 ▶ train MSE: 0.6023, val MSE: 0.3804
Gen 1038/8000 ▶ train MSE: 0.6023, val MSE: 0.3804
Gen 1039/8000 ▶ train MSE: 0.6023, val MSE: 0.3834
Gen 1040/8000 ▶ train MSE: 0.6023, val MSE: 0.3834
Gen 1041/8000 ▶ train MSE: 0.6023, val MSE: 0.3834
Gen 1042/8000 ▶ train MSE: 0.6023, val MSE: 0.3834
Gen 1043/8000 ▶ train MSE: 0.6023, val MSE: 0.3834
Gen 1044/8000 ▶ train MSE: 0.6022, val MSE: 0.3813
Gen 1045/8000 ▶ train MSE: 0.6022, val MSE: 0.3804
Gen 1046/8000 ▶ train MSE: 0.6022, val MSE: 0.3806
Gen 1047/8000 ▶ train MSE: 0.6021, val MSE: 0.3802
Gen 1048/8000 ▶ train MSE: 0.6020, val MSE: 0.3801
Gen 1049/8000 ▶ train MSE: 0.6020, val MSE: 0.3801
Gen 1050/8000 ▶ train MSE: 0.6019, val MSE: 0.3793
Gen 1051/8000 ▶ train MSE: 0.6019, val MSE: 0.3793
Gen 1052/8000 ▶ train MSE: 0.6019, val MSE: 0.3793
Gen 1053/8000 ▶ train MSE: 0.6019, val MSE: 0.3793
Gen 1054/8000 ▶ train MSE: 0.6019, val MSE: 0.3793
Gen 1055/8000 ▶ train MSE: 0.6019, val MSE: 0.3793
Gen 1056/8000 ▶ train MSE: 0.6019, val MSE: 0.3793
Gen 1057/8000 ▶ train MSE: 0.6018, val MSE: 0.3794
Gen 1058/8000 ▶ train MSE: 0.6018, val MSE: 0.3794
Gen 1059/8000 ▶ train MSE: 0.6016, val MSE: 0.3783
Gen 1060/8000 ▶ train MSE: 0.6016, val MSE: 0.3783
Gen 1061/8000 ▶ train MSE: 0.6016, val MSE: 0.3783
Gen 1062/8000 ▶ train MSE: 0.6016, val MSE: 0.3783
Gen 1063/8000 ▶ train MSE: 0.6016, val MSE: 0.3783
Gen 1064/8000 ▶ train MSE: 0.6016, val MSE: 0.3783
Gen 1065/8000 ▶ train MSE: 0.6016, val MSE: 0.3783
Gen 1066/8000 ▶ train MSE: 0.6015, val MSE: 0.3779
Gen 1067/8000 ▶ train MSE: 0.6015, val MSE: 0.3779
Gen 1068/8000 ▶ train MSE: 0.6015, val MSE: 0.3779
Gen 1069/8000 ▶ train MSE: 0.6015, val MSE: 0.3779
Gen 1070/8000 ▶ train MSE: 0.6015, val MSE: 0.3779
Gen 1071/8000 ▶ train MSE: 0.6015, val MSE: 0.3779
Gen 1072/8000 ▶ train MSE: 0.6015, val MSE: 0.3779
Gen 1073/8000 ▶ train MSE: 0.6015, val MSE: 0.3766
Gen 1074/8000 ▶ train MSE: 0.6015, val MSE: 0.3798
Gen 1075/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1076/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1077/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1078/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1079/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1080/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1081/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1082/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1083/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1084/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1085/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1086/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1087/8000 ▶ train MSE: 0.6011, val MSE: 0.3781
Gen 1088/8000 ▶ train MSE: 0.6011, val MSE: 0.3766
Gen 1089/8000 ▶ train MSE: 0.6011, val MSE: 0.3766
Gen 1090/8000 ▶ train MSE: 0.6011, val MSE: 0.3782
Gen 1091/8000 ▶ train MSE: 0.6011, val MSE: 0.3782
Gen 1092/8000 ▶ train MSE: 0.6011, val MSE: 0.3782
Gen 1093/8000 ▶ train MSE: 0.6011, val MSE: 0.3782
Gen 1094/8000 ▶ train MSE: 0.6011, val MSE: 0.3782
Gen 1095/8000 ▶ train MSE: 0.6010, val MSE: 0.3775
Gen 1096/8000 ▶ train MSE: 0.6010, val MSE: 0.3775
Gen 1097/8000 ▶ train MSE: 0.6010, val MSE: 0.3775
Gen 1098/8000 ▶ train MSE: 0.6010, val MSE: 0.3775
Gen 1099/8000 ▶ train MSE: 0.6010, val MSE: 0.3775
Gen 1100/8000 ▶ train MSE: 0.6010, val MSE: 0.3775
Gen 1101/8000 ▶ train MSE: 0.6010, val MSE: 0.3775
Gen 1102/8000 ▶ train MSE: 0.6010, val MSE: 0.3775
Gen 1103/8000 ▶ train MSE: 0.6010, val MSE: 0.3763
Gen 1104/8000 ▶ train MSE: 0.6009, val MSE: 0.3786
Gen 1105/8000 ▶ train MSE: 0.6008, val MSE: 0.3753
Gen 1106/8000 ▶ train MSE: 0.6008, val MSE: 0.3753
Gen 1107/8000 ▶ train MSE: 0.6008, val MSE: 0.3753
Gen 1108/8000 ▶ train MSE: 0.6008, val MSE: 0.3753
Gen 1109/8000 ▶ train MSE: 0.6007, val MSE: 0.3761
Gen 1110/8000 ▶ train MSE: 0.6007, val MSE: 0.3761
Gen 1111/8000 ▶ train MSE: 0.6007, val MSE: 0.3765
Gen 1112/8000 ▶ train MSE: 0.6007, val MSE: 0.3765
Gen 1113/8000 ▶ train MSE: 0.6007, val MSE: 0.3765
Gen 1114/8000 ▶ train MSE: 0.6007, val MSE: 0.3771
Gen 1115/8000 ▶ train MSE: 0.6007, val MSE: 0.3771
Gen 1116/8000 ▶ train MSE: 0.6007, val MSE: 0.3771
Gen 1117/8000 ▶ train MSE: 0.6006, val MSE: 0.3762
Gen 1118/8000 ▶ train MSE: 0.6006, val MSE: 0.3762
Gen 1119/8000 ▶ train MSE: 0.6006, val MSE: 0.3757
Gen 1120/8000 ▶ train MSE: 0.6004, val MSE: 0.3757
Gen 1121/8000 ▶ train MSE: 0.6004, val MSE: 0.3757
Gen 1122/8000 ▶ train MSE: 0.6004, val MSE: 0.3757
Gen 1123/8000 ▶ train MSE: 0.6004, val MSE: 0.3757
Gen 1124/8000 ▶ train MSE: 0.6004, val MSE: 0.3757
Gen 1125/8000 ▶ train MSE: 0.6004, val MSE: 0.3741
Gen 1126/8000 ▶ train MSE: 0.6004, val MSE: 0.3741
Gen 1127/8000 ▶ train MSE: 0.6002, val MSE: 0.3746
Gen 1128/8000 ▶ train MSE: 0.6002, val MSE: 0.3745
Gen 1129/8000 ▶ train MSE: 0.6002, val MSE: 0.3745
Gen 1130/8000 ▶ train MSE: 0.6002, val MSE: 0.3745
Gen 1131/8000 ▶ train MSE: 0.6002, val MSE: 0.3745
Gen 1132/8000 ▶ train MSE: 0.6002, val MSE: 0.3735
Gen 1133/8000 ▶ train MSE: 0.6002, val MSE: 0.3735
Gen 1134/8000 ▶ train MSE: 0.6002, val MSE: 0.3735
Gen 1135/8000 ▶ train MSE: 0.6002, val MSE: 0.3735
Gen 1136/8000 ▶ train MSE: 0.6002, val MSE: 0.3735
Gen 1137/8000 ▶ train MSE: 0.6001, val MSE: 0.3728
Gen 1138/8000 ▶ train MSE: 0.6001, val MSE: 0.3728
Gen 1139/8000 ▶ train MSE: 0.6000, val MSE: 0.3738
Gen 1140/8000 ▶ train MSE: 0.6000, val MSE: 0.3738
Gen 1141/8000 ▶ train MSE: 0.6000, val MSE: 0.3738
Gen 1142/8000 ▶ train MSE: 0.6000, val MSE: 0.3738
Gen 1143/8000 ▶ train MSE: 0.6000, val MSE: 0.3738
Gen 1144/8000 ▶ train MSE: 0.5997, val MSE: 0.3747
Gen 1145/8000 ▶ train MSE: 0.5997, val MSE: 0.3747
Gen 1146/8000 ▶ train MSE: 0.5997, val MSE: 0.3747
Gen 1147/8000 ▶ train MSE: 0.5997, val MSE: 0.3747
Gen 1148/8000 ▶ train MSE: 0.5997, val MSE: 0.3747
Gen 1149/8000 ▶ train MSE: 0.5997, val MSE: 0.3747
Gen 1150/8000 ▶ train MSE: 0.5997, val MSE: 0.3747
Gen 1151/8000 ▶ train MSE: 0.5997, val MSE: 0.3747
Gen 1152/8000 ▶ train MSE: 0.5997, val MSE: 0.3747
Gen 1153/8000 ▶ train MSE: 0.5997, val MSE: 0.3747
Gen 1154/8000 ▶ train MSE: 0.5996, val MSE: 0.3735
Gen 1155/8000 ▶ train MSE: 0.5996, val MSE: 0.3735
Gen 1156/8000 ▶ train MSE: 0.5996, val MSE: 0.3735
Gen 1157/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1158/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1159/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1160/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1161/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1162/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1163/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1164/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1165/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1166/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1167/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1168/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1169/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1170/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1171/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1172/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1173/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1174/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1175/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1176/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1177/8000 ▶ train MSE: 0.5995, val MSE: 0.3728
Gen 1178/8000 ▶ train MSE: 0.5994, val MSE: 0.3741
Gen 1179/8000 ▶ train MSE: 0.5994, val MSE: 0.3741
Gen 1180/8000 ▶ train MSE: 0.5994, val MSE: 0.3741
Gen 1181/8000 ▶ train MSE: 0.5993, val MSE: 0.3733
Gen 1182/8000 ▶ train MSE: 0.5993, val MSE: 0.3733
Gen 1183/8000 ▶ train MSE: 0.5993, val MSE: 0.3733
Gen 1184/8000 ▶ train MSE: 0.5993, val MSE: 0.3733
Gen 1185/8000 ▶ train MSE: 0.5993, val MSE: 0.3733
Gen 1186/8000 ▶ train MSE: 0.5993, val MSE: 0.3734
Gen 1187/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1188/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1189/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1190/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1191/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1192/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1193/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1194/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1195/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1196/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1197/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1198/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1199/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1200/8000 ▶ train MSE: 0.5990, val MSE: 0.3746
Gen 1201/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1202/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1203/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1204/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1205/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1206/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1207/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1208/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1209/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1210/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1211/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1212/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1213/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1214/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1215/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1216/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1217/8000 ▶ train MSE: 0.5990, val MSE: 0.3736
Gen 1218/8000 ▶ train MSE: 0.5990, val MSE: 0.3740
Gen 1219/8000 ▶ train MSE: 0.5989, val MSE: 0.3729
Gen 1220/8000 ▶ train MSE: 0.5989, val MSE: 0.3729
Gen 1221/8000 ▶ train MSE: 0.5989, val MSE: 0.3729
Gen 1222/8000 ▶ train MSE: 0.5987, val MSE: 0.3739
Gen 1223/8000 ▶ train MSE: 0.5987, val MSE: 0.3739
Gen 1224/8000 ▶ train MSE: 0.5987, val MSE: 0.3739
Gen 1225/8000 ▶ train MSE: 0.5987, val MSE: 0.3739
Gen 1226/8000 ▶ train MSE: 0.5987, val MSE: 0.3739
Gen 1227/8000 ▶ train MSE: 0.5987, val MSE: 0.3739
Gen 1228/8000 ▶ train MSE: 0.5987, val MSE: 0.3735
Gen 1229/8000 ▶ train MSE: 0.5986, val MSE: 0.3736
Gen 1230/8000 ▶ train MSE: 0.5986, val MSE: 0.3736
Gen 1231/8000 ▶ train MSE: 0.5986, val MSE: 0.3734
Gen 1232/8000 ▶ train MSE: 0.5986, val MSE: 0.3734
Gen 1233/8000 ▶ train MSE: 0.5984, val MSE: 0.3731
Gen 1234/8000 ▶ train MSE: 0.5984, val MSE: 0.3731
Gen 1235/8000 ▶ train MSE: 0.5984, val MSE: 0.3731
Gen 1236/8000 ▶ train MSE: 0.5984, val MSE: 0.3731
Gen 1237/8000 ▶ train MSE: 0.5984, val MSE: 0.3731
Gen 1238/8000 ▶ train MSE: 0.5984, val MSE: 0.3731
Gen 1239/8000 ▶ train MSE: 0.5984, val MSE: 0.3730
Gen 1240/8000 ▶ train MSE: 0.5984, val MSE: 0.3730
Gen 1241/8000 ▶ train MSE: 0.5984, val MSE: 0.3730
Gen 1242/8000 ▶ train MSE: 0.5984, val MSE: 0.3730
Gen 1243/8000 ▶ train MSE: 0.5984, val MSE: 0.3730
Gen 1244/8000 ▶ train MSE: 0.5984, val MSE: 0.3730
Gen 1245/8000 ▶ train MSE: 0.5984, val MSE: 0.3730
Gen 1246/8000 ▶ train MSE: 0.5983, val MSE: 0.3724
Gen 1247/8000 ▶ train MSE: 0.5983, val MSE: 0.3724
Gen 1248/8000 ▶ train MSE: 0.5983, val MSE: 0.3724
Gen 1249/8000 ▶ train MSE: 0.5983, val MSE: 0.3724
Gen 1250/8000 ▶ train MSE: 0.5983, val MSE: 0.3724
Gen 1251/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1252/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1253/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1254/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1255/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1256/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1257/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1258/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1259/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1260/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1261/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1262/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1263/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1264/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1265/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1266/8000 ▶ train MSE: 0.5982, val MSE: 0.3722
Gen 1267/8000 ▶ train MSE: 0.5981, val MSE: 0.3725
Gen 1268/8000 ▶ train MSE: 0.5981, val MSE: 0.3725
Gen 1269/8000 ▶ train MSE: 0.5981, val MSE: 0.3725
Gen 1270/8000 ▶ train MSE: 0.5981, val MSE: 0.3725
Gen 1271/8000 ▶ train MSE: 0.5981, val MSE: 0.3725
Gen 1272/8000 ▶ train MSE: 0.5981, val MSE: 0.3725
Gen 1273/8000 ▶ train MSE: 0.5981, val MSE: 0.3725
Gen 1274/8000 ▶ train MSE: 0.5981, val MSE: 0.3725
Gen 1275/8000 ▶ train MSE: 0.5981, val MSE: 0.3725
Gen 1276/8000 ▶ train MSE: 0.5981, val MSE: 0.3730
Gen 1277/8000 ▶ train MSE: 0.5981, val MSE: 0.3730
Gen 1278/8000 ▶ train MSE: 0.5981, val MSE: 0.3730
Gen 1279/8000 ▶ train MSE: 0.5981, val MSE: 0.3730
Gen 1280/8000 ▶ train MSE: 0.5981, val MSE: 0.3724
Gen 1281/8000 ▶ train MSE: 0.5981, val MSE: 0.3720
Gen 1282/8000 ▶ train MSE: 0.5981, val MSE: 0.3720
Gen 1283/8000 ▶ train MSE: 0.5981, val MSE: 0.3720
Gen 1284/8000 ▶ train MSE: 0.5981, val MSE: 0.3720
Gen 1285/8000 ▶ train MSE: 0.5981, val MSE: 0.3720
Gen 1286/8000 ▶ train MSE: 0.5981, val MSE: 0.3720
Gen 1287/8000 ▶ train MSE: 0.5981, val MSE: 0.3720
Gen 1288/8000 ▶ train MSE: 0.5981, val MSE: 0.3720
Gen 1289/8000 ▶ train MSE: 0.5979, val MSE: 0.3716
Gen 1290/8000 ▶ train MSE: 0.5978, val MSE: 0.3725
Gen 1291/8000 ▶ train MSE: 0.5978, val MSE: 0.3725
Gen 1292/8000 ▶ train MSE: 0.5978, val MSE: 0.3725
Gen 1293/8000 ▶ train MSE: 0.5978, val MSE: 0.3725
Gen 1294/8000 ▶ train MSE: 0.5978, val MSE: 0.3725
Gen 1295/8000 ▶ train MSE: 0.5978, val MSE: 0.3725
Gen 1296/8000 ▶ train MSE: 0.5978, val MSE: 0.3725
Gen 1297/8000 ▶ train MSE: 0.5978, val MSE: 0.3725
Gen 1298/8000 ▶ train MSE: 0.5978, val MSE: 0.3725
Gen 1299/8000 ▶ train MSE: 0.5978, val MSE: 0.3713
Gen 1300/8000 ▶ train MSE: 0.5978, val MSE: 0.3713
Gen 1301/8000 ▶ train MSE: 0.5978, val MSE: 0.3713
Gen 1302/8000 ▶ train MSE: 0.5978, val MSE: 0.3713
Gen 1303/8000 ▶ train MSE: 0.5978, val MSE: 0.3713
Gen 1304/8000 ▶ train MSE: 0.5978, val MSE: 0.3727
Gen 1305/8000 ▶ train MSE: 0.5978, val MSE: 0.3727
Gen 1306/8000 ▶ train MSE: 0.5977, val MSE: 0.3723
Gen 1307/8000 ▶ train MSE: 0.5977, val MSE: 0.3723
Gen 1308/8000 ▶ train MSE: 0.5977, val MSE: 0.3723
Gen 1309/8000 ▶ train MSE: 0.5977, val MSE: 0.3723
Gen 1310/8000 ▶ train MSE: 0.5977, val MSE: 0.3723
Gen 1311/8000 ▶ train MSE: 0.5977, val MSE: 0.3718
Gen 1312/8000 ▶ train MSE: 0.5977, val MSE: 0.3718
Gen 1313/8000 ▶ train MSE: 0.5977, val MSE: 0.3718
Gen 1314/8000 ▶ train MSE: 0.5977, val MSE: 0.3718
Gen 1315/8000 ▶ train MSE: 0.5977, val MSE: 0.3718
Gen 1316/8000 ▶ train MSE: 0.5977, val MSE: 0.3718
Gen 1317/8000 ▶ train MSE: 0.5977, val MSE: 0.3718
Gen 1318/8000 ▶ train MSE: 0.5976, val MSE: 0.3736
Gen 1319/8000 ▶ train MSE: 0.5976, val MSE: 0.3736
Gen 1320/8000 ▶ train MSE: 0.5976, val MSE: 0.3736
Gen 1321/8000 ▶ train MSE: 0.5976, val MSE: 0.3736
Gen 1322/8000 ▶ train MSE: 0.5976, val MSE: 0.3736
Gen 1323/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1324/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1325/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1326/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1327/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1328/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1329/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1330/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1331/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1332/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1333/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1334/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1335/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1336/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1337/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1338/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1339/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1340/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1341/8000 ▶ train MSE: 0.5974, val MSE: 0.3715
Gen 1342/8000 ▶ train MSE: 0.5974, val MSE: 0.3728
Gen 1343/8000 ▶ train MSE: 0.5974, val MSE: 0.3728
Gen 1344/8000 ▶ train MSE: 0.5974, val MSE: 0.3728
Gen 1345/8000 ▶ train MSE: 0.5974, val MSE: 0.3728
Gen 1346/8000 ▶ train MSE: 0.5974, val MSE: 0.3728
Gen 1347/8000 ▶ train MSE: 0.5974, val MSE: 0.3728
Gen 1348/8000 ▶ train MSE: 0.5974, val MSE: 0.3728
Gen 1349/8000 ▶ train MSE: 0.5974, val MSE: 0.3728
Gen 1350/8000 ▶ train MSE: 0.5974, val MSE: 0.3728
Gen 1351/8000 ▶ train MSE: 0.5974, val MSE: 0.3728
Gen 1352/8000 ▶ train MSE: 0.5973, val MSE: 0.3715
Gen 1353/8000 ▶ train MSE: 0.5973, val MSE: 0.3715
Gen 1354/8000 ▶ train MSE: 0.5973, val MSE: 0.3715
Gen 1355/8000 ▶ train MSE: 0.5973, val MSE: 0.3715
Gen 1356/8000 ▶ train MSE: 0.5973, val MSE: 0.3715
Gen 1357/8000 ▶ train MSE: 0.5973, val MSE: 0.3715
Gen 1358/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1359/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1360/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1361/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1362/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1363/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1364/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1365/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1366/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1367/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1368/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1369/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1370/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1371/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1372/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1373/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1374/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1375/8000 ▶ train MSE: 0.5970, val MSE: 0.3729
Gen 1376/8000 ▶ train MSE: 0.5970, val MSE: 0.3724
Gen 1377/8000 ▶ train MSE: 0.5970, val MSE: 0.3724
Gen 1378/8000 ▶ train MSE: 0.5970, val MSE: 0.3724
Gen 1379/8000 ▶ train MSE: 0.5970, val MSE: 0.3713
Gen 1380/8000 ▶ train MSE: 0.5969, val MSE: 0.3725
Gen 1381/8000 ▶ train MSE: 0.5969, val MSE: 0.3725
Gen 1382/8000 ▶ train MSE: 0.5969, val MSE: 0.3728
Gen 1383/8000 ▶ train MSE: 0.5969, val MSE: 0.3728
Gen 1384/8000 ▶ train MSE: 0.5969, val MSE: 0.3728
Gen 1385/8000 ▶ train MSE: 0.5969, val MSE: 0.3728
Gen 1386/8000 ▶ train MSE: 0.5969, val MSE: 0.3728
Gen 1387/8000 ▶ train MSE: 0.5968, val MSE: 0.3722
Gen 1388/8000 ▶ train MSE: 0.5968, val MSE: 0.3712
Gen 1389/8000 ▶ train MSE: 0.5968, val MSE: 0.3712
Gen 1390/8000 ▶ train MSE: 0.5968, val MSE: 0.3712
Gen 1391/8000 ▶ train MSE: 0.5968, val MSE: 0.3717
Gen 1392/8000 ▶ train MSE: 0.5968, val MSE: 0.3715
Gen 1393/8000 ▶ train MSE: 0.5968, val MSE: 0.3715
Gen 1394/8000 ▶ train MSE: 0.5968, val MSE: 0.3715
Gen 1395/8000 ▶ train MSE: 0.5967, val MSE: 0.3731
Gen 1396/8000 ▶ train MSE: 0.5967, val MSE: 0.3723
Gen 1397/8000 ▶ train MSE: 0.5967, val MSE: 0.3723
Gen 1398/8000 ▶ train MSE: 0.5967, val MSE: 0.3723
Gen 1399/8000 ▶ train MSE: 0.5967, val MSE: 0.3723
Gen 1400/8000 ▶ train MSE: 0.5967, val MSE: 0.3723
Gen 1401/8000 ▶ train MSE: 0.5967, val MSE: 0.3723
Gen 1402/8000 ▶ train MSE: 0.5967, val MSE: 0.3723
Gen 1403/8000 ▶ train MSE: 0.5967, val MSE: 0.3720
Gen 1404/8000 ▶ train MSE: 0.5967, val MSE: 0.3725
Gen 1405/8000 ▶ train MSE: 0.5966, val MSE: 0.3720
Gen 1406/8000 ▶ train MSE: 0.5966, val MSE: 0.3718
Gen 1407/8000 ▶ train MSE: 0.5966, val MSE: 0.3713
Gen 1408/8000 ▶ train MSE: 0.5966, val MSE: 0.3713
Gen 1409/8000 ▶ train MSE: 0.5965, val MSE: 0.3728
Gen 1410/8000 ▶ train MSE: 0.5965, val MSE: 0.3728
Gen 1411/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1412/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1413/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1414/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1415/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1416/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1417/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1418/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1419/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1420/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1421/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1422/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1423/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1424/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1425/8000 ▶ train MSE: 0.5965, val MSE: 0.3712
Gen 1426/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1427/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1428/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1429/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1430/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1431/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1432/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1433/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1434/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1435/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1436/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1437/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1438/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1439/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1440/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1441/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1442/8000 ▶ train MSE: 0.5964, val MSE: 0.3712
Gen 1443/8000 ▶ train MSE: 0.5964, val MSE: 0.3705
Gen 1444/8000 ▶ train MSE: 0.5964, val MSE: 0.3705
Gen 1445/8000 ▶ train MSE: 0.5964, val MSE: 0.3705
Gen 1446/8000 ▶ train MSE: 0.5964, val MSE: 0.3705
Gen 1447/8000 ▶ train MSE: 0.5964, val MSE: 0.3705
Gen 1448/8000 ▶ train MSE: 0.5964, val MSE: 0.3705
Gen 1449/8000 ▶ train MSE: 0.5964, val MSE: 0.3705
Gen 1450/8000 ▶ train MSE: 0.5964, val MSE: 0.3705
Gen 1451/8000 ▶ train MSE: 0.5963, val MSE: 0.3710
Gen 1452/8000 ▶ train MSE: 0.5962, val MSE: 0.3716
Gen 1453/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1454/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1455/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1456/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1457/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1458/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1459/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1460/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1461/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1462/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1463/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1464/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1465/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1466/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1467/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1468/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1469/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1470/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1471/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1472/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1473/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1474/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1475/8000 ▶ train MSE: 0.5961, val MSE: 0.3712
Gen 1476/8000 ▶ train MSE: 0.5960, val MSE: 0.3713
Gen 1477/8000 ▶ train MSE: 0.5960, val MSE: 0.3713
Gen 1478/8000 ▶ train MSE: 0.5960, val MSE: 0.3713
Gen 1479/8000 ▶ train MSE: 0.5960, val MSE: 0.3713
Gen 1480/8000 ▶ train MSE: 0.5960, val MSE: 0.3713
Gen 1481/8000 ▶ train MSE: 0.5960, val MSE: 0.3713
Gen 1482/8000 ▶ train MSE: 0.5960, val MSE: 0.3713
Gen 1483/8000 ▶ train MSE: 0.5960, val MSE: 0.3713
Gen 1484/8000 ▶ train MSE: 0.5960, val MSE: 0.3713
Gen 1485/8000 ▶ train MSE: 0.5960, val MSE: 0.3713
Gen 1486/8000 ▶ train MSE: 0.5960, val MSE: 0.3713
Gen 1487/8000 ▶ train MSE: 0.5959, val MSE: 0.3712
Gen 1488/8000 ▶ train MSE: 0.5959, val MSE: 0.3712
Gen 1489/8000 ▶ train MSE: 0.5959, val MSE: 0.3712
Gen 1490/8000 ▶ train MSE: 0.5959, val MSE: 0.3712
Gen 1491/8000 ▶ train MSE: 0.5959, val MSE: 0.3712
Gen 1492/8000 ▶ train MSE: 0.5959, val MSE: 0.3712
Gen 1493/8000 ▶ train MSE: 0.5959, val MSE: 0.3712
Gen 1494/8000 ▶ train MSE: 0.5959, val MSE: 0.3712
Gen 1495/8000 ▶ train MSE: 0.5959, val MSE: 0.3712
Gen 1496/8000 ▶ train MSE: 0.5959, val MSE: 0.3712
Gen 1497/8000 ▶ train MSE: 0.5959, val MSE: 0.3712
Gen 1498/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1499/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1500/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1501/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1502/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1503/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1504/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1505/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1506/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1507/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1508/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1509/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1510/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1511/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1512/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1513/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1514/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1515/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1516/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1517/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1518/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1519/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1520/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1521/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1522/8000 ▶ train MSE: 0.5959, val MSE: 0.3697
Gen 1523/8000 ▶ train MSE: 0.5958, val MSE: 0.3718
Gen 1524/8000 ▶ train MSE: 0.5958, val MSE: 0.3718
Gen 1525/8000 ▶ train MSE: 0.5958, val MSE: 0.3718
Gen 1526/8000 ▶ train MSE: 0.5958, val MSE: 0.3718
Gen 1527/8000 ▶ train MSE: 0.5958, val MSE: 0.3718
Gen 1528/8000 ▶ train MSE: 0.5958, val MSE: 0.3698
Gen 1529/8000 ▶ train MSE: 0.5958, val MSE: 0.3698
Gen 1530/8000 ▶ train MSE: 0.5958, val MSE: 0.3698
Gen 1531/8000 ▶ train MSE: 0.5958, val MSE: 0.3698
Gen 1532/8000 ▶ train MSE: 0.5958, val MSE: 0.3698
Gen 1533/8000 ▶ train MSE: 0.5958, val MSE: 0.3698
Gen 1534/8000 ▶ train MSE: 0.5958, val MSE: 0.3698
Gen 1535/8000 ▶ train MSE: 0.5958, val MSE: 0.3698
Gen 1536/8000 ▶ train MSE: 0.5958, val MSE: 0.3709
Gen 1537/8000 ▶ train MSE: 0.5958, val MSE: 0.3709
Gen 1538/8000 ▶ train MSE: 0.5958, val MSE: 0.3709
Gen 1539/8000 ▶ train MSE: 0.5958, val MSE: 0.3709
Gen 1540/8000 ▶ train MSE: 0.5958, val MSE: 0.3709
Gen 1541/8000 ▶ train MSE: 0.5958, val MSE: 0.3709
Gen 1542/8000 ▶ train MSE: 0.5958, val MSE: 0.3709
Gen 1543/8000 ▶ train MSE: 0.5958, val MSE: 0.3709
Gen 1544/8000 ▶ train MSE: 0.5957, val MSE: 0.3704
Gen 1545/8000 ▶ train MSE: 0.5957, val MSE: 0.3704
Gen 1546/8000 ▶ train MSE: 0.5957, val MSE: 0.3704
Gen 1547/8000 ▶ train MSE: 0.5957, val MSE: 0.3704
Gen 1548/8000 ▶ train MSE: 0.5957, val MSE: 0.3704
Gen 1549/8000 ▶ train MSE: 0.5957, val MSE: 0.3704
Gen 1550/8000 ▶ train MSE: 0.5957, val MSE: 0.3704
Gen 1551/8000 ▶ train MSE: 0.5956, val MSE: 0.3714
Gen 1552/8000 ▶ train MSE: 0.5956, val MSE: 0.3714
Gen 1553/8000 ▶ train MSE: 0.5956, val MSE: 0.3714
Gen 1554/8000 ▶ train MSE: 0.5956, val MSE: 0.3714
Gen 1555/8000 ▶ train MSE: 0.5956, val MSE: 0.3714
Gen 1556/8000 ▶ train MSE: 0.5956, val MSE: 0.3714
Gen 1557/8000 ▶ train MSE: 0.5956, val MSE: 0.3714
Gen 1558/8000 ▶ train MSE: 0.5956, val MSE: 0.3714
Gen 1559/8000 ▶ train MSE: 0.5956, val MSE: 0.3714
Gen 1560/8000 ▶ train MSE: 0.5956, val MSE: 0.3714
Gen 1561/8000 ▶ train MSE: 0.5956, val MSE: 0.3714
Gen 1562/8000 ▶ train MSE: 0.5956, val MSE: 0.3710
Gen 1563/8000 ▶ train MSE: 0.5956, val MSE: 0.3710
Gen 1564/8000 ▶ train MSE: 0.5955, val MSE: 0.3705
Gen 1565/8000 ▶ train MSE: 0.5955, val MSE: 0.3688
Gen 1566/8000 ▶ train MSE: 0.5955, val MSE: 0.3688
Gen 1567/8000 ▶ train MSE: 0.5955, val MSE: 0.3688
Gen 1568/8000 ▶ train MSE: 0.5955, val MSE: 0.3688
Gen 1569/8000 ▶ train MSE: 0.5955, val MSE: 0.3688
Gen 1570/8000 ▶ train MSE: 0.5955, val MSE: 0.3688
Gen 1571/8000 ▶ train MSE: 0.5955, val MSE: 0.3688
Gen 1572/8000 ▶ train MSE: 0.5955, val MSE: 0.3688
Gen 1573/8000 ▶ train MSE: 0.5955, val MSE: 0.3688
Gen 1574/8000 ▶ train MSE: 0.5955, val MSE: 0.3688
Gen 1575/8000 ▶ train MSE: 0.5954, val MSE: 0.3705
Gen 1576/8000 ▶ train MSE: 0.5954, val MSE: 0.3705
Gen 1577/8000 ▶ train MSE: 0.5954, val MSE: 0.3707
Gen 1578/8000 ▶ train MSE: 0.5954, val MSE: 0.3707
Gen 1579/8000 ▶ train MSE: 0.5954, val MSE: 0.3707
Gen 1580/8000 ▶ train MSE: 0.5954, val MSE: 0.3707
Gen 1581/8000 ▶ train MSE: 0.5954, val MSE: 0.3707
Gen 1582/8000 ▶ train MSE: 0.5954, val MSE: 0.3701
Gen 1583/8000 ▶ train MSE: 0.5954, val MSE: 0.3701
Gen 1584/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1585/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1586/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1587/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1588/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1589/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1590/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1591/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1592/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1593/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1594/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1595/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1596/8000 ▶ train MSE: 0.5952, val MSE: 0.3703
Gen 1597/8000 ▶ train MSE: 0.5951, val MSE: 0.3704
Gen 1598/8000 ▶ train MSE: 0.5951, val MSE: 0.3704
Gen 1599/8000 ▶ train MSE: 0.5951, val MSE: 0.3704
Gen 1600/8000 ▶ train MSE: 0.5951, val MSE: 0.3704
Gen 1601/8000 ▶ train MSE: 0.5951, val MSE: 0.3704
Gen 1602/8000 ▶ train MSE: 0.5951, val MSE: 0.3704
Gen 1603/8000 ▶ train MSE: 0.5951, val MSE: 0.3704
Gen 1604/8000 ▶ train MSE: 0.5951, val MSE: 0.3704
Gen 1605/8000 ▶ train MSE: 0.5951, val MSE: 0.3705
Gen 1606/8000 ▶ train MSE: 0.5951, val MSE: 0.3705
Gen 1607/8000 ▶ train MSE: 0.5951, val MSE: 0.3705
Gen 1608/8000 ▶ train MSE: 0.5951, val MSE: 0.3705
Gen 1609/8000 ▶ train MSE: 0.5951, val MSE: 0.3705
Gen 1610/8000 ▶ train MSE: 0.5951, val MSE: 0.3705
Gen 1611/8000 ▶ train MSE: 0.5951, val MSE: 0.3705
Gen 1612/8000 ▶ train MSE: 0.5951, val MSE: 0.3705
Gen 1613/8000 ▶ train MSE: 0.5951, val MSE: 0.3703
Gen 1614/8000 ▶ train MSE: 0.5951, val MSE: 0.3703
Gen 1615/8000 ▶ train MSE: 0.5951, val MSE: 0.3703
Gen 1616/8000 ▶ train MSE: 0.5951, val MSE: 0.3703
Gen 1617/8000 ▶ train MSE: 0.5951, val MSE: 0.3703
Gen 1618/8000 ▶ train MSE: 0.5951, val MSE: 0.3703
Gen 1619/8000 ▶ train MSE: 0.5951, val MSE: 0.3703
Gen 1620/8000 ▶ train MSE: 0.5951, val MSE: 0.3703
Gen 1621/8000 ▶ train MSE: 0.5950, val MSE: 0.3702
Gen 1622/8000 ▶ train MSE: 0.5950, val MSE: 0.3702
Gen 1623/8000 ▶ train MSE: 0.5950, val MSE: 0.3702
Gen 1624/8000 ▶ train MSE: 0.5950, val MSE: 0.3705
Gen 1625/8000 ▶ train MSE: 0.5950, val MSE: 0.3705
Gen 1626/8000 ▶ train MSE: 0.5950, val MSE: 0.3705
Gen 1627/8000 ▶ train MSE: 0.5950, val MSE: 0.3705
Gen 1628/8000 ▶ train MSE: 0.5950, val MSE: 0.3703
Gen 1629/8000 ▶ train MSE: 0.5950, val MSE: 0.3703
Gen 1630/8000 ▶ train MSE: 0.5950, val MSE: 0.3703
Gen 1631/8000 ▶ train MSE: 0.5950, val MSE: 0.3703
Gen 1632/8000 ▶ train MSE: 0.5949, val MSE: 0.3700
Gen 1633/8000 ▶ train MSE: 0.5949, val MSE: 0.3700
Gen 1634/8000 ▶ train MSE: 0.5949, val MSE: 0.3703
Gen 1635/8000 ▶ train MSE: 0.5949, val MSE: 0.3703
Gen 1636/8000 ▶ train MSE: 0.5949, val MSE: 0.3703
Gen 1637/8000 ▶ train MSE: 0.5949, val MSE: 0.3703
Gen 1638/8000 ▶ train MSE: 0.5949, val MSE: 0.3703
Gen 1639/8000 ▶ train MSE: 0.5949, val MSE: 0.3703
Gen 1640/8000 ▶ train MSE: 0.5949, val MSE: 0.3703
Gen 1641/8000 ▶ train MSE: 0.5949, val MSE: 0.3703
Gen 1642/8000 ▶ train MSE: 0.5949, val MSE: 0.3703
Gen 1643/8000 ▶ train MSE: 0.5949, val MSE: 0.3703
Gen 1644/8000 ▶ train MSE: 0.5949, val MSE: 0.3703
Gen 1645/8000 ▶ train MSE: 0.5949, val MSE: 0.3700
Gen 1646/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1647/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1648/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1649/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1650/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1651/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1652/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1653/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1654/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1655/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1656/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1657/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1658/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1659/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1660/8000 ▶ train MSE: 0.5948, val MSE: 0.3686
Gen 1661/8000 ▶ train MSE: 0.5948, val MSE: 0.3698
Gen 1662/8000 ▶ train MSE: 0.5948, val MSE: 0.3698
Gen 1663/8000 ▶ train MSE: 0.5948, val MSE: 0.3698
Gen 1664/8000 ▶ train MSE: 0.5948, val MSE: 0.3698
Gen 1665/8000 ▶ train MSE: 0.5948, val MSE: 0.3698
Gen 1666/8000 ▶ train MSE: 0.5948, val MSE: 0.3698
Gen 1667/8000 ▶ train MSE: 0.5948, val MSE: 0.3698
Gen 1668/8000 ▶ train MSE: 0.5948, val MSE: 0.3698
Gen 1669/8000 ▶ train MSE: 0.5947, val MSE: 0.3697
Gen 1670/8000 ▶ train MSE: 0.5947, val MSE: 0.3697
Gen 1671/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1672/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1673/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1674/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1675/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1676/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1677/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1678/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1679/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1680/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1681/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1682/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1683/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1684/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1685/8000 ▶ train MSE: 0.5946, val MSE: 0.3696
Gen 1686/8000 ▶ train MSE: 0.5946, val MSE: 0.3704
Gen 1687/8000 ▶ train MSE: 0.5946, val MSE: 0.3704
Gen 1688/8000 ▶ train MSE: 0.5946, val MSE: 0.3704
Gen 1689/8000 ▶ train MSE: 0.5946, val MSE: 0.3704
Gen 1690/8000 ▶ train MSE: 0.5946, val MSE: 0.3704
Gen 1691/8000 ▶ train MSE: 0.5946, val MSE: 0.3697
Gen 1692/8000 ▶ train MSE: 0.5946, val MSE: 0.3697
Gen 1693/8000 ▶ train MSE: 0.5946, val MSE: 0.3701
Gen 1694/8000 ▶ train MSE: 0.5946, val MSE: 0.3701
Gen 1695/8000 ▶ train MSE: 0.5945, val MSE: 0.3701
Gen 1696/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1697/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1698/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1699/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1700/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1701/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1702/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1703/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1704/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1705/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1706/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1707/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1708/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1709/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1710/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1711/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1712/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1713/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1714/8000 ▶ train MSE: 0.5944, val MSE: 0.3696
Gen 1715/8000 ▶ train MSE: 0.5944, val MSE: 0.3690
Gen 1716/8000 ▶ train MSE: 0.5944, val MSE: 0.3685
Gen 1717/8000 ▶ train MSE: 0.5944, val MSE: 0.3685
Gen 1718/8000 ▶ train MSE: 0.5944, val MSE: 0.3685
Gen 1719/8000 ▶ train MSE: 0.5944, val MSE: 0.3685
Gen 1720/8000 ▶ train MSE: 0.5944, val MSE: 0.3685
Gen 1721/8000 ▶ train MSE: 0.5944, val MSE: 0.3685
Gen 1722/8000 ▶ train MSE: 0.5944, val MSE: 0.3685
Gen 1723/8000 ▶ train MSE: 0.5944, val MSE: 0.3693
Gen 1724/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1725/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1726/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1727/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1728/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1729/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1730/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1731/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1732/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1733/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1734/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1735/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1736/8000 ▶ train MSE: 0.5943, val MSE: 0.3681
Gen 1737/8000 ▶ train MSE: 0.5943, val MSE: 0.3679
Gen 1738/8000 ▶ train MSE: 0.5943, val MSE: 0.3679
Gen 1739/8000 ▶ train MSE: 0.5943, val MSE: 0.3679
Gen 1740/8000 ▶ train MSE: 0.5943, val MSE: 0.3687
Gen 1741/8000 ▶ train MSE: 0.5942, val MSE: 0.3691
Gen 1742/8000 ▶ train MSE: 0.5942, val MSE: 0.3691
Gen 1743/8000 ▶ train MSE: 0.5942, val MSE: 0.3691
Gen 1744/8000 ▶ train MSE: 0.5942, val MSE: 0.3691
Gen 1745/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1746/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1747/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1748/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1749/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1750/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1751/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1752/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1753/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1754/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1755/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1756/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1757/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1758/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1759/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1760/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1761/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1762/8000 ▶ train MSE: 0.5941, val MSE: 0.3680
Gen 1763/8000 ▶ train MSE: 0.5941, val MSE: 0.3684
Gen 1764/8000 ▶ train MSE: 0.5941, val MSE: 0.3684
Gen 1765/8000 ▶ train MSE: 0.5941, val MSE: 0.3684
Gen 1766/8000 ▶ train MSE: 0.5941, val MSE: 0.3684
Gen 1767/8000 ▶ train MSE: 0.5941, val MSE: 0.3684
Gen 1768/8000 ▶ train MSE: 0.5941, val MSE: 0.3684
Gen 1769/8000 ▶ train MSE: 0.5941, val MSE: 0.3683
Gen 1770/8000 ▶ train MSE: 0.5941, val MSE: 0.3681
Gen 1771/8000 ▶ train MSE: 0.5941, val MSE: 0.3688
Gen 1772/8000 ▶ train MSE: 0.5941, val MSE: 0.3688
Gen 1773/8000 ▶ train MSE: 0.5941, val MSE: 0.3688
Gen 1774/8000 ▶ train MSE: 0.5941, val MSE: 0.3684
Gen 1775/8000 ▶ train MSE: 0.5941, val MSE: 0.3671
Gen 1776/8000 ▶ train MSE: 0.5941, val MSE: 0.3671
Gen 1777/8000 ▶ train MSE: 0.5941, val MSE: 0.3671
Gen 1778/8000 ▶ train MSE: 0.5940, val MSE: 0.3683
Gen 1779/8000 ▶ train MSE: 0.5940, val MSE: 0.3683
Gen 1780/8000 ▶ train MSE: 0.5940, val MSE: 0.3683
Gen 1781/8000 ▶ train MSE: 0.5940, val MSE: 0.3684
Gen 1782/8000 ▶ train MSE: 0.5940, val MSE: 0.3684
Gen 1783/8000 ▶ train MSE: 0.5940, val MSE: 0.3679
Gen 1784/8000 ▶ train MSE: 0.5940, val MSE: 0.3679
Gen 1785/8000 ▶ train MSE: 0.5940, val MSE: 0.3679
Gen 1786/8000 ▶ train MSE: 0.5940, val MSE: 0.3679
Gen 1787/8000 ▶ train MSE: 0.5940, val MSE: 0.3679
Gen 1788/8000 ▶ train MSE: 0.5939, val MSE: 0.3675
Gen 1789/8000 ▶ train MSE: 0.5939, val MSE: 0.3671
Gen 1790/8000 ▶ train MSE: 0.5939, val MSE: 0.3671
Gen 1791/8000 ▶ train MSE: 0.5939, val MSE: 0.3671
Gen 1792/8000 ▶ train MSE: 0.5939, val MSE: 0.3671
Gen 1793/8000 ▶ train MSE: 0.5939, val MSE: 0.3671
Gen 1794/8000 ▶ train MSE: 0.5939, val MSE: 0.3671
Gen 1795/8000 ▶ train MSE: 0.5939, val MSE: 0.3671
Gen 1796/8000 ▶ train MSE: 0.5939, val MSE: 0.3671
Gen 1797/8000 ▶ train MSE: 0.5939, val MSE: 0.3671
Gen 1798/8000 ▶ train MSE: 0.5939, val MSE: 0.3671
Gen 1799/8000 ▶ train MSE: 0.5938, val MSE: 0.3676
Gen 1800/8000 ▶ train MSE: 0.5938, val MSE: 0.3676
Gen 1801/8000 ▶ train MSE: 0.5938, val MSE: 0.3676
Gen 1802/8000 ▶ train MSE: 0.5938, val MSE: 0.3676
Gen 1803/8000 ▶ train MSE: 0.5938, val MSE: 0.3676
Gen 1804/8000 ▶ train MSE: 0.5938, val MSE: 0.3676
Gen 1805/8000 ▶ train MSE: 0.5938, val MSE: 0.3675
Gen 1806/8000 ▶ train MSE: 0.5938, val MSE: 0.3675
Gen 1807/8000 ▶ train MSE: 0.5938, val MSE: 0.3675
Gen 1808/8000 ▶ train MSE: 0.5938, val MSE: 0.3675
Gen 1809/8000 ▶ train MSE: 0.5936, val MSE: 0.3672
Gen 1810/8000 ▶ train MSE: 0.5936, val MSE: 0.3672
Gen 1811/8000 ▶ train MSE: 0.5936, val MSE: 0.3672
Gen 1812/8000 ▶ train MSE: 0.5936, val MSE: 0.3672
Gen 1813/8000 ▶ train MSE: 0.5936, val MSE: 0.3672
Gen 1814/8000 ▶ train MSE: 0.5936, val MSE: 0.3672
Gen 1815/8000 ▶ train MSE: 0.5936, val MSE: 0.3672
Gen 1816/8000 ▶ train MSE: 0.5936, val MSE: 0.3672
Gen 1817/8000 ▶ train MSE: 0.5936, val MSE: 0.3672
Gen 1818/8000 ▶ train MSE: 0.5936, val MSE: 0.3672
Gen 1819/8000 ▶ train MSE: 0.5936, val MSE: 0.3672
Gen 1820/8000 ▶ train MSE: 0.5936, val MSE: 0.3675
Gen 1821/8000 ▶ train MSE: 0.5936, val MSE: 0.3675
Gen 1822/8000 ▶ train MSE: 0.5936, val MSE: 0.3675
Gen 1823/8000 ▶ train MSE: 0.5936, val MSE: 0.3675
Gen 1824/8000 ▶ train MSE: 0.5936, val MSE: 0.3675
Gen 1825/8000 ▶ train MSE: 0.5936, val MSE: 0.3675
Gen 1826/8000 ▶ train MSE: 0.5936, val MSE: 0.3675
Gen 1827/8000 ▶ train MSE: 0.5936, val MSE: 0.3654
Gen 1828/8000 ▶ train MSE: 0.5936, val MSE: 0.3654
Gen 1829/8000 ▶ train MSE: 0.5936, val MSE: 0.3654
Gen 1830/8000 ▶ train MSE: 0.5936, val MSE: 0.3654
Gen 1831/8000 ▶ train MSE: 0.5936, val MSE: 0.3654
Gen 1832/8000 ▶ train MSE: 0.5936, val MSE: 0.3654
Gen 1833/8000 ▶ train MSE: 0.5935, val MSE: 0.3665
Gen 1834/8000 ▶ train MSE: 0.5935, val MSE: 0.3665
Gen 1835/8000 ▶ train MSE: 0.5935, val MSE: 0.3665
Gen 1836/8000 ▶ train MSE: 0.5935, val MSE: 0.3665
Gen 1837/8000 ▶ train MSE: 0.5935, val MSE: 0.3665
Gen 1838/8000 ▶ train MSE: 0.5935, val MSE: 0.3665
Gen 1839/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1840/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1841/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1842/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1843/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1844/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1845/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1846/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1847/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1848/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1849/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1850/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1851/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1852/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1853/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1854/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1855/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1856/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1857/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1858/8000 ▶ train MSE: 0.5934, val MSE: 0.3651
Gen 1859/8000 ▶ train MSE: 0.5934, val MSE: 0.3661
Gen 1860/8000 ▶ train MSE: 0.5934, val MSE: 0.3661
Gen 1861/8000 ▶ train MSE: 0.5934, val MSE: 0.3661
Gen 1862/8000 ▶ train MSE: 0.5934, val MSE: 0.3661
Gen 1863/8000 ▶ train MSE: 0.5934, val MSE: 0.3661
Gen 1864/8000 ▶ train MSE: 0.5934, val MSE: 0.3655
Gen 1865/8000 ▶ train MSE: 0.5934, val MSE: 0.3655
Gen 1866/8000 ▶ train MSE: 0.5934, val MSE: 0.3655
Gen 1867/8000 ▶ train MSE: 0.5934, val MSE: 0.3666
Gen 1868/8000 ▶ train MSE: 0.5934, val MSE: 0.3666
Gen 1869/8000 ▶ train MSE: 0.5934, val MSE: 0.3666
Gen 1870/8000 ▶ train MSE: 0.5934, val MSE: 0.3666
Gen 1871/8000 ▶ train MSE: 0.5933, val MSE: 0.3662
Gen 1872/8000 ▶ train MSE: 0.5933, val MSE: 0.3651
Gen 1873/8000 ▶ train MSE: 0.5933, val MSE: 0.3661
Gen 1874/8000 ▶ train MSE: 0.5933, val MSE: 0.3661
Gen 1875/8000 ▶ train MSE: 0.5932, val MSE: 0.3652
Gen 1876/8000 ▶ train MSE: 0.5932, val MSE: 0.3652
Gen 1877/8000 ▶ train MSE: 0.5932, val MSE: 0.3652
Gen 1878/8000 ▶ train MSE: 0.5932, val MSE: 0.3652
Gen 1879/8000 ▶ train MSE: 0.5932, val MSE: 0.3654
Gen 1880/8000 ▶ train MSE: 0.5932, val MSE: 0.3646
Gen 1881/8000 ▶ train MSE: 0.5932, val MSE: 0.3646
Gen 1882/8000 ▶ train MSE: 0.5932, val MSE: 0.3646
Gen 1883/8000 ▶ train MSE: 0.5932, val MSE: 0.3646
Gen 1884/8000 ▶ train MSE: 0.5932, val MSE: 0.3646
Gen 1885/8000 ▶ train MSE: 0.5932, val MSE: 0.3652
Gen 1886/8000 ▶ train MSE: 0.5932, val MSE: 0.3652
Gen 1887/8000 ▶ train MSE: 0.5930, val MSE: 0.3655
Gen 1888/8000 ▶ train MSE: 0.5930, val MSE: 0.3655
Gen 1889/8000 ▶ train MSE: 0.5930, val MSE: 0.3655
Gen 1890/8000 ▶ train MSE: 0.5930, val MSE: 0.3655
Gen 1891/8000 ▶ train MSE: 0.5930, val MSE: 0.3655
Gen 1892/8000 ▶ train MSE: 0.5930, val MSE: 0.3655
Gen 1893/8000 ▶ train MSE: 0.5930, val MSE: 0.3655
Gen 1894/8000 ▶ train MSE: 0.5929, val MSE: 0.3638
Gen 1895/8000 ▶ train MSE: 0.5929, val MSE: 0.3638
Gen 1896/8000 ▶ train MSE: 0.5929, val MSE: 0.3638
Gen 1897/8000 ▶ train MSE: 0.5929, val MSE: 0.3638
Gen 1898/8000 ▶ train MSE: 0.5929, val MSE: 0.3638
Gen 1899/8000 ▶ train MSE: 0.5929, val MSE: 0.3638
Gen 1900/8000 ▶ train MSE: 0.5929, val MSE: 0.3638
Gen 1901/8000 ▶ train MSE: 0.5929, val MSE: 0.3638
Gen 1902/8000 ▶ train MSE: 0.5928, val MSE: 0.3659
Gen 1903/8000 ▶ train MSE: 0.5928, val MSE: 0.3659
Gen 1904/8000 ▶ train MSE: 0.5928, val MSE: 0.3659
Gen 1905/8000 ▶ train MSE: 0.5928, val MSE: 0.3659
Gen 1906/8000 ▶ train MSE: 0.5928, val MSE: 0.3659
Gen 1907/8000 ▶ train MSE: 0.5928, val MSE: 0.3659
Gen 1908/8000 ▶ train MSE: 0.5928, val MSE: 0.3659
Gen 1909/8000 ▶ train MSE: 0.5928, val MSE: 0.3659
Gen 1910/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1911/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1912/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1913/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1914/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1915/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1916/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1917/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1918/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1919/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1920/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1921/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1922/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1923/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1924/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1925/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1926/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1927/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1928/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1929/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1930/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1931/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1932/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1933/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1934/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1935/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1936/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1937/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1938/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1939/8000 ▶ train MSE: 0.5927, val MSE: 0.3642
Gen 1940/8000 ▶ train MSE: 0.5927, val MSE: 0.3643
Gen 1941/8000 ▶ train MSE: 0.5927, val MSE: 0.3643
Gen 1942/8000 ▶ train MSE: 0.5927, val MSE: 0.3643
Gen 1943/8000 ▶ train MSE: 0.5927, val MSE: 0.3643
Gen 1944/8000 ▶ train MSE: 0.5927, val MSE: 0.3643
Gen 1945/8000 ▶ train MSE: 0.5927, val MSE: 0.3643
Gen 1946/8000 ▶ train MSE: 0.5926, val MSE: 0.3642
Gen 1947/8000 ▶ train MSE: 0.5926, val MSE: 0.3642
Gen 1948/8000 ▶ train MSE: 0.5926, val MSE: 0.3642
Gen 1949/8000 ▶ train MSE: 0.5926, val MSE: 0.3649
Gen 1950/8000 ▶ train MSE: 0.5926, val MSE: 0.3649
Gen 1951/8000 ▶ train MSE: 0.5926, val MSE: 0.3649
Gen 1952/8000 ▶ train MSE: 0.5925, val MSE: 0.3642
Gen 1953/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1954/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1955/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1956/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1957/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1958/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1959/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1960/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1961/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1962/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1963/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1964/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1965/8000 ▶ train MSE: 0.5925, val MSE: 0.3643
Gen 1966/8000 ▶ train MSE: 0.5925, val MSE: 0.3644
Gen 1967/8000 ▶ train MSE: 0.5925, val MSE: 0.3644
Gen 1968/8000 ▶ train MSE: 0.5924, val MSE: 0.3643
Gen 1969/8000 ▶ train MSE: 0.5924, val MSE: 0.3643
Gen 1970/8000 ▶ train MSE: 0.5924, val MSE: 0.3643
Gen 1971/8000 ▶ train MSE: 0.5923, val MSE: 0.3637
Gen 1972/8000 ▶ train MSE: 0.5923, val MSE: 0.3637
Gen 1973/8000 ▶ train MSE: 0.5923, val MSE: 0.3637
Gen 1974/8000 ▶ train MSE: 0.5923, val MSE: 0.3642
Gen 1975/8000 ▶ train MSE: 0.5923, val MSE: 0.3642
Gen 1976/8000 ▶ train MSE: 0.5923, val MSE: 0.3645
Gen 1977/8000 ▶ train MSE: 0.5923, val MSE: 0.3645
Gen 1978/8000 ▶ train MSE: 0.5921, val MSE: 0.3633
Gen 1979/8000 ▶ train MSE: 0.5921, val MSE: 0.3633
Gen 1980/8000 ▶ train MSE: 0.5921, val MSE: 0.3633
Gen 1981/8000 ▶ train MSE: 0.5921, val MSE: 0.3633
Gen 1982/8000 ▶ train MSE: 0.5921, val MSE: 0.3633
Gen 1983/8000 ▶ train MSE: 0.5921, val MSE: 0.3633
Gen 1984/8000 ▶ train MSE: 0.5921, val MSE: 0.3633
Gen 1985/8000 ▶ train MSE: 0.5921, val MSE: 0.3633
Gen 1986/8000 ▶ train MSE: 0.5921, val MSE: 0.3633
Gen 1987/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1988/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1989/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1990/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1991/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1992/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1993/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1994/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1995/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1996/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1997/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1998/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 1999/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 2000/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 2001/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 2002/8000 ▶ train MSE: 0.5920, val MSE: 0.3637
Gen 2003/8000 ▶ train MSE: 0.5919, val MSE: 0.3634
Gen 2004/8000 ▶ train MSE: 0.5919, val MSE: 0.3634
Gen 2005/8000 ▶ train MSE: 0.5919, val MSE: 0.3634
Gen 2006/8000 ▶ train MSE: 0.5919, val MSE: 0.3634
Gen 2007/8000 ▶ train MSE: 0.5919, val MSE: 0.3634
Gen 2008/8000 ▶ train MSE: 0.5919, val MSE: 0.3634
Gen 2009/8000 ▶ train MSE: 0.5919, val MSE: 0.3642
Gen 2010/8000 ▶ train MSE: 0.5919, val MSE: 0.3642
Gen 2011/8000 ▶ train MSE: 0.5919, val MSE: 0.3642
Gen 2012/8000 ▶ train MSE: 0.5919, val MSE: 0.3640
Gen 2013/8000 ▶ train MSE: 0.5919, val MSE: 0.3631
Gen 2014/8000 ▶ train MSE: 0.5919, val MSE: 0.3631
Gen 2015/8000 ▶ train MSE: 0.5919, val MSE: 0.3631
Gen 2016/8000 ▶ train MSE: 0.5919, val MSE: 0.3631
Gen 2017/8000 ▶ train MSE: 0.5918, val MSE: 0.3633
Gen 2018/8000 ▶ train MSE: 0.5918, val MSE: 0.3633
Gen 2019/8000 ▶ train MSE: 0.5918, val MSE: 0.3633
Gen 2020/8000 ▶ train MSE: 0.5918, val MSE: 0.3633
Gen 2021/8000 ▶ train MSE: 0.5918, val MSE: 0.3635
Gen 2022/8000 ▶ train MSE: 0.5917, val MSE: 0.3625
Gen 2023/8000 ▶ train MSE: 0.5917, val MSE: 0.3625
Gen 2024/8000 ▶ train MSE: 0.5917, val MSE: 0.3625
Gen 2025/8000 ▶ train MSE: 0.5917, val MSE: 0.3634
Gen 2026/8000 ▶ train MSE: 0.5917, val MSE: 0.3634
Gen 2027/8000 ▶ train MSE: 0.5917, val MSE: 0.3634
Gen 2028/8000 ▶ train MSE: 0.5917, val MSE: 0.3634
Gen 2029/8000 ▶ train MSE: 0.5917, val MSE: 0.3634
Gen 2030/8000 ▶ train MSE: 0.5917, val MSE: 0.3634
Gen 2031/8000 ▶ train MSE: 0.5917, val MSE: 0.3634
Gen 2032/8000 ▶ train MSE: 0.5917, val MSE: 0.3634
Gen 2033/8000 ▶ train MSE: 0.5915, val MSE: 0.3621
Gen 2034/8000 ▶ train MSE: 0.5915, val MSE: 0.3621
Gen 2035/8000 ▶ train MSE: 0.5915, val MSE: 0.3621
Gen 2036/8000 ▶ train MSE: 0.5915, val MSE: 0.3621
Gen 2037/8000 ▶ train MSE: 0.5914, val MSE: 0.3630
Gen 2038/8000 ▶ train MSE: 0.5914, val MSE: 0.3630
Gen 2039/8000 ▶ train MSE: 0.5914, val MSE: 0.3630
Gen 2040/8000 ▶ train MSE: 0.5914, val MSE: 0.3630
Gen 2041/8000 ▶ train MSE: 0.5914, val MSE: 0.3630
Gen 2042/8000 ▶ train MSE: 0.5914, val MSE: 0.3630
Gen 2043/8000 ▶ train MSE: 0.5914, val MSE: 0.3630
Gen 2044/8000 ▶ train MSE: 0.5914, val MSE: 0.3630
Gen 2045/8000 ▶ train MSE: 0.5914, val MSE: 0.3630
Gen 2046/8000 ▶ train MSE: 0.5914, val MSE: 0.3630
Gen 2047/8000 ▶ train MSE: 0.5914, val MSE: 0.3630
Gen 2048/8000 ▶ train MSE: 0.5914, val MSE: 0.3617
Gen 2049/8000 ▶ train MSE: 0.5914, val MSE: 0.3617
Gen 2050/8000 ▶ train MSE: 0.5914, val MSE: 0.3617
Gen 2051/8000 ▶ train MSE: 0.5914, val MSE: 0.3617
Gen 2052/8000 ▶ train MSE: 0.5914, val MSE: 0.3617
Gen 2053/8000 ▶ train MSE: 0.5914, val MSE: 0.3617
Gen 2054/8000 ▶ train MSE: 0.5914, val MSE: 0.3617
Gen 2055/8000 ▶ train MSE: 0.5914, val MSE: 0.3617
Gen 2056/8000 ▶ train MSE: 0.5914, val MSE: 0.3617
Gen 2057/8000 ▶ train MSE: 0.5914, val MSE: 0.3616
Gen 2058/8000 ▶ train MSE: 0.5914, val MSE: 0.3616
Gen 2059/8000 ▶ train MSE: 0.5914, val MSE: 0.3616
Gen 2060/8000 ▶ train MSE: 0.5913, val MSE: 0.3615
Gen 2061/8000 ▶ train MSE: 0.5913, val MSE: 0.3615
Gen 2062/8000 ▶ train MSE: 0.5913, val MSE: 0.3612
Gen 2063/8000 ▶ train MSE: 0.5913, val MSE: 0.3612
Gen 2064/8000 ▶ train MSE: 0.5913, val MSE: 0.3614
Gen 2065/8000 ▶ train MSE: 0.5913, val MSE: 0.3614
Gen 2066/8000 ▶ train MSE: 0.5913, val MSE: 0.3616
Gen 2067/8000 ▶ train MSE: 0.5913, val MSE: 0.3616
Gen 2068/8000 ▶ train MSE: 0.5913, val MSE: 0.3616
Gen 2069/8000 ▶ train MSE: 0.5913, val MSE: 0.3616
Gen 2070/8000 ▶ train MSE: 0.5912, val MSE: 0.3609
Gen 2071/8000 ▶ train MSE: 0.5911, val MSE: 0.3606
Gen 2072/8000 ▶ train MSE: 0.5911, val MSE: 0.3606
Gen 2073/8000 ▶ train MSE: 0.5911, val MSE: 0.3606
Gen 2074/8000 ▶ train MSE: 0.5911, val MSE: 0.3606
Gen 2075/8000 ▶ train MSE: 0.5911, val MSE: 0.3606
Gen 2076/8000 ▶ train MSE: 0.5911, val MSE: 0.3618
Gen 2077/8000 ▶ train MSE: 0.5911, val MSE: 0.3618
Gen 2078/8000 ▶ train MSE: 0.5911, val MSE: 0.3618
Gen 2079/8000 ▶ train MSE: 0.5911, val MSE: 0.3611
Gen 2080/8000 ▶ train MSE: 0.5911, val MSE: 0.3611
Gen 2081/8000 ▶ train MSE: 0.5911, val MSE: 0.3611
Gen 2082/8000 ▶ train MSE: 0.5911, val MSE: 0.3611
Gen 2083/8000 ▶ train MSE: 0.5911, val MSE: 0.3611
Gen 2084/8000 ▶ train MSE: 0.5911, val MSE: 0.3611
Gen 2085/8000 ▶ train MSE: 0.5911, val MSE: 0.3611
Gen 2086/8000 ▶ train MSE: 0.5910, val MSE: 0.3604
Gen 2087/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2088/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2089/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2090/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2091/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2092/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2093/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2094/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2095/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2096/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2097/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2098/8000 ▶ train MSE: 0.5910, val MSE: 0.3608
Gen 2099/8000 ▶ train MSE: 0.5909, val MSE: 0.3611
Gen 2100/8000 ▶ train MSE: 0.5909, val MSE: 0.3611
Gen 2101/8000 ▶ train MSE: 0.5909, val MSE: 0.3611
Gen 2102/8000 ▶ train MSE: 0.5909, val MSE: 0.3611
Gen 2103/8000 ▶ train MSE: 0.5909, val MSE: 0.3611
Gen 2104/8000 ▶ train MSE: 0.5909, val MSE: 0.3611
Gen 2105/8000 ▶ train MSE: 0.5909, val MSE: 0.3611
Gen 2106/8000 ▶ train MSE: 0.5909, val MSE: 0.3611
Gen 2107/8000 ▶ train MSE: 0.5909, val MSE: 0.3611
Gen 2108/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2109/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2110/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2111/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2112/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2113/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2114/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2115/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2116/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2117/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2118/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2119/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2120/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2121/8000 ▶ train MSE: 0.5908, val MSE: 0.3609
Gen 2122/8000 ▶ train MSE: 0.5908, val MSE: 0.3615
Gen 2123/8000 ▶ train MSE: 0.5908, val MSE: 0.3615
Gen 2124/8000 ▶ train MSE: 0.5908, val MSE: 0.3615
Gen 2125/8000 ▶ train MSE: 0.5908, val MSE: 0.3615
Gen 2126/8000 ▶ train MSE: 0.5908, val MSE: 0.3615
Gen 2127/8000 ▶ train MSE: 0.5907, val MSE: 0.3609
Gen 2128/8000 ▶ train MSE: 0.5907, val MSE: 0.3609
Gen 2129/8000 ▶ train MSE: 0.5907, val MSE: 0.3609
Gen 2130/8000 ▶ train MSE: 0.5907, val MSE: 0.3609
Gen 2131/8000 ▶ train MSE: 0.5907, val MSE: 0.3609
Gen 2132/8000 ▶ train MSE: 0.5907, val MSE: 0.3609
Gen 2133/8000 ▶ train MSE: 0.5907, val MSE: 0.3609
Gen 2134/8000 ▶ train MSE: 0.5907, val MSE: 0.3609
Gen 2135/8000 ▶ train MSE: 0.5907, val MSE: 0.3609
Gen 2136/8000 ▶ train MSE: 0.5907, val MSE: 0.3609
Gen 2137/8000 ▶ train MSE: 0.5907, val MSE: 0.3609
Gen 2138/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2139/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2140/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2141/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2142/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2143/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2144/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2145/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2146/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2147/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2148/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2149/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2150/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2151/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2152/8000 ▶ train MSE: 0.5907, val MSE: 0.3597
Gen 2153/8000 ▶ train MSE: 0.5907, val MSE: 0.3599
Gen 2154/8000 ▶ train MSE: 0.5907, val MSE: 0.3599
Gen 2155/8000 ▶ train MSE: 0.5907, val MSE: 0.3599
Gen 2156/8000 ▶ train MSE: 0.5907, val MSE: 0.3599
Gen 2157/8000 ▶ train MSE: 0.5906, val MSE: 0.3608
Gen 2158/8000 ▶ train MSE: 0.5906, val MSE: 0.3608
Gen 2159/8000 ▶ train MSE: 0.5906, val MSE: 0.3608
Gen 2160/8000 ▶ train MSE: 0.5906, val MSE: 0.3608
Gen 2161/8000 ▶ train MSE: 0.5906, val MSE: 0.3608
Gen 2162/8000 ▶ train MSE: 0.5906, val MSE: 0.3608
Gen 2163/8000 ▶ train MSE: 0.5906, val MSE: 0.3608
Gen 2164/8000 ▶ train MSE: 0.5906, val MSE: 0.3608
Gen 2165/8000 ▶ train MSE: 0.5906, val MSE: 0.3608
Gen 2166/8000 ▶ train MSE: 0.5906, val MSE: 0.3608
Gen 2167/8000 ▶ train MSE: 0.5905, val MSE: 0.3605
Gen 2168/8000 ▶ train MSE: 0.5905, val MSE: 0.3605
Gen 2169/8000 ▶ train MSE: 0.5905, val MSE: 0.3605
Gen 2170/8000 ▶ train MSE: 0.5905, val MSE: 0.3601
Gen 2171/8000 ▶ train MSE: 0.5905, val MSE: 0.3601
Gen 2172/8000 ▶ train MSE: 0.5905, val MSE: 0.3601
Gen 2173/8000 ▶ train MSE: 0.5905, val MSE: 0.3601
Gen 2174/8000 ▶ train MSE: 0.5905, val MSE: 0.3601
Gen 2175/8000 ▶ train MSE: 0.5905, val MSE: 0.3594
Gen 2176/8000 ▶ train MSE: 0.5905, val MSE: 0.3594
Gen 2177/8000 ▶ train MSE: 0.5905, val MSE: 0.3594
Gen 2178/8000 ▶ train MSE: 0.5905, val MSE: 0.3595
Gen 2179/8000 ▶ train MSE: 0.5905, val MSE: 0.3595
Gen 2180/8000 ▶ train MSE: 0.5904, val MSE: 0.3600
Gen 2181/8000 ▶ train MSE: 0.5904, val MSE: 0.3600
Gen 2182/8000 ▶ train MSE: 0.5904, val MSE: 0.3600
Gen 2183/8000 ▶ train MSE: 0.5904, val MSE: 0.3600
Gen 2184/8000 ▶ train MSE: 0.5904, val MSE: 0.3600
Gen 2185/8000 ▶ train MSE: 0.5904, val MSE: 0.3600
Gen 2186/8000 ▶ train MSE: 0.5904, val MSE: 0.3600
Gen 2187/8000 ▶ train MSE: 0.5904, val MSE: 0.3600
Gen 2188/8000 ▶ train MSE: 0.5904, val MSE: 0.3600
Gen 2189/8000 ▶ train MSE: 0.5904, val MSE: 0.3600
Gen 2190/8000 ▶ train MSE: 0.5904, val MSE: 0.3598
Gen 2191/8000 ▶ train MSE: 0.5904, val MSE: 0.3598
Gen 2192/8000 ▶ train MSE: 0.5904, val MSE: 0.3598
Gen 2193/8000 ▶ train MSE: 0.5904, val MSE: 0.3598
Gen 2194/8000 ▶ train MSE: 0.5904, val MSE: 0.3598
Gen 2195/8000 ▶ train MSE: 0.5904, val MSE: 0.3598
Gen 2196/8000 ▶ train MSE: 0.5904, val MSE: 0.3598
Gen 2197/8000 ▶ train MSE: 0.5904, val MSE: 0.3598
Gen 2198/8000 ▶ train MSE: 0.5904, val MSE: 0.3598
Gen 2199/8000 ▶ train MSE: 0.5904, val MSE: 0.3598
Gen 2200/8000 ▶ train MSE: 0.5904, val MSE: 0.3598
Gen 2201/8000 ▶ train MSE: 0.5904, val MSE: 0.3601
Gen 2202/8000 ▶ train MSE: 0.5904, val MSE: 0.3601
Gen 2203/8000 ▶ train MSE: 0.5904, val MSE: 0.3601
Gen 2204/8000 ▶ train MSE: 0.5904, val MSE: 0.3601
Gen 2205/8000 ▶ train MSE: 0.5904, val MSE: 0.3601
Gen 2206/8000 ▶ train MSE: 0.5904, val MSE: 0.3601
Gen 2207/8000 ▶ train MSE: 0.5904, val MSE: 0.3601
Gen 2208/8000 ▶ train MSE: 0.5904, val MSE: 0.3601
Gen 2209/8000 ▶ train MSE: 0.5904, val MSE: 0.3601
Gen 2210/8000 ▶ train MSE: 0.5903, val MSE: 0.3607
Gen 2211/8000 ▶ train MSE: 0.5903, val MSE: 0.3607
Gen 2212/8000 ▶ train MSE: 0.5903, val MSE: 0.3607
Gen 2213/8000 ▶ train MSE: 0.5903, val MSE: 0.3607
Gen 2214/8000 ▶ train MSE: 0.5903, val MSE: 0.3607
Gen 2215/8000 ▶ train MSE: 0.5903, val MSE: 0.3603
Gen 2216/8000 ▶ train MSE: 0.5903, val MSE: 0.3603
Gen 2217/8000 ▶ train MSE: 0.5903, val MSE: 0.3603
Gen 2218/8000 ▶ train MSE: 0.5903, val MSE: 0.3603
Gen 2219/8000 ▶ train MSE: 0.5903, val MSE: 0.3603
Gen 2220/8000 ▶ train MSE: 0.5903, val MSE: 0.3603
Gen 2221/8000 ▶ train MSE: 0.5903, val MSE: 0.3603
Gen 2222/8000 ▶ train MSE: 0.5903, val MSE: 0.3603
Gen 2223/8000 ▶ train MSE: 0.5903, val MSE: 0.3603
Gen 2224/8000 ▶ train MSE: 0.5903, val MSE: 0.3603
Gen 2225/8000 ▶ train MSE: 0.5903, val MSE: 0.3603
Gen 2226/8000 ▶ train MSE: 0.5903, val MSE: 0.3607
Gen 2227/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2228/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2229/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2230/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2231/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2232/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2233/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2234/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2235/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2236/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2237/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2238/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2239/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2240/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2241/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2242/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2243/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2244/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2245/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2246/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2247/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2248/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2249/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2250/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2251/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2252/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2253/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2254/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2255/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2256/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2257/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2258/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2259/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2260/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2261/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2262/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2263/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2264/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2265/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2266/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2267/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2268/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2269/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2270/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2271/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2272/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2273/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2274/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2275/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2276/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2277/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2278/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2279/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2280/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2281/8000 ▶ train MSE: 0.5901, val MSE: 0.3602
Gen 2282/8000 ▶ train MSE: 0.5901, val MSE: 0.3596
Gen 2283/8000 ▶ train MSE: 0.5901, val MSE: 0.3600
Gen 2284/8000 ▶ train MSE: 0.5901, val MSE: 0.3600
Gen 2285/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2286/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2287/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2288/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2289/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2290/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2291/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2292/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2293/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2294/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2295/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2296/8000 ▶ train MSE: 0.5901, val MSE: 0.3601
Gen 2297/8000 ▶ train MSE: 0.5901, val MSE: 0.3600
Gen 2298/8000 ▶ train MSE: 0.5901, val MSE: 0.3600
Gen 2299/8000 ▶ train MSE: 0.5901, val MSE: 0.3600
Gen 2300/8000 ▶ train MSE: 0.5901, val MSE: 0.3600
Gen 2301/8000 ▶ train MSE: 0.5901, val MSE: 0.3600
Gen 2302/8000 ▶ train MSE: 0.5901, val MSE: 0.3597
Gen 2303/8000 ▶ train MSE: 0.5901, val MSE: 0.3597
Gen 2304/8000 ▶ train MSE: 0.5901, val MSE: 0.3597
Gen 2305/8000 ▶ train MSE: 0.5900, val MSE: 0.3589
Gen 2306/8000 ▶ train MSE: 0.5900, val MSE: 0.3589
Gen 2307/8000 ▶ train MSE: 0.5900, val MSE: 0.3589
Gen 2308/8000 ▶ train MSE: 0.5900, val MSE: 0.3589
Gen 2309/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2310/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2311/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2312/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2313/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2314/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2315/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2316/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2317/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2318/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2319/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2320/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2321/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2322/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2323/8000 ▶ train MSE: 0.5900, val MSE: 0.3592
Gen 2324/8000 ▶ train MSE: 0.5899, val MSE: 0.3592
Gen 2325/8000 ▶ train MSE: 0.5899, val MSE: 0.3592
Gen 2326/8000 ▶ train MSE: 0.5899, val MSE: 0.3592
Gen 2327/8000 ▶ train MSE: 0.5899, val MSE: 0.3592
Gen 2328/8000 ▶ train MSE: 0.5899, val MSE: 0.3592
Gen 2329/8000 ▶ train MSE: 0.5899, val MSE: 0.3589
Gen 2330/8000 ▶ train MSE: 0.5899, val MSE: 0.3589
Gen 2331/8000 ▶ train MSE: 0.5899, val MSE: 0.3589
Gen 2332/8000 ▶ train MSE: 0.5899, val MSE: 0.3589
Gen 2333/8000 ▶ train MSE: 0.5899, val MSE: 0.3589
Gen 2334/8000 ▶ train MSE: 0.5899, val MSE: 0.3601
Gen 2335/8000 ▶ train MSE: 0.5899, val MSE: 0.3601
Gen 2336/8000 ▶ train MSE: 0.5899, val MSE: 0.3601
Gen 2337/8000 ▶ train MSE: 0.5899, val MSE: 0.3601
Gen 2338/8000 ▶ train MSE: 0.5899, val MSE: 0.3601
Gen 2339/8000 ▶ train MSE: 0.5899, val MSE: 0.3601
Gen 2340/8000 ▶ train MSE: 0.5899, val MSE: 0.3601
Gen 2341/8000 ▶ train MSE: 0.5899, val MSE: 0.3601
Gen 2342/8000 ▶ train MSE: 0.5899, val MSE: 0.3601
Gen 2343/8000 ▶ train MSE: 0.5899, val MSE: 0.3601
Gen 2344/8000 ▶ train MSE: 0.5898, val MSE: 0.3594
Gen 2345/8000 ▶ train MSE: 0.5898, val MSE: 0.3594
Gen 2346/8000 ▶ train MSE: 0.5898, val MSE: 0.3593
Gen 2347/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2348/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2349/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2350/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2351/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2352/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2353/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2354/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2355/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2356/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2357/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2358/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2359/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2360/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2361/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2362/8000 ▶ train MSE: 0.5897, val MSE: 0.3598
Gen 2363/8000 ▶ train MSE: 0.5897, val MSE: 0.3596
Gen 2364/8000 ▶ train MSE: 0.5897, val MSE: 0.3578
Gen 2365/8000 ▶ train MSE: 0.5897, val MSE: 0.3578
Gen 2366/8000 ▶ train MSE: 0.5897, val MSE: 0.3578
Gen 2367/8000 ▶ train MSE: 0.5897, val MSE: 0.3581
Gen 2368/8000 ▶ train MSE: 0.5897, val MSE: 0.3581
Gen 2369/8000 ▶ train MSE: 0.5897, val MSE: 0.3581
Gen 2370/8000 ▶ train MSE: 0.5897, val MSE: 0.3584
Gen 2371/8000 ▶ train MSE: 0.5897, val MSE: 0.3584
Gen 2372/8000 ▶ train MSE: 0.5897, val MSE: 0.3584
Gen 2373/8000 ▶ train MSE: 0.5897, val MSE: 0.3584
Gen 2374/8000 ▶ train MSE: 0.5897, val MSE: 0.3579
Gen 2375/8000 ▶ train MSE: 0.5897, val MSE: 0.3579
Gen 2376/8000 ▶ train MSE: 0.5897, val MSE: 0.3579
Gen 2377/8000 ▶ train MSE: 0.5896, val MSE: 0.3585
Gen 2378/8000 ▶ train MSE: 0.5896, val MSE: 0.3585
Gen 2379/8000 ▶ train MSE: 0.5896, val MSE: 0.3585
Gen 2380/8000 ▶ train MSE: 0.5896, val MSE: 0.3585
Gen 2381/8000 ▶ train MSE: 0.5896, val MSE: 0.3585
Gen 2382/8000 ▶ train MSE: 0.5896, val MSE: 0.3585
Gen 2383/8000 ▶ train MSE: 0.5896, val MSE: 0.3585
Gen 2384/8000 ▶ train MSE: 0.5896, val MSE: 0.3585
Gen 2385/8000 ▶ train MSE: 0.5896, val MSE: 0.3589
Gen 2386/8000 ▶ train MSE: 0.5896, val MSE: 0.3589
Gen 2387/8000 ▶ train MSE: 0.5896, val MSE: 0.3589
Gen 2388/8000 ▶ train MSE: 0.5896, val MSE: 0.3589
Gen 2389/8000 ▶ train MSE: 0.5896, val MSE: 0.3589
Gen 2390/8000 ▶ train MSE: 0.5896, val MSE: 0.3589
Gen 2391/8000 ▶ train MSE: 0.5896, val MSE: 0.3589
Gen 2392/8000 ▶ train MSE: 0.5896, val MSE: 0.3589
Gen 2393/8000 ▶ train MSE: 0.5896, val MSE: 0.3589
Gen 2394/8000 ▶ train MSE: 0.5895, val MSE: 0.3581
Gen 2395/8000 ▶ train MSE: 0.5895, val MSE: 0.3581
Gen 2396/8000 ▶ train MSE: 0.5895, val MSE: 0.3581
Gen 2397/8000 ▶ train MSE: 0.5895, val MSE: 0.3581
Gen 2398/8000 ▶ train MSE: 0.5895, val MSE: 0.3581
Gen 2399/8000 ▶ train MSE: 0.5895, val MSE: 0.3581
Gen 2400/8000 ▶ train MSE: 0.5895, val MSE: 0.3581
Gen 2401/8000 ▶ train MSE: 0.5895, val MSE: 0.3581
Gen 2402/8000 ▶ train MSE: 0.5895, val MSE: 0.3580
Gen 2403/8000 ▶ train MSE: 0.5895, val MSE: 0.3580
Gen 2404/8000 ▶ train MSE: 0.5895, val MSE: 0.3580
Gen 2405/8000 ▶ train MSE: 0.5895, val MSE: 0.3580
Gen 2406/8000 ▶ train MSE: 0.5895, val MSE: 0.3580
Gen 2407/8000 ▶ train MSE: 0.5895, val MSE: 0.3587
Gen 2408/8000 ▶ train MSE: 0.5895, val MSE: 0.3587
Gen 2409/8000 ▶ train MSE: 0.5895, val MSE: 0.3587
Gen 2410/8000 ▶ train MSE: 0.5895, val MSE: 0.3588
Gen 2411/8000 ▶ train MSE: 0.5895, val MSE: 0.3588
Gen 2412/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2413/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2414/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2415/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2416/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2417/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2418/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2419/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2420/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2421/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2422/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2423/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2424/8000 ▶ train MSE: 0.5894, val MSE: 0.3581
Gen 2425/8000 ▶ train MSE: 0.5894, val MSE: 0.3585
Gen 2426/8000 ▶ train MSE: 0.5894, val MSE: 0.3585
Gen 2427/8000 ▶ train MSE: 0.5894, val MSE: 0.3585
Gen 2428/8000 ▶ train MSE: 0.5894, val MSE: 0.3585
Gen 2429/8000 ▶ train MSE: 0.5893, val MSE: 0.3569
Gen 2430/8000 ▶ train MSE: 0.5893, val MSE: 0.3569
Gen 2431/8000 ▶ train MSE: 0.5893, val MSE: 0.3569
Gen 2432/8000 ▶ train MSE: 0.5893, val MSE: 0.3569
Gen 2433/8000 ▶ train MSE: 0.5893, val MSE: 0.3569
Gen 2434/8000 ▶ train MSE: 0.5893, val MSE: 0.3569
Gen 2435/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2436/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2437/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2438/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2439/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2440/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2441/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2442/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2443/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2444/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2445/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2446/8000 ▶ train MSE: 0.5892, val MSE: 0.3588
Gen 2447/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2448/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2449/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2450/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2451/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2452/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2453/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2454/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2455/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2456/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2457/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2458/8000 ▶ train MSE: 0.5892, val MSE: 0.3577
Gen 2459/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2460/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2461/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2462/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2463/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2464/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2465/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2466/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2467/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2468/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2469/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2470/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2471/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2472/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2473/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2474/8000 ▶ train MSE: 0.5891, val MSE: 0.3572
Gen 2475/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2476/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2477/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2478/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2479/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2480/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2481/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2482/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2483/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2484/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2485/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2486/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2487/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2488/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2489/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2490/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2491/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2492/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2493/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2494/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2495/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2496/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2497/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2498/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2499/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2500/8000 ▶ train MSE: 0.5890, val MSE: 0.3559
Gen 2501/8000 ▶ train MSE: 0.5890, val MSE: 0.3571
Gen 2502/8000 ▶ train MSE: 0.5890, val MSE: 0.3571
Gen 2503/8000 ▶ train MSE: 0.5889, val MSE: 0.3571
Gen 2504/8000 ▶ train MSE: 0.5889, val MSE: 0.3571
Gen 2505/8000 ▶ train MSE: 0.5889, val MSE: 0.3571
Gen 2506/8000 ▶ train MSE: 0.5889, val MSE: 0.3571
Gen 2507/8000 ▶ train MSE: 0.5889, val MSE: 0.3566
Gen 2508/8000 ▶ train MSE: 0.5889, val MSE: 0.3566
Gen 2509/8000 ▶ train MSE: 0.5889, val MSE: 0.3566
Gen 2510/8000 ▶ train MSE: 0.5889, val MSE: 0.3566
Gen 2511/8000 ▶ train MSE: 0.5889, val MSE: 0.3566
Gen 2512/8000 ▶ train MSE: 0.5889, val MSE: 0.3566
Gen 2513/8000 ▶ train MSE: 0.5889, val MSE: 0.3566
Gen 2514/8000 ▶ train MSE: 0.5889, val MSE: 0.3566
Gen 2515/8000 ▶ train MSE: 0.5889, val MSE: 0.3566
Gen 2516/8000 ▶ train MSE: 0.5889, val MSE: 0.3566
Gen 2517/8000 ▶ train MSE: 0.5889, val MSE: 0.3566
Gen 2518/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2519/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2520/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2521/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2522/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2523/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2524/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2525/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2526/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2527/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2528/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2529/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2530/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2531/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2532/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2533/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2534/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2535/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2536/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2537/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2538/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2539/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2540/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2541/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2542/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2543/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2544/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2545/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2546/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2547/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2548/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2549/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2550/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2551/8000 ▶ train MSE: 0.5889, val MSE: 0.3568
Gen 2552/8000 ▶ train MSE: 0.5889, val MSE: 0.3570
Gen 2553/8000 ▶ train MSE: 0.5889, val MSE: 0.3570
Gen 2554/8000 ▶ train MSE: 0.5889, val MSE: 0.3570
Gen 2555/8000 ▶ train MSE: 0.5889, val MSE: 0.3570
Gen 2556/8000 ▶ train MSE: 0.5889, val MSE: 0.3570
Gen 2557/8000 ▶ train MSE: 0.5889, val MSE: 0.3570
Gen 2558/8000 ▶ train MSE: 0.5889, val MSE: 0.3570
Gen 2559/8000 ▶ train MSE: 0.5889, val MSE: 0.3570
Gen 2560/8000 ▶ train MSE: 0.5889, val MSE: 0.3565
Gen 2561/8000 ▶ train MSE: 0.5889, val MSE: 0.3565
Gen 2562/8000 ▶ train MSE: 0.5889, val MSE: 0.3565
Gen 2563/8000 ▶ train MSE: 0.5889, val MSE: 0.3565
Gen 2564/8000 ▶ train MSE: 0.5889, val MSE: 0.3565
Gen 2565/8000 ▶ train MSE: 0.5889, val MSE: 0.3572
Gen 2566/8000 ▶ train MSE: 0.5889, val MSE: 0.3572
Gen 2567/8000 ▶ train MSE: 0.5889, val MSE: 0.3572
Gen 2568/8000 ▶ train MSE: 0.5889, val MSE: 0.3572
Gen 2569/8000 ▶ train MSE: 0.5889, val MSE: 0.3563
Gen 2570/8000 ▶ train MSE: 0.5889, val MSE: 0.3563
Gen 2571/8000 ▶ train MSE: 0.5889, val MSE: 0.3563
Gen 2572/8000 ▶ train MSE: 0.5889, val MSE: 0.3563
Gen 2573/8000 ▶ train MSE: 0.5888, val MSE: 0.3561
Gen 2574/8000 ▶ train MSE: 0.5888, val MSE: 0.3561
Gen 2575/8000 ▶ train MSE: 0.5888, val MSE: 0.3561
Gen 2576/8000 ▶ train MSE: 0.5888, val MSE: 0.3564
Gen 2577/8000 ▶ train MSE: 0.5888, val MSE: 0.3564
Gen 2578/8000 ▶ train MSE: 0.5888, val MSE: 0.3564
Gen 2579/8000 ▶ train MSE: 0.5888, val MSE: 0.3564
Gen 2580/8000 ▶ train MSE: 0.5888, val MSE: 0.3564
Gen 2581/8000 ▶ train MSE: 0.5888, val MSE: 0.3564
Gen 2582/8000 ▶ train MSE: 0.5888, val MSE: 0.3564
Gen 2583/8000 ▶ train MSE: 0.5888, val MSE: 0.3557
Gen 2584/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2585/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2586/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2587/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2588/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2589/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2590/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2591/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2592/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2593/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2594/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2595/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2596/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2597/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2598/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2599/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2600/8000 ▶ train MSE: 0.5887, val MSE: 0.3561
Gen 2601/8000 ▶ train MSE: 0.5887, val MSE: 0.3561
Gen 2602/8000 ▶ train MSE: 0.5887, val MSE: 0.3561
Gen 2603/8000 ▶ train MSE: 0.5887, val MSE: 0.3561
Gen 2604/8000 ▶ train MSE: 0.5887, val MSE: 0.3561
Gen 2605/8000 ▶ train MSE: 0.5887, val MSE: 0.3561
Gen 2606/8000 ▶ train MSE: 0.5887, val MSE: 0.3561
Gen 2607/8000 ▶ train MSE: 0.5887, val MSE: 0.3561
Gen 2608/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2609/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2610/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2611/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2612/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2613/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2614/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2615/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2616/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2617/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2618/8000 ▶ train MSE: 0.5887, val MSE: 0.3564
Gen 2619/8000 ▶ train MSE: 0.5887, val MSE: 0.3568
Gen 2620/8000 ▶ train MSE: 0.5887, val MSE: 0.3568
Gen 2621/8000 ▶ train MSE: 0.5887, val MSE: 0.3568
Gen 2622/8000 ▶ train MSE: 0.5887, val MSE: 0.3560
Gen 2623/8000 ▶ train MSE: 0.5887, val MSE: 0.3560
Gen 2624/8000 ▶ train MSE: 0.5887, val MSE: 0.3560
Gen 2625/8000 ▶ train MSE: 0.5887, val MSE: 0.3560
Gen 2626/8000 ▶ train MSE: 0.5887, val MSE: 0.3560
Gen 2627/8000 ▶ train MSE: 0.5887, val MSE: 0.3560
Gen 2628/8000 ▶ train MSE: 0.5887, val MSE: 0.3560
Gen 2629/8000 ▶ train MSE: 0.5887, val MSE: 0.3560
Gen 2630/8000 ▶ train MSE: 0.5887, val MSE: 0.3560
Gen 2631/8000 ▶ train MSE: 0.5887, val MSE: 0.3560
Gen 2632/8000 ▶ train MSE: 0.5887, val MSE: 0.3560
Gen 2633/8000 ▶ train MSE: 0.5886, val MSE: 0.3572
Gen 2634/8000 ▶ train MSE: 0.5886, val MSE: 0.3572
Gen 2635/8000 ▶ train MSE: 0.5886, val MSE: 0.3572
Gen 2636/8000 ▶ train MSE: 0.5886, val MSE: 0.3572
Gen 2637/8000 ▶ train MSE: 0.5886, val MSE: 0.3572
Gen 2638/8000 ▶ train MSE: 0.5886, val MSE: 0.3572
Gen 2639/8000 ▶ train MSE: 0.5886, val MSE: 0.3572
Gen 2640/8000 ▶ train MSE: 0.5886, val MSE: 0.3572
Gen 2641/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2642/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2643/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2644/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2645/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2646/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2647/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2648/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2649/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2650/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2651/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2652/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2653/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2654/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2655/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2656/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2657/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2658/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2659/8000 ▶ train MSE: 0.5885, val MSE: 0.3569
Gen 2660/8000 ▶ train MSE: 0.5885, val MSE: 0.3567
Gen 2661/8000 ▶ train MSE: 0.5885, val MSE: 0.3567
Gen 2662/8000 ▶ train MSE: 0.5885, val MSE: 0.3567
Gen 2663/8000 ▶ train MSE: 0.5885, val MSE: 0.3567
Gen 2664/8000 ▶ train MSE: 0.5885, val MSE: 0.3567
Gen 2665/8000 ▶ train MSE: 0.5885, val MSE: 0.3567
Gen 2666/8000 ▶ train MSE: 0.5885, val MSE: 0.3567
Gen 2667/8000 ▶ train MSE: 0.5885, val MSE: 0.3560
Gen 2668/8000 ▶ train MSE: 0.5885, val MSE: 0.3567
Gen 2669/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2670/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2671/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2672/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2673/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2674/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2675/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2676/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2677/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2678/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2679/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2680/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2681/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2682/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2683/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2684/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2685/8000 ▶ train MSE: 0.5884, val MSE: 0.3548
Gen 2686/8000 ▶ train MSE: 0.5884, val MSE: 0.3560
Gen 2687/8000 ▶ train MSE: 0.5884, val MSE: 0.3560
Gen 2688/8000 ▶ train MSE: 0.5884, val MSE: 0.3560
Gen 2689/8000 ▶ train MSE: 0.5884, val MSE: 0.3560
Gen 2690/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2691/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2692/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2693/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2694/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2695/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2696/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2697/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2698/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2699/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2700/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2701/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2702/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2703/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2704/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2705/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2706/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2707/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2708/8000 ▶ train MSE: 0.5883, val MSE: 0.3562
Gen 2709/8000 ▶ train MSE: 0.5883, val MSE: 0.3552
Gen 2710/8000 ▶ train MSE: 0.5883, val MSE: 0.3556
Gen 2711/8000 ▶ train MSE: 0.5883, val MSE: 0.3556
Gen 2712/8000 ▶ train MSE: 0.5882, val MSE: 0.3557
Gen 2713/8000 ▶ train MSE: 0.5882, val MSE: 0.3557
Gen 2714/8000 ▶ train MSE: 0.5882, val MSE: 0.3557
Gen 2715/8000 ▶ train MSE: 0.5882, val MSE: 0.3557
Gen 2716/8000 ▶ train MSE: 0.5882, val MSE: 0.3557
Gen 2717/8000 ▶ train MSE: 0.5882, val MSE: 0.3557
Gen 2718/8000 ▶ train MSE: 0.5881, val MSE: 0.3548
Gen 2719/8000 ▶ train MSE: 0.5881, val MSE: 0.3548
Gen 2720/8000 ▶ train MSE: 0.5881, val MSE: 0.3548
Gen 2721/8000 ▶ train MSE: 0.5881, val MSE: 0.3556
Gen 2722/8000 ▶ train MSE: 0.5881, val MSE: 0.3556
Gen 2723/8000 ▶ train MSE: 0.5881, val MSE: 0.3556
Gen 2724/8000 ▶ train MSE: 0.5881, val MSE: 0.3556
Gen 2725/8000 ▶ train MSE: 0.5881, val MSE: 0.3556
Gen 2726/8000 ▶ train MSE: 0.5881, val MSE: 0.3556
Gen 2727/8000 ▶ train MSE: 0.5881, val MSE: 0.3556
Gen 2728/8000 ▶ train MSE: 0.5881, val MSE: 0.3556
Gen 2729/8000 ▶ train MSE: 0.5881, val MSE: 0.3556
Gen 2730/8000 ▶ train MSE: 0.5881, val MSE: 0.3556
Gen 2731/8000 ▶ train MSE: 0.5881, val MSE: 0.3556
Gen 2732/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2733/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2734/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2735/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2736/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2737/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2738/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2739/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2740/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2741/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2742/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2743/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2744/8000 ▶ train MSE: 0.5881, val MSE: 0.3549
Gen 2745/8000 ▶ train MSE: 0.5881, val MSE: 0.3543
Gen 2746/8000 ▶ train MSE: 0.5881, val MSE: 0.3543
Gen 2747/8000 ▶ train MSE: 0.5881, val MSE: 0.3543
Gen 2748/8000 ▶ train MSE: 0.5881, val MSE: 0.3543
Gen 2749/8000 ▶ train MSE: 0.5881, val MSE: 0.3543
Gen 2750/8000 ▶ train MSE: 0.5880, val MSE: 0.3549
Gen 2751/8000 ▶ train MSE: 0.5880, val MSE: 0.3549
Gen 2752/8000 ▶ train MSE: 0.5880, val MSE: 0.3549
Gen 2753/8000 ▶ train MSE: 0.5880, val MSE: 0.3549
Gen 2754/8000 ▶ train MSE: 0.5880, val MSE: 0.3549
Gen 2755/8000 ▶ train MSE: 0.5880, val MSE: 0.3553
Gen 2756/8000 ▶ train MSE: 0.5880, val MSE: 0.3550
Gen 2757/8000 ▶ train MSE: 0.5880, val MSE: 0.3550
Gen 2758/8000 ▶ train MSE: 0.5880, val MSE: 0.3550
Gen 2759/8000 ▶ train MSE: 0.5880, val MSE: 0.3550
Gen 2760/8000 ▶ train MSE: 0.5879, val MSE: 0.3555
Gen 2761/8000 ▶ train MSE: 0.5879, val MSE: 0.3555
Gen 2762/8000 ▶ train MSE: 0.5879, val MSE: 0.3555
Gen 2763/8000 ▶ train MSE: 0.5879, val MSE: 0.3555
Gen 2764/8000 ▶ train MSE: 0.5879, val MSE: 0.3555
Gen 2765/8000 ▶ train MSE: 0.5879, val MSE: 0.3555
Gen 2766/8000 ▶ train MSE: 0.5879, val MSE: 0.3555
Gen 2767/8000 ▶ train MSE: 0.5879, val MSE: 0.3555
Gen 2768/8000 ▶ train MSE: 0.5879, val MSE: 0.3550
Gen 2769/8000 ▶ train MSE: 0.5879, val MSE: 0.3553
Gen 2770/8000 ▶ train MSE: 0.5879, val MSE: 0.3553
Gen 2771/8000 ▶ train MSE: 0.5879, val MSE: 0.3553
Gen 2772/8000 ▶ train MSE: 0.5879, val MSE: 0.3553
Gen 2773/8000 ▶ train MSE: 0.5879, val MSE: 0.3553
Gen 2774/8000 ▶ train MSE: 0.5879, val MSE: 0.3546
Gen 2775/8000 ▶ train MSE: 0.5878, val MSE: 0.3539
Gen 2776/8000 ▶ train MSE: 0.5878, val MSE: 0.3539
Gen 2777/8000 ▶ train MSE: 0.5878, val MSE: 0.3539
Gen 2778/8000 ▶ train MSE: 0.5878, val MSE: 0.3539
Gen 2779/8000 ▶ train MSE: 0.5878, val MSE: 0.3539
Gen 2780/8000 ▶ train MSE: 0.5878, val MSE: 0.3546
Gen 2781/8000 ▶ train MSE: 0.5878, val MSE: 0.3546
Gen 2782/8000 ▶ train MSE: 0.5878, val MSE: 0.3546
Gen 2783/8000 ▶ train MSE: 0.5878, val MSE: 0.3546
Gen 2784/8000 ▶ train MSE: 0.5878, val MSE: 0.3546
Gen 2785/8000 ▶ train MSE: 0.5878, val MSE: 0.3546
Gen 2786/8000 ▶ train MSE: 0.5878, val MSE: 0.3546
Gen 2787/8000 ▶ train MSE: 0.5878, val MSE: 0.3546
Gen 2788/8000 ▶ train MSE: 0.5878, val MSE: 0.3546
Gen 2789/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2790/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2791/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2792/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2793/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2794/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2795/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2796/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2797/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2798/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2799/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2800/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2801/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2802/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2803/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2804/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2805/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2806/8000 ▶ train MSE: 0.5877, val MSE: 0.3534
Gen 2807/8000 ▶ train MSE: 0.5877, val MSE: 0.3544
Gen 2808/8000 ▶ train MSE: 0.5877, val MSE: 0.3544
Gen 2809/8000 ▶ train MSE: 0.5876, val MSE: 0.3534
Gen 2810/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2811/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2812/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2813/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2814/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2815/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2816/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2817/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2818/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2819/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2820/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2821/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2822/8000 ▶ train MSE: 0.5876, val MSE: 0.3538
Gen 2823/8000 ▶ train MSE: 0.5876, val MSE: 0.3545
Gen 2824/8000 ▶ train MSE: 0.5876, val MSE: 0.3545
Gen 2825/8000 ▶ train MSE: 0.5876, val MSE: 0.3545
Gen 2826/8000 ▶ train MSE: 0.5876, val MSE: 0.3545
Gen 2827/8000 ▶ train MSE: 0.5876, val MSE: 0.3545
Gen 2828/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2829/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2830/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2831/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2832/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2833/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2834/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2835/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2836/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2837/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2838/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2839/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2840/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2841/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2842/8000 ▶ train MSE: 0.5875, val MSE: 0.3537
Gen 2843/8000 ▶ train MSE: 0.5875, val MSE: 0.3530
Gen 2844/8000 ▶ train MSE: 0.5875, val MSE: 0.3530
Gen 2845/8000 ▶ train MSE: 0.5875, val MSE: 0.3530
Gen 2846/8000 ▶ train MSE: 0.5875, val MSE: 0.3530
Gen 2847/8000 ▶ train MSE: 0.5875, val MSE: 0.3535
Gen 2848/8000 ▶ train MSE: 0.5875, val MSE: 0.3535
Gen 2849/8000 ▶ train MSE: 0.5875, val MSE: 0.3535
Gen 2850/8000 ▶ train MSE: 0.5875, val MSE: 0.3535
Gen 2851/8000 ▶ train MSE: 0.5875, val MSE: 0.3535
Gen 2852/8000 ▶ train MSE: 0.5874, val MSE: 0.3534
Gen 2853/8000 ▶ train MSE: 0.5874, val MSE: 0.3534
Gen 2854/8000 ▶ train MSE: 0.5874, val MSE: 0.3534
Gen 2855/8000 ▶ train MSE: 0.5874, val MSE: 0.3534
Gen 2856/8000 ▶ train MSE: 0.5874, val MSE: 0.3534
Gen 2857/8000 ▶ train MSE: 0.5874, val MSE: 0.3534
Gen 2858/8000 ▶ train MSE: 0.5874, val MSE: 0.3534
Gen 2859/8000 ▶ train MSE: 0.5874, val MSE: 0.3534
Gen 2860/8000 ▶ train MSE: 0.5874, val MSE: 0.3531
Gen 2861/8000 ▶ train MSE: 0.5874, val MSE: 0.3531
Gen 2862/8000 ▶ train MSE: 0.5874, val MSE: 0.3531
Gen 2863/8000 ▶ train MSE: 0.5874, val MSE: 0.3531
Gen 2864/8000 ▶ train MSE: 0.5874, val MSE: 0.3531
Gen 2865/8000 ▶ train MSE: 0.5874, val MSE: 0.3531
Gen 2866/8000 ▶ train MSE: 0.5873, val MSE: 0.3533
Gen 2867/8000 ▶ train MSE: 0.5873, val MSE: 0.3533
Gen 2868/8000 ▶ train MSE: 0.5873, val MSE: 0.3533
Gen 2869/8000 ▶ train MSE: 0.5873, val MSE: 0.3533
Gen 2870/8000 ▶ train MSE: 0.5873, val MSE: 0.3533
Gen 2871/8000 ▶ train MSE: 0.5873, val MSE: 0.3533
Gen 2872/8000 ▶ train MSE: 0.5873, val MSE: 0.3533
Gen 2873/8000 ▶ train MSE: 0.5873, val MSE: 0.3533
Gen 2874/8000 ▶ train MSE: 0.5873, val MSE: 0.3533
Gen 2875/8000 ▶ train MSE: 0.5873, val MSE: 0.3533
Gen 2876/8000 ▶ train MSE: 0.5873, val MSE: 0.3533
Gen 2877/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2878/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2879/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2880/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2881/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2882/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2883/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2884/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2885/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2886/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2887/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2888/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2889/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2890/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2891/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2892/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2893/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2894/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2895/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2896/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2897/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2898/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2899/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2900/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2901/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2902/8000 ▶ train MSE: 0.5873, val MSE: 0.3535
Gen 2903/8000 ▶ train MSE: 0.5873, val MSE: 0.3536
Gen 2904/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2905/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2906/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2907/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2908/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2909/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2910/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2911/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2912/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2913/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2914/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2915/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2916/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2917/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2918/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2919/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2920/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2921/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2922/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2923/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2924/8000 ▶ train MSE: 0.5873, val MSE: 0.3530
Gen 2925/8000 ▶ train MSE: 0.5872, val MSE: 0.3534
Gen 2926/8000 ▶ train MSE: 0.5872, val MSE: 0.3534
Gen 2927/8000 ▶ train MSE: 0.5872, val MSE: 0.3534
Gen 2928/8000 ▶ train MSE: 0.5872, val MSE: 0.3534
Gen 2929/8000 ▶ train MSE: 0.5872, val MSE: 0.3534
Gen 2930/8000 ▶ train MSE: 0.5872, val MSE: 0.3543
Gen 2931/8000 ▶ train MSE: 0.5872, val MSE: 0.3545
Gen 2932/8000 ▶ train MSE: 0.5872, val MSE: 0.3545
Gen 2933/8000 ▶ train MSE: 0.5872, val MSE: 0.3528
Gen 2934/8000 ▶ train MSE: 0.5872, val MSE: 0.3528
Gen 2935/8000 ▶ train MSE: 0.5872, val MSE: 0.3528
Gen 2936/8000 ▶ train MSE: 0.5872, val MSE: 0.3535
Gen 2937/8000 ▶ train MSE: 0.5872, val MSE: 0.3535
Gen 2938/8000 ▶ train MSE: 0.5872, val MSE: 0.3535
Gen 2939/8000 ▶ train MSE: 0.5872, val MSE: 0.3535
Gen 2940/8000 ▶ train MSE: 0.5872, val MSE: 0.3535
Gen 2941/8000 ▶ train MSE: 0.5872, val MSE: 0.3535
Gen 2942/8000 ▶ train MSE: 0.5872, val MSE: 0.3535
Gen 2943/8000 ▶ train MSE: 0.5872, val MSE: 0.3535
Gen 2944/8000 ▶ train MSE: 0.5871, val MSE: 0.3535
Gen 2945/8000 ▶ train MSE: 0.5871, val MSE: 0.3535
Gen 2946/8000 ▶ train MSE: 0.5871, val MSE: 0.3535
Gen 2947/8000 ▶ train MSE: 0.5871, val MSE: 0.3532
Gen 2948/8000 ▶ train MSE: 0.5871, val MSE: 0.3532
Gen 2949/8000 ▶ train MSE: 0.5871, val MSE: 0.3532
Gen 2950/8000 ▶ train MSE: 0.5871, val MSE: 0.3532
Gen 2951/8000 ▶ train MSE: 0.5871, val MSE: 0.3532
Gen 2952/8000 ▶ train MSE: 0.5871, val MSE: 0.3532
Gen 2953/8000 ▶ train MSE: 0.5871, val MSE: 0.3527
Gen 2954/8000 ▶ train MSE: 0.5871, val MSE: 0.3527
Gen 2955/8000 ▶ train MSE: 0.5871, val MSE: 0.3527
Gen 2956/8000 ▶ train MSE: 0.5871, val MSE: 0.3527
Gen 2957/8000 ▶ train MSE: 0.5871, val MSE: 0.3527
Gen 2958/8000 ▶ train MSE: 0.5871, val MSE: 0.3527
Gen 2959/8000 ▶ train MSE: 0.5871, val MSE: 0.3527
Gen 2960/8000 ▶ train MSE: 0.5871, val MSE: 0.3527
Gen 2961/8000 ▶ train MSE: 0.5871, val MSE: 0.3527
Gen 2962/8000 ▶ train MSE: 0.5871, val MSE: 0.3539
Gen 2963/8000 ▶ train MSE: 0.5871, val MSE: 0.3539
Gen 2964/8000 ▶ train MSE: 0.5871, val MSE: 0.3539
Gen 2965/8000 ▶ train MSE: 0.5871, val MSE: 0.3539
Gen 2966/8000 ▶ train MSE: 0.5871, val MSE: 0.3539
Gen 2967/8000 ▶ train MSE: 0.5870, val MSE: 0.3536
Gen 2968/8000 ▶ train MSE: 0.5870, val MSE: 0.3536
Gen 2969/8000 ▶ train MSE: 0.5870, val MSE: 0.3536
Gen 2970/8000 ▶ train MSE: 0.5870, val MSE: 0.3532
Gen 2971/8000 ▶ train MSE: 0.5870, val MSE: 0.3532
Gen 2972/8000 ▶ train MSE: 0.5870, val MSE: 0.3532
Gen 2973/8000 ▶ train MSE: 0.5870, val MSE: 0.3532
Gen 2974/8000 ▶ train MSE: 0.5869, val MSE: 0.3534
Gen 2975/8000 ▶ train MSE: 0.5869, val MSE: 0.3534
Gen 2976/8000 ▶ train MSE: 0.5869, val MSE: 0.3534
Gen 2977/8000 ▶ train MSE: 0.5869, val MSE: 0.3534
Gen 2978/8000 ▶ train MSE: 0.5869, val MSE: 0.3534
Gen 2979/8000 ▶ train MSE: 0.5869, val MSE: 0.3534
Gen 2980/8000 ▶ train MSE: 0.5869, val MSE: 0.3534
Gen 2981/8000 ▶ train MSE: 0.5869, val MSE: 0.3534
Gen 2982/8000 ▶ train MSE: 0.5869, val MSE: 0.3534
Gen 2983/8000 ▶ train MSE: 0.5869, val MSE: 0.3531
Gen 2984/8000 ▶ train MSE: 0.5869, val MSE: 0.3531
Gen 2985/8000 ▶ train MSE: 0.5869, val MSE: 0.3531
Gen 2986/8000 ▶ train MSE: 0.5869, val MSE: 0.3531
Gen 2987/8000 ▶ train MSE: 0.5869, val MSE: 0.3531
Gen 2988/8000 ▶ train MSE: 0.5868, val MSE: 0.3527
Gen 2989/8000 ▶ train MSE: 0.5868, val MSE: 0.3527
Gen 2990/8000 ▶ train MSE: 0.5868, val MSE: 0.3527
Gen 2991/8000 ▶ train MSE: 0.5868, val MSE: 0.3527
Gen 2992/8000 ▶ train MSE: 0.5868, val MSE: 0.3527
Gen 2993/8000 ▶ train MSE: 0.5868, val MSE: 0.3527
Gen 2994/8000 ▶ train MSE: 0.5868, val MSE: 0.3527
Gen 2995/8000 ▶ train MSE: 0.5868, val MSE: 0.3527
Gen 2996/8000 ▶ train MSE: 0.5868, val MSE: 0.3527
Gen 2997/8000 ▶ train MSE: 0.5868, val MSE: 0.3521
Gen 2998/8000 ▶ train MSE: 0.5868, val MSE: 0.3521
Gen 2999/8000 ▶ train MSE: 0.5868, val MSE: 0.3521
Gen 3000/8000 ▶ train MSE: 0.5868, val MSE: 0.3521
Gen 3001/8000 ▶ train MSE: 0.5868, val MSE: 0.3521
Gen 3002/8000 ▶ train MSE: 0.5868, val MSE: 0.3521
Gen 3003/8000 ▶ train MSE: 0.5868, val MSE: 0.3521
Gen 3004/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3005/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3006/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3007/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3008/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3009/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3010/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3011/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3012/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3013/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3014/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3015/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3016/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3017/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3018/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3019/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3020/8000 ▶ train MSE: 0.5867, val MSE: 0.3521
Gen 3021/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3022/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3023/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3024/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3025/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3026/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3027/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3028/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3029/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3030/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3031/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3032/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3033/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3034/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3035/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3036/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3037/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3038/8000 ▶ train MSE: 0.5867, val MSE: 0.3524
Gen 3039/8000 ▶ train MSE: 0.5867, val MSE: 0.3522
Gen 3040/8000 ▶ train MSE: 0.5867, val MSE: 0.3522
Gen 3041/8000 ▶ train MSE: 0.5867, val MSE: 0.3527
Gen 3042/8000 ▶ train MSE: 0.5867, val MSE: 0.3527
Gen 3043/8000 ▶ train MSE: 0.5867, val MSE: 0.3527
Gen 3044/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3045/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3046/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3047/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3048/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3049/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3050/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3051/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3052/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3053/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3054/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3055/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3056/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3057/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3058/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3059/8000 ▶ train MSE: 0.5867, val MSE: 0.3520
Gen 3060/8000 ▶ train MSE: 0.5866, val MSE: 0.3518
Gen 3061/8000 ▶ train MSE: 0.5866, val MSE: 0.3518
Gen 3062/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3063/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3064/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3065/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3066/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3067/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3068/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3069/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3070/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3071/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3072/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3073/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3074/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3075/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3076/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3077/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3078/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3079/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3080/8000 ▶ train MSE: 0.5865, val MSE: 0.3518
Gen 3081/8000 ▶ train MSE: 0.5865, val MSE: 0.3519
Gen 3082/8000 ▶ train MSE: 0.5865, val MSE: 0.3519
Gen 3083/8000 ▶ train MSE: 0.5865, val MSE: 0.3519
Gen 3084/8000 ▶ train MSE: 0.5865, val MSE: 0.3519
Gen 3085/8000 ▶ train MSE: 0.5865, val MSE: 0.3519
Gen 3086/8000 ▶ train MSE: 0.5865, val MSE: 0.3519
Gen 3087/8000 ▶ train MSE: 0.5865, val MSE: 0.3519
Gen 3088/8000 ▶ train MSE: 0.5865, val MSE: 0.3519
Gen 3089/8000 ▶ train MSE: 0.5865, val MSE: 0.3519
Gen 3090/8000 ▶ train MSE: 0.5865, val MSE: 0.3519
Gen 3091/8000 ▶ train MSE: 0.5864, val MSE: 0.3522
Gen 3092/8000 ▶ train MSE: 0.5864, val MSE: 0.3522
Gen 3093/8000 ▶ train MSE: 0.5864, val MSE: 0.3522
Gen 3094/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3095/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3096/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3097/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3098/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3099/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3100/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3101/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3102/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3103/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3104/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3105/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3106/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3107/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3108/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3109/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3110/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3111/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3112/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3113/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3114/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3115/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3116/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3117/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3118/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3119/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3120/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3121/8000 ▶ train MSE: 0.5864, val MSE: 0.3525
Gen 3122/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3123/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3124/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3125/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3126/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3127/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3128/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3129/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3130/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3131/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3132/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3133/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3134/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3135/8000 ▶ train MSE: 0.5864, val MSE: 0.3531
Gen 3136/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3137/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3138/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3139/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3140/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3141/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3142/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3143/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3144/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3145/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3146/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3147/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3148/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3149/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3150/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3151/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3152/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3153/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3154/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3155/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3156/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3157/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3158/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3159/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3160/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3161/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3162/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3163/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3164/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3165/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3166/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3167/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3168/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3169/8000 ▶ train MSE: 0.5863, val MSE: 0.3530
Gen 3170/8000 ▶ train MSE: 0.5862, val MSE: 0.3524
Gen 3171/8000 ▶ train MSE: 0.5862, val MSE: 0.3524
Gen 3172/8000 ▶ train MSE: 0.5862, val MSE: 0.3524
Gen 3173/8000 ▶ train MSE: 0.5862, val MSE: 0.3524
Gen 3174/8000 ▶ train MSE: 0.5862, val MSE: 0.3524
Gen 3175/8000 ▶ train MSE: 0.5862, val MSE: 0.3524
Gen 3176/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3177/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3178/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3179/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3180/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3181/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3182/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3183/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3184/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3185/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3186/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3187/8000 ▶ train MSE: 0.5862, val MSE: 0.3530
Gen 3188/8000 ▶ train MSE: 0.5862, val MSE: 0.3530
Gen 3189/8000 ▶ train MSE: 0.5862, val MSE: 0.3530
Gen 3190/8000 ▶ train MSE: 0.5862, val MSE: 0.3530
Gen 3191/8000 ▶ train MSE: 0.5862, val MSE: 0.3530
Gen 3192/8000 ▶ train MSE: 0.5862, val MSE: 0.3520
Gen 3193/8000 ▶ train MSE: 0.5862, val MSE: 0.3520
Gen 3194/8000 ▶ train MSE: 0.5862, val MSE: 0.3520
Gen 3195/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3196/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3197/8000 ▶ train MSE: 0.5862, val MSE: 0.3528
Gen 3198/8000 ▶ train MSE: 0.5862, val MSE: 0.3521
Gen 3199/8000 ▶ train MSE: 0.5862, val MSE: 0.3521
Gen 3200/8000 ▶ train MSE: 0.5862, val MSE: 0.3521
Gen 3201/8000 ▶ train MSE: 0.5862, val MSE: 0.3521
Gen 3202/8000 ▶ train MSE: 0.5862, val MSE: 0.3521
Gen 3203/8000 ▶ train MSE: 0.5862, val MSE: 0.3521
Gen 3204/8000 ▶ train MSE: 0.5862, val MSE: 0.3521
Gen 3205/8000 ▶ train MSE: 0.5862, val MSE: 0.3521
Gen 3206/8000 ▶ train MSE: 0.5862, val MSE: 0.3521
Gen 3207/8000 ▶ train MSE: 0.5862, val MSE: 0.3521
Gen 3208/8000 ▶ train MSE: 0.5862, val MSE: 0.3521
Gen 3209/8000 ▶ train MSE: 0.5861, val MSE: 0.3520
Gen 3210/8000 ▶ train MSE: 0.5861, val MSE: 0.3520
Gen 3211/8000 ▶ train MSE: 0.5861, val MSE: 0.3520
Gen 3212/8000 ▶ train MSE: 0.5861, val MSE: 0.3520
Gen 3213/8000 ▶ train MSE: 0.5861, val MSE: 0.3520
Gen 3214/8000 ▶ train MSE: 0.5861, val MSE: 0.3520
Gen 3215/8000 ▶ train MSE: 0.5861, val MSE: 0.3520
Gen 3216/8000 ▶ train MSE: 0.5861, val MSE: 0.3519
Gen 3217/8000 ▶ train MSE: 0.5861, val MSE: 0.3519
Gen 3218/8000 ▶ train MSE: 0.5861, val MSE: 0.3517
Gen 3219/8000 ▶ train MSE: 0.5861, val MSE: 0.3517
Gen 3220/8000 ▶ train MSE: 0.5861, val MSE: 0.3517
Gen 3221/8000 ▶ train MSE: 0.5861, val MSE: 0.3517
Gen 3222/8000 ▶ train MSE: 0.5861, val MSE: 0.3517
Gen 3223/8000 ▶ train MSE: 0.5861, val MSE: 0.3517
Gen 3224/8000 ▶ train MSE: 0.5861, val MSE: 0.3517
Gen 3225/8000 ▶ train MSE: 0.5861, val MSE: 0.3517
Gen 3226/8000 ▶ train MSE: 0.5860, val MSE: 0.3514
Gen 3227/8000 ▶ train MSE: 0.5860, val MSE: 0.3514
Gen 3228/8000 ▶ train MSE: 0.5860, val MSE: 0.3517
Gen 3229/8000 ▶ train MSE: 0.5860, val MSE: 0.3517
Gen 3230/8000 ▶ train MSE: 0.5860, val MSE: 0.3517
Gen 3231/8000 ▶ train MSE: 0.5860, val MSE: 0.3517
Gen 3232/8000 ▶ train MSE: 0.5860, val MSE: 0.3517
Gen 3233/8000 ▶ train MSE: 0.5860, val MSE: 0.3517
Gen 3234/8000 ▶ train MSE: 0.5860, val MSE: 0.3517
Gen 3235/8000 ▶ train MSE: 0.5860, val MSE: 0.3517
Gen 3236/8000 ▶ train MSE: 0.5860, val MSE: 0.3517
Gen 3237/8000 ▶ train MSE: 0.5860, val MSE: 0.3517
Gen 3238/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3239/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3240/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3241/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3242/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3243/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3244/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3245/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3246/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3247/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3248/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3249/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3250/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3251/8000 ▶ train MSE: 0.5860, val MSE: 0.3502
Gen 3252/8000 ▶ train MSE: 0.5859, val MSE: 0.3506
Gen 3253/8000 ▶ train MSE: 0.5859, val MSE: 0.3506
Gen 3254/8000 ▶ train MSE: 0.5859, val MSE: 0.3506
Gen 3255/8000 ▶ train MSE: 0.5859, val MSE: 0.3506
Gen 3256/8000 ▶ train MSE: 0.5859, val MSE: 0.3506
Gen 3257/8000 ▶ train MSE: 0.5859, val MSE: 0.3506
Gen 3258/8000 ▶ train MSE: 0.5859, val MSE: 0.3506
Gen 3259/8000 ▶ train MSE: 0.5859, val MSE: 0.3506
Gen 3260/8000 ▶ train MSE: 0.5859, val MSE: 0.3506
Gen 3261/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3262/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3263/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3264/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3265/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3266/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3267/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3268/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3269/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3270/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3271/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3272/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3273/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3274/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3275/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3276/8000 ▶ train MSE: 0.5859, val MSE: 0.3512
Gen 3277/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3278/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3279/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3280/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3281/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3282/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3283/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3284/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3285/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3286/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3287/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3288/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3289/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3290/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3291/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3292/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3293/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3294/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3295/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3296/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3297/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3298/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3299/8000 ▶ train MSE: 0.5858, val MSE: 0.3509
Gen 3300/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3301/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3302/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3303/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3304/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3305/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3306/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3307/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3308/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3309/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3310/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3311/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3312/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3313/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3314/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3315/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3316/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3317/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3318/8000 ▶ train MSE: 0.5856, val MSE: 0.3501
Gen 3319/8000 ▶ train MSE: 0.5856, val MSE: 0.3500
Gen 3320/8000 ▶ train MSE: 0.5856, val MSE: 0.3500
Gen 3321/8000 ▶ train MSE: 0.5856, val MSE: 0.3500
Gen 3322/8000 ▶ train MSE: 0.5856, val MSE: 0.3500
Gen 3323/8000 ▶ train MSE: 0.5856, val MSE: 0.3500
Gen 3324/8000 ▶ train MSE: 0.5856, val MSE: 0.3500
Gen 3325/8000 ▶ train MSE: 0.5856, val MSE: 0.3500
Gen 3326/8000 ▶ train MSE: 0.5856, val MSE: 0.3500
Gen 3327/8000 ▶ train MSE: 0.5856, val MSE: 0.3500
Gen 3328/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3329/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3330/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3331/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3332/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3333/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3334/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3335/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3336/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3337/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3338/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3339/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3340/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3341/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3342/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3343/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3344/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3345/8000 ▶ train MSE: 0.5855, val MSE: 0.3497
Gen 3346/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3347/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3348/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3349/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3350/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3351/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3352/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3353/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3354/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3355/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3356/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3357/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3358/8000 ▶ train MSE: 0.5855, val MSE: 0.3499
Gen 3359/8000 ▶ train MSE: 0.5855, val MSE: 0.3493
Gen 3360/8000 ▶ train MSE: 0.5854, val MSE: 0.3493
Gen 3361/8000 ▶ train MSE: 0.5854, val MSE: 0.3493
Gen 3362/8000 ▶ train MSE: 0.5854, val MSE: 0.3493
Gen 3363/8000 ▶ train MSE: 0.5854, val MSE: 0.3493
Gen 3364/8000 ▶ train MSE: 0.5854, val MSE: 0.3493
Gen 3365/8000 ▶ train MSE: 0.5854, val MSE: 0.3493
Gen 3366/8000 ▶ train MSE: 0.5854, val MSE: 0.3493
Gen 3367/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3368/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3369/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3370/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3371/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3372/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3373/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3374/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3375/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3376/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3377/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3378/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3379/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3380/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3381/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3382/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3383/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3384/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3385/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3386/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3387/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3388/8000 ▶ train MSE: 0.5854, val MSE: 0.3490
Gen 3389/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3390/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3391/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3392/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3393/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3394/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3395/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3396/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3397/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3398/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3399/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3400/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3401/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3402/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3403/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3404/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3405/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3406/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3407/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3408/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3409/8000 ▶ train MSE: 0.5853, val MSE: 0.3495
Gen 3410/8000 ▶ train MSE: 0.5853, val MSE: 0.3494
Gen 3411/8000 ▶ train MSE: 0.5853, val MSE: 0.3494
Gen 3412/8000 ▶ train MSE: 0.5853, val MSE: 0.3494
Gen 3413/8000 ▶ train MSE: 0.5853, val MSE: 0.3494
Gen 3414/8000 ▶ train MSE: 0.5853, val MSE: 0.3494
Gen 3415/8000 ▶ train MSE: 0.5853, val MSE: 0.3494
Gen 3416/8000 ▶ train MSE: 0.5853, val MSE: 0.3494
Gen 3417/8000 ▶ train MSE: 0.5853, val MSE: 0.3494
Gen 3418/8000 ▶ train MSE: 0.5853, val MSE: 0.3494
Gen 3419/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3420/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3421/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3422/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3423/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3424/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3425/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3426/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3427/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3428/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3429/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3430/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3431/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3432/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3433/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3434/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3435/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3436/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3437/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3438/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3439/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3440/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3441/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3442/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3443/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3444/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3445/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3446/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3447/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3448/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3449/8000 ▶ train MSE: 0.5852, val MSE: 0.3484
Gen 3450/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3451/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3452/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3453/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3454/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3455/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3456/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3457/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3458/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3459/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3460/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3461/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3462/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3463/8000 ▶ train MSE: 0.5852, val MSE: 0.3482
Gen 3464/8000 ▶ train MSE: 0.5851, val MSE: 0.3486
Gen 3465/8000 ▶ train MSE: 0.5851, val MSE: 0.3486
Gen 3466/8000 ▶ train MSE: 0.5851, val MSE: 0.3486
Gen 3467/8000 ▶ train MSE: 0.5851, val MSE: 0.3486
Gen 3468/8000 ▶ train MSE: 0.5851, val MSE: 0.3485
Gen 3469/8000 ▶ train MSE: 0.5851, val MSE: 0.3489
Gen 3470/8000 ▶ train MSE: 0.5851, val MSE: 0.3478
Gen 3471/8000 ▶ train MSE: 0.5851, val MSE: 0.3478
Gen 3472/8000 ▶ train MSE: 0.5851, val MSE: 0.3478
Gen 3473/8000 ▶ train MSE: 0.5851, val MSE: 0.3485
Gen 3474/8000 ▶ train MSE: 0.5850, val MSE: 0.3484
Gen 3475/8000 ▶ train MSE: 0.5850, val MSE: 0.3484
Gen 3476/8000 ▶ train MSE: 0.5850, val MSE: 0.3484
Gen 3477/8000 ▶ train MSE: 0.5850, val MSE: 0.3484
Gen 3478/8000 ▶ train MSE: 0.5850, val MSE: 0.3484
Gen 3479/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3480/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3481/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3482/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3483/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3484/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3485/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3486/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3487/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3488/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3489/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3490/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3491/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3492/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3493/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3494/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3495/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3496/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3497/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3498/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3499/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3500/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3501/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3502/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3503/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3504/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3505/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3506/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3507/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3508/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3509/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3510/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3511/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3512/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3513/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3514/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3515/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3516/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3517/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3518/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3519/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3520/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3521/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3522/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3523/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3524/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3525/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3526/8000 ▶ train MSE: 0.5850, val MSE: 0.3485
Gen 3527/8000 ▶ train MSE: 0.5850, val MSE: 0.3481
Gen 3528/8000 ▶ train MSE: 0.5850, val MSE: 0.3481
Gen 3529/8000 ▶ train MSE: 0.5850, val MSE: 0.3481
Gen 3530/8000 ▶ train MSE: 0.5850, val MSE: 0.3481
Gen 3531/8000 ▶ train MSE: 0.5850, val MSE: 0.3481
Gen 3532/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3533/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3534/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3535/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3536/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3537/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3538/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3539/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3540/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3541/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3542/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3543/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3544/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3545/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3546/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3547/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3548/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3549/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3550/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3551/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3552/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3553/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3554/8000 ▶ train MSE: 0.5849, val MSE: 0.3487
Gen 3555/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3556/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3557/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3558/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3559/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3560/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3561/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3562/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3563/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3564/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3565/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3566/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3567/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3568/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3569/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3570/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3571/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3572/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3573/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3574/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3575/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3576/8000 ▶ train MSE: 0.5849, val MSE: 0.3489
Gen 3577/8000 ▶ train MSE: 0.5849, val MSE: 0.3489
Gen 3578/8000 ▶ train MSE: 0.5849, val MSE: 0.3489
Gen 3579/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3580/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3581/8000 ▶ train MSE: 0.5849, val MSE: 0.3478
Gen 3582/8000 ▶ train MSE: 0.5849, val MSE: 0.3483
Gen 3583/8000 ▶ train MSE: 0.5849, val MSE: 0.3483
Gen 3584/8000 ▶ train MSE: 0.5849, val MSE: 0.3483
Gen 3585/8000 ▶ train MSE: 0.5849, val MSE: 0.3483
Gen 3586/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3587/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3588/8000 ▶ train MSE: 0.5849, val MSE: 0.3484
Gen 3589/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3590/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3591/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3592/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3593/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3594/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3595/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3596/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3597/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3598/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3599/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3600/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3601/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3602/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3603/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3604/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3605/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3606/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3607/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3608/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3609/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3610/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3611/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3612/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3613/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3614/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3615/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3616/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3617/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3618/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3619/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3620/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3621/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3622/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3623/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3624/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3625/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3626/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3627/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3628/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3629/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3630/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3631/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3632/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3633/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3634/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3635/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3636/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3637/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3638/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3639/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3640/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3641/8000 ▶ train MSE: 0.5848, val MSE: 0.3488
Gen 3642/8000 ▶ train MSE: 0.5848, val MSE: 0.3479
Gen 3643/8000 ▶ train MSE: 0.5848, val MSE: 0.3479
Gen 3644/8000 ▶ train MSE: 0.5848, val MSE: 0.3480
Gen 3645/8000 ▶ train MSE: 0.5848, val MSE: 0.3480
Gen 3646/8000 ▶ train MSE: 0.5848, val MSE: 0.3480
Gen 3647/8000 ▶ train MSE: 0.5848, val MSE: 0.3480
Gen 3648/8000 ▶ train MSE: 0.5848, val MSE: 0.3480
Gen 3649/8000 ▶ train MSE: 0.5848, val MSE: 0.3480
Gen 3650/8000 ▶ train MSE: 0.5848, val MSE: 0.3480
Gen 3651/8000 ▶ train MSE: 0.5848, val MSE: 0.3480
Gen 3652/8000 ▶ train MSE: 0.5848, val MSE: 0.3479
Gen 3653/8000 ▶ train MSE: 0.5848, val MSE: 0.3479
Gen 3654/8000 ▶ train MSE: 0.5847, val MSE: 0.3478
Gen 3655/8000 ▶ train MSE: 0.5847, val MSE: 0.3478
Gen 3656/8000 ▶ train MSE: 0.5847, val MSE: 0.3480
Gen 3657/8000 ▶ train MSE: 0.5847, val MSE: 0.3480
Gen 3658/8000 ▶ train MSE: 0.5847, val MSE: 0.3480
Gen 3659/8000 ▶ train MSE: 0.5847, val MSE: 0.3483
Gen 3660/8000 ▶ train MSE: 0.5847, val MSE: 0.3483
Gen 3661/8000 ▶ train MSE: 0.5847, val MSE: 0.3483
Gen 3662/8000 ▶ train MSE: 0.5847, val MSE: 0.3483
Gen 3663/8000 ▶ train MSE: 0.5847, val MSE: 0.3483
Gen 3664/8000 ▶ train MSE: 0.5847, val MSE: 0.3483
Gen 3665/8000 ▶ train MSE: 0.5847, val MSE: 0.3483
Gen 3666/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3667/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3668/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3669/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3670/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3671/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3672/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3673/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3674/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3675/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3676/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3677/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3678/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3679/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3680/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3681/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3682/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3683/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3684/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3685/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3686/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3687/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3688/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3689/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3690/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3691/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3692/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3693/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3694/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3695/8000 ▶ train MSE: 0.5847, val MSE: 0.3481
Gen 3696/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3697/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3698/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3699/8000 ▶ train MSE: 0.5847, val MSE: 0.3482
Gen 3700/8000 ▶ train MSE: 0.5846, val MSE: 0.3481
Gen 3701/8000 ▶ train MSE: 0.5846, val MSE: 0.3481
Gen 3702/8000 ▶ train MSE: 0.5846, val MSE: 0.3481
Gen 3703/8000 ▶ train MSE: 0.5846, val MSE: 0.3481
Gen 3704/8000 ▶ train MSE: 0.5846, val MSE: 0.3481
Gen 3705/8000 ▶ train MSE: 0.5846, val MSE: 0.3476
Gen 3706/8000 ▶ train MSE: 0.5846, val MSE: 0.3476
Gen 3707/8000 ▶ train MSE: 0.5846, val MSE: 0.3476
Gen 3708/8000 ▶ train MSE: 0.5846, val MSE: 0.3476
Gen 3709/8000 ▶ train MSE: 0.5846, val MSE: 0.3476
Gen 3710/8000 ▶ train MSE: 0.5846, val MSE: 0.3476
Gen 3711/8000 ▶ train MSE: 0.5846, val MSE: 0.3476
Gen 3712/8000 ▶ train MSE: 0.5846, val MSE: 0.3476
Gen 3713/8000 ▶ train MSE: 0.5846, val MSE: 0.3476
Gen 3714/8000 ▶ train MSE: 0.5846, val MSE: 0.3476
Gen 3715/8000 ▶ train MSE: 0.5846, val MSE: 0.3476
Gen 3716/8000 ▶ train MSE: 0.5846, val MSE: 0.3475
Gen 3717/8000 ▶ train MSE: 0.5846, val MSE: 0.3475
Gen 3718/8000 ▶ train MSE: 0.5846, val MSE: 0.3475
Gen 3719/8000 ▶ train MSE: 0.5846, val MSE: 0.3475
Gen 3720/8000 ▶ train MSE: 0.5846, val MSE: 0.3475
Gen 3721/8000 ▶ train MSE: 0.5846, val MSE: 0.3475
Gen 3722/8000 ▶ train MSE: 0.5846, val MSE: 0.3475
Gen 3723/8000 ▶ train MSE: 0.5846, val MSE: 0.3475
Gen 3724/8000 ▶ train MSE: 0.5846, val MSE: 0.3473
Gen 3725/8000 ▶ train MSE: 0.5846, val MSE: 0.3477
Gen 3726/8000 ▶ train MSE: 0.5846, val MSE: 0.3483
Gen 3727/8000 ▶ train MSE: 0.5846, val MSE: 0.3483
Gen 3728/8000 ▶ train MSE: 0.5846, val MSE: 0.3483
Gen 3729/8000 ▶ train MSE: 0.5846, val MSE: 0.3483
Gen 3730/8000 ▶ train MSE: 0.5846, val MSE: 0.3483
Gen 3731/8000 ▶ train MSE: 0.5846, val MSE: 0.3483
Gen 3732/8000 ▶ train MSE: 0.5846, val MSE: 0.3483
Gen 3733/8000 ▶ train MSE: 0.5846, val MSE: 0.3478
Gen 3734/8000 ▶ train MSE: 0.5846, val MSE: 0.3478
Gen 3735/8000 ▶ train MSE: 0.5846, val MSE: 0.3478
Gen 3736/8000 ▶ train MSE: 0.5846, val MSE: 0.3475
Gen 3737/8000 ▶ train MSE: 0.5846, val MSE: 0.3482
Gen 3738/8000 ▶ train MSE: 0.5846, val MSE: 0.3474
Gen 3739/8000 ▶ train MSE: 0.5846, val MSE: 0.3474
Gen 3740/8000 ▶ train MSE: 0.5845, val MSE: 0.3478
Gen 3741/8000 ▶ train MSE: 0.5845, val MSE: 0.3478
Gen 3742/8000 ▶ train MSE: 0.5845, val MSE: 0.3478
Gen 3743/8000 ▶ train MSE: 0.5845, val MSE: 0.3478
Gen 3744/8000 ▶ train MSE: 0.5845, val MSE: 0.3479
Gen 3745/8000 ▶ train MSE: 0.5845, val MSE: 0.3474
Gen 3746/8000 ▶ train MSE: 0.5845, val MSE: 0.3474
Gen 3747/8000 ▶ train MSE: 0.5845, val MSE: 0.3474
Gen 3748/8000 ▶ train MSE: 0.5845, val MSE: 0.3473
Gen 3749/8000 ▶ train MSE: 0.5845, val MSE: 0.3473
Gen 3750/8000 ▶ train MSE: 0.5845, val MSE: 0.3473
Gen 3751/8000 ▶ train MSE: 0.5845, val MSE: 0.3473
Gen 3752/8000 ▶ train MSE: 0.5844, val MSE: 0.3470
Gen 3753/8000 ▶ train MSE: 0.5844, val MSE: 0.3470
Gen 3754/8000 ▶ train MSE: 0.5844, val MSE: 0.3470
Gen 3755/8000 ▶ train MSE: 0.5844, val MSE: 0.3470
Gen 3756/8000 ▶ train MSE: 0.5844, val MSE: 0.3470
Gen 3757/8000 ▶ train MSE: 0.5844, val MSE: 0.3470
Gen 3758/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3759/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3760/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3761/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3762/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3763/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3764/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3765/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3766/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3767/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3768/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3769/8000 ▶ train MSE: 0.5844, val MSE: 0.3472
Gen 3770/8000 ▶ train MSE: 0.5844, val MSE: 0.3475
Gen 3771/8000 ▶ train MSE: 0.5844, val MSE: 0.3475
Gen 3772/8000 ▶ train MSE: 0.5844, val MSE: 0.3467
Gen 3773/8000 ▶ train MSE: 0.5844, val MSE: 0.3467
Gen 3774/8000 ▶ train MSE: 0.5844, val MSE: 0.3467
Gen 3775/8000 ▶ train MSE: 0.5844, val MSE: 0.3467
Gen 3776/8000 ▶ train MSE: 0.5844, val MSE: 0.3467
Gen 3777/8000 ▶ train MSE: 0.5844, val MSE: 0.3467
Gen 3778/8000 ▶ train MSE: 0.5843, val MSE: 0.3475
Gen 3779/8000 ▶ train MSE: 0.5843, val MSE: 0.3475
Gen 3780/8000 ▶ train MSE: 0.5843, val MSE: 0.3475
Gen 3781/8000 ▶ train MSE: 0.5843, val MSE: 0.3475
Gen 3782/8000 ▶ train MSE: 0.5843, val MSE: 0.3475
Gen 3783/8000 ▶ train MSE: 0.5843, val MSE: 0.3475
Gen 3784/8000 ▶ train MSE: 0.5843, val MSE: 0.3477
Gen 3785/8000 ▶ train MSE: 0.5843, val MSE: 0.3477
Gen 3786/8000 ▶ train MSE: 0.5843, val MSE: 0.3477
Gen 3787/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3788/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3789/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3790/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3791/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3792/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3793/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3794/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3795/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3796/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3797/8000 ▶ train MSE: 0.5843, val MSE: 0.3478
Gen 3798/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3799/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3800/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3801/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3802/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3803/8000 ▶ train MSE: 0.5843, val MSE: 0.3474
Gen 3804/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3805/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3806/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3807/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3808/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3809/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3810/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3811/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3812/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3813/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3814/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3815/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3816/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3817/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3818/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3819/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3820/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3821/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3822/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3823/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3824/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3825/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3826/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3827/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3828/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3829/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3830/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3831/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3832/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3833/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3834/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3835/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3836/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3837/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3838/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3839/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3840/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3841/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3842/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3843/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3844/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3845/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3846/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3847/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3848/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3849/8000 ▶ train MSE: 0.5842, val MSE: 0.3481
Gen 3850/8000 ▶ train MSE: 0.5841, val MSE: 0.3476
Gen 3851/8000 ▶ train MSE: 0.5841, val MSE: 0.3476
Gen 3852/8000 ▶ train MSE: 0.5841, val MSE: 0.3476
Gen 3853/8000 ▶ train MSE: 0.5841, val MSE: 0.3476
Gen 3854/8000 ▶ train MSE: 0.5841, val MSE: 0.3476
Gen 3855/8000 ▶ train MSE: 0.5841, val MSE: 0.3476
Gen 3856/8000 ▶ train MSE: 0.5841, val MSE: 0.3476
Gen 3857/8000 ▶ train MSE: 0.5841, val MSE: 0.3476
Gen 3858/8000 ▶ train MSE: 0.5841, val MSE: 0.3476
Gen 3859/8000 ▶ train MSE: 0.5841, val MSE: 0.3476
Gen 3860/8000 ▶ train MSE: 0.5841, val MSE: 0.3477
Gen 3861/8000 ▶ train MSE: 0.5840, val MSE: 0.3472
Gen 3862/8000 ▶ train MSE: 0.5840, val MSE: 0.3472
Gen 3863/8000 ▶ train MSE: 0.5840, val MSE: 0.3472
Gen 3864/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3865/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3866/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3867/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3868/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3869/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3870/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3871/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3872/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3873/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3874/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3875/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3876/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3877/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3878/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3879/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3880/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3881/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3882/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3883/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3884/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3885/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3886/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3887/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3888/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3889/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3890/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3891/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3892/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3893/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3894/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3895/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3896/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3897/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3898/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3899/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3900/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3901/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3902/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3903/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3904/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3905/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3906/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3907/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3908/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3909/8000 ▶ train MSE: 0.5840, val MSE: 0.3476
Gen 3910/8000 ▶ train MSE: 0.5840, val MSE: 0.3477
Gen 3911/8000 ▶ train MSE: 0.5840, val MSE: 0.3477
Gen 3912/8000 ▶ train MSE: 0.5840, val MSE: 0.3477
Gen 3913/8000 ▶ train MSE: 0.5840, val MSE: 0.3477
Gen 3914/8000 ▶ train MSE: 0.5839, val MSE: 0.3474
Gen 3915/8000 ▶ train MSE: 0.5839, val MSE: 0.3474
Gen 3916/8000 ▶ train MSE: 0.5839, val MSE: 0.3478
Gen 3917/8000 ▶ train MSE: 0.5839, val MSE: 0.3478
Gen 3918/8000 ▶ train MSE: 0.5839, val MSE: 0.3473
Gen 3919/8000 ▶ train MSE: 0.5839, val MSE: 0.3473
Gen 3920/8000 ▶ train MSE: 0.5839, val MSE: 0.3473
Gen 3921/8000 ▶ train MSE: 0.5839, val MSE: 0.3473
Gen 3922/8000 ▶ train MSE: 0.5839, val MSE: 0.3473
Gen 3923/8000 ▶ train MSE: 0.5839, val MSE: 0.3473
Gen 3924/8000 ▶ train MSE: 0.5839, val MSE: 0.3473
Gen 3925/8000 ▶ train MSE: 0.5839, val MSE: 0.3473
Gen 3926/8000 ▶ train MSE: 0.5839, val MSE: 0.3473
Gen 3927/8000 ▶ train MSE: 0.5838, val MSE: 0.3468
Gen 3928/8000 ▶ train MSE: 0.5838, val MSE: 0.3468
Gen 3929/8000 ▶ train MSE: 0.5838, val MSE: 0.3468
Gen 3930/8000 ▶ train MSE: 0.5838, val MSE: 0.3468
Gen 3931/8000 ▶ train MSE: 0.5838, val MSE: 0.3468
Gen 3932/8000 ▶ train MSE: 0.5838, val MSE: 0.3471
Gen 3933/8000 ▶ train MSE: 0.5838, val MSE: 0.3471
Gen 3934/8000 ▶ train MSE: 0.5838, val MSE: 0.3467
Gen 3935/8000 ▶ train MSE: 0.5838, val MSE: 0.3467
Gen 3936/8000 ▶ train MSE: 0.5838, val MSE: 0.3467
Gen 3937/8000 ▶ train MSE: 0.5838, val MSE: 0.3467
Gen 3938/8000 ▶ train MSE: 0.5838, val MSE: 0.3467
Gen 3939/8000 ▶ train MSE: 0.5838, val MSE: 0.3467
Gen 3940/8000 ▶ train MSE: 0.5838, val MSE: 0.3467
Gen 3941/8000 ▶ train MSE: 0.5838, val MSE: 0.3469
Gen 3942/8000 ▶ train MSE: 0.5838, val MSE: 0.3469
Gen 3943/8000 ▶ train MSE: 0.5838, val MSE: 0.3469
Gen 3944/8000 ▶ train MSE: 0.5838, val MSE: 0.3469
Gen 3945/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3946/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3947/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3948/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3949/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3950/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3951/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3952/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3953/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3954/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3955/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3956/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3957/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3958/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3959/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3960/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3961/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3962/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3963/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3964/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3965/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3966/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3967/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3968/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3969/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3970/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3971/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3972/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3973/8000 ▶ train MSE: 0.5837, val MSE: 0.3473
Gen 3974/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3975/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3976/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3977/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3978/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3979/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3980/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3981/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3982/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3983/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3984/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3985/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3986/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3987/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3988/8000 ▶ train MSE: 0.5837, val MSE: 0.3466
Gen 3989/8000 ▶ train MSE: 0.5837, val MSE: 0.3464
Gen 3990/8000 ▶ train MSE: 0.5837, val MSE: 0.3464
Gen 3991/8000 ▶ train MSE: 0.5837, val MSE: 0.3464
Gen 3992/8000 ▶ train MSE: 0.5837, val MSE: 0.3464
Gen 3993/8000 ▶ train MSE: 0.5837, val MSE: 0.3464
Gen 3994/8000 ▶ train MSE: 0.5837, val MSE: 0.3463
Gen 3995/8000 ▶ train MSE: 0.5837, val MSE: 0.3463
Gen 3996/8000 ▶ train MSE: 0.5837, val MSE: 0.3463
Gen 3997/8000 ▶ train MSE: 0.5837, val MSE: 0.3463
Gen 3998/8000 ▶ train MSE: 0.5837, val MSE: 0.3467
Gen 3999/8000 ▶ train MSE: 0.5837, val MSE: 0.3467
Gen 4000/8000 ▶ train MSE: 0.5837, val MSE: 0.3467
Gen 4001/8000 ▶ train MSE: 0.5837, val MSE: 0.3468
Gen 4002/8000 ▶ train MSE: 0.5837, val MSE: 0.3468
Gen 4003/8000 ▶ train MSE: 0.5837, val MSE: 0.3468
Gen 4004/8000 ▶ train MSE: 0.5837, val MSE: 0.3468
Gen 4005/8000 ▶ train MSE: 0.5837, val MSE: 0.3468
Gen 4006/8000 ▶ train MSE: 0.5837, val MSE: 0.3468
Gen 4007/8000 ▶ train MSE: 0.5836, val MSE: 0.3462
Gen 4008/8000 ▶ train MSE: 0.5836, val MSE: 0.3462
Gen 4009/8000 ▶ train MSE: 0.5836, val MSE: 0.3462
Gen 4010/8000 ▶ train MSE: 0.5836, val MSE: 0.3459
Gen 4011/8000 ▶ train MSE: 0.5836, val MSE: 0.3459
Gen 4012/8000 ▶ train MSE: 0.5836, val MSE: 0.3459
Gen 4013/8000 ▶ train MSE: 0.5836, val MSE: 0.3459
Gen 4014/8000 ▶ train MSE: 0.5836, val MSE: 0.3459
Gen 4015/8000 ▶ train MSE: 0.5836, val MSE: 0.3457
Gen 4016/8000 ▶ train MSE: 0.5836, val MSE: 0.3457
Gen 4017/8000 ▶ train MSE: 0.5836, val MSE: 0.3457
Gen 4018/8000 ▶ train MSE: 0.5836, val MSE: 0.3458
Gen 4019/8000 ▶ train MSE: 0.5836, val MSE: 0.3458
Gen 4020/8000 ▶ train MSE: 0.5836, val MSE: 0.3458
Gen 4021/8000 ▶ train MSE: 0.5836, val MSE: 0.3458
Gen 4022/8000 ▶ train MSE: 0.5836, val MSE: 0.3458
Gen 4023/8000 ▶ train MSE: 0.5836, val MSE: 0.3458
Gen 4024/8000 ▶ train MSE: 0.5836, val MSE: 0.3452
Gen 4025/8000 ▶ train MSE: 0.5836, val MSE: 0.3452
Gen 4026/8000 ▶ train MSE: 0.5836, val MSE: 0.3452
Gen 4027/8000 ▶ train MSE: 0.5835, val MSE: 0.3458
Gen 4028/8000 ▶ train MSE: 0.5835, val MSE: 0.3458
Gen 4029/8000 ▶ train MSE: 0.5835, val MSE: 0.3458
Gen 4030/8000 ▶ train MSE: 0.5835, val MSE: 0.3458
Gen 4031/8000 ▶ train MSE: 0.5835, val MSE: 0.3458
Gen 4032/8000 ▶ train MSE: 0.5835, val MSE: 0.3458
Gen 4033/8000 ▶ train MSE: 0.5835, val MSE: 0.3454
Gen 4034/8000 ▶ train MSE: 0.5835, val MSE: 0.3454
Gen 4035/8000 ▶ train MSE: 0.5835, val MSE: 0.3454
Gen 4036/8000 ▶ train MSE: 0.5835, val MSE: 0.3454
Gen 4037/8000 ▶ train MSE: 0.5835, val MSE: 0.3454
Gen 4038/8000 ▶ train MSE: 0.5835, val MSE: 0.3454
Gen 4039/8000 ▶ train MSE: 0.5835, val MSE: 0.3454
Gen 4040/8000 ▶ train MSE: 0.5835, val MSE: 0.3454
Gen 4041/8000 ▶ train MSE: 0.5834, val MSE: 0.3454
Gen 4042/8000 ▶ train MSE: 0.5834, val MSE: 0.3454
Gen 4043/8000 ▶ train MSE: 0.5834, val MSE: 0.3454
Gen 4044/8000 ▶ train MSE: 0.5834, val MSE: 0.3454
Gen 4045/8000 ▶ train MSE: 0.5834, val MSE: 0.3455
Gen 4046/8000 ▶ train MSE: 0.5834, val MSE: 0.3455
Gen 4047/8000 ▶ train MSE: 0.5834, val MSE: 0.3455
Gen 4048/8000 ▶ train MSE: 0.5834, val MSE: 0.3455
Gen 4049/8000 ▶ train MSE: 0.5834, val MSE: 0.3451
Gen 4050/8000 ▶ train MSE: 0.5834, val MSE: 0.3451
Gen 4051/8000 ▶ train MSE: 0.5834, val MSE: 0.3451
Gen 4052/8000 ▶ train MSE: 0.5834, val MSE: 0.3451
Gen 4053/8000 ▶ train MSE: 0.5834, val MSE: 0.3451
Gen 4054/8000 ▶ train MSE: 0.5834, val MSE: 0.3451
Gen 4055/8000 ▶ train MSE: 0.5833, val MSE: 0.3454
Gen 4056/8000 ▶ train MSE: 0.5833, val MSE: 0.3454
Gen 4057/8000 ▶ train MSE: 0.5833, val MSE: 0.3454
Gen 4058/8000 ▶ train MSE: 0.5833, val MSE: 0.3454
Gen 4059/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4060/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4061/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4062/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4063/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4064/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4065/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4066/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4067/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4068/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4069/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4070/8000 ▶ train MSE: 0.5833, val MSE: 0.3455
Gen 4071/8000 ▶ train MSE: 0.5833, val MSE: 0.3452
Gen 4072/8000 ▶ train MSE: 0.5833, val MSE: 0.3452
Gen 4073/8000 ▶ train MSE: 0.5833, val MSE: 0.3452
Gen 4074/8000 ▶ train MSE: 0.5833, val MSE: 0.3452
Gen 4075/8000 ▶ train MSE: 0.5833, val MSE: 0.3452
Gen 4076/8000 ▶ train MSE: 0.5833, val MSE: 0.3452
Gen 4077/8000 ▶ train MSE: 0.5833, val MSE: 0.3452
Gen 4078/8000 ▶ train MSE: 0.5832, val MSE: 0.3445
Gen 4079/8000 ▶ train MSE: 0.5832, val MSE: 0.3445
Gen 4080/8000 ▶ train MSE: 0.5832, val MSE: 0.3445
Gen 4081/8000 ▶ train MSE: 0.5832, val MSE: 0.3445
Gen 4082/8000 ▶ train MSE: 0.5832, val MSE: 0.3445
Gen 4083/8000 ▶ train MSE: 0.5832, val MSE: 0.3445
Gen 4084/8000 ▶ train MSE: 0.5832, val MSE: 0.3445
Gen 4085/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4086/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4087/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4088/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4089/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4090/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4091/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4092/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4093/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4094/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4095/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4096/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4097/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4098/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4099/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4100/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4101/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4102/8000 ▶ train MSE: 0.5832, val MSE: 0.3449
Gen 4103/8000 ▶ train MSE: 0.5832, val MSE: 0.3450
Gen 4104/8000 ▶ train MSE: 0.5832, val MSE: 0.3450
Gen 4105/8000 ▶ train MSE: 0.5832, val MSE: 0.3450
Gen 4106/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4107/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4108/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4109/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4110/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4111/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4112/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4113/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4114/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4115/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4116/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4117/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4118/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4119/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4120/8000 ▶ train MSE: 0.5831, val MSE: 0.3455
Gen 4121/8000 ▶ train MSE: 0.5831, val MSE: 0.3454
Gen 4122/8000 ▶ train MSE: 0.5831, val MSE: 0.3454
Gen 4123/8000 ▶ train MSE: 0.5831, val MSE: 0.3454
Gen 4124/8000 ▶ train MSE: 0.5831, val MSE: 0.3454
Gen 4125/8000 ▶ train MSE: 0.5831, val MSE: 0.3454
Gen 4126/8000 ▶ train MSE: 0.5831, val MSE: 0.3454
Gen 4127/8000 ▶ train MSE: 0.5831, val MSE: 0.3454
Gen 4128/8000 ▶ train MSE: 0.5830, val MSE: 0.3440
Gen 4129/8000 ▶ train MSE: 0.5830, val MSE: 0.3440
Gen 4130/8000 ▶ train MSE: 0.5830, val MSE: 0.3440
Gen 4131/8000 ▶ train MSE: 0.5830, val MSE: 0.3440
Gen 4132/8000 ▶ train MSE: 0.5830, val MSE: 0.3440
Gen 4133/8000 ▶ train MSE: 0.5830, val MSE: 0.3440
Gen 4134/8000 ▶ train MSE: 0.5830, val MSE: 0.3440
Gen 4135/8000 ▶ train MSE: 0.5830, val MSE: 0.3448
Gen 4136/8000 ▶ train MSE: 0.5830, val MSE: 0.3448
Gen 4137/8000 ▶ train MSE: 0.5830, val MSE: 0.3448
Gen 4138/8000 ▶ train MSE: 0.5830, val MSE: 0.3448
Gen 4139/8000 ▶ train MSE: 0.5830, val MSE: 0.3448
Gen 4140/8000 ▶ train MSE: 0.5830, val MSE: 0.3448
Gen 4141/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4142/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4143/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4144/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4145/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4146/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4147/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4148/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4149/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4150/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4151/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4152/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4153/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4154/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4155/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4156/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4157/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4158/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4159/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4160/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4161/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4162/8000 ▶ train MSE: 0.5830, val MSE: 0.3458
Gen 4163/8000 ▶ train MSE: 0.5830, val MSE: 0.3453
Gen 4164/8000 ▶ train MSE: 0.5830, val MSE: 0.3453
Gen 4165/8000 ▶ train MSE: 0.5830, val MSE: 0.3453
Gen 4166/8000 ▶ train MSE: 0.5830, val MSE: 0.3453
Gen 4167/8000 ▶ train MSE: 0.5830, val MSE: 0.3453
Gen 4168/8000 ▶ train MSE: 0.5830, val MSE: 0.3453
Gen 4169/8000 ▶ train MSE: 0.5830, val MSE: 0.3453
Gen 4170/8000 ▶ train MSE: 0.5830, val MSE: 0.3453
Gen 4171/8000 ▶ train MSE: 0.5830, val MSE: 0.3453
Gen 4172/8000 ▶ train MSE: 0.5830, val MSE: 0.3453
Gen 4173/8000 ▶ train MSE: 0.5830, val MSE: 0.3457
Gen 4174/8000 ▶ train MSE: 0.5830, val MSE: 0.3457
Gen 4175/8000 ▶ train MSE: 0.5830, val MSE: 0.3457
Gen 4176/8000 ▶ train MSE: 0.5830, val MSE: 0.3457
Gen 4177/8000 ▶ train MSE: 0.5830, val MSE: 0.3454
Gen 4178/8000 ▶ train MSE: 0.5830, val MSE: 0.3454
Gen 4179/8000 ▶ train MSE: 0.5830, val MSE: 0.3454
Gen 4180/8000 ▶ train MSE: 0.5830, val MSE: 0.3452
Gen 4181/8000 ▶ train MSE: 0.5830, val MSE: 0.3452
Gen 4182/8000 ▶ train MSE: 0.5830, val MSE: 0.3452
Gen 4183/8000 ▶ train MSE: 0.5830, val MSE: 0.3452
Gen 4184/8000 ▶ train MSE: 0.5829, val MSE: 0.3453
Gen 4185/8000 ▶ train MSE: 0.5829, val MSE: 0.3453
Gen 4186/8000 ▶ train MSE: 0.5829, val MSE: 0.3453
Gen 4187/8000 ▶ train MSE: 0.5829, val MSE: 0.3453
Gen 4188/8000 ▶ train MSE: 0.5829, val MSE: 0.3453
Gen 4189/8000 ▶ train MSE: 0.5829, val MSE: 0.3453
Gen 4190/8000 ▶ train MSE: 0.5829, val MSE: 0.3453
Gen 4191/8000 ▶ train MSE: 0.5829, val MSE: 0.3453
Gen 4192/8000 ▶ train MSE: 0.5829, val MSE: 0.3460
Gen 4193/8000 ▶ train MSE: 0.5829, val MSE: 0.3460
Gen 4194/8000 ▶ train MSE: 0.5829, val MSE: 0.3460
Gen 4195/8000 ▶ train MSE: 0.5829, val MSE: 0.3448
Gen 4196/8000 ▶ train MSE: 0.5829, val MSE: 0.3448
Gen 4197/8000 ▶ train MSE: 0.5829, val MSE: 0.3448
Gen 4198/8000 ▶ train MSE: 0.5829, val MSE: 0.3456
Gen 4199/8000 ▶ train MSE: 0.5829, val MSE: 0.3456
Gen 4200/8000 ▶ train MSE: 0.5829, val MSE: 0.3450
Gen 4201/8000 ▶ train MSE: 0.5829, val MSE: 0.3450
Gen 4202/8000 ▶ train MSE: 0.5829, val MSE: 0.3450
Gen 4203/8000 ▶ train MSE: 0.5829, val MSE: 0.3455
Gen 4204/8000 ▶ train MSE: 0.5829, val MSE: 0.3455
Gen 4205/8000 ▶ train MSE: 0.5828, val MSE: 0.3451
Gen 4206/8000 ▶ train MSE: 0.5828, val MSE: 0.3451
Gen 4207/8000 ▶ train MSE: 0.5828, val MSE: 0.3451
Gen 4208/8000 ▶ train MSE: 0.5828, val MSE: 0.3451
Gen 4209/8000 ▶ train MSE: 0.5828, val MSE: 0.3451
Gen 4210/8000 ▶ train MSE: 0.5828, val MSE: 0.3451
Gen 4211/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4212/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4213/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4214/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4215/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4216/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4217/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4218/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4219/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4220/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4221/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4222/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4223/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4224/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4225/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4226/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4227/8000 ▶ train MSE: 0.5828, val MSE: 0.3450
Gen 4228/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4229/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4230/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4231/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4232/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4233/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4234/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4235/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4236/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4237/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4238/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4239/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4240/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4241/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4242/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4243/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4244/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4245/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4246/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4247/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4248/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4249/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4250/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4251/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4252/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4253/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4254/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4255/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4256/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4257/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4258/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4259/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4260/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4261/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4262/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4263/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4264/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4265/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4266/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4267/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4268/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4269/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4270/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4271/8000 ▶ train MSE: 0.5828, val MSE: 0.3452
Gen 4272/8000 ▶ train MSE: 0.5828, val MSE: 0.3465
Gen 4273/8000 ▶ train MSE: 0.5828, val MSE: 0.3465
Gen 4274/8000 ▶ train MSE: 0.5828, val MSE: 0.3465
Gen 4275/8000 ▶ train MSE: 0.5828, val MSE: 0.3465
Gen 4276/8000 ▶ train MSE: 0.5828, val MSE: 0.3465
Gen 4277/8000 ▶ train MSE: 0.5828, val MSE: 0.3465
Gen 4278/8000 ▶ train MSE: 0.5828, val MSE: 0.3465
Gen 4279/8000 ▶ train MSE: 0.5828, val MSE: 0.3465
Gen 4280/8000 ▶ train MSE: 0.5827, val MSE: 0.3453
Gen 4281/8000 ▶ train MSE: 0.5827, val MSE: 0.3453
Gen 4282/8000 ▶ train MSE: 0.5827, val MSE: 0.3453
Gen 4283/8000 ▶ train MSE: 0.5827, val MSE: 0.3453
Gen 4284/8000 ▶ train MSE: 0.5827, val MSE: 0.3453
Gen 4285/8000 ▶ train MSE: 0.5827, val MSE: 0.3453
Gen 4286/8000 ▶ train MSE: 0.5827, val MSE: 0.3453
Gen 4287/8000 ▶ train MSE: 0.5827, val MSE: 0.3453
Gen 4288/8000 ▶ train MSE: 0.5827, val MSE: 0.3461
Gen 4289/8000 ▶ train MSE: 0.5827, val MSE: 0.3461
Gen 4290/8000 ▶ train MSE: 0.5827, val MSE: 0.3461
Gen 4291/8000 ▶ train MSE: 0.5827, val MSE: 0.3455
Gen 4292/8000 ▶ train MSE: 0.5827, val MSE: 0.3455
Gen 4293/8000 ▶ train MSE: 0.5827, val MSE: 0.3455
Gen 4294/8000 ▶ train MSE: 0.5827, val MSE: 0.3455
Gen 4295/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4296/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4297/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4298/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4299/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4300/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4301/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4302/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4303/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4304/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4305/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4306/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4307/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4308/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4309/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4310/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4311/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4312/8000 ▶ train MSE: 0.5827, val MSE: 0.3456
Gen 4313/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4314/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4315/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4316/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4317/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4318/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4319/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4320/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4321/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4322/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4323/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4324/8000 ▶ train MSE: 0.5826, val MSE: 0.3460
Gen 4325/8000 ▶ train MSE: 0.5826, val MSE: 0.3460
Gen 4326/8000 ▶ train MSE: 0.5826, val MSE: 0.3460
Gen 4327/8000 ▶ train MSE: 0.5826, val MSE: 0.3460
Gen 4328/8000 ▶ train MSE: 0.5826, val MSE: 0.3460
Gen 4329/8000 ▶ train MSE: 0.5826, val MSE: 0.3460
Gen 4330/8000 ▶ train MSE: 0.5826, val MSE: 0.3460
Gen 4331/8000 ▶ train MSE: 0.5826, val MSE: 0.3460
Gen 4332/8000 ▶ train MSE: 0.5826, val MSE: 0.3460
Gen 4333/8000 ▶ train MSE: 0.5826, val MSE: 0.3460
Gen 4334/8000 ▶ train MSE: 0.5826, val MSE: 0.3460
Gen 4335/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4336/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4337/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4338/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4339/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4340/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4341/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4342/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4343/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4344/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4345/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4346/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4347/8000 ▶ train MSE: 0.5826, val MSE: 0.3459
Gen 4348/8000 ▶ train MSE: 0.5826, val MSE: 0.3458
Gen 4349/8000 ▶ train MSE: 0.5826, val MSE: 0.3458
Gen 4350/8000 ▶ train MSE: 0.5826, val MSE: 0.3458
Gen 4351/8000 ▶ train MSE: 0.5826, val MSE: 0.3458
Gen 4352/8000 ▶ train MSE: 0.5826, val MSE: 0.3458
Gen 4353/8000 ▶ train MSE: 0.5826, val MSE: 0.3458
Gen 4354/8000 ▶ train MSE: 0.5826, val MSE: 0.3458
Gen 4355/8000 ▶ train MSE: 0.5826, val MSE: 0.3458
Gen 4356/8000 ▶ train MSE: 0.5826, val MSE: 0.3458
Gen 4357/8000 ▶ train MSE: 0.5826, val MSE: 0.3458
Gen 4358/8000 ▶ train MSE: 0.5826, val MSE: 0.3458
Gen 4359/8000 ▶ train MSE: 0.5825, val MSE: 0.3457
Gen 4360/8000 ▶ train MSE: 0.5825, val MSE: 0.3457
Gen 4361/8000 ▶ train MSE: 0.5825, val MSE: 0.3457
Gen 4362/8000 ▶ train MSE: 0.5825, val MSE: 0.3457
Gen 4363/8000 ▶ train MSE: 0.5825, val MSE: 0.3457
Gen 4364/8000 ▶ train MSE: 0.5825, val MSE: 0.3457
Gen 4365/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4366/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4367/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4368/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4369/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4370/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4371/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4372/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4373/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4374/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4375/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4376/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4377/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4378/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4379/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4380/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4381/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4382/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4383/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4384/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4385/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4386/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4387/8000 ▶ train MSE: 0.5825, val MSE: 0.3455
Gen 4388/8000 ▶ train MSE: 0.5825, val MSE: 0.3458
Gen 4389/8000 ▶ train MSE: 0.5825, val MSE: 0.3458
Gen 4390/8000 ▶ train MSE: 0.5825, val MSE: 0.3458
Gen 4391/8000 ▶ train MSE: 0.5825, val MSE: 0.3458
Gen 4392/8000 ▶ train MSE: 0.5825, val MSE: 0.3458
Gen 4393/8000 ▶ train MSE: 0.5825, val MSE: 0.3458
Gen 4394/8000 ▶ train MSE: 0.5825, val MSE: 0.3458
Gen 4395/8000 ▶ train MSE: 0.5825, val MSE: 0.3458
Gen 4396/8000 ▶ train MSE: 0.5825, val MSE: 0.3458
Gen 4397/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4398/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4399/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4400/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4401/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4402/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4403/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4404/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4405/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4406/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4407/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4408/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4409/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4410/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4411/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4412/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4413/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4414/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4415/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4416/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4417/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4418/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4419/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4420/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4421/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4422/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4423/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4424/8000 ▶ train MSE: 0.5825, val MSE: 0.3446
Gen 4425/8000 ▶ train MSE: 0.5825, val MSE: 0.3459
Gen 4426/8000 ▶ train MSE: 0.5825, val MSE: 0.3459
Gen 4427/8000 ▶ train MSE: 0.5825, val MSE: 0.3459
Gen 4428/8000 ▶ train MSE: 0.5825, val MSE: 0.3459
Gen 4429/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4430/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4431/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4432/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4433/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4434/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4435/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4436/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4437/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4438/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4439/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4440/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4441/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4442/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4443/8000 ▶ train MSE: 0.5824, val MSE: 0.3457
Gen 4444/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4445/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4446/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4447/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4448/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4449/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4450/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4451/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4452/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4453/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4454/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4455/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4456/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4457/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4458/8000 ▶ train MSE: 0.5824, val MSE: 0.3459
Gen 4459/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4460/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4461/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4462/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4463/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4464/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4465/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4466/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4467/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4468/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4469/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4470/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4471/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4472/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4473/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4474/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4475/8000 ▶ train MSE: 0.5824, val MSE: 0.3454
Gen 4476/8000 ▶ train MSE: 0.5823, val MSE: 0.3458
Gen 4477/8000 ▶ train MSE: 0.5823, val MSE: 0.3458
Gen 4478/8000 ▶ train MSE: 0.5823, val MSE: 0.3458
Gen 4479/8000 ▶ train MSE: 0.5823, val MSE: 0.3458
Gen 4480/8000 ▶ train MSE: 0.5823, val MSE: 0.3458
Gen 4481/8000 ▶ train MSE: 0.5823, val MSE: 0.3458
Gen 4482/8000 ▶ train MSE: 0.5823, val MSE: 0.3458
Gen 4483/8000 ▶ train MSE: 0.5823, val MSE: 0.3458
Gen 4484/8000 ▶ train MSE: 0.5823, val MSE: 0.3458
Gen 4485/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4486/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4487/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4488/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4489/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4490/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4491/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4492/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4493/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4494/8000 ▶ train MSE: 0.5823, val MSE: 0.3459
Gen 4495/8000 ▶ train MSE: 0.5823, val MSE: 0.3459
Gen 4496/8000 ▶ train MSE: 0.5823, val MSE: 0.3459
Gen 4497/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4498/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4499/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4500/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4501/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4502/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4503/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4504/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4505/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4506/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4507/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4508/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4509/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4510/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4511/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4512/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4513/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4514/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4515/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4516/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4517/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4518/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4519/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4520/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4521/8000 ▶ train MSE: 0.5823, val MSE: 0.3457
Gen 4522/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4523/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4524/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4525/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4526/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4527/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4528/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4529/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4530/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4531/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4532/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4533/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4534/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4535/8000 ▶ train MSE: 0.5823, val MSE: 0.3460
Gen 4536/8000 ▶ train MSE: 0.5823, val MSE: 0.3454
Gen 4537/8000 ▶ train MSE: 0.5823, val MSE: 0.3454
Gen 4538/8000 ▶ train MSE: 0.5823, val MSE: 0.3454
Gen 4539/8000 ▶ train MSE: 0.5823, val MSE: 0.3454
Gen 4540/8000 ▶ train MSE: 0.5823, val MSE: 0.3454
Gen 4541/8000 ▶ train MSE: 0.5823, val MSE: 0.3454
Gen 4542/8000 ▶ train MSE: 0.5823, val MSE: 0.3454
Gen 4543/8000 ▶ train MSE: 0.5823, val MSE: 0.3454
Gen 4544/8000 ▶ train MSE: 0.5823, val MSE: 0.3454
Gen 4545/8000 ▶ train MSE: 0.5823, val MSE: 0.3456
Gen 4546/8000 ▶ train MSE: 0.5823, val MSE: 0.3456
Gen 4547/8000 ▶ train MSE: 0.5823, val MSE: 0.3456
Gen 4548/8000 ▶ train MSE: 0.5823, val MSE: 0.3458
Gen 4549/8000 ▶ train MSE: 0.5823, val MSE: 0.3458
Gen 4550/8000 ▶ train MSE: 0.5822, val MSE: 0.3456
Gen 4551/8000 ▶ train MSE: 0.5822, val MSE: 0.3456
Gen 4552/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4553/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4554/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4555/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4556/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4557/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4558/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4559/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4560/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4561/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4562/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4563/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4564/8000 ▶ train MSE: 0.5822, val MSE: 0.3458
Gen 4565/8000 ▶ train MSE: 0.5822, val MSE: 0.3458
Gen 4566/8000 ▶ train MSE: 0.5822, val MSE: 0.3458
Gen 4567/8000 ▶ train MSE: 0.5822, val MSE: 0.3458
Gen 4568/8000 ▶ train MSE: 0.5822, val MSE: 0.3458
Gen 4569/8000 ▶ train MSE: 0.5822, val MSE: 0.3458
Gen 4570/8000 ▶ train MSE: 0.5822, val MSE: 0.3458
Gen 4571/8000 ▶ train MSE: 0.5822, val MSE: 0.3458
Gen 4572/8000 ▶ train MSE: 0.5822, val MSE: 0.3459
Gen 4573/8000 ▶ train MSE: 0.5822, val MSE: 0.3459
Gen 4574/8000 ▶ train MSE: 0.5822, val MSE: 0.3459
Gen 4575/8000 ▶ train MSE: 0.5822, val MSE: 0.3459
Gen 4576/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4577/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4578/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4579/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4580/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4581/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4582/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4583/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4584/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4585/8000 ▶ train MSE: 0.5822, val MSE: 0.3461
Gen 4586/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4587/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4588/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4589/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4590/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4591/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4592/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4593/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4594/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4595/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4596/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4597/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4598/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4599/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4600/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4601/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4602/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4603/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4604/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4605/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4606/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4607/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4608/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4609/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4610/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4611/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4612/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4613/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4614/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4615/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4616/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4617/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4618/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4619/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4620/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4621/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4622/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4623/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4624/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4625/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4626/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4627/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4628/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4629/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4630/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4631/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4632/8000 ▶ train MSE: 0.5821, val MSE: 0.3451
Gen 4633/8000 ▶ train MSE: 0.5821, val MSE: 0.3450
Gen 4634/8000 ▶ train MSE: 0.5821, val MSE: 0.3450
Gen 4635/8000 ▶ train MSE: 0.5821, val MSE: 0.3450
Gen 4636/8000 ▶ train MSE: 0.5821, val MSE: 0.3450
Gen 4637/8000 ▶ train MSE: 0.5821, val MSE: 0.3450
Gen 4638/8000 ▶ train MSE: 0.5821, val MSE: 0.3450
Gen 4639/8000 ▶ train MSE: 0.5821, val MSE: 0.3450
Gen 4640/8000 ▶ train MSE: 0.5821, val MSE: 0.3450
Gen 4641/8000 ▶ train MSE: 0.5821, val MSE: 0.3450
Gen 4642/8000 ▶ train MSE: 0.5821, val MSE: 0.3450
Gen 4643/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4644/8000 ▶ train MSE: 0.5821, val MSE: 0.3457
Gen 4645/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4646/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4647/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4648/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4649/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4650/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4651/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4652/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4653/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4654/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4655/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4656/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4657/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4658/8000 ▶ train MSE: 0.5820, val MSE: 0.3457
Gen 4659/8000 ▶ train MSE: 0.5820, val MSE: 0.3456
Gen 4660/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4661/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4662/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4663/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4664/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4665/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4666/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4667/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4668/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4669/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4670/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4671/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4672/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4673/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4674/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4675/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4676/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4677/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4678/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4679/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4680/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4681/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4682/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4683/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4684/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4685/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4686/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4687/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4688/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4689/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4690/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4691/8000 ▶ train MSE: 0.5820, val MSE: 0.3442
Gen 4692/8000 ▶ train MSE: 0.5820, val MSE: 0.3443
Gen 4693/8000 ▶ train MSE: 0.5820, val MSE: 0.3443
Gen 4694/8000 ▶ train MSE: 0.5820, val MSE: 0.3443
Gen 4695/8000 ▶ train MSE: 0.5820, val MSE: 0.3443
Gen 4696/8000 ▶ train MSE: 0.5820, val MSE: 0.3443
Gen 4697/8000 ▶ train MSE: 0.5820, val MSE: 0.3443
Gen 4698/8000 ▶ train MSE: 0.5820, val MSE: 0.3443
Gen 4699/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4700/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4701/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4702/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4703/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4704/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4705/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4706/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4707/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4708/8000 ▶ train MSE: 0.5819, val MSE: 0.3440
Gen 4709/8000 ▶ train MSE: 0.5819, val MSE: 0.3440
Gen 4710/8000 ▶ train MSE: 0.5819, val MSE: 0.3440
Gen 4711/8000 ▶ train MSE: 0.5819, val MSE: 0.3440
Gen 4712/8000 ▶ train MSE: 0.5819, val MSE: 0.3440
Gen 4713/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4714/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4715/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4716/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4717/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4718/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4719/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4720/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4721/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4722/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4723/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4724/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4725/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4726/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4727/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4728/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4729/8000 ▶ train MSE: 0.5819, val MSE: 0.3439
Gen 4730/8000 ▶ train MSE: 0.5819, val MSE: 0.3446
Gen 4731/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4732/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4733/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4734/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4735/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4736/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4737/8000 ▶ train MSE: 0.5819, val MSE: 0.3442
Gen 4738/8000 ▶ train MSE: 0.5818, val MSE: 0.3442
Gen 4739/8000 ▶ train MSE: 0.5818, val MSE: 0.3442
Gen 4740/8000 ▶ train MSE: 0.5818, val MSE: 0.3442
Gen 4741/8000 ▶ train MSE: 0.5818, val MSE: 0.3442
Gen 4742/8000 ▶ train MSE: 0.5818, val MSE: 0.3442
Gen 4743/8000 ▶ train MSE: 0.5818, val MSE: 0.3442
Gen 4744/8000 ▶ train MSE: 0.5818, val MSE: 0.3442
Gen 4745/8000 ▶ train MSE: 0.5818, val MSE: 0.3442
Gen 4746/8000 ▶ train MSE: 0.5818, val MSE: 0.3442
Gen 4747/8000 ▶ train MSE: 0.5818, val MSE: 0.3449
Gen 4748/8000 ▶ train MSE: 0.5818, val MSE: 0.3449
Gen 4749/8000 ▶ train MSE: 0.5818, val MSE: 0.3449
Gen 4750/8000 ▶ train MSE: 0.5818, val MSE: 0.3449
Gen 4751/8000 ▶ train MSE: 0.5818, val MSE: 0.3449
Gen 4752/8000 ▶ train MSE: 0.5818, val MSE: 0.3449
Gen 4753/8000 ▶ train MSE: 0.5818, val MSE: 0.3441
Gen 4754/8000 ▶ train MSE: 0.5818, val MSE: 0.3441
Gen 4755/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4756/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4757/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4758/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4759/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4760/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4761/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4762/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4763/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4764/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4765/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4766/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4767/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4768/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4769/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4770/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4771/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4772/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4773/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4774/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4775/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4776/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4777/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4778/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4779/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4780/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4781/8000 ▶ train MSE: 0.5818, val MSE: 0.3444
Gen 4782/8000 ▶ train MSE: 0.5818, val MSE: 0.3450
Gen 4783/8000 ▶ train MSE: 0.5818, val MSE: 0.3450
Gen 4784/8000 ▶ train MSE: 0.5818, val MSE: 0.3450
Gen 4785/8000 ▶ train MSE: 0.5818, val MSE: 0.3450
Gen 4786/8000 ▶ train MSE: 0.5818, val MSE: 0.3450
Gen 4787/8000 ▶ train MSE: 0.5818, val MSE: 0.3450
Gen 4788/8000 ▶ train MSE: 0.5818, val MSE: 0.3450
Gen 4789/8000 ▶ train MSE: 0.5818, val MSE: 0.3450
Gen 4790/8000 ▶ train MSE: 0.5818, val MSE: 0.3446
Gen 4791/8000 ▶ train MSE: 0.5818, val MSE: 0.3446
Gen 4792/8000 ▶ train MSE: 0.5818, val MSE: 0.3446
Gen 4793/8000 ▶ train MSE: 0.5818, val MSE: 0.3446
Gen 4794/8000 ▶ train MSE: 0.5818, val MSE: 0.3447
Gen 4795/8000 ▶ train MSE: 0.5818, val MSE: 0.3447
Gen 4796/8000 ▶ train MSE: 0.5818, val MSE: 0.3447
Gen 4797/8000 ▶ train MSE: 0.5817, val MSE: 0.3447
Gen 4798/8000 ▶ train MSE: 0.5817, val MSE: 0.3447
Gen 4799/8000 ▶ train MSE: 0.5817, val MSE: 0.3447
Gen 4800/8000 ▶ train MSE: 0.5817, val MSE: 0.3447
Gen 4801/8000 ▶ train MSE: 0.5817, val MSE: 0.3447
Gen 4802/8000 ▶ train MSE: 0.5817, val MSE: 0.3447
Gen 4803/8000 ▶ train MSE: 0.5817, val MSE: 0.3447
Gen 4804/8000 ▶ train MSE: 0.5817, val MSE: 0.3447
Gen 4805/8000 ▶ train MSE: 0.5817, val MSE: 0.3447
Gen 4806/8000 ▶ train MSE: 0.5817, val MSE: 0.3447
Gen 4807/8000 ▶ train MSE: 0.5817, val MSE: 0.3446
Gen 4808/8000 ▶ train MSE: 0.5817, val MSE: 0.3446
Gen 4809/8000 ▶ train MSE: 0.5817, val MSE: 0.3446
Gen 4810/8000 ▶ train MSE: 0.5817, val MSE: 0.3446
Gen 4811/8000 ▶ train MSE: 0.5817, val MSE: 0.3446
Gen 4812/8000 ▶ train MSE: 0.5817, val MSE: 0.3446
Gen 4813/8000 ▶ train MSE: 0.5817, val MSE: 0.3446
Gen 4814/8000 ▶ train MSE: 0.5817, val MSE: 0.3446
Gen 4815/8000 ▶ train MSE: 0.5817, val MSE: 0.3446
Gen 4816/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4817/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4818/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4819/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4820/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4821/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4822/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4823/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4824/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4825/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4826/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4827/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4828/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4829/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4830/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4831/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4832/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4833/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4834/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4835/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4836/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4837/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4838/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4839/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4840/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4841/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4842/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4843/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4844/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4845/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4846/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4847/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4848/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4849/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4850/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4851/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4852/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4853/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4854/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4855/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4856/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4857/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4858/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4859/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4860/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4861/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4862/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4863/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4864/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4865/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4866/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4867/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4868/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4869/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4870/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4871/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4872/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4873/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4874/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4875/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4876/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4877/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4878/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4879/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4880/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4881/8000 ▶ train MSE: 0.5816, val MSE: 0.3445
Gen 4882/8000 ▶ train MSE: 0.5816, val MSE: 0.3444
Gen 4883/8000 ▶ train MSE: 0.5816, val MSE: 0.3444
Gen 4884/8000 ▶ train MSE: 0.5816, val MSE: 0.3444
Gen 4885/8000 ▶ train MSE: 0.5816, val MSE: 0.3444
Gen 4886/8000 ▶ train MSE: 0.5816, val MSE: 0.3444
Gen 4887/8000 ▶ train MSE: 0.5816, val MSE: 0.3444
Gen 4888/8000 ▶ train MSE: 0.5816, val MSE: 0.3444
Gen 4889/8000 ▶ train MSE: 0.5816, val MSE: 0.3444
Gen 4890/8000 ▶ train MSE: 0.5816, val MSE: 0.3444
Gen 4891/8000 ▶ train MSE: 0.5816, val MSE: 0.3444
Gen 4892/8000 ▶ train MSE: 0.5816, val MSE: 0.3444
Gen 4893/8000 ▶ train MSE: 0.5815, val MSE: 0.3445
Gen 4894/8000 ▶ train MSE: 0.5815, val MSE: 0.3445
Gen 4895/8000 ▶ train MSE: 0.5815, val MSE: 0.3445
Gen 4896/8000 ▶ train MSE: 0.5815, val MSE: 0.3445
Gen 4897/8000 ▶ train MSE: 0.5815, val MSE: 0.3445
Gen 4898/8000 ▶ train MSE: 0.5815, val MSE: 0.3445
Gen 4899/8000 ▶ train MSE: 0.5815, val MSE: 0.3445
Gen 4900/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4901/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4902/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4903/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4904/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4905/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4906/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4907/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4908/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4909/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4910/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4911/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4912/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4913/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4914/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4915/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4916/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4917/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4918/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4919/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4920/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4921/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4922/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4923/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4924/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4925/8000 ▶ train MSE: 0.5815, val MSE: 0.3443
Gen 4926/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4927/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4928/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4929/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4930/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4931/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4932/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4933/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4934/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4935/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4936/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4937/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4938/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4939/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4940/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4941/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4942/8000 ▶ train MSE: 0.5815, val MSE: 0.3442
Gen 4943/8000 ▶ train MSE: 0.5815, val MSE: 0.3438
Gen 4944/8000 ▶ train MSE: 0.5815, val MSE: 0.3438
Gen 4945/8000 ▶ train MSE: 0.5815, val MSE: 0.3438
Gen 4946/8000 ▶ train MSE: 0.5815, val MSE: 0.3438
Gen 4947/8000 ▶ train MSE: 0.5815, val MSE: 0.3438
Gen 4948/8000 ▶ train MSE: 0.5815, val MSE: 0.3438
Gen 4949/8000 ▶ train MSE: 0.5815, val MSE: 0.3438
Gen 4950/8000 ▶ train MSE: 0.5815, val MSE: 0.3438
Gen 4951/8000 ▶ train MSE: 0.5815, val MSE: 0.3438
Gen 4952/8000 ▶ train MSE: 0.5815, val MSE: 0.3438
Gen 4953/8000 ▶ train MSE: 0.5815, val MSE: 0.3438
Gen 4954/8000 ▶ train MSE: 0.5815, val MSE: 0.3440
Gen 4955/8000 ▶ train MSE: 0.5815, val MSE: 0.3440
Gen 4956/8000 ▶ train MSE: 0.5815, val MSE: 0.3440
Gen 4957/8000 ▶ train MSE: 0.5815, val MSE: 0.3440
Gen 4958/8000 ▶ train MSE: 0.5815, val MSE: 0.3440
Gen 4959/8000 ▶ train MSE: 0.5815, val MSE: 0.3440
Gen 4960/8000 ▶ train MSE: 0.5815, val MSE: 0.3440
Gen 4961/8000 ▶ train MSE: 0.5814, val MSE: 0.3440
Gen 4962/8000 ▶ train MSE: 0.5814, val MSE: 0.3440
Gen 4963/8000 ▶ train MSE: 0.5814, val MSE: 0.3440
Gen 4964/8000 ▶ train MSE: 0.5814, val MSE: 0.3440
Gen 4965/8000 ▶ train MSE: 0.5814, val MSE: 0.3448
Gen 4966/8000 ▶ train MSE: 0.5814, val MSE: 0.3439
Gen 4967/8000 ▶ train MSE: 0.5814, val MSE: 0.3439
Gen 4968/8000 ▶ train MSE: 0.5814, val MSE: 0.3439
Gen 4969/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4970/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4971/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4972/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4973/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4974/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4975/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4976/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4977/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4978/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4979/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4980/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4981/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4982/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4983/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4984/8000 ▶ train MSE: 0.5814, val MSE: 0.3434
Gen 4985/8000 ▶ train MSE: 0.5814, val MSE: 0.3438
Gen 4986/8000 ▶ train MSE: 0.5814, val MSE: 0.3438
Gen 4987/8000 ▶ train MSE: 0.5814, val MSE: 0.3438
Gen 4988/8000 ▶ train MSE: 0.5814, val MSE: 0.3439
Gen 4989/8000 ▶ train MSE: 0.5814, val MSE: 0.3439
Gen 4990/8000 ▶ train MSE: 0.5814, val MSE: 0.3439
Gen 4991/8000 ▶ train MSE: 0.5814, val MSE: 0.3439
Gen 4992/8000 ▶ train MSE: 0.5814, val MSE: 0.3439
Gen 4993/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 4994/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 4995/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 4996/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 4997/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 4998/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 4999/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 5000/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 5001/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 5002/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 5003/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 5004/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 5005/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 5006/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 5007/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 5008/8000 ▶ train MSE: 0.5813, val MSE: 0.3428
Gen 5009/8000 ▶ train MSE: 0.5813, val MSE: 0.3435
Gen 5010/8000 ▶ train MSE: 0.5813, val MSE: 0.3435
Gen 5011/8000 ▶ train MSE: 0.5813, val MSE: 0.3435
Gen 5012/8000 ▶ train MSE: 0.5813, val MSE: 0.3435
Gen 5013/8000 ▶ train MSE: 0.5813, val MSE: 0.3435
Gen 5014/8000 ▶ train MSE: 0.5813, val MSE: 0.3435
Gen 5015/8000 ▶ train MSE: 0.5813, val MSE: 0.3435
Gen 5016/8000 ▶ train MSE: 0.5813, val MSE: 0.3435
Gen 5017/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5018/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5019/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5020/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5021/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5022/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5023/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5024/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5025/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5026/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5027/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5028/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5029/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5030/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5031/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5032/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5033/8000 ▶ train MSE: 0.5813, val MSE: 0.3424
Gen 5034/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5035/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5036/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5037/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5038/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5039/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5040/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5041/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5042/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5043/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5044/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5045/8000 ▶ train MSE: 0.5813, val MSE: 0.3431
Gen 5046/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5047/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5048/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5049/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5050/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5051/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5052/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5053/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5054/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5055/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5056/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5057/8000 ▶ train MSE: 0.5812, val MSE: 0.3435
Gen 5058/8000 ▶ train MSE: 0.5812, val MSE: 0.3435
Gen 5059/8000 ▶ train MSE: 0.5812, val MSE: 0.3435
Gen 5060/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5061/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5062/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5063/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5064/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5065/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5066/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5067/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5068/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5069/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5070/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5071/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5072/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5073/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5074/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5075/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5076/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5077/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5078/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5079/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5080/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5081/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5082/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5083/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5084/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5085/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5086/8000 ▶ train MSE: 0.5812, val MSE: 0.3431
Gen 5087/8000 ▶ train MSE: 0.5812, val MSE: 0.3426
Gen 5088/8000 ▶ train MSE: 0.5812, val MSE: 0.3426
Gen 5089/8000 ▶ train MSE: 0.5812, val MSE: 0.3426
Gen 5090/8000 ▶ train MSE: 0.5812, val MSE: 0.3426
Gen 5091/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5092/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5093/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5094/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5095/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5096/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5097/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5098/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5099/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5100/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5101/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5102/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5103/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5104/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5105/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5106/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5107/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5108/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5109/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5110/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5111/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5112/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5113/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5114/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5115/8000 ▶ train MSE: 0.5812, val MSE: 0.3429
Gen 5116/8000 ▶ train MSE: 0.5812, val MSE: 0.3427
Gen 5117/8000 ▶ train MSE: 0.5811, val MSE: 0.3426
Gen 5118/8000 ▶ train MSE: 0.5811, val MSE: 0.3426
Gen 5119/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5120/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5121/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5122/8000 ▶ train MSE: 0.5811, val MSE: 0.3429
Gen 5123/8000 ▶ train MSE: 0.5811, val MSE: 0.3429
Gen 5124/8000 ▶ train MSE: 0.5811, val MSE: 0.3429
Gen 5125/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5126/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5127/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5128/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5129/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5130/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5131/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5132/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5133/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5134/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5135/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5136/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5137/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5138/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5139/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5140/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5141/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5142/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5143/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5144/8000 ▶ train MSE: 0.5811, val MSE: 0.3424
Gen 5145/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5146/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5147/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5148/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5149/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5150/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5151/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5152/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5153/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5154/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5155/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5156/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5157/8000 ▶ train MSE: 0.5811, val MSE: 0.3425
Gen 5158/8000 ▶ train MSE: 0.5811, val MSE: 0.3435
Gen 5159/8000 ▶ train MSE: 0.5811, val MSE: 0.3435
Gen 5160/8000 ▶ train MSE: 0.5811, val MSE: 0.3435
Gen 5161/8000 ▶ train MSE: 0.5811, val MSE: 0.3427
Gen 5162/8000 ▶ train MSE: 0.5811, val MSE: 0.3427
Gen 5163/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5164/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5165/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5166/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5167/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5168/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5169/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5170/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5171/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5172/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5173/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5174/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5175/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5176/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5177/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5178/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5179/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5180/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5181/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5182/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5183/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5184/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5185/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5186/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5187/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5188/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5189/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5190/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5191/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5192/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5193/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5194/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5195/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5196/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5197/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5198/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5199/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5200/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5201/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5202/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5203/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5204/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5205/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5206/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5207/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5208/8000 ▶ train MSE: 0.5810, val MSE: 0.3420
Gen 5209/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5210/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5211/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5212/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5213/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5214/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5215/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5216/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5217/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5218/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5219/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5220/8000 ▶ train MSE: 0.5810, val MSE: 0.3415
Gen 5221/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5222/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5223/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5224/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5225/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5226/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5227/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5228/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5229/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5230/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5231/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5232/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5233/8000 ▶ train MSE: 0.5810, val MSE: 0.3421
Gen 5234/8000 ▶ train MSE: 0.5809, val MSE: 0.3412
Gen 5235/8000 ▶ train MSE: 0.5809, val MSE: 0.3412
Gen 5236/8000 ▶ train MSE: 0.5809, val MSE: 0.3412
Gen 5237/8000 ▶ train MSE: 0.5809, val MSE: 0.3412
Gen 5238/8000 ▶ train MSE: 0.5809, val MSE: 0.3412
Gen 5239/8000 ▶ train MSE: 0.5809, val MSE: 0.3412
Gen 5240/8000 ▶ train MSE: 0.5809, val MSE: 0.3412
Gen 5241/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5242/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5243/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5244/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5245/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5246/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5247/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5248/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5249/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5250/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5251/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5252/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5253/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5254/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5255/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5256/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5257/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5258/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5259/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5260/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5261/8000 ▶ train MSE: 0.5809, val MSE: 0.3416
Gen 5262/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5263/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5264/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5265/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5266/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5267/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5268/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5269/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5270/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5271/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5272/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5273/8000 ▶ train MSE: 0.5809, val MSE: 0.3417
Gen 5274/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5275/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5276/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5277/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5278/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5279/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5280/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5281/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5282/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5283/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5284/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5285/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5286/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5287/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5288/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5289/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5290/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5291/8000 ▶ train MSE: 0.5809, val MSE: 0.3418
Gen 5292/8000 ▶ train MSE: 0.5808, val MSE: 0.3412
Gen 5293/8000 ▶ train MSE: 0.5808, val MSE: 0.3412
Gen 5294/8000 ▶ train MSE: 0.5808, val MSE: 0.3412
Gen 5295/8000 ▶ train MSE: 0.5808, val MSE: 0.3412
Gen 5296/8000 ▶ train MSE: 0.5808, val MSE: 0.3412
Gen 5297/8000 ▶ train MSE: 0.5808, val MSE: 0.3412
Gen 5298/8000 ▶ train MSE: 0.5808, val MSE: 0.3412
Gen 5299/8000 ▶ train MSE: 0.5808, val MSE: 0.3412
Gen 5300/8000 ▶ train MSE: 0.5808, val MSE: 0.3412
Gen 5301/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5302/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5303/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5304/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5305/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5306/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5307/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5308/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5309/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5310/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5311/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5312/8000 ▶ train MSE: 0.5808, val MSE: 0.3413
Gen 5313/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5314/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5315/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5316/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5317/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5318/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5319/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5320/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5321/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5322/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5323/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5324/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5325/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5326/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5327/8000 ▶ train MSE: 0.5808, val MSE: 0.3416
Gen 5328/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5329/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5330/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5331/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5332/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5333/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5334/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5335/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5336/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5337/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5338/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5339/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5340/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5341/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5342/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5343/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5344/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5345/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5346/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5347/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5348/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5349/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5350/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5351/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5352/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5353/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5354/8000 ▶ train MSE: 0.5807, val MSE: 0.3412
Gen 5355/8000 ▶ train MSE: 0.5807, val MSE: 0.3412
Gen 5356/8000 ▶ train MSE: 0.5807, val MSE: 0.3412
Gen 5357/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5358/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5359/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5360/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5361/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5362/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5363/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5364/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5365/8000 ▶ train MSE: 0.5807, val MSE: 0.3405
Gen 5366/8000 ▶ train MSE: 0.5807, val MSE: 0.3402
Gen 5367/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5368/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5369/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5370/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5371/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5372/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5373/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5374/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5375/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5376/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5377/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5378/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5379/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5380/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5381/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5382/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5383/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5384/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5385/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5386/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5387/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5388/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5389/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5390/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5391/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5392/8000 ▶ train MSE: 0.5807, val MSE: 0.3410
Gen 5393/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5394/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5395/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5396/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5397/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5398/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5399/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5400/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5401/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5402/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5403/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5404/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5405/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5406/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5407/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5408/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5409/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5410/8000 ▶ train MSE: 0.5806, val MSE: 0.3407
Gen 5411/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5412/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5413/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5414/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5415/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5416/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5417/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5418/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5419/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5420/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5421/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5422/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5423/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5424/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5425/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5426/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5427/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5428/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5429/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5430/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5431/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5432/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5433/8000 ▶ train MSE: 0.5806, val MSE: 0.3403
Gen 5434/8000 ▶ train MSE: 0.5806, val MSE: 0.3413
Gen 5435/8000 ▶ train MSE: 0.5806, val MSE: 0.3404
Gen 5436/8000 ▶ train MSE: 0.5806, val MSE: 0.3404
Gen 5437/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5438/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5439/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5440/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5441/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5442/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5443/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5444/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5445/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5446/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5447/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5448/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5449/8000 ▶ train MSE: 0.5806, val MSE: 0.3405
Gen 5450/8000 ▶ train MSE: 0.5806, val MSE: 0.3408
Gen 5451/8000 ▶ train MSE: 0.5806, val MSE: 0.3408
Gen 5452/8000 ▶ train MSE: 0.5806, val MSE: 0.3408
Gen 5453/8000 ▶ train MSE: 0.5806, val MSE: 0.3408
Gen 5454/8000 ▶ train MSE: 0.5806, val MSE: 0.3408
Gen 5455/8000 ▶ train MSE: 0.5806, val MSE: 0.3408
Gen 5456/8000 ▶ train MSE: 0.5806, val MSE: 0.3408
Gen 5457/8000 ▶ train MSE: 0.5806, val MSE: 0.3408
Gen 5458/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5459/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5460/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5461/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5462/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5463/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5464/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5465/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5466/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5467/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5468/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5469/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5470/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5471/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5472/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5473/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5474/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5475/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5476/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5477/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5478/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5479/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5480/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5481/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5482/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5483/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5484/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5485/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5486/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5487/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5488/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5489/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5490/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5491/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5492/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5493/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5494/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5495/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5496/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5497/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5498/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5499/8000 ▶ train MSE: 0.5805, val MSE: 0.3408
Gen 5500/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5501/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5502/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5503/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5504/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5505/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5506/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5507/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5508/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5509/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5510/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5511/8000 ▶ train MSE: 0.5805, val MSE: 0.3403
Gen 5512/8000 ▶ train MSE: 0.5804, val MSE: 0.3415
Gen 5513/8000 ▶ train MSE: 0.5804, val MSE: 0.3415
Gen 5514/8000 ▶ train MSE: 0.5804, val MSE: 0.3415
Gen 5515/8000 ▶ train MSE: 0.5804, val MSE: 0.3415
Gen 5516/8000 ▶ train MSE: 0.5804, val MSE: 0.3415
Gen 5517/8000 ▶ train MSE: 0.5804, val MSE: 0.3415
Gen 5518/8000 ▶ train MSE: 0.5804, val MSE: 0.3400
Gen 5519/8000 ▶ train MSE: 0.5804, val MSE: 0.3400
Gen 5520/8000 ▶ train MSE: 0.5804, val MSE: 0.3400
Gen 5521/8000 ▶ train MSE: 0.5804, val MSE: 0.3400
Gen 5522/8000 ▶ train MSE: 0.5804, val MSE: 0.3400
Gen 5523/8000 ▶ train MSE: 0.5804, val MSE: 0.3400
Gen 5524/8000 ▶ train MSE: 0.5804, val MSE: 0.3400
Gen 5525/8000 ▶ train MSE: 0.5804, val MSE: 0.3400
Gen 5526/8000 ▶ train MSE: 0.5804, val MSE: 0.3406
Gen 5527/8000 ▶ train MSE: 0.5804, val MSE: 0.3406
Gen 5528/8000 ▶ train MSE: 0.5804, val MSE: 0.3406
Gen 5529/8000 ▶ train MSE: 0.5804, val MSE: 0.3406
Gen 5530/8000 ▶ train MSE: 0.5804, val MSE: 0.3406
Gen 5531/8000 ▶ train MSE: 0.5804, val MSE: 0.3406
Gen 5532/8000 ▶ train MSE: 0.5804, val MSE: 0.3406
Gen 5533/8000 ▶ train MSE: 0.5804, val MSE: 0.3406
Gen 5534/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5535/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5536/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5537/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5538/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5539/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5540/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5541/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5542/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5543/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5544/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5545/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5546/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5547/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5548/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5549/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5550/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5551/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5552/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5553/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5554/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5555/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5556/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5557/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5558/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5559/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5560/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5561/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5562/8000 ▶ train MSE: 0.5804, val MSE: 0.3407
Gen 5563/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5564/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5565/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5566/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5567/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5568/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5569/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5570/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5571/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5572/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5573/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5574/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5575/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5576/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5577/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5578/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5579/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5580/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5581/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5582/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5583/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5584/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5585/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5586/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5587/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5588/8000 ▶ train MSE: 0.5804, val MSE: 0.3397
Gen 5589/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5590/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5591/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5592/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5593/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5594/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5595/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5596/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5597/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5598/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5599/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5600/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5601/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5602/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5603/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5604/8000 ▶ train MSE: 0.5804, val MSE: 0.3409
Gen 5605/8000 ▶ train MSE: 0.5803, val MSE: 0.3397
Gen 5606/8000 ▶ train MSE: 0.5803, val MSE: 0.3397
Gen 5607/8000 ▶ train MSE: 0.5803, val MSE: 0.3397
Gen 5608/8000 ▶ train MSE: 0.5803, val MSE: 0.3397
Gen 5609/8000 ▶ train MSE: 0.5803, val MSE: 0.3397
Gen 5610/8000 ▶ train MSE: 0.5803, val MSE: 0.3397
Gen 5611/8000 ▶ train MSE: 0.5803, val MSE: 0.3397
Gen 5612/8000 ▶ train MSE: 0.5803, val MSE: 0.3397
Gen 5613/8000 ▶ train MSE: 0.5803, val MSE: 0.3407
Gen 5614/8000 ▶ train MSE: 0.5803, val MSE: 0.3407
Gen 5615/8000 ▶ train MSE: 0.5803, val MSE: 0.3407
Gen 5616/8000 ▶ train MSE: 0.5803, val MSE: 0.3407
Gen 5617/8000 ▶ train MSE: 0.5803, val MSE: 0.3407
Gen 5618/8000 ▶ train MSE: 0.5803, val MSE: 0.3407
Gen 5619/8000 ▶ train MSE: 0.5803, val MSE: 0.3407
Gen 5620/8000 ▶ train MSE: 0.5803, val MSE: 0.3407
Gen 5621/8000 ▶ train MSE: 0.5803, val MSE: 0.3407
Gen 5622/8000 ▶ train MSE: 0.5803, val MSE: 0.3407
Gen 5623/8000 ▶ train MSE: 0.5803, val MSE: 0.3407
Gen 5624/8000 ▶ train MSE: 0.5803, val MSE: 0.3400
Gen 5625/8000 ▶ train MSE: 0.5803, val MSE: 0.3400
Gen 5626/8000 ▶ train MSE: 0.5803, val MSE: 0.3400
Gen 5627/8000 ▶ train MSE: 0.5803, val MSE: 0.3398
Gen 5628/8000 ▶ train MSE: 0.5803, val MSE: 0.3398
Gen 5629/8000 ▶ train MSE: 0.5803, val MSE: 0.3398
Gen 5630/8000 ▶ train MSE: 0.5803, val MSE: 0.3398
Gen 5631/8000 ▶ train MSE: 0.5803, val MSE: 0.3398
Gen 5632/8000 ▶ train MSE: 0.5803, val MSE: 0.3398
Gen 5633/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5634/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5635/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5636/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5637/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5638/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5639/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5640/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5641/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5642/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5643/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5644/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5645/8000 ▶ train MSE: 0.5802, val MSE: 0.3406
Gen 5646/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5647/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5648/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5649/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5650/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5651/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5652/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5653/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5654/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5655/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5656/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5657/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5658/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5659/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5660/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5661/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5662/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5663/8000 ▶ train MSE: 0.5802, val MSE: 0.3395
Gen 5664/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5665/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5666/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5667/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5668/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5669/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5670/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5671/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5672/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5673/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5674/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5675/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5676/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5677/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5678/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5679/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5680/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5681/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5682/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5683/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5684/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5685/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5686/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5687/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5688/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5689/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5690/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5691/8000 ▶ train MSE: 0.5802, val MSE: 0.3400
Gen 5692/8000 ▶ train MSE: 0.5802, val MSE: 0.3407
Gen 5693/8000 ▶ train MSE: 0.5801, val MSE: 0.3404
Gen 5694/8000 ▶ train MSE: 0.5801, val MSE: 0.3404
Gen 5695/8000 ▶ train MSE: 0.5801, val MSE: 0.3404
Gen 5696/8000 ▶ train MSE: 0.5801, val MSE: 0.3404
Gen 5697/8000 ▶ train MSE: 0.5801, val MSE: 0.3404
Gen 5698/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5699/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5700/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5701/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5702/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5703/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5704/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5705/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5706/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5707/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5708/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5709/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5710/8000 ▶ train MSE: 0.5801, val MSE: 0.3403
Gen 5711/8000 ▶ train MSE: 0.5801, val MSE: 0.3402
Gen 5712/8000 ▶ train MSE: 0.5801, val MSE: 0.3402
Gen 5713/8000 ▶ train MSE: 0.5801, val MSE: 0.3402
Gen 5714/8000 ▶ train MSE: 0.5801, val MSE: 0.3402
Gen 5715/8000 ▶ train MSE: 0.5801, val MSE: 0.3402
Gen 5716/8000 ▶ train MSE: 0.5801, val MSE: 0.3402
Gen 5717/8000 ▶ train MSE: 0.5801, val MSE: 0.3402
Gen 5718/8000 ▶ train MSE: 0.5801, val MSE: 0.3402
Gen 5719/8000 ▶ train MSE: 0.5801, val MSE: 0.3402
Gen 5720/8000 ▶ train MSE: 0.5801, val MSE: 0.3402
Gen 5721/8000 ▶ train MSE: 0.5801, val MSE: 0.3402
Gen 5722/8000 ▶ train MSE: 0.5801, val MSE: 0.3399
Gen 5723/8000 ▶ train MSE: 0.5801, val MSE: 0.3399
Gen 5724/8000 ▶ train MSE: 0.5801, val MSE: 0.3399
Gen 5725/8000 ▶ train MSE: 0.5801, val MSE: 0.3399
Gen 5726/8000 ▶ train MSE: 0.5801, val MSE: 0.3399
Gen 5727/8000 ▶ train MSE: 0.5801, val MSE: 0.3399
Gen 5728/8000 ▶ train MSE: 0.5801, val MSE: 0.3399
Gen 5729/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5730/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5731/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5732/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5733/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5734/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5735/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5736/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5737/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5738/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5739/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5740/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5741/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5742/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5743/8000 ▶ train MSE: 0.5800, val MSE: 0.3395
Gen 5744/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5745/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5746/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5747/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5748/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5749/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5750/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5751/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5752/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5753/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5754/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5755/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5756/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5757/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5758/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5759/8000 ▶ train MSE: 0.5799, val MSE: 0.3390
Gen 5760/8000 ▶ train MSE: 0.5799, val MSE: 0.3393
Gen 5761/8000 ▶ train MSE: 0.5799, val MSE: 0.3393
Gen 5762/8000 ▶ train MSE: 0.5799, val MSE: 0.3393
Gen 5763/8000 ▶ train MSE: 0.5799, val MSE: 0.3393
Gen 5764/8000 ▶ train MSE: 0.5799, val MSE: 0.3393
Gen 5765/8000 ▶ train MSE: 0.5799, val MSE: 0.3393
Gen 5766/8000 ▶ train MSE: 0.5799, val MSE: 0.3393
Gen 5767/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5768/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5769/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5770/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5771/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5772/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5773/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5774/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5775/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5776/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5777/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5778/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5779/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5780/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5781/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5782/8000 ▶ train MSE: 0.5798, val MSE: 0.3390
Gen 5783/8000 ▶ train MSE: 0.5798, val MSE: 0.3393
Gen 5784/8000 ▶ train MSE: 0.5798, val MSE: 0.3393
Gen 5785/8000 ▶ train MSE: 0.5798, val MSE: 0.3393
Gen 5786/8000 ▶ train MSE: 0.5798, val MSE: 0.3397
Gen 5787/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5788/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5789/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5790/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5791/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5792/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5793/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5794/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5795/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5796/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5797/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5798/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5799/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5800/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5801/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5802/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5803/8000 ▶ train MSE: 0.5798, val MSE: 0.3394
Gen 5804/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5805/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5806/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5807/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5808/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5809/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5810/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5811/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5812/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5813/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5814/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5815/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5816/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5817/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5818/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5819/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5820/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5821/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5822/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5823/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5824/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5825/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5826/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5827/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5828/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5829/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5830/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5831/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5832/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5833/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5834/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5835/8000 ▶ train MSE: 0.5798, val MSE: 0.3389
Gen 5836/8000 ▶ train MSE: 0.5797, val MSE: 0.3390
Gen 5837/8000 ▶ train MSE: 0.5797, val MSE: 0.3385
Gen 5838/8000 ▶ train MSE: 0.5797, val MSE: 0.3385
Gen 5839/8000 ▶ train MSE: 0.5797, val MSE: 0.3385
Gen 5840/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5841/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5842/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5843/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5844/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5845/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5846/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5847/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5848/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5849/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5850/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5851/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5852/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5853/8000 ▶ train MSE: 0.5797, val MSE: 0.3388
Gen 5854/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5855/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5856/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5857/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5858/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5859/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5860/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5861/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5862/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5863/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5864/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5865/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5866/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5867/8000 ▶ train MSE: 0.5796, val MSE: 0.3394
Gen 5868/8000 ▶ train MSE: 0.5796, val MSE: 0.3387
Gen 5869/8000 ▶ train MSE: 0.5796, val MSE: 0.3387
Gen 5870/8000 ▶ train MSE: 0.5796, val MSE: 0.3387
Gen 5871/8000 ▶ train MSE: 0.5796, val MSE: 0.3387
Gen 5872/8000 ▶ train MSE: 0.5796, val MSE: 0.3387
Gen 5873/8000 ▶ train MSE: 0.5796, val MSE: 0.3386
Gen 5874/8000 ▶ train MSE: 0.5796, val MSE: 0.3386
Gen 5875/8000 ▶ train MSE: 0.5796, val MSE: 0.3386
Gen 5876/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5877/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5878/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5879/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5880/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5881/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5882/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5883/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5884/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5885/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5886/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5887/8000 ▶ train MSE: 0.5796, val MSE: 0.3385
Gen 5888/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5889/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5890/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5891/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5892/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5893/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5894/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5895/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5896/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5897/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5898/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5899/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5900/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5901/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5902/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5903/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5904/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5905/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5906/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5907/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5908/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5909/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5910/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5911/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5912/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5913/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5914/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5915/8000 ▶ train MSE: 0.5795, val MSE: 0.3385
Gen 5916/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5917/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5918/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5919/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5920/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5921/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5922/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5923/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5924/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5925/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5926/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5927/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5928/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5929/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5930/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5931/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5932/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5933/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5934/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5935/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5936/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5937/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5938/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5939/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5940/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5941/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5942/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5943/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5944/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5945/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5946/8000 ▶ train MSE: 0.5795, val MSE: 0.3389
Gen 5947/8000 ▶ train MSE: 0.5795, val MSE: 0.3388
Gen 5948/8000 ▶ train MSE: 0.5795, val MSE: 0.3388
Gen 5949/8000 ▶ train MSE: 0.5795, val MSE: 0.3388
Gen 5950/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5951/8000 ▶ train MSE: 0.5795, val MSE: 0.3385
Gen 5952/8000 ▶ train MSE: 0.5795, val MSE: 0.3385
Gen 5953/8000 ▶ train MSE: 0.5795, val MSE: 0.3385
Gen 5954/8000 ▶ train MSE: 0.5795, val MSE: 0.3385
Gen 5955/8000 ▶ train MSE: 0.5795, val MSE: 0.3385
Gen 5956/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5957/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5958/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5959/8000 ▶ train MSE: 0.5795, val MSE: 0.3390
Gen 5960/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5961/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5962/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5963/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5964/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5965/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5966/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5967/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5968/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5969/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5970/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5971/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5972/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5973/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5974/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5975/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5976/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5977/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5978/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5979/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5980/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5981/8000 ▶ train MSE: 0.5794, val MSE: 0.3392
Gen 5982/8000 ▶ train MSE: 0.5794, val MSE: 0.3389
Gen 5983/8000 ▶ train MSE: 0.5794, val MSE: 0.3389
Gen 5984/8000 ▶ train MSE: 0.5794, val MSE: 0.3389
Gen 5985/8000 ▶ train MSE: 0.5794, val MSE: 0.3389
Gen 5986/8000 ▶ train MSE: 0.5794, val MSE: 0.3389
Gen 5987/8000 ▶ train MSE: 0.5794, val MSE: 0.3390
Gen 5988/8000 ▶ train MSE: 0.5794, val MSE: 0.3390
Gen 5989/8000 ▶ train MSE: 0.5794, val MSE: 0.3393
Gen 5990/8000 ▶ train MSE: 0.5794, val MSE: 0.3393
Gen 5991/8000 ▶ train MSE: 0.5794, val MSE: 0.3389
Gen 5992/8000 ▶ train MSE: 0.5794, val MSE: 0.3389
Gen 5993/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 5994/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 5995/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 5996/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 5997/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 5998/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 5999/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 6000/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 6001/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 6002/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 6003/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 6004/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 6005/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 6006/8000 ▶ train MSE: 0.5794, val MSE: 0.3391
Gen 6007/8000 ▶ train MSE: 0.5793, val MSE: 0.3389
Gen 6008/8000 ▶ train MSE: 0.5793, val MSE: 0.3389
Gen 6009/8000 ▶ train MSE: 0.5793, val MSE: 0.3389
Gen 6010/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6011/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6012/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6013/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6014/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6015/8000 ▶ train MSE: 0.5793, val MSE: 0.3381
Gen 6016/8000 ▶ train MSE: 0.5793, val MSE: 0.3381
Gen 6017/8000 ▶ train MSE: 0.5793, val MSE: 0.3381
Gen 6018/8000 ▶ train MSE: 0.5793, val MSE: 0.3381
Gen 6019/8000 ▶ train MSE: 0.5793, val MSE: 0.3381
Gen 6020/8000 ▶ train MSE: 0.5793, val MSE: 0.3381
Gen 6021/8000 ▶ train MSE: 0.5793, val MSE: 0.3381
Gen 6022/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6023/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6024/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6025/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6026/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6027/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6028/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6029/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6030/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6031/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6032/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6033/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6034/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6035/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6036/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6037/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6038/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6039/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6040/8000 ▶ train MSE: 0.5793, val MSE: 0.3387
Gen 6041/8000 ▶ train MSE: 0.5793, val MSE: 0.3395
Gen 6042/8000 ▶ train MSE: 0.5793, val MSE: 0.3395
Gen 6043/8000 ▶ train MSE: 0.5793, val MSE: 0.3395
Gen 6044/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6045/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6046/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6047/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6048/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6049/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6050/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6051/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6052/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6053/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6054/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6055/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6056/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6057/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6058/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6059/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6060/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6061/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6062/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6063/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6064/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6065/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6066/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6067/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6068/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6069/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6070/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6071/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6072/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6073/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6074/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6075/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6076/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6077/8000 ▶ train MSE: 0.5792, val MSE: 0.3389
Gen 6078/8000 ▶ train MSE: 0.5792, val MSE: 0.3388
Gen 6079/8000 ▶ train MSE: 0.5792, val MSE: 0.3388
Gen 6080/8000 ▶ train MSE: 0.5792, val MSE: 0.3388
Gen 6081/8000 ▶ train MSE: 0.5792, val MSE: 0.3388
Gen 6082/8000 ▶ train MSE: 0.5792, val MSE: 0.3388
Gen 6083/8000 ▶ train MSE: 0.5792, val MSE: 0.3388
Gen 6084/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6085/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6086/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6087/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6088/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6089/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6090/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6091/8000 ▶ train MSE: 0.5792, val MSE: 0.3393
Gen 6092/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6093/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6094/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6095/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6096/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6097/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6098/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6099/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6100/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6101/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6102/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6103/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6104/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6105/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6106/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6107/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6108/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6109/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6110/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6111/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6112/8000 ▶ train MSE: 0.5792, val MSE: 0.3391
Gen 6113/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6114/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6115/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6116/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6117/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6118/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6119/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6120/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6121/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6122/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6123/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6124/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6125/8000 ▶ train MSE: 0.5792, val MSE: 0.3386
Gen 6126/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6127/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6128/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6129/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6130/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6131/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6132/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6133/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6134/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6135/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6136/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6137/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6138/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6139/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6140/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6141/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6142/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6143/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6144/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6145/8000 ▶ train MSE: 0.5791, val MSE: 0.3390
Gen 6146/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6147/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6148/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6149/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6150/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6151/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6152/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6153/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6154/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6155/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6156/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6157/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6158/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6159/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6160/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6161/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6162/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6163/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6164/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6165/8000 ▶ train MSE: 0.5791, val MSE: 0.3389
Gen 6166/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6167/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6168/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6169/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6170/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6171/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6172/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6173/8000 ▶ train MSE: 0.5791, val MSE: 0.3386
Gen 6174/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6175/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6176/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6177/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6178/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6179/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6180/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6181/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6182/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6183/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6184/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6185/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6186/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6187/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6188/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6189/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6190/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6191/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6192/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6193/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6194/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6195/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6196/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6197/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6198/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6199/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6200/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6201/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6202/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6203/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6204/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6205/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6206/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6207/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6208/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6209/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6210/8000 ▶ train MSE: 0.5791, val MSE: 0.3394
Gen 6211/8000 ▶ train MSE: 0.5791, val MSE: 0.3384
Gen 6212/8000 ▶ train MSE: 0.5791, val MSE: 0.3395
Gen 6213/8000 ▶ train MSE: 0.5791, val MSE: 0.3395
Gen 6214/8000 ▶ train MSE: 0.5791, val MSE: 0.3395
Gen 6215/8000 ▶ train MSE: 0.5791, val MSE: 0.3395
Gen 6216/8000 ▶ train MSE: 0.5791, val MSE: 0.3395
Gen 6217/8000 ▶ train MSE: 0.5791, val MSE: 0.3395
Gen 6218/8000 ▶ train MSE: 0.5791, val MSE: 0.3395
Gen 6219/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6220/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6221/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6222/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6223/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6224/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6225/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6226/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6227/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6228/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6229/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6230/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6231/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6232/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6233/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6234/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6235/8000 ▶ train MSE: 0.5790, val MSE: 0.3391
Gen 6236/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6237/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6238/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6239/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6240/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6241/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6242/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6243/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6244/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6245/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6246/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6247/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6248/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6249/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6250/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6251/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6252/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6253/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6254/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6255/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6256/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6257/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6258/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6259/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6260/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6261/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6262/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6263/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6264/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6265/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6266/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6267/8000 ▶ train MSE: 0.5790, val MSE: 0.3390
Gen 6268/8000 ▶ train MSE: 0.5790, val MSE: 0.3390
Gen 6269/8000 ▶ train MSE: 0.5790, val MSE: 0.3390
Gen 6270/8000 ▶ train MSE: 0.5790, val MSE: 0.3390
Gen 6271/8000 ▶ train MSE: 0.5790, val MSE: 0.3390
Gen 6272/8000 ▶ train MSE: 0.5790, val MSE: 0.3390
Gen 6273/8000 ▶ train MSE: 0.5790, val MSE: 0.3390
Gen 6274/8000 ▶ train MSE: 0.5790, val MSE: 0.3390
Gen 6275/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6276/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6277/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6278/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6279/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6280/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6281/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6282/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6283/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6284/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6285/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6286/8000 ▶ train MSE: 0.5790, val MSE: 0.3389
Gen 6287/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6288/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6289/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6290/8000 ▶ train MSE: 0.5790, val MSE: 0.3385
Gen 6291/8000 ▶ train MSE: 0.5790, val MSE: 0.3390
Gen 6292/8000 ▶ train MSE: 0.5789, val MSE: 0.3387
Gen 6293/8000 ▶ train MSE: 0.5789, val MSE: 0.3387
Gen 6294/8000 ▶ train MSE: 0.5789, val MSE: 0.3387
Gen 6295/8000 ▶ train MSE: 0.5789, val MSE: 0.3387
Gen 6296/8000 ▶ train MSE: 0.5789, val MSE: 0.3387
Gen 6297/8000 ▶ train MSE: 0.5789, val MSE: 0.3387
Gen 6298/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6299/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6300/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6301/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6302/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6303/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6304/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6305/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6306/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6307/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6308/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6309/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6310/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6311/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6312/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6313/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6314/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6315/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6316/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6317/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6318/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6319/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6320/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6321/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6322/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6323/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6324/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6325/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6326/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6327/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6328/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6329/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6330/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6331/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6332/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6333/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6334/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6335/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6336/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6337/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6338/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6339/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6340/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6341/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6342/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6343/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6344/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6345/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6346/8000 ▶ train MSE: 0.5789, val MSE: 0.3384
Gen 6347/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6348/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6349/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6350/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6351/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6352/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6353/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6354/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6355/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6356/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6357/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6358/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6359/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6360/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6361/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6362/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6363/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6364/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6365/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6366/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6367/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6368/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6369/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6370/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6371/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6372/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6373/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6374/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6375/8000 ▶ train MSE: 0.5789, val MSE: 0.3378
Gen 6376/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6377/8000 ▶ train MSE: 0.5789, val MSE: 0.3386
Gen 6378/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6379/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6380/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6381/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6382/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6383/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6384/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6385/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6386/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6387/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6388/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6389/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6390/8000 ▶ train MSE: 0.5788, val MSE: 0.3385
Gen 6391/8000 ▶ train MSE: 0.5788, val MSE: 0.3379
Gen 6392/8000 ▶ train MSE: 0.5788, val MSE: 0.3379
Gen 6393/8000 ▶ train MSE: 0.5788, val MSE: 0.3379
Gen 6394/8000 ▶ train MSE: 0.5788, val MSE: 0.3379
Gen 6395/8000 ▶ train MSE: 0.5788, val MSE: 0.3379
Gen 6396/8000 ▶ train MSE: 0.5788, val MSE: 0.3379
Gen 6397/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6398/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6399/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6400/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6401/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6402/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6403/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6404/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6405/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6406/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6407/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6408/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6409/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6410/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6411/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6412/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6413/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6414/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6415/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6416/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6417/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6418/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6419/8000 ▶ train MSE: 0.5788, val MSE: 0.3387
Gen 6420/8000 ▶ train MSE: 0.5788, val MSE: 0.3379
Gen 6421/8000 ▶ train MSE: 0.5788, val MSE: 0.3379
Gen 6422/8000 ▶ train MSE: 0.5788, val MSE: 0.3383
Gen 6423/8000 ▶ train MSE: 0.5788, val MSE: 0.3383
Gen 6424/8000 ▶ train MSE: 0.5788, val MSE: 0.3383
Gen 6425/8000 ▶ train MSE: 0.5788, val MSE: 0.3383
Gen 6426/8000 ▶ train MSE: 0.5788, val MSE: 0.3384
Gen 6427/8000 ▶ train MSE: 0.5788, val MSE: 0.3384
Gen 6428/8000 ▶ train MSE: 0.5788, val MSE: 0.3384
Gen 6429/8000 ▶ train MSE: 0.5788, val MSE: 0.3384
Gen 6430/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6431/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6432/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6433/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6434/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6435/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6436/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6437/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6438/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6439/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6440/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6441/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6442/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6443/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6444/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6445/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6446/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6447/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6448/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6449/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6450/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6451/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6452/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6453/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6454/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6455/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6456/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6457/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6458/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6459/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6460/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6461/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6462/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6463/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6464/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6465/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6466/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6467/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6468/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6469/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6470/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6471/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6472/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6473/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6474/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6475/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6476/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6477/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6478/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6479/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6480/8000 ▶ train MSE: 0.5788, val MSE: 0.3378
Gen 6481/8000 ▶ train MSE: 0.5788, val MSE: 0.3383
Gen 6482/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6483/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6484/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6485/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6486/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6487/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6488/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6489/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6490/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6491/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6492/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6493/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6494/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6495/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6496/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6497/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6498/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6499/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6500/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6501/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6502/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6503/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6504/8000 ▶ train MSE: 0.5787, val MSE: 0.3380
Gen 6505/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6506/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6507/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6508/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6509/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6510/8000 ▶ train MSE: 0.5787, val MSE: 0.3377
Gen 6511/8000 ▶ train MSE: 0.5787, val MSE: 0.3377
Gen 6512/8000 ▶ train MSE: 0.5787, val MSE: 0.3377
Gen 6513/8000 ▶ train MSE: 0.5787, val MSE: 0.3377
Gen 6514/8000 ▶ train MSE: 0.5787, val MSE: 0.3384
Gen 6515/8000 ▶ train MSE: 0.5787, val MSE: 0.3384
Gen 6516/8000 ▶ train MSE: 0.5787, val MSE: 0.3384
Gen 6517/8000 ▶ train MSE: 0.5787, val MSE: 0.3384
Gen 6518/8000 ▶ train MSE: 0.5787, val MSE: 0.3384
Gen 6519/8000 ▶ train MSE: 0.5787, val MSE: 0.3384
Gen 6520/8000 ▶ train MSE: 0.5787, val MSE: 0.3384
Gen 6521/8000 ▶ train MSE: 0.5787, val MSE: 0.3384
Gen 6522/8000 ▶ train MSE: 0.5787, val MSE: 0.3384
Gen 6523/8000 ▶ train MSE: 0.5787, val MSE: 0.3375
Gen 6524/8000 ▶ train MSE: 0.5787, val MSE: 0.3375
Gen 6525/8000 ▶ train MSE: 0.5787, val MSE: 0.3375
Gen 6526/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6527/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6528/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6529/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6530/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6531/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6532/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6533/8000 ▶ train MSE: 0.5787, val MSE: 0.3379
Gen 6534/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6535/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6536/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6537/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6538/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6539/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6540/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6541/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6542/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6543/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6544/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6545/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6546/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6547/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6548/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6549/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6550/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6551/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6552/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6553/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6554/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6555/8000 ▶ train MSE: 0.5786, val MSE: 0.3381
Gen 6556/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6557/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6558/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6559/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6560/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6561/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6562/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6563/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6564/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6565/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6566/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6567/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6568/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6569/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6570/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6571/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6572/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6573/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6574/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6575/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6576/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6577/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6578/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6579/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6580/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6581/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6582/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6583/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6584/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6585/8000 ▶ train MSE: 0.5786, val MSE: 0.3376
Gen 6586/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6587/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6588/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6589/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6590/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6591/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6592/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6593/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6594/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6595/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6596/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6597/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6598/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6599/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6600/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6601/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6602/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6603/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6604/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6605/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6606/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6607/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6608/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6609/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6610/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6611/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6612/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6613/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6614/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6615/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6616/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6617/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6618/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6619/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6620/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6621/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6622/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6623/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6624/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6625/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6626/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6627/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6628/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6629/8000 ▶ train MSE: 0.5785, val MSE: 0.3385
Gen 6630/8000 ▶ train MSE: 0.5785, val MSE: 0.3382
Gen 6631/8000 ▶ train MSE: 0.5785, val MSE: 0.3382
Gen 6632/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6633/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6634/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6635/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6636/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6637/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6638/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6639/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6640/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6641/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6642/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6643/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6644/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6645/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6646/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6647/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6648/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6649/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6650/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6651/8000 ▶ train MSE: 0.5785, val MSE: 0.3379
Gen 6652/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6653/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6654/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6655/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6656/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6657/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6658/8000 ▶ train MSE: 0.5785, val MSE: 0.3375
Gen 6659/8000 ▶ train MSE: 0.5785, val MSE: 0.3373
Gen 6660/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6661/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6662/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6663/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6664/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6665/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6666/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6667/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6668/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6669/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6670/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6671/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6672/8000 ▶ train MSE: 0.5784, val MSE: 0.3382
Gen 6673/8000 ▶ train MSE: 0.5784, val MSE: 0.3382
Gen 6674/8000 ▶ train MSE: 0.5784, val MSE: 0.3382
Gen 6675/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6676/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6677/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6678/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6679/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6680/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6681/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6682/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6683/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6684/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6685/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6686/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6687/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6688/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6689/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6690/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6691/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6692/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6693/8000 ▶ train MSE: 0.5784, val MSE: 0.3380
Gen 6694/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6695/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6696/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6697/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6698/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6699/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6700/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6701/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6702/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6703/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6704/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6705/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6706/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6707/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6708/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6709/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6710/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6711/8000 ▶ train MSE: 0.5784, val MSE: 0.3384
Gen 6712/8000 ▶ train MSE: 0.5784, val MSE: 0.3384
Gen 6713/8000 ▶ train MSE: 0.5784, val MSE: 0.3384
Gen 6714/8000 ▶ train MSE: 0.5784, val MSE: 0.3384
Gen 6715/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6716/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6717/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6718/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6719/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6720/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6721/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6722/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6723/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6724/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6725/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6726/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6727/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6728/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6729/8000 ▶ train MSE: 0.5784, val MSE: 0.3378
Gen 6730/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6731/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6732/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6733/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6734/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6735/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6736/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6737/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6738/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6739/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6740/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6741/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6742/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6743/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6744/8000 ▶ train MSE: 0.5784, val MSE: 0.3375
Gen 6745/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6746/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6747/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6748/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6749/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6750/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6751/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6752/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6753/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6754/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6755/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6756/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6757/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6758/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6759/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6760/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6761/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6762/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6763/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6764/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6765/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6766/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6767/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6768/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6769/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6770/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6771/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6772/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6773/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6774/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6775/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6776/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6777/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6778/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6779/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6780/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6781/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6782/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6783/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6784/8000 ▶ train MSE: 0.5783, val MSE: 0.3376
Gen 6785/8000 ▶ train MSE: 0.5783, val MSE: 0.3378
Gen 6786/8000 ▶ train MSE: 0.5783, val MSE: 0.3378
Gen 6787/8000 ▶ train MSE: 0.5783, val MSE: 0.3378
Gen 6788/8000 ▶ train MSE: 0.5783, val MSE: 0.3378
Gen 6789/8000 ▶ train MSE: 0.5783, val MSE: 0.3378
Gen 6790/8000 ▶ train MSE: 0.5783, val MSE: 0.3378
Gen 6791/8000 ▶ train MSE: 0.5783, val MSE: 0.3378
Gen 6792/8000 ▶ train MSE: 0.5783, val MSE: 0.3377
Gen 6793/8000 ▶ train MSE: 0.5783, val MSE: 0.3377
Gen 6794/8000 ▶ train MSE: 0.5783, val MSE: 0.3377
Gen 6795/8000 ▶ train MSE: 0.5783, val MSE: 0.3377
Gen 6796/8000 ▶ train MSE: 0.5783, val MSE: 0.3377
Gen 6797/8000 ▶ train MSE: 0.5783, val MSE: 0.3377
Gen 6798/8000 ▶ train MSE: 0.5783, val MSE: 0.3377
Gen 6799/8000 ▶ train MSE: 0.5783, val MSE: 0.3377
Gen 6800/8000 ▶ train MSE: 0.5783, val MSE: 0.3377
Gen 6801/8000 ▶ train MSE: 0.5783, val MSE: 0.3377
Gen 6802/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6803/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6804/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6805/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6806/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6807/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6808/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6809/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6810/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6811/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6812/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6813/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6814/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6815/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6816/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6817/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6818/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6819/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6820/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6821/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6822/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6823/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6824/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6825/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6826/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6827/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6828/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6829/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6830/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6831/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6832/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6833/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6834/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6835/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6836/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6837/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6838/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6839/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6840/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6841/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6842/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6843/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6844/8000 ▶ train MSE: 0.5783, val MSE: 0.3375
Gen 6845/8000 ▶ train MSE: 0.5782, val MSE: 0.3379
Gen 6846/8000 ▶ train MSE: 0.5782, val MSE: 0.3379
Gen 6847/8000 ▶ train MSE: 0.5782, val MSE: 0.3379
Gen 6848/8000 ▶ train MSE: 0.5782, val MSE: 0.3379
Gen 6849/8000 ▶ train MSE: 0.5782, val MSE: 0.3379
Gen 6850/8000 ▶ train MSE: 0.5782, val MSE: 0.3379
Gen 6851/8000 ▶ train MSE: 0.5782, val MSE: 0.3379
Gen 6852/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6853/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6854/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6855/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6856/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6857/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6858/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6859/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6860/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6861/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6862/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6863/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6864/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6865/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6866/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6867/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6868/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6869/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6870/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6871/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6872/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6873/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6874/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6875/8000 ▶ train MSE: 0.5782, val MSE: 0.3375
Gen 6876/8000 ▶ train MSE: 0.5782, val MSE: 0.3373
Gen 6877/8000 ▶ train MSE: 0.5782, val MSE: 0.3373
Gen 6878/8000 ▶ train MSE: 0.5782, val MSE: 0.3373
Gen 6879/8000 ▶ train MSE: 0.5782, val MSE: 0.3373
Gen 6880/8000 ▶ train MSE: 0.5782, val MSE: 0.3370
Gen 6881/8000 ▶ train MSE: 0.5782, val MSE: 0.3370
Gen 6882/8000 ▶ train MSE: 0.5782, val MSE: 0.3370
Gen 6883/8000 ▶ train MSE: 0.5782, val MSE: 0.3370
Gen 6884/8000 ▶ train MSE: 0.5781, val MSE: 0.3374
Gen 6885/8000 ▶ train MSE: 0.5781, val MSE: 0.3374
Gen 6886/8000 ▶ train MSE: 0.5781, val MSE: 0.3374
Gen 6887/8000 ▶ train MSE: 0.5781, val MSE: 0.3374
Gen 6888/8000 ▶ train MSE: 0.5781, val MSE: 0.3374
Gen 6889/8000 ▶ train MSE: 0.5781, val MSE: 0.3374
Gen 6890/8000 ▶ train MSE: 0.5781, val MSE: 0.3374
Gen 6891/8000 ▶ train MSE: 0.5781, val MSE: 0.3374
Gen 6892/8000 ▶ train MSE: 0.5781, val MSE: 0.3374
Gen 6893/8000 ▶ train MSE: 0.5781, val MSE: 0.3374
Gen 6894/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6895/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6896/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6897/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6898/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6899/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6900/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6901/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6902/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6903/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6904/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6905/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6906/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6907/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6908/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6909/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6910/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6911/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6912/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6913/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6914/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6915/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6916/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6917/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6918/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6919/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6920/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6921/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6922/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6923/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6924/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6925/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6926/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6927/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6928/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6929/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6930/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6931/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6932/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6933/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6934/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6935/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6936/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6937/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6938/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6939/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6940/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6941/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6942/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6943/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6944/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6945/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6946/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6947/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6948/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6949/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6950/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6951/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6952/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6953/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6954/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6955/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6956/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6957/8000 ▶ train MSE: 0.5781, val MSE: 0.3376
Gen 6958/8000 ▶ train MSE: 0.5781, val MSE: 0.3371
Gen 6959/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6960/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6961/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6962/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6963/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6964/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6965/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6966/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6967/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6968/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6969/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6970/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6971/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6972/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6973/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6974/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6975/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6976/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6977/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6978/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6979/8000 ▶ train MSE: 0.5780, val MSE: 0.3371
Gen 6980/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6981/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6982/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6983/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6984/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6985/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6986/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6987/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6988/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6989/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6990/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6991/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6992/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6993/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6994/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6995/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6996/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6997/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6998/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 6999/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7000/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7001/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7002/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7003/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7004/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7005/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7006/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7007/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7008/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7009/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7010/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7011/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7012/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7013/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7014/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7015/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7016/8000 ▶ train MSE: 0.5780, val MSE: 0.3374
Gen 7017/8000 ▶ train MSE: 0.5780, val MSE: 0.3371
Gen 7018/8000 ▶ train MSE: 0.5780, val MSE: 0.3371
Gen 7019/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7020/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7021/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7022/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7023/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7024/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7025/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7026/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7027/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7028/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7029/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7030/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7031/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7032/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7033/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7034/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7035/8000 ▶ train MSE: 0.5780, val MSE: 0.3367
Gen 7036/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7037/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7038/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7039/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7040/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7041/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7042/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7043/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7044/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7045/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7046/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7047/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7048/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7049/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7050/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7051/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7052/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7053/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7054/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7055/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7056/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7057/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7058/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7059/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7060/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7061/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7062/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7063/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7064/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7065/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7066/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7067/8000 ▶ train MSE: 0.5779, val MSE: 0.3371
Gen 7068/8000 ▶ train MSE: 0.5779, val MSE: 0.3365
Gen 7069/8000 ▶ train MSE: 0.5779, val MSE: 0.3365
Gen 7070/8000 ▶ train MSE: 0.5779, val MSE: 0.3366
Gen 7071/8000 ▶ train MSE: 0.5779, val MSE: 0.3366
Gen 7072/8000 ▶ train MSE: 0.5779, val MSE: 0.3365
Gen 7073/8000 ▶ train MSE: 0.5779, val MSE: 0.3365
Gen 7074/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7075/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7076/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7077/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7078/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7079/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7080/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7081/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7082/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7083/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7084/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7085/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7086/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7087/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7088/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7089/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7090/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7091/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7092/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7093/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7094/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7095/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7096/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7097/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7098/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7099/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7100/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7101/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7102/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7103/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7104/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7105/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7106/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7107/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7108/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7109/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7110/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7111/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7112/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7113/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7114/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7115/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7116/8000 ▶ train MSE: 0.5778, val MSE: 0.3373
Gen 7117/8000 ▶ train MSE: 0.5778, val MSE: 0.3370
Gen 7118/8000 ▶ train MSE: 0.5778, val MSE: 0.3370
Gen 7119/8000 ▶ train MSE: 0.5778, val MSE: 0.3370
Gen 7120/8000 ▶ train MSE: 0.5778, val MSE: 0.3370
Gen 7121/8000 ▶ train MSE: 0.5778, val MSE: 0.3370
Gen 7122/8000 ▶ train MSE: 0.5778, val MSE: 0.3370
Gen 7123/8000 ▶ train MSE: 0.5778, val MSE: 0.3370
Gen 7124/8000 ▶ train MSE: 0.5778, val MSE: 0.3367
Gen 7125/8000 ▶ train MSE: 0.5778, val MSE: 0.3367
Gen 7126/8000 ▶ train MSE: 0.5778, val MSE: 0.3367
Gen 7127/8000 ▶ train MSE: 0.5778, val MSE: 0.3367
Gen 7128/8000 ▶ train MSE: 0.5778, val MSE: 0.3367
Gen 7129/8000 ▶ train MSE: 0.5778, val MSE: 0.3367
Gen 7130/8000 ▶ train MSE: 0.5778, val MSE: 0.3367
Gen 7131/8000 ▶ train MSE: 0.5778, val MSE: 0.3367
Gen 7132/8000 ▶ train MSE: 0.5778, val MSE: 0.3367
Gen 7133/8000 ▶ train MSE: 0.5778, val MSE: 0.3367
Gen 7134/8000 ▶ train MSE: 0.5778, val MSE: 0.3366
Gen 7135/8000 ▶ train MSE: 0.5778, val MSE: 0.3366
Gen 7136/8000 ▶ train MSE: 0.5778, val MSE: 0.3365
Gen 7137/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7138/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7139/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7140/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7141/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7142/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7143/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7144/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7145/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7146/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7147/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7148/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7149/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7150/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7151/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7152/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7153/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7154/8000 ▶ train MSE: 0.5777, val MSE: 0.3368
Gen 7155/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7156/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7157/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7158/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7159/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7160/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7161/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7162/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7163/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7164/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7165/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7166/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7167/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7168/8000 ▶ train MSE: 0.5777, val MSE: 0.3361
Gen 7169/8000 ▶ train MSE: 0.5777, val MSE: 0.3371
Gen 7170/8000 ▶ train MSE: 0.5777, val MSE: 0.3371
Gen 7171/8000 ▶ train MSE: 0.5777, val MSE: 0.3371
Gen 7172/8000 ▶ train MSE: 0.5777, val MSE: 0.3371
Gen 7173/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7174/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7175/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7176/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7177/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7178/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7179/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7180/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7181/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7182/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7183/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7184/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7185/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7186/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7187/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7188/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7189/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7190/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7191/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7192/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7193/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7194/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7195/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7196/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7197/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7198/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7199/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7200/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7201/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7202/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7203/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7204/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7205/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7206/8000 ▶ train MSE: 0.5777, val MSE: 0.3362
Gen 7207/8000 ▶ train MSE: 0.5777, val MSE: 0.3362
Gen 7208/8000 ▶ train MSE: 0.5777, val MSE: 0.3362
Gen 7209/8000 ▶ train MSE: 0.5777, val MSE: 0.3362
Gen 7210/8000 ▶ train MSE: 0.5777, val MSE: 0.3362
Gen 7211/8000 ▶ train MSE: 0.5777, val MSE: 0.3362
Gen 7212/8000 ▶ train MSE: 0.5777, val MSE: 0.3362
Gen 7213/8000 ▶ train MSE: 0.5777, val MSE: 0.3370
Gen 7214/8000 ▶ train MSE: 0.5777, val MSE: 0.3367
Gen 7215/8000 ▶ train MSE: 0.5777, val MSE: 0.3367
Gen 7216/8000 ▶ train MSE: 0.5777, val MSE: 0.3367
Gen 7217/8000 ▶ train MSE: 0.5777, val MSE: 0.3367
Gen 7218/8000 ▶ train MSE: 0.5777, val MSE: 0.3367
Gen 7219/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7220/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7221/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7222/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7223/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7224/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7225/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7226/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7227/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7228/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7229/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7230/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7231/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7232/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7233/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7234/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7235/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7236/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7237/8000 ▶ train MSE: 0.5777, val MSE: 0.3365
Gen 7238/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7239/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7240/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7241/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7242/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7243/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7244/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7245/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7246/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7247/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7248/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7249/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7250/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7251/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7252/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7253/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7254/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7255/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7256/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7257/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7258/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7259/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7260/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7261/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7262/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7263/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7264/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7265/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7266/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7267/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7268/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7269/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7270/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7271/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7272/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7273/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7274/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7275/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7276/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7277/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7278/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7279/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7280/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7281/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7282/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7283/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7284/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7285/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7286/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7287/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7288/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7289/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7290/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7291/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7292/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7293/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7294/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7295/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7296/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7297/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7298/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7299/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7300/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7301/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7302/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7303/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7304/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7305/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7306/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7307/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7308/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7309/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7310/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7311/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7312/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7313/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7314/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7315/8000 ▶ train MSE: 0.5776, val MSE: 0.3361
Gen 7316/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7317/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7318/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7319/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7320/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7321/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7322/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7323/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7324/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7325/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7326/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7327/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7328/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7329/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7330/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7331/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7332/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7333/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7334/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7335/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7336/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7337/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7338/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7339/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7340/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7341/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7342/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7343/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7344/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7345/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7346/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7347/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7348/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7349/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7350/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7351/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7352/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7353/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7354/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7355/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7356/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7357/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7358/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7359/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7360/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7361/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7362/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7363/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7364/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7365/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7366/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7367/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7368/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7369/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7370/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7371/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7372/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7373/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7374/8000 ▶ train MSE: 0.5776, val MSE: 0.3357
Gen 7375/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7376/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7377/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7378/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7379/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7380/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7381/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7382/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7383/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7384/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7385/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7386/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7387/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7388/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7389/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7390/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7391/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7392/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7393/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7394/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7395/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7396/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7397/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7398/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7399/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7400/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7401/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7402/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7403/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7404/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7405/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7406/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7407/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7408/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7409/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7410/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7411/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7412/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7413/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7414/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7415/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7416/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7417/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7418/8000 ▶ train MSE: 0.5776, val MSE: 0.3363
Gen 7419/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7420/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7421/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7422/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7423/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7424/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7425/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7426/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7427/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7428/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7429/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7430/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7431/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7432/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7433/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7434/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7435/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7436/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7437/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7438/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7439/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7440/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7441/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7442/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7443/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7444/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7445/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7446/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7447/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7448/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7449/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7450/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7451/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7452/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7453/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7454/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7455/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7456/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7457/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7458/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7459/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7460/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7461/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7462/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7463/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7464/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7465/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7466/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7467/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7468/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7469/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7470/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7471/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7472/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7473/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7474/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7475/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7476/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7477/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7478/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7479/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7480/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7481/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7482/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7483/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7484/8000 ▶ train MSE: 0.5775, val MSE: 0.3365
Gen 7485/8000 ▶ train MSE: 0.5775, val MSE: 0.3362
Gen 7486/8000 ▶ train MSE: 0.5775, val MSE: 0.3362
Gen 7487/8000 ▶ train MSE: 0.5775, val MSE: 0.3362
Gen 7488/8000 ▶ train MSE: 0.5775, val MSE: 0.3362
Gen 7489/8000 ▶ train MSE: 0.5775, val MSE: 0.3364
Gen 7490/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7491/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7492/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7493/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7494/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7495/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7496/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7497/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7498/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7499/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7500/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7501/8000 ▶ train MSE: 0.5774, val MSE: 0.3360
Gen 7502/8000 ▶ train MSE: 0.5774, val MSE: 0.3360
Gen 7503/8000 ▶ train MSE: 0.5774, val MSE: 0.3360
Gen 7504/8000 ▶ train MSE: 0.5774, val MSE: 0.3362
Gen 7505/8000 ▶ train MSE: 0.5774, val MSE: 0.3362
Gen 7506/8000 ▶ train MSE: 0.5774, val MSE: 0.3357
Gen 7507/8000 ▶ train MSE: 0.5774, val MSE: 0.3357
Gen 7508/8000 ▶ train MSE: 0.5774, val MSE: 0.3357
Gen 7509/8000 ▶ train MSE: 0.5774, val MSE: 0.3357
Gen 7510/8000 ▶ train MSE: 0.5774, val MSE: 0.3357
Gen 7511/8000 ▶ train MSE: 0.5774, val MSE: 0.3357
Gen 7512/8000 ▶ train MSE: 0.5774, val MSE: 0.3357
Gen 7513/8000 ▶ train MSE: 0.5774, val MSE: 0.3357
Gen 7514/8000 ▶ train MSE: 0.5774, val MSE: 0.3357
Gen 7515/8000 ▶ train MSE: 0.5774, val MSE: 0.3357
Gen 7516/8000 ▶ train MSE: 0.5774, val MSE: 0.3357
Gen 7517/8000 ▶ train MSE: 0.5774, val MSE: 0.3363
Gen 7518/8000 ▶ train MSE: 0.5774, val MSE: 0.3363
Gen 7519/8000 ▶ train MSE: 0.5774, val MSE: 0.3363
Gen 7520/8000 ▶ train MSE: 0.5774, val MSE: 0.3363
Gen 7521/8000 ▶ train MSE: 0.5774, val MSE: 0.3363
Gen 7522/8000 ▶ train MSE: 0.5774, val MSE: 0.3363
Gen 7523/8000 ▶ train MSE: 0.5774, val MSE: 0.3363
Gen 7524/8000 ▶ train MSE: 0.5774, val MSE: 0.3363
Gen 7525/8000 ▶ train MSE: 0.5774, val MSE: 0.3363
Gen 7526/8000 ▶ train MSE: 0.5774, val MSE: 0.3363
Gen 7527/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7528/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7529/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7530/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7531/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7532/8000 ▶ train MSE: 0.5774, val MSE: 0.3359
Gen 7533/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7534/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7535/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7536/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7537/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7538/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7539/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7540/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7541/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7542/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7543/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7544/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7545/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7546/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7547/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7548/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7549/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7550/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7551/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7552/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7553/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7554/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7555/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7556/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7557/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7558/8000 ▶ train MSE: 0.5774, val MSE: 0.3354
Gen 7559/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7560/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7561/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7562/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7563/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7564/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7565/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7566/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7567/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7568/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7569/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7570/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7571/8000 ▶ train MSE: 0.5774, val MSE: 0.3358
Gen 7572/8000 ▶ train MSE: 0.5773, val MSE: 0.3359
Gen 7573/8000 ▶ train MSE: 0.5773, val MSE: 0.3359
Gen 7574/8000 ▶ train MSE: 0.5773, val MSE: 0.3359
Gen 7575/8000 ▶ train MSE: 0.5773, val MSE: 0.3359
Gen 7576/8000 ▶ train MSE: 0.5773, val MSE: 0.3359
Gen 7577/8000 ▶ train MSE: 0.5773, val MSE: 0.3359
Gen 7578/8000 ▶ train MSE: 0.5773, val MSE: 0.3359
Gen 7579/8000 ▶ train MSE: 0.5773, val MSE: 0.3359
Gen 7580/8000 ▶ train MSE: 0.5773, val MSE: 0.3359
Gen 7581/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7582/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7583/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7584/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7585/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7586/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7587/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7588/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7589/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7590/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7591/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7592/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7593/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7594/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7595/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7596/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7597/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7598/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7599/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7600/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7601/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7602/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7603/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7604/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7605/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7606/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7607/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7608/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7609/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7610/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7611/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7612/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7613/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7614/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7615/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7616/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7617/8000 ▶ train MSE: 0.5773, val MSE: 0.3351
Gen 7618/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7619/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7620/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7621/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7622/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7623/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7624/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7625/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7626/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7627/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7628/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7629/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7630/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7631/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7632/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7633/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7634/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7635/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7636/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7637/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7638/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7639/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7640/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7641/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7642/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7643/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7644/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7645/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7646/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7647/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7648/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7649/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7650/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7651/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7652/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7653/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7654/8000 ▶ train MSE: 0.5773, val MSE: 0.3356
Gen 7655/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7656/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7657/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7658/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7659/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7660/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7661/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7662/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7663/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7664/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7665/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7666/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7667/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7668/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7669/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7670/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7671/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7672/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7673/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7674/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7675/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7676/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7677/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7678/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7679/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7680/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7681/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7682/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7683/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7684/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7685/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7686/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7687/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7688/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7689/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7690/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7691/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7692/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7693/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7694/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7695/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7696/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7697/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7698/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7699/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7700/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7701/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7702/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7703/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7704/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7705/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7706/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7707/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7708/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7709/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7710/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7711/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7712/8000 ▶ train MSE: 0.5772, val MSE: 0.3355
Gen 7713/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7714/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7715/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7716/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7717/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7718/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7719/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7720/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7721/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7722/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7723/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7724/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7725/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7726/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7727/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7728/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7729/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7730/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7731/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7732/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7733/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7734/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7735/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7736/8000 ▶ train MSE: 0.5772, val MSE: 0.3351
Gen 7737/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7738/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7739/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7740/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7741/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7742/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7743/8000 ▶ train MSE: 0.5772, val MSE: 0.3359
Gen 7744/8000 ▶ train MSE: 0.5772, val MSE: 0.3359
Gen 7745/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7746/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7747/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7748/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7749/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7750/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7751/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7752/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7753/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7754/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7755/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7756/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7757/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7758/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7759/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7760/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7761/8000 ▶ train MSE: 0.5772, val MSE: 0.3357
Gen 7762/8000 ▶ train MSE: 0.5772, val MSE: 0.3353
Gen 7763/8000 ▶ train MSE: 0.5772, val MSE: 0.3353
Gen 7764/8000 ▶ train MSE: 0.5772, val MSE: 0.3353
Gen 7765/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7766/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7767/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7768/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7769/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7770/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7771/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7772/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7773/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7774/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7775/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7776/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7777/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7778/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7779/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7780/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7781/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7782/8000 ▶ train MSE: 0.5772, val MSE: 0.3361
Gen 7783/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7784/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7785/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7786/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7787/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7788/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7789/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7790/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7791/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7792/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7793/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7794/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7795/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7796/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7797/8000 ▶ train MSE: 0.5772, val MSE: 0.3356
Gen 7798/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7799/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7800/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7801/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7802/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7803/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7804/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7805/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7806/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7807/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7808/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7809/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7810/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7811/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7812/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7813/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7814/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7815/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7816/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7817/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7818/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7819/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7820/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7821/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7822/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7823/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7824/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7825/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7826/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7827/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7828/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7829/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7830/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7831/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7832/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7833/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7834/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7835/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7836/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7837/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7838/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7839/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7840/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7841/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7842/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7843/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7844/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7845/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7846/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7847/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7848/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7849/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7850/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7851/8000 ▶ train MSE: 0.5771, val MSE: 0.3357
Gen 7852/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7853/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7854/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7855/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7856/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7857/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7858/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7859/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7860/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7861/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7862/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7863/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7864/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7865/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7866/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7867/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7868/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7869/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7870/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7871/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7872/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7873/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7874/8000 ▶ train MSE: 0.5771, val MSE: 0.3354
Gen 7875/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7876/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7877/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7878/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7879/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7880/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7881/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7882/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7883/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7884/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7885/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7886/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7887/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7888/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7889/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7890/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7891/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7892/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7893/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7894/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7895/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7896/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7897/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7898/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7899/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7900/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7901/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7902/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7903/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7904/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7905/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7906/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7907/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7908/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7909/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7910/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7911/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7912/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7913/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7914/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7915/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7916/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7917/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7918/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7919/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7920/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7921/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7922/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7923/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7924/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7925/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7926/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7927/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7928/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7929/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7930/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7931/8000 ▶ train MSE: 0.5770, val MSE: 0.3351
Gen 7932/8000 ▶ train MSE: 0.5770, val MSE: 0.3357
Gen 7933/8000 ▶ train MSE: 0.5770, val MSE: 0.3357
Gen 7934/8000 ▶ train MSE: 0.5770, val MSE: 0.3356
Gen 7935/8000 ▶ train MSE: 0.5770, val MSE: 0.3356
Gen 7936/8000 ▶ train MSE: 0.5770, val MSE: 0.3356
Gen 7937/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7938/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7939/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7940/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7941/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7942/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7943/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7944/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7945/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7946/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7947/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7948/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7949/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7950/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7951/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7952/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7953/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7954/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7955/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7956/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7957/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7958/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7959/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7960/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7961/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7962/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7963/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7964/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7965/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7966/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7967/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7968/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7969/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7970/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7971/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7972/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7973/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7974/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7975/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7976/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7977/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7978/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7979/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7980/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7981/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7982/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7983/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7984/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7985/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7986/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7987/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7988/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7989/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7990/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7991/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7992/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7993/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7994/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7995/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7996/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7997/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7998/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 7999/8000 ▶ train MSE: 0.5770, val MSE: 0.3352
Gen 8000/8000 ▶ train MSE: 0.5770, val MSE: 0.3352

✅ GA+SGD done!  Final Train MSE: 0.5770, Val MSE: 0.3351
✅ Best model weights, genome and metadata saved to ./checkpoints/
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=36b5a71a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Final-test-set-evaluation-of-all-three-optimisers-best-models:">Final test set evaluation of all three optimisers best models:<a class="anchor-link" href="#Final-test-set-evaluation-of-all-three-optimisers-best-models:">¶</a></h4><h4 id="NOTE-:-All-cells-before-this-must-be-run-before-running-this-one.">NOTE : All cells before this must be run before running this one.<a class="anchor-link" href="#NOTE-:-All-cells-before-this-must-be-run-before-running-this-one.">¶</a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=993aa8dc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># ─── 3) Evaluate each checkpointed model ────────────────────────────────────</span>
<span class="sd">"""</span>
<span class="sd">Evaluates the performance of each trained model (GA, SGD, L-BFGS) on the test set.</span>

<span class="sd">For each method:</span>
<span class="sd">- Loads model architecture and weights from checkpoint</span>
<span class="sd">- Runs forward pass on test data (scaled)</span>
<span class="sd">- Converts predictions back to original scale</span>
<span class="sd">- Computes test MSE and RMSE in both scaled and original units</span>
<span class="sd">- Logs results and saves predictions for downstream use</span>
<span class="sd">"""</span>


<span class="c1">#  Evaluate each checkpointed model </span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">all_preds</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">rmse_results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">method</span><span class="p">,</span> <span class="n">ckpt_dir</span><span class="p">,</span> <span class="n">weight_file</span><span class="p">,</span> <span class="n">meta_file</span> <span class="ow">in</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">"GA"</span><span class="p">,</span>    <span class="s2">"checkpoints"</span><span class="p">,</span>         <span class="s2">"best_model_weights.pth"</span><span class="p">,</span>  <span class="s2">"best_model_meta.json"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"SGD"</span><span class="p">,</span>   <span class="s2">"checkpoints_sgd"</span><span class="p">,</span>     <span class="s2">"sgd_final_weights.pth"</span><span class="p">,</span>   <span class="s2">"sgd_model_meta.json"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"L-BFGS"</span><span class="p">,</span><span class="s2">"checkpoints_lbfgs"</span><span class="p">,</span>   <span class="s2">"lbfgs_final_weights.pth"</span><span class="p">,</span> <span class="s2">"lbfgs_model_meta.json"</span><span class="p">),</span>
<span class="p">]:</span>
    <span class="c1"># load architecture meta</span>
    <span class="n">meta</span>   <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ckpt_dir</span><span class="p">,</span> <span class="n">meta_file</span><span class="p">),</span> <span class="s2">"r"</span><span class="p">))</span>
    <span class="n">arch</span>   <span class="o">=</span> <span class="n">meta</span><span class="p">[</span><span class="s2">"arch"</span><span class="p">]</span>
    <span class="n">model</span>  <span class="o">=</span> <span class="n">build_model_from_arch</span><span class="p">(</span><span class="n">arch</span><span class="p">)</span>
    <span class="c1"># load weights</span>
    <span class="n">state</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ckpt_dir</span><span class="p">,</span> <span class="n">weight_file</span><span class="p">),</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="c1"># forward on test set</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">y_pred_scaled</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">y_pred_orig</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_scaled</span><span class="p">)</span>
    <span class="n">y_true_orig</span> <span class="o">=</span> <span class="n">y_test_np</span>
    <span class="c1"># MSE in scaled and original units</span>
    <span class="n">mse_scaled</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_pred_scaled</span> <span class="o">-</span> <span class="n">y_test</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">mse_original</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_pred_orig</span>   <span class="o">-</span> <span class="n">y_true_orig</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">rmse_original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse_original</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">method</span><span class="si">:</span><span class="s2">6s</span><span class="si">}</span><span class="s2"> → Test MSE (scaled):   </span><span class="si">{</span><span class="n">mse_scaled</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">method</span><span class="si">:</span><span class="s2">6s</span><span class="si">}</span><span class="s2"> → Test MSE (original): </span><span class="si">{</span><span class="n">mse_original</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">method</span><span class="si">:</span><span class="s2">6s</span><span class="si">}</span><span class="s2"> → Test RMSE (original): </span><span class="si">{</span><span class="n">rmse_original</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">method</span><span class="p">,</span> <span class="n">mse_scaled</span><span class="p">,</span> <span class="n">mse_original</span><span class="p">))</span>
    <span class="n">rmse_results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">method</span><span class="p">,</span> <span class="n">rmse_original</span><span class="p">))</span>
    <span class="n">all_preds</span><span class="p">[</span><span class="n">method</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_orig</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

<span class="c1">#  5) Bar‐plot of test MSE (original units) </span>
<span class="n">methods</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">orig_mses</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">results</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">methods</span><span class="p">,</span> <span class="n">orig_mses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">"C0"</span><span class="p">,</span><span class="s2">"C1"</span><span class="p">,</span><span class="s2">"C2"</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Test MSE (original units)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Test-set Performance by Method"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">"--"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">#  5b) Bar‐plot of test RMSE (original units) </span>
<span class="n">methods_rmse</span><span class="p">,</span> <span class="n">rmses</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">rmse_results</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">methods_rmse</span><span class="p">,</span> <span class="n">rmses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">"C0"</span><span class="p">,</span> <span class="s2">"C1"</span><span class="p">,</span> <span class="s2">"C2"</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Test RMSE (original units)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Test-set RMSE by Method"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">"--"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>GA     → Test MSE (scaled):   0.332427
GA     → Test MSE (original): 67.030509
GA     → Test RMSE (original): 8.1872

SGD    → Test MSE (scaled):   0.504205
SGD    → Test MSE (original): 101.667889
SGD    → Test RMSE (original): 10.0830

L-BFGS → Test MSE (scaled):   0.354872
L-BFGS → Test MSE (original): 71.556357
L-BFGS → Test RMSE (original): 8.4591

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
