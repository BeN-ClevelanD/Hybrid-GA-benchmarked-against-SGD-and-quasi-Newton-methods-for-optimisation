<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>project_code</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMTUwIDE1MCA1NDEuOSAyOTUuMyI+CiAgPGcgY2xhc3M9ImpwLWljb24tYnJhbmQyIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxREFGQiI+CiAgICA8cGF0aCBkPSJNNjY2LjMgMjk2LjVjMC0zMi41LTQwLjctNjMuMy0xMDMuMS04Mi40IDE0LjQtNjMuNiA4LTExNC4yLTIwLjItMTMwLjQtNi41LTMuOC0xNC4xLTUuNi0yMi40LTUuNnYyMi4zYzQuNiAwIDguMy45IDExLjQgMi42IDEzLjYgNy44IDE5LjUgMzcuNSAxNC45IDc1LjctMS4xIDkuNC0yLjkgMTkuMy01LjEgMjkuNC0xOS42LTQuOC00MS04LjUtNjMuNS0xMC45LTEzLjUtMTguNS0yNy41LTM1LjMtNDEuNi01MCAzMi42LTMwLjMgNjMuMi00Ni45IDg0LTQ2LjlWNzhjLTI3LjUgMC02My41IDE5LjYtOTkuOSA1My42LTM2LjQtMzMuOC03Mi40LTUzLjItOTkuOS01My4ydjIyLjNjMjAuNyAwIDUxLjQgMTYuNSA4NCA0Ni42LTE0IDE0LjctMjggMzEuNC00MS4zIDQ5LjktMjIuNiAyLjQtNDQgNi4xLTYzLjYgMTEtMi4zLTEwLTQtMTkuNy01LjItMjktNC43LTM4LjIgMS4xLTY3LjkgMTQuNi03NS44IDMtMS44IDYuOS0yLjYgMTEuNS0yLjZWNzguNWMtOC40IDAtMTYgMS44LTIyLjYgNS42LTI4LjEgMTYuMi0zNC40IDY2LjctMTkuOSAxMzAuMS02Mi4yIDE5LjItMTAyLjcgNDkuOS0xMDIuNyA4Mi4zIDAgMzIuNSA0MC43IDYzLjMgMTAzLjEgODIuNC0xNC40IDYzLjYtOCAxMTQuMiAyMC4yIDEzMC40IDYuNSAzLjggMTQuMSA1LjYgMjIuNSA1LjYgMjcuNSAwIDYzLjUtMTkuNiA5OS45LTUzLjYgMzYuNCAzMy44IDcyLjQgNTMuMiA5OS45IDUzLjIgOC40IDAgMTYtMS44IDIyLjYtNS42IDI4LjEtMTYuMiAzNC40LTY2LjcgMTkuOS0xMzAuMSA2Mi0xOS4xIDEwMi41LTQ5LjkgMTAyLjUtODIuM3ptLTEzMC4yLTY2LjdjLTMuNyAxMi45LTguMyAyNi4yLTEzLjUgMzkuNS00LjEtOC04LjQtMTYtMTMuMS0yNC00LjYtOC05LjUtMTUuOC0xNC40LTIzLjQgMTQuMiAyLjEgMjcuOSA0LjcgNDEgNy45em0tNDUuOCAxMDYuNWMtNy44IDEzLjUtMTUuOCAyNi4zLTI0LjEgMzguMi0xNC45IDEuMy0zMCAyLTQ1LjIgMi0xNS4xIDAtMzAuMi0uNy00NS0xLjktOC4zLTExLjktMTYuNC0yNC42LTI0LjItMzgtNy42LTEzLjEtMTQuNS0yNi40LTIwLjgtMzkuOCA2LjItMTMuNCAxMy4yLTI2LjggMjAuNy0zOS45IDcuOC0xMy41IDE1LjgtMjYuMyAyNC4xLTM4LjIgMTQuOS0xLjMgMzAtMiA0NS4yLTIgMTUuMSAwIDMwLjIuNyA0NSAxLjkgOC4zIDExLjkgMTYuNCAyNC42IDI0LjIgMzggNy42IDEzLjEgMTQuNSAyNi40IDIwLjggMzkuOC02LjMgMTMuNC0xMy4yIDI2LjgtMjAuNyAzOS45em0zMi4zLTEzYzUuNCAxMy40IDEwIDI2LjggMTMuOCAzOS44LTEzLjEgMy4yLTI2LjkgNS45LTQxLjIgOCA0LjktNy43IDkuOC0xNS42IDE0LjQtMjMuNyA0LjYtOCA4LjktMTYuMSAxMy0yNC4xek00MjEuMiA0MzBjLTkuMy05LjYtMTguNi0yMC4zLTI3LjgtMzIgOSAuNCAxOC4yLjcgMjcuNS43IDkuNCAwIDE4LjctLjIgMjcuOC0uNy05IDExLjctMTguMyAyMi40LTI3LjUgMzJ6bS03NC40LTU4LjljLTE0LjItMi4xLTI3LjktNC43LTQxLTcuOSAzLjctMTIuOSA4LjMtMjYuMiAxMy41LTM5LjUgNC4xIDggOC40IDE2IDEzLjEgMjQgNC43IDggOS41IDE1LjggMTQuNCAyMy40ek00MjAuNyAxNjNjOS4zIDkuNiAxOC42IDIwLjMgMjcuOCAzMi05LS40LTE4LjItLjctMjcuNS0uNy05LjQgMC0xOC43LjItMjcuOC43IDktMTEuNyAxOC4zLTIyLjQgMjcuNS0zMnptLTc0IDU4LjljLTQuOSA3LjctOS44IDE1LjYtMTQuNCAyMy43LTQuNiA4LTguOSAxNi0xMyAyNC01LjQtMTMuNC0xMC0yNi44LTEzLjgtMzkuOCAxMy4xLTMuMSAyNi45LTUuOCA0MS4yLTcuOXptLTkwLjUgMTI1LjJjLTM1LjQtMTUuMS01OC4zLTM0LjktNTguMy01MC42IDAtMTUuNyAyMi45LTM1LjYgNTguMy01MC42IDguNi0zLjcgMTgtNyAyNy43LTEwLjEgNS43IDE5LjYgMTMuMiA0MCAyMi41IDYwLjktOS4yIDIwLjgtMTYuNiA0MS4xLTIyLjIgNjAuNi05LjktMy4xLTE5LjMtNi41LTI4LTEwLjJ6TTMxMCA0OTBjLTEzLjYtNy44LTE5LjUtMzcuNS0xNC45LTc1LjcgMS4xLTkuNCAyLjktMTkuMyA1LjEtMjkuNCAxOS42IDQuOCA0MSA4LjUgNjMuNSAxMC45IDEzLjUgMTguNSAyNy41IDM1LjMgNDEuNiA1MC0zMi42IDMwLjMtNjMuMiA0Ni45LTg0IDQ2LjktNC41LS4xLTguMy0xLTExLjMtMi43em0yMzcuMi03Ni4yYzQuNyAzOC4yLTEuMSA2Ny45LTE0LjYgNzUuOC0zIDEuOC02LjkgMi42LTExLjUgMi42LTIwLjcgMC01MS40LTE2LjUtODQtNDYuNiAxNC0xNC43IDI4LTMxLjQgNDEuMy00OS45IDIyLjYtMi40IDQ0LTYuMSA2My42LTExIDIuMyAxMC4xIDQuMSAxOS44IDUuMiAyOS4xem0zOC41LTY2LjdjLTguNiAzLjctMTggNy0yNy43IDEwLjEtNS43LTE5LjYtMTMuMi00MC0yMi41LTYwLjkgOS4yLTIwLjggMTYuNi00MS4xIDIyLjItNjAuNiA5LjkgMy4xIDE5LjMgNi41IDI4LjEgMTAuMiAzNS40IDE1LjEgNTguMyAzNC45IDU4LjMgNTAuNi0uMSAxNS43LTIzIDM1LjYtNTguNCA1MC42ek0zMjAuOCA3OC40eiIvPgogICAgPGNpcmNsZSBjeD0iNDIwLjkiIGN5PSIyOTYuNSIgcj0iNDUuNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                messageStyle: 'none',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=37d0e86f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Comparative-Study-of-Optimisation-Algorithms-for-Feed-Forward-Neural-Networks+">Comparative Study of Optimisation Algorithms for Feed-Forward Neural Networks+<a class="anchor-link" href="#Comparative-Study-of-Optimisation-Algorithms-for-Feed-Forward-Neural-Networks+"></a></h1><p>Ben Cleveland - 25504843
DJ Swanevelder - 250205269</p>
<h4 id="Notes:">Notes:<a class="anchor-link" href="#Notes:"></a></h4><ul>
<li>Not all imports are in one place: Sometimes there are piecewise imports in each block. Purposeful, so that not all blocks have to be run in a linearly dependent order.</li>
<li>Docstrings are provided for documentation</li>
<li>Some code has been removed: Some initially tried models and such were removed a long time ago during experimentation: What remains are the important codeblocks, as well as all code that was used in final results, tests or models</li>
</ul>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=0a1e21e2">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Initial-imports-and-setup">Initial imports and setup<a class="anchor-link" href="#Initial-imports-and-setup"></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f00f9b09">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span> 
<span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span> 

<span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Load CSVs</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'Training.csv'</span><span class="p">)</span>
<span class="n">val_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'Validation.csv'</span><span class="p">)</span>

<span class="c1"># Features and targets</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[[</span><span class="s1">'x1'</span><span class="p">,</span> <span class="s1">'x2'</span><span class="p">,</span> <span class="s1">'x3'</span><span class="p">,</span> <span class="s1">'x4'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">'y'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">X_val</span> <span class="o">=</span> <span class="n">val_df</span><span class="p">[[</span><span class="s1">'x1'</span><span class="p">,</span> <span class="s1">'x2'</span><span class="p">,</span> <span class="s1">'x3'</span><span class="p">,</span> <span class="s1">'x4'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">val_df</span><span class="p">[</span><span class="s1">'y'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Standardise</span>
<span class="n">scaler_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">scaler_y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler_X</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">X_val</span> <span class="o">=</span> <span class="n">scaler_X</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_val</span><span class="p">)</span>

<span class="c1"># Convert to tensors</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=a4a72b9a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Architecture-tuning-using-grid-search:">Architecture tuning using grid search:<a class="anchor-link" href="#Architecture-tuning-using-grid-search:"></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8b5c4d03">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Grid Search for Neural Network Architecture Evaluation</span>

<span class="sd">This script performs a simple grid search to evaluate different fully connected</span>
<span class="sd">neural network architectures on a regression task using PyTorch. The model is trained</span>
<span class="sd">on scaled inputs from a training CSV file and validated on a separate validation CSV.</span>

<span class="sd">Steps:</span>
<span class="sd">-------</span>
<span class="sd">1. **Imports and Reproducibility**: Sets global random seeds for NumPy and PyTorch.</span>
<span class="sd">2. **Data Loading and Preprocessing**:</span>
<span class="sd">   - Loads training and validation data from CSV files: 'Training.csv' and 'Validation.csv'.</span>
<span class="sd">   - Applies `StandardScaler` separately on `X` (features) and `y` (targets).</span>
<span class="sd">   - Converts data to PyTorch tensors.</span>
<span class="sd">3. **Architecture Grid Setup**:</span>
<span class="sd">   - A manually defined grid of `(n_layers, n_units)` combinations is iterated over.</span>
<span class="sd">4. **Training and Evaluation**:</span>
<span class="sd">   - For each architecture, a model is built using `nn.Sequential`.</span>
<span class="sd">   - Weights are initialized using Xavier normal.</span>
<span class="sd">   - The model is trained using SGD for 10 epochs.</span>
<span class="sd">   - Both training and validation Mean Squared Error (MSE) are recorded.</span>
<span class="sd">5. **Results Summary**:</span>
<span class="sd">   - Results are saved in a DataFrame and sorted by validation MSE.</span>
<span class="sd">   - The best performing architecture is displayed.</span>

<span class="sd">Returns:</span>
<span class="sd">--------</span>
<span class="sd">Prints a summary table of all architectures sorted by validation MSE,</span>
<span class="sd">as well as the best-performing architecture.</span>

<span class="sd">Instructions for Modifying:</span>
<span class="sd">----------------------------</span>
<span class="sd">- **Input Data**: Replace `'Training.csv'` and `'Validation.csv'` with your own file paths if needed.</span>
<span class="sd">- **Feature Columns**: Modify `['x1','x2','x3','x4']` if your feature column names differ.</span>
<span class="sd">- **Architecture Grid**: Modify the `arch_grid` list to test other layer/unit combinations.</span>
<span class="sd">- **Epochs**: Change the number of training epochs by adjusting the `range(10)` line.</span>
<span class="sd">- **Loss and Optimizer**: Swap out `nn.MSELoss()` or `optim.SGD(...)` for alternatives (e.g., Adam).</span>
<span class="sd">- **Device**: The script currently uses CPU only. To enable GPU, change `device = torch.device("cpu")` to:</span>
<span class="sd">  `device = torch.device("cuda" if torch.cuda.is_available() else "cpu")`</span>
<span class="sd">- **Result Export**: You can write the `df` DataFrame to CSV using `df.to_csv("results.csv", index=False)` if needed.</span>

<span class="sd">Example Output:</span>
<span class="sd">---------------</span>
<span class="sd">Grid-search results (sorted by val MSE):</span>

<span class="sd">   n_layers  n_units  train_mse   val_mse</span>
<span class="sd">0         2       24   0.989429  0.987574</span>
<span class="sd">1         1       24   0.991087  0.987586</span>
<span class="sd">...</span>
<span class="sd">  Best architecture: 2 layers  24 units  (val MSE=0.9876)</span>


<span class="sd">"""</span>

<span class="c1"># 1) Imports &amp; Global Seeding</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># reproducibility</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 2) Load &amp; Preprocess Data (CPU only)</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'Training.csv'</span><span class="p">)</span>
<span class="n">val_df</span>   <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'Validation.csv'</span><span class="p">)</span>

<span class="c1"># split out features / targets</span>
<span class="n">X_train_np</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[[</span><span class="s1">'x1'</span><span class="p">,</span><span class="s1">'x2'</span><span class="p">,</span><span class="s1">'x3'</span><span class="p">,</span><span class="s1">'x4'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">'y'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_val_np</span>   <span class="o">=</span> <span class="n">val_df</span><span class="p">[[</span><span class="s1">'x1'</span><span class="p">,</span><span class="s1">'x2'</span><span class="p">,</span><span class="s1">'x3'</span><span class="p">,</span><span class="s1">'x4'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_val_np</span>   <span class="o">=</span> <span class="n">val_df</span><span class="p">[</span><span class="s1">'y'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">)</span>
<span class="n">scaler_y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y_train_np</span><span class="p">)</span>
<span class="n">X_train_np</span> <span class="o">=</span> <span class="n">scaler_X</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">)</span>
<span class="n">X_val_np</span>   <span class="o">=</span> <span class="n">scaler_X</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val_np</span><span class="p">)</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_train_np</span><span class="p">)</span>
<span class="n">y_val_np</span>   <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_val_np</span><span class="p">)</span>

<span class="c1"># to torch tensors</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_train_np</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_train_np</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_val</span>   <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_val_np</span><span class="p">,</span>   <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_val</span>   <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_val_np</span><span class="p">,</span>   <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># 3) Grid of architectures to try</span>

<span class="n">arch_grid</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span><span class="p">),</span>  
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">24</span><span class="p">),</span>   
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>  
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>  
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>   
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>   
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>  
<span class="p">]</span>

<span class="c1"># 4) Training loop</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">n_units</span> <span class="ow">in</span> <span class="n">arch_grid</span><span class="p">:</span>

    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_f</span>   <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">n_units</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">n_units</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

  
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

   
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># train 10 epochs</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># evaluate</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">train_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">val_mse</span>   <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_val</span><span class="p">),</span>   <span class="n">y_val</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
        <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="n">n_layers</span><span class="p">,</span>
        <span class="s2">"n_units"</span><span class="p">:</span>    <span class="n">n_units</span><span class="p">,</span>
        <span class="s2">"train_mse"</span><span class="p">:</span>  <span class="n">train_mse</span><span class="p">,</span>
        <span class="s2">"val_mse"</span><span class="p">:</span>    <span class="n">val_mse</span>
    <span class="p">})</span>

<span class="c1"># 5) Summarize</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">"val_mse"</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Grid-search results (sorted by val MSE):</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="n">best</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">  Best architecture: "</span>
      <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">n_layers</span><span class="si">}</span><span class="s2"> layers  </span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">n_units</span><span class="si">}</span><span class="s2"> units  "</span>
      <span class="sa">f</span><span class="s2">"(val MSE=</span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">val_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Grid-search results (sorted by val MSE):

   n_layers  n_units  train_mse   val_mse
0         2       24   0.989429  0.987574
1         1       24   0.991087  0.987586
2         2       20   0.989687  0.988779
3         2       16   0.991191  0.990116
4         1       16   0.995888  0.994841
5         2        8   0.995200  0.996152
6         1        8   0.998301  0.999128

  Best architecture: 2.0 layers  24.0 units  (val MSE=0.9876)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=33f1b375">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Heaptmap-for-displaying-arch:">Heaptmap for displaying arch:<a class="anchor-link" href="#Heaptmap-for-displaying-arch:"></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=561f5316">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[62]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># 1) Reconstruct your grid-search results</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span> <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.987574</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span> <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.987586</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span> <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.988779</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.990116</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.994841</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.996152</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'n_units'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="s1">'val_mse'</span><span class="p">:</span> <span class="mf">0.999128</span><span class="p">},</span>
<span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># 2) Pivot into matrix form</span>
<span class="n">pivot</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s1">'n_layers'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s1">'n_units'</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s1">'val_mse'</span><span class="p">)</span>

<span class="c1"># 3) Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pivot</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'plasma'</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">)</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">'Validation MSE'</span><span class="p">)</span>

<span class="c1"># 4) Axes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pivot</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">pivot</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pivot</span><span class="o">.</span><span class="n">index</span><span class="p">)),</span> <span class="n">pivot</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Number of Units'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Number of Layers'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Validation MSE Heatmap'</span><span class="p">)</span>

<span class="c1"># 5) Annotate each cell with white text</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pivot</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pivot</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">pivot</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span>
                 <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=99a3e448">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Using-Optuna-to-do-hyperparameter-tuning-for-our-stochastic-gradient-descent-implementation:-Finding-optimal-learning-rate-and-momentum-value.">Using Optuna to do hyperparameter tuning for our stochastic gradient descent implementation: Finding optimal learning rate and momentum value.<a class="anchor-link" href="#Using-Optuna-to-do-hyperparameter-tuning-for-our-stochastic-gradient-descent-implementation:-Finding-optimal-learning-rate-and-momentum-value."></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2980ba10">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Script for hyperparameter tuning using Optuna on a fixed neural network architecture.</span>

<span class="sd">This script builds a simple feedforward neural network (FFN) with two hidden layers </span>
<span class="sd">and performs hyperparameter optimization using Optuna, specifically tuning learning </span>
<span class="sd">rate (`lr`) and momentum (`momentum`) for stochastic gradient descent (SGD). </span>
<span class="sd">The training is done using a batch size of 1 to simulate pure SGD. The goal is to </span>
<span class="sd">minimize the mean squared error (MSE) on the validation set.</span>

<span class="sd">Sections:</span>
<span class="sd">---------</span>
<span class="sd">1. Fixed Model Definition:</span>
<span class="sd">    - Uses 2 hidden layers with 24 ReLU units each (modifiable).</span>
<span class="sd">    - Xavier initialization for weights and zero initialization for biases.</span>

<span class="sd">2. Optuna Search Objective:</span>
<span class="sd">    - Tunes learning rate (log-uniform between 1e-5 and 1e-1).</span>
<span class="sd">    - Tunes momentum (uniform between 0 and 0.99).</span>
<span class="sd">    - Trains for 10 epochs using batch size = 1.</span>

<span class="sd">3. Search Execution:</span>
<span class="sd">    - Runs 20 trials of optimization.</span>
<span class="sd">    - Reports the best validation MSE and corresponding hyperparameters.</span>

<span class="sd">How to Modify:</span>
<span class="sd">--------------</span>
<span class="sd"> **Architecture**: Change the number of hidden layers or units per layer </span>
<span class="sd">  by editing `build_fixed_model()`. For example, change range(2) to range(3) for </span>
<span class="sd">  3 hidden layers or change 24 to any other value for hidden size.</span>

<span class="sd"> **Loss Function**: Change `nn.MSELoss()` to other loss functions like MAE if needed.</span>

<span class="sd"> **Search Space**: To tune additional hyperparameters (e.g., batch size, number of units),</span>
<span class="sd">  add them to the `trial.suggest_*` section in `objective()` and modify `build_fixed_model()` accordingly.</span>

<span class="sd"> **Training Epochs**: Change the loop range in `objective()` (currently 10) to control how long </span>
<span class="sd">  each trial is trained for.</span>

<span class="sd"> **Batch Size**: To switch from pure SGD to minibatch gradient descent, change </span>
<span class="sd">  `batch_size=1` to another value in the DataLoader inside `objective()`.</span>

<span class="sd">Dependencies:</span>
<span class="sd">-------------</span>
<span class="sd">- torch</span>
<span class="sd">- numpy</span>
<span class="sd">- pandas</span>
<span class="sd">- optuna</span>
<span class="sd">- sklearn</span>

<span class="sd">Expected Inputs:</span>
<span class="sd">----------------</span>
<span class="sd">- `X_train`, `y_train`: torch.Tensors (features and labels for training)</span>
<span class="sd">- `X_val`, `y_val`: torch.Tensors (features and labels for validation)</span>
<span class="sd">  These must be preloaded in memory before running this script.</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Console log showing best hyperparameters and corresponding validation MSE.</span>

<span class="sd">"""</span>


<span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># reproducibility</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 1) Fixed architecture builder</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_fixed_model</span><span class="p">():</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_f</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>   
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>        
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">24</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="mi">24</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    <span class="c1"># Xavier init</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
<span class="c1"># 2) Optuna objective  only lr &amp; momentum, fix batch_size=1</span>
<span class="k">def</span><span class="w"> </span><span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="c1"># Two params to tune</span>
    <span class="n">lr</span>       <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_loguniform</span><span class="p">(</span><span class="s2">"lr"</span><span class="p">,</span>      <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span>  <span class="s2">"momentum"</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>

    <span class="c1">#  DataLoader with batch_size=1</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

   <span class="c1"># Build model, loss &amp; optimizer</span>
    <span class="n">model</span>     <span class="o">=</span> <span class="n">build_fixed_model</span><span class="p">()</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>

    <span class="c1">#  Train for 80 epochs</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1">#  Validation</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_val</span><span class="p">),</span> <span class="n">y_val</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">val_mse</span>

<span class="c1"># 3) Run the search</span>
<span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">"minimize"</span><span class="p">)</span>
<span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 4) Report best</span>
<span class="n">best</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">=== Best Hyperparameters (pure SGD) ==="</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  lr:       </span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">]</span><span class="si">:</span><span class="s2">.5g</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  momentum: </span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">'momentum'</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best Val MSE: </span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">value</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>[I 2025-05-28 18:42:30,770] A new study created in memory with name: no-name-29cfe234-123d-4b6d-a9b0-7c3f774a1328
  0%|          | 0/20 [00:00&lt;?, ?it/s]/tmp/ipykernel_5499/2176302026.py:31: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr       = trial.suggest_loguniform("lr",      1e-5, 1e-1)
Best trial: 0. Best value: 1.01949:   5%|         | 1/20 [01:14&lt;23:44, 74.96s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:43:45,730] Trial 0 finished with value: 1.019486665725708 and parameters: {'lr': 0.07722597496556405, 'momentum': 0.68862897675173}. Best is trial 0 with value: 1.019486665725708.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  10%|         | 2/20 [02:30&lt;22:32, 75.13s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:45:00,975] Trial 1 finished with value: 0.8632642030715942 and parameters: {'lr': 0.002491034301035869, 'momentum': 0.6706817605204999}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  15%|        | 3/20 [03:44&lt;21:11, 74.78s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:46:15,345] Trial 2 finished with value: 1.008341908454895 and parameters: {'lr': 0.02926027577397731, 'momentum': 0.11218386033409711}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  20%|        | 4/20 [04:59&lt;19:54, 74.65s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:47:29,802] Trial 3 finished with value: 0.9851898550987244 and parameters: {'lr': 0.00893774459776824, 'momentum': 0.5399565006032588}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  25%|       | 5/20 [06:13&lt;18:39, 74.63s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:48:44,391] Trial 4 finished with value: 0.9964484572410583 and parameters: {'lr': 4.80326942588253e-05, 'momentum': 0.08237666225046819}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  30%|       | 6/20 [07:27&lt;17:22, 74.50s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:49:58,629] Trial 5 finished with value: 0.9977979063987732 and parameters: {'lr': 1.3879333944853265e-05, 'momentum': 0.621887448671995}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  35%|      | 7/20 [08:42&lt;16:09, 74.55s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 18:51:13,284] Trial 6 failed with parameters: {'lr': 0.0693368760229596, 'momentum': 0.6561466262131057} because of the following error: The value nan is not acceptable.
[W 2025-05-28 18:51:13,287] Trial 6 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  40%|      | 8/20 [09:56&lt;14:53, 74.49s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:52:27,660] Trial 7 finished with value: 0.8716850280761719 and parameters: {'lr': 0.004083592137636449, 'momentum': 0.1948592692793183}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  45%|     | 9/20 [11:11&lt;13:39, 74.50s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:53:42,167] Trial 8 finished with value: 0.9257376790046692 and parameters: {'lr': 0.009614876416667221, 'momentum': 0.39769380793625425}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  50%|     | 10/20 [12:26&lt;12:25, 74.55s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:54:56,846] Trial 9 finished with value: 0.9045374989509583 and parameters: {'lr': 0.0022883990537292363, 'momentum': 0.7195431992810583}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  55%|    | 11/20 [13:40&lt;11:10, 74.46s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:56:11,099] Trial 10 finished with value: 0.9899379014968872 and parameters: {'lr': 0.00010944626772812799, 'momentum': 0.3367779849354335}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  60%|    | 12/20 [14:54&lt;09:55, 74.45s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:57:25,513] Trial 11 finished with value: 0.9432251453399658 and parameters: {'lr': 0.0002997376745477513, 'momentum': 0.9790257780084946}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 1. Best value: 0.863264:  65%|   | 13/20 [16:08&lt;08:39, 74.28s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:58:39,417] Trial 12 finished with value: 0.965964138507843 and parameters: {'lr': 0.0015662068029184029, 'momentum': 0.2451522199571834}. Best is trial 1 with value: 0.8632642030715942.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 13. Best value: 0.850097:  70%|   | 14/20 [17:22&lt;07:25, 74.24s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 18:59:53,570] Trial 13 finished with value: 0.8500973582267761 and parameters: {'lr': 0.0005680703916186882, 'momentum': 0.8732847818815674}. Best is trial 13 with value: 0.8500973582267761.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692:  75%|  | 15/20 [18:36&lt;06:10, 74.19s/it] </pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:01:07,647] Trial 14 finished with value: 0.7969197630882263 and parameters: {'lr': 0.0004872119511159354, 'momentum': 0.8766164307306744}. Best is trial 14 with value: 0.7969197630882263.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692:  80%|  | 16/20 [19:50&lt;04:56, 74.16s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:02:21,717] Trial 15 finished with value: 0.908641517162323 and parameters: {'lr': 0.00037484338849541106, 'momentum': 0.9445571334860606}. Best is trial 14 with value: 0.7969197630882263.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692:  85%| | 17/20 [21:04&lt;03:42, 74.10s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:03:35,671] Trial 16 finished with value: 0.8646453022956848 and parameters: {'lr': 0.00048517453693837256, 'momentum': 0.8251558057957358}. Best is trial 14 with value: 0.7969197630882263.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692:  90%| | 18/20 [22:19&lt;02:28, 74.10s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:04:49,793] Trial 17 finished with value: 0.9784354567527771 and parameters: {'lr': 9.173144461117463e-05, 'momentum': 0.8073786826673773}. Best is trial 14 with value: 0.7969197630882263.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692:  95%|| 19/20 [23:32&lt;01:14, 74.05s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:06:03,735] Trial 18 finished with value: 0.9212556481361389 and parameters: {'lr': 0.0008223197638385264, 'momentum': 0.8744406649023867}. Best is trial 14 with value: 0.7969197630882263.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 14. Best value: 0.79692: 100%|| 20/20 [24:46&lt;00:00, 74.32s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 19:07:17,238] Trial 19 finished with value: 0.9900429248809814 and parameters: {'lr': 0.00016307254326817386, 'momentum': 0.522156351647615}. Best is trial 14 with value: 0.7969197630882263.

=== Best Hyperparameters (pure SGD) ===
  lr:       0.00048721
  momentum: 0.8766
Best Val MSE: 0.79692
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=93d78023">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="SGD-run-using-parameters-found-with-optuna:">SGD run using parameters found with optuna:<a class="anchor-link" href="#SGD-run-using-parameters-found-with-optuna:"></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=db20497a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Final Feed-Forward Network (FFN) Training Script with Best Hyperparameters (SGD)</span>

<span class="sd">Overview:</span>
<span class="sd">---------</span>
<span class="sd">This script trains a fixed-architecture FFN on preprocessed training data using</span>
<span class="sd">Stochastic Gradient Descent (SGD) with Optuna-optimized hyperparameters. It logs</span>
<span class="sd">performance metrics (MSE) per epoch, plots loss curves, and saves the model, weights,</span>
<span class="sd">and architecture metadata for future reuse.</span>

<span class="sd">Sections:</span>
<span class="sd">---------</span>
<span class="sd">1. Device Setup          Forces CPU or uses CUDA if available.</span>
<span class="sd">2. Hyperparameter Dict   Update this section with new best hyperparameters if needed.</span>
<span class="sd">3. Data Preparation      Assumes X_train, y_train, X_val, y_val are already scaled and defined.</span>
<span class="sd">4. Model Construction    Dynamically builds a model with specified layers/activation.</span>
<span class="sd">5. Training Loop         Runs for 100 epochs with SGD, logs per-epoch MSE to CSV.</span>
<span class="sd">6. Performance Plotting  Visualises train vs val loss curves.</span>
<span class="sd">7. Checkpointing         Saves model weights, flattened vector, and architecture metadata.</span>

<span class="sd">Customisation Instructions:</span>
<span class="sd">---------------------------</span>
<span class="sd">- To change the architecture:</span>
<span class="sd">   Edit `best_params["n_layers"]` or `["n_units"]`, and optionally the activation function.</span>
<span class="sd">- To change the training duration:</span>
<span class="sd">   Modify the `epochs` variable (currently set to 100).</span>
<span class="sd">- To use a different batch size or optimizer:</span>
<span class="sd">   Update the `best_params["batch_size"]` and/or swap out `optim.SGD` with another optimizer.</span>
<span class="sd">- To enable GPU:</span>
<span class="sd">   Uncomment the `cuda` line in device setup.</span>
<span class="sd">- To retrain with different data:</span>
<span class="sd">   Make sure to redefine `X_train`, `y_train`, `X_val`, `y_val` with properly preprocessed tensors.</span>

<span class="sd">Output:</span>
<span class="sd">-------</span>
<span class="sd">- `checkpoints_sgd/sgd_final_weights.pth`         Trained PyTorch state_dict.</span>
<span class="sd">- `checkpoints_sgd/sgd_final_genome.npy`          Flattened model parameter vector.</span>
<span class="sd">- `checkpoints_sgd/sgd_model_meta.json`           Model and training metadata.</span>
<span class="sd">- `sgd_stats.csv`                                 CSV log of train/val MSE per epoch.</span>
<span class="sd">- Train vs Val loss plot displayed at end.</span>
<span class="sd">"""</span>

<span class="c1">#  Final FFN Training with Best Hyperparameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># 1) Device: CPU </span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="n">best_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"lr"</span><span class="p">:</span>         <span class="mf">0.00048721</span><span class="p">,</span>
    <span class="s2">"momentum"</span><span class="p">:</span>   <span class="mf">0.8766</span><span class="p">,</span>
    <span class="s2">"batch_size"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"n_units"</span><span class="p">:</span>    <span class="mi">24</span><span class="p">,</span>
    <span class="s2">"activation"</span><span class="p">:</span> <span class="s2">"ReLU"</span>
<span class="p">}</span>
<span class="c1"># 3) Prepare DataLoaders (assumes X_train, y_train, X_val, y_val already defined &amp; scaled)</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">val_ds</span>   <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">],</span>
                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># we'll just evaluate on the full validation set each epoch</span>
<span class="n">Xv</span><span class="p">,</span> <span class="n">yv</span> <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 4) Build the model</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">in_f</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">Act</span>  <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
    <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
    <span class="n">in_f</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 5) Optimizer &amp; loss</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                      <span class="n">lr</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span>
                      <span class="n">momentum</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"momentum"</span><span class="p">])</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">SGD_LOG_FN</span> <span class="o">=</span> <span class="s2">"sgd_stats.csv"</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">SGD_LOG_FN</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">"epoch,epoch_time,train_mse,val_mse</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1"># 6) Train for 100 epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">running</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">running</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">xb</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">train_mse</span> <span class="o">=</span> <span class="n">running</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_mse</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">Xv</span><span class="p">),</span> <span class="n">yv</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_mse</span><span class="p">)</span>
    <span class="n">epoch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">SGD_LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">epoch_time</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">train_mse</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">val_mse</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
   
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s2">3d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">  train MSE: </span><span class="si">{</span><span class="n">train_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">val_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> Training complete!"</span><span class="p">)</span>

<span class="c1"># 7) Plot curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_losses</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Final FFN Training &amp; Validation Loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span><span class="o">,</span><span class="w"> </span><span class="nn">json</span><span class="o">,</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span>

<span class="c1"># Saving params for later re use:</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">"checkpoints_sgd"</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 1) Save the PyTorch state_dict</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
           <span class="s2">"checkpoints_sgd/sgd_final_weights.pth"</span><span class="p">)</span>

<span class="c1"># 2) Save the flat parameter vector (so you can reload via vector_to_parameters)</span>
<span class="n">genome_sgd</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"checkpoints_sgd/sgd_final_genome.npy"</span><span class="p">,</span> <span class="n">genome_sgd</span><span class="p">)</span>

<span class="c1"># 3) Save the architecture &amp; SGD hyperparams so you can rebuild the same model later</span>
<span class="n">meta_sgd</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"arch"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">],</span>
        <span class="s2">"n_units"</span><span class="p">:</span>    <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">],</span>
        <span class="s2">"activation"</span><span class="p">:</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s2">"sgd_params"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"lr"</span><span class="p">:</span>       <span class="n">best_params</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span>
        <span class="s2">"momentum"</span><span class="p">:</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"momentum"</span><span class="p">],</span>
        <span class="s2">"batch_size"</span><span class="p">:</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">],</span>
        <span class="s2">"epochs"</span><span class="p">:</span>    <span class="n">epochs</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"checkpoints_sgd/sgd_model_meta.json"</span><span class="p">,</span><span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">meta_sgd</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> SGDtrained model and metadata saved to `checkpoints_sgd/`"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cpu

Epoch   1/100  train MSE: 1.0027, val MSE: 0.9923
Epoch   2/100  train MSE: 0.9970, val MSE: 0.9871
Epoch   3/100  train MSE: 0.9940, val MSE: 0.9912
Epoch   4/100  train MSE: 0.9920, val MSE: 0.9927
Epoch   5/100  train MSE: 0.9908, val MSE: 0.9839
Epoch   6/100  train MSE: 0.9884, val MSE: 0.9777
Epoch   7/100  train MSE: 0.9875, val MSE: 0.9817
Epoch   8/100  train MSE: 0.9840, val MSE: 0.9875
Epoch   9/100  train MSE: 0.9798, val MSE: 0.9822
Epoch  10/100  train MSE: 0.9705, val MSE: 0.9191
Epoch  11/100  train MSE: 0.9517, val MSE: 0.8930
Epoch  12/100  train MSE: 0.9389, val MSE: 0.8855
Epoch  13/100  train MSE: 0.9265, val MSE: 0.8699
Epoch  14/100  train MSE: 0.9180, val MSE: 0.8634
Epoch  15/100  train MSE: 0.9126, val MSE: 0.8345
Epoch  16/100  train MSE: 0.9068, val MSE: 0.8344
Epoch  17/100  train MSE: 0.9012, val MSE: 0.8410
Epoch  18/100  train MSE: 0.8934, val MSE: 0.8000
Epoch  19/100  train MSE: 0.8846, val MSE: 0.7904
Epoch  20/100  train MSE: 0.8795, val MSE: 0.7870
Epoch  21/100  train MSE: 0.8638, val MSE: 0.7344
Epoch  22/100  train MSE: 0.8420, val MSE: 0.7129
Epoch  23/100  train MSE: 0.8257, val MSE: 0.7024
Epoch  24/100  train MSE: 0.8131, val MSE: 0.6714
Epoch  25/100  train MSE: 0.8010, val MSE: 0.6892
Epoch  26/100  train MSE: 0.7911, val MSE: 0.6552
Epoch  27/100  train MSE: 0.7783, val MSE: 0.6357
Epoch  28/100  train MSE: 0.7695, val MSE: 0.5944
Epoch  29/100  train MSE: 0.7676, val MSE: 0.6008
Epoch  30/100  train MSE: 0.7631, val MSE: 0.6278
Epoch  31/100  train MSE: 0.7573, val MSE: 0.6090
Epoch  32/100  train MSE: 0.7562, val MSE: 0.5965
Epoch  33/100  train MSE: 0.7523, val MSE: 0.5988
Epoch  34/100  train MSE: 0.7494, val MSE: 0.5927
Epoch  35/100  train MSE: 0.7429, val MSE: 0.5939
Epoch  36/100  train MSE: 0.7417, val MSE: 0.5846
Epoch  37/100  train MSE: 0.7372, val MSE: 0.5912
Epoch  38/100  train MSE: 0.7297, val MSE: 0.5761
Epoch  39/100  train MSE: 0.7294, val MSE: 0.5668
Epoch  40/100  train MSE: 0.7289, val MSE: 0.5726
Epoch  41/100  train MSE: 0.7263, val MSE: 0.5957
Epoch  42/100  train MSE: 0.7253, val MSE: 0.5361
Epoch  43/100  train MSE: 0.7246, val MSE: 0.5359
Epoch  44/100  train MSE: 0.7210, val MSE: 0.5702
Epoch  45/100  train MSE: 0.7213, val MSE: 0.6333
Epoch  46/100  train MSE: 0.7203, val MSE: 0.5624
Epoch  47/100  train MSE: 0.7194, val MSE: 0.5596
Epoch  48/100  train MSE: 0.7186, val MSE: 0.5529
Epoch  49/100  train MSE: 0.7162, val MSE: 0.5251
Epoch  50/100  train MSE: 0.7143, val MSE: 0.5610
Epoch  51/100  train MSE: 0.7145, val MSE: 0.5601
Epoch  52/100  train MSE: 0.7142, val MSE: 0.5127
Epoch  53/100  train MSE: 0.7138, val MSE: 0.5340
Epoch  54/100  train MSE: 0.7112, val MSE: 0.5767
Epoch  55/100  train MSE: 0.7116, val MSE: 0.5227
Epoch  56/100  train MSE: 0.7125, val MSE: 0.5177
Epoch  57/100  train MSE: 0.7105, val MSE: 0.5475
Epoch  58/100  train MSE: 0.7101, val MSE: 0.5247
Epoch  59/100  train MSE: 0.7117, val MSE: 0.5013
Epoch  60/100  train MSE: 0.7096, val MSE: 0.5332
Epoch  61/100  train MSE: 0.7110, val MSE: 0.5622
Epoch  62/100  train MSE: 0.7091, val MSE: 0.5358
Epoch  63/100  train MSE: 0.7097, val MSE: 0.5416
Epoch  64/100  train MSE: 0.7086, val MSE: 0.5348
Epoch  65/100  train MSE: 0.7091, val MSE: 0.5439
Epoch  66/100  train MSE: 0.7099, val MSE: 0.5217
Epoch  67/100  train MSE: 0.7081, val MSE: 0.5677
Epoch  68/100  train MSE: 0.7086, val MSE: 0.5454
Epoch  69/100  train MSE: 0.7098, val MSE: 0.5249
Epoch  70/100  train MSE: 0.7082, val MSE: 0.5075
Epoch  71/100  train MSE: 0.7086, val MSE: 0.5346
Epoch  72/100  train MSE: 0.7065, val MSE: 0.5194
Epoch  73/100  train MSE: 0.7078, val MSE: 0.5315
Epoch  74/100  train MSE: 0.7058, val MSE: 0.5479
Epoch  75/100  train MSE: 0.7081, val MSE: 0.5534
Epoch  76/100  train MSE: 0.7062, val MSE: 0.5462
Epoch  77/100  train MSE: 0.7062, val MSE: 0.5383
Epoch  78/100  train MSE: 0.7070, val MSE: 0.5786
Epoch  79/100  train MSE: 0.7042, val MSE: 0.5318
Epoch  80/100  train MSE: 0.7064, val MSE: 0.6006
Epoch  81/100  train MSE: 0.7056, val MSE: 0.5234
Epoch  82/100  train MSE: 0.7049, val MSE: 0.5324
Epoch  83/100  train MSE: 0.7055, val MSE: 0.5010
Epoch  84/100  train MSE: 0.7041, val MSE: 0.5954
Epoch  85/100  train MSE: 0.7036, val MSE: 0.5131
Epoch  86/100  train MSE: 0.7029, val MSE: 0.5305
Epoch  87/100  train MSE: 0.7025, val MSE: 0.5180
Epoch  88/100  train MSE: 0.7027, val MSE: 0.5715
Epoch  89/100  train MSE: 0.7009, val MSE: 0.5061
Epoch  90/100  train MSE: 0.6991, val MSE: 0.5205
Epoch  91/100  train MSE: 0.6980, val MSE: 0.4923
Epoch  92/100  train MSE: 0.7001, val MSE: 0.5231
Epoch  93/100  train MSE: 0.6968, val MSE: 0.5967
Epoch  94/100  train MSE: 0.6974, val MSE: 0.5254
Epoch  95/100  train MSE: 0.6974, val MSE: 0.5126
Epoch  96/100  train MSE: 0.6959, val MSE: 0.5894
Epoch  97/100  train MSE: 0.6957, val MSE: 0.5198
Epoch  98/100  train MSE: 0.6960, val MSE: 0.5050
Epoch  99/100  train MSE: 0.6952, val MSE: 0.4812
Epoch 100/100  train MSE: 0.6956, val MSE: 0.5245

 Training complete!
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
 SGDtrained model and metadata saved to `checkpoints_sgd/`
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e68a654a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Using-Optuna-to-do-hyperparameter-tuning-for-our-L-BFGS-implementation:-Finding-optimal-learning-rate,-max-iterations,-and-epochs.">Using Optuna to do hyperparameter tuning for our L-BFGS implementation: Finding optimal learning rate, max iterations, and epochs.<a class="anchor-link" href="#Using-Optuna-to-do-hyperparameter-tuning-for-our-L-BFGS-implementation:-Finding-optimal-learning-rate,-max-iterations,-and-epochs."></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=ce4c71e6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">L-BFGS Optimisation with Optuna for Feed-Forward Neural Network Training</span>
<span class="sd">=========================================================================</span>

<span class="sd">This script implements hyperparameter optimisation for training a Feed-Forward Neural Network (FFN)</span>
<span class="sd">using the Limited-memory BFGS (L-BFGS) algorithm. It is designed as part of a comparative study of</span>
<span class="sd">SGD, L-BFGS, and Genetic Algorithms on a multimodal loss landscape, as specified in the project brief.</span>

<span class="sd">The workflow includes:</span>
<span class="sd">1. Model architecture setup using a fixed Optuna-tuned FFN configuration.</span>
<span class="sd">2. Weight initialisation using configurable schemes (Xavier, Kaiming).</span>
<span class="sd">3. L-BFGS hyperparameter tuning via Optuna over learning rate, max_iter, and training epochs.</span>
<span class="sd">4. Final retraining of the best model with loss curves plotted for visual comparison.</span>
<span class="sd">5. Exposure of the trained model and hyperparameters for downstream use.</span>

<span class="sd">Instructions for Use</span>
<span class="sd">---------------------</span>
<span class="sd">- Ensure that `X_train`, `y_train`, `X_val`, and `y_val` tensors are pre-defined in your workspace.</span>
<span class="sd">- Execute this script after defining these tensors to perform L-BFGS hyperparameter search and training.</span>
<span class="sd">- Modify `arch_params` and `init_scheme` to experiment with different FFN configurations.</span>
<span class="sd">- Adjust the Optuna search space within `objective()` for further tuning.</span>

<span class="sd">Changing the Architecture</span>
<span class="sd">--------------------------</span>
<span class="sd">- The FFN architecture is defined by `arch_params`, which controls the number of layers, units per layer,</span>
<span class="sd">  and activation function. Only standard PyTorch activations (e.g., "ReLU", "Tanh") are supported.</span>
<span class="sd">- To change the architecture, update `arch_params` accordingly.</span>

<span class="sd">Changing the Initialisation Scheme</span>
<span class="sd">-----------------------------------</span>
<span class="sd">- Set `init_scheme` to one of: 'xavier_normal', 'xavier_uniform', 'kaiming_normal', or 'kaiming_uniform'.</span>

<span class="sd">Customising the Optuna Search</span>
<span class="sd">------------------------------</span>
<span class="sd">- The `objective()` function defines the search space:</span>
<span class="sd">    * `lr`: Learning rate (log-uniform)</span>
<span class="sd">    * `max_iter`: L-BFGS internal iterations per outer step</span>
<span class="sd">    * `epochs`: Total number of epochs to run optimizer.step()</span>
<span class="sd">- Modify the ranges or add new parameters as needed.</span>

<span class="sd">Outputs</span>
<span class="sd">-------</span>
<span class="sd">- Prints the best hyperparameters and validation MSE.</span>
<span class="sd">- Plots training and validation MSE curves.</span>
<span class="sd">- Exposes:</span>
<span class="sd">    * `best_model_lbfgs`: trained FFN model</span>
<span class="sd">    * `best_params_lbfgs`: dictionary of best Optuna hyperparameters</span>

<span class="sd">"""</span>

<span class="c1">#  L-BFGS + Optuna Hyperparameter Search on Best FFN Architecture</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>

<span class="c1"># 1) Device</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>

<span class="c1"># 2) Move data to device</span>
<span class="n">X_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 3) Best FFN architecture from Optuna + init scheme</span>
<span class="n">arch_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"n_units"</span><span class="p">:</span>    <span class="mi">24</span><span class="p">,</span>
    <span class="s2">"activation"</span><span class="p">:</span> <span class="s2">"ReLU"</span>
<span class="p">}</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="k">def</span><span class="w"> </span><span class="nf">initialize_weights</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">init_scheme</span> <span class="o">==</span> <span class="s1">'xavier_normal'</span><span class="p">:</span>
                <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">init_scheme</span> <span class="o">==</span> <span class="s1">'xavier_uniform'</span><span class="p">:</span>
                <span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">init_scheme</span> <span class="o">==</span> <span class="s1">'kaiming_normal'</span><span class="p">:</span>
                <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">init_scheme</span> <span class="o">==</span> <span class="s1">'kaiming_uniform'</span><span class="p">:</span>
                <span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_f</span> <span class="o">=</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span>  <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">initialize_weights</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># 4) Optuna objective</span>
<span class="k">def</span><span class="w"> </span><span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="n">lr</span>       <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_loguniform</span><span class="p">(</span><span class="s2">"lr"</span><span class="p">,</span>      <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s2">"max_iter"</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span>
    <span class="n">epochs</span>   <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">"epochs"</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    
    <span class="n">model</span>     <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
    
    <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y_train_dev</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
        <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">val_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
    <span class="c1"># report final validation loss</span>
    <span class="k">return</span> <span class="n">val_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># 5) Run the search</span>
<span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">"minimize"</span><span class="p">)</span>
<span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 6) Retrieve best hyperparameters</span>
<span class="n">best</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> Best L-BFGS Config:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best Validation MSE: </span><span class="si">{</span><span class="n">best</span><span class="o">.</span><span class="n">value</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># 7) Re-train best model to record curves</span>
<span class="n">best_lr</span>       <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">]</span>
<span class="n">best_max_iter</span> <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">"max_iter"</span><span class="p">]</span>
<span class="n">best_epochs</span>   <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">"epochs"</span><span class="p">]</span>
<span class="n">model</span>     <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">best_lr</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">best_max_iter</span><span class="p">)</span>
<span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">best_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y_train_dev</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="p">(</span><span class="n">best_epochs</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">best_epochs</span><span class="si">}</span><span class="s2">  train MSE: </span><span class="si">{</span><span class="n">train_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">val_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> Final L-BFGS training complete!"</span><span class="p">)</span>

<span class="c1"># 8) Plot the loss curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_losses</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"L-BFGS Training &amp; Validation Loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 9) Expose best model &amp; params for downstream use</span>
<span class="n">best_model_lbfgs</span> <span class="o">=</span> <span class="n">model</span>
<span class="n">best_params_lbfgs</span> <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>[I 2025-05-28 14:19:21,785] A new study created in memory with name: no-name-3a70d6ab-00f9-4d41-9a48-558d13cc2b69
  0%|          | 0/20 [00:00&lt;?, ?it/s]/tmp/ipykernel_5499/59883290.py:57: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr       = trial.suggest_loguniform("lr",      1e-2, 10.0)
  5%|         | 1/20 [00:11&lt;03:41, 11.64s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:19:33,431] Trial 0 failed with parameters: {'lr': 2.08981369921552, 'max_iter': 50, 'epochs': 40} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:19:33,434] Trial 0 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre> 10%|         | 2/20 [00:13&lt;01:50,  6.16s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:19:35,750] Trial 1 failed with parameters: {'lr': 2.2316699775559283, 'max_iter': 10, 'epochs': 40} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:19:35,752] Trial 1 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 2. Best value: 0.635706:  15%|        | 3/20 [00:20&lt;01:44,  6.16s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:19:41,907] Trial 2 finished with value: 0.6357057094573975 and parameters: {'lr': 0.041762790769269996, 'max_iter': 10, 'epochs': 100}. Best is trial 2 with value: 0.6357057094573975.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 3. Best value: 0.431403:  20%|        | 4/20 [00:26&lt;01:43,  6.44s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:19:48,779] Trial 3 finished with value: 0.4314032196998596 and parameters: {'lr': 0.06168203583807849, 'max_iter': 20, 'epochs': 60}. Best is trial 3 with value: 0.4314032196998596.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  25%|       | 5/20 [00:34&lt;01:39,  6.65s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:19:55,807] Trial 4 finished with value: 0.3471372723579407 and parameters: {'lr': 1.3615240458927713, 'max_iter': 20, 'epochs': 60}. Best is trial 4 with value: 0.3471372723579407.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  30%|       | 6/20 [00:39&lt;01:29,  6.36s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:20:01,597] Trial 5 finished with value: 0.4124971330165863 and parameters: {'lr': 0.10160487871637558, 'max_iter': 10, 'epochs': 100}. Best is trial 4 with value: 0.3471372723579407.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  35%|      | 7/20 [01:04&lt;02:41, 12.41s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:20:26,453] Trial 6 failed with parameters: {'lr': 4.5260308447550655, 'max_iter': 50, 'epochs': 100} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:20:26,455] Trial 6 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  40%|      | 8/20 [01:27&lt;03:10, 15.84s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:20:49,639] Trial 7 failed with parameters: {'lr': 0.011931180028593898, 'max_iter': 50, 'epochs': 80} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:20:49,642] Trial 7 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  45%|     | 9/20 [01:44&lt;02:55, 15.96s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:21:05,860] Trial 8 failed with parameters: {'lr': 1.9211059060929936, 'max_iter': 50, 'epochs': 60} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:21:05,863] Trial 8 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  50%|     | 10/20 [01:48&lt;02:02, 12.27s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:21:09,877] Trial 9 failed with parameters: {'lr': 9.009499918610667, 'max_iter': 20, 'epochs': 40} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:21:09,879] Trial 9 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  55%|    | 11/20 [01:51&lt;01:25,  9.54s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:21:13,220] Trial 10 finished with value: 0.4225972294807434 and parameters: {'lr': 0.2792263989562056, 'max_iter': 10, 'epochs': 60}. Best is trial 4 with value: 0.3471372723579407.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  60%|    | 12/20 [01:59&lt;01:12,  9.01s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:21:21,020] Trial 11 failed with parameters: {'lr': 4.108316983113093, 'max_iter': 20, 'epochs': 80} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:21:21,023] Trial 11 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  65%|   | 13/20 [02:00&lt;00:46,  6.66s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:21:22,281] Trial 12 finished with value: 0.7582546472549438 and parameters: {'lr': 0.16338714361958181, 'max_iter': 10, 'epochs': 20}. Best is trial 4 with value: 0.3471372723579407.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  70%|   | 14/20 [02:02&lt;00:31,  5.26s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:21:24,309] Trial 13 failed with parameters: {'lr': 9.385566437479376, 'max_iter': 20, 'epochs': 20} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:21:24,312] Trial 13 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  75%|  | 15/20 [02:28&lt;00:56, 11.36s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:21:49,808] Trial 14 failed with parameters: {'lr': 7.919467830773519, 'max_iter': 50, 'epochs': 100} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:21:49,811] Trial 14 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 4. Best value: 0.347137:  80%|  | 16/20 [02:30&lt;00:34,  8.67s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:21:52,220] Trial 15 finished with value: 0.9797548651695251 and parameters: {'lr': 0.01422571893517124, 'max_iter': 10, 'epochs': 40}. Best is trial 4 with value: 0.3471372723579407.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 16. Best value: 0.210479:  85%| | 17/20 [02:34&lt;00:22,  7.42s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:21:56,730] Trial 16 finished with value: 0.2104787528514862 and parameters: {'lr': 0.6843929977747965, 'max_iter': 10, 'epochs': 80}. Best is trial 16 with value: 0.2104787528514862.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 16. Best value: 0.210479:  90%| | 18/20 [02:39&lt;00:12,  6.43s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:22:00,843] Trial 17 failed with parameters: {'lr': 2.7875555078900844, 'max_iter': 10, 'epochs': 80} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:22:00,846] Trial 17 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 16. Best value: 0.210479:  95%|| 19/20 [02:44&lt;00:06,  6.26s/it]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[W 2025-05-28 14:22:06,714] Trial 18 failed with parameters: {'lr': 0.03772030648482473, 'max_iter': 10, 'epochs': 100} because of the following error: The value nan is not acceptable.
[W 2025-05-28 14:22:06,716] Trial 18 failed with value nan.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Best trial: 19. Best value: 0.184061: 100%|| 20/20 [02:56&lt;00:00,  8.84s/it]
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[I 2025-05-28 14:22:18,656] Trial 19 finished with value: 0.1840611696243286 and parameters: {'lr': 0.9669461523303573, 'max_iter': 20, 'epochs': 100}. Best is trial 19 with value: 0.1840611696243286.

 Best L-BFGS Config:
  lr: 0.9669461523303573
  max_iter: 20
  epochs: 100
Best Validation MSE: 0.18406

Epoch 1/100  train MSE: 1.4198, val MSE: 0.9929
Epoch 20/100  train MSE: 0.7260, val MSE: 0.5253
Epoch 40/100  train MSE: 0.6450, val MSE: 0.4130
Epoch 60/100  train MSE: 0.5921, val MSE: 0.3304
Epoch 80/100  train MSE: 0.5516, val MSE: 0.2728
Epoch 100/100  train MSE: 0.5408, val MSE: 0.2614

 Final L-BFGS training complete!
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAArMAAAGJCAYAAACZ7rtNAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdkVJREFUeJzt3Xd8FNX6x/HPtmx6I5XQiwIqRTqIoNKRi10BFcGrVwVF+VnAhthQvFexYL0CehVB7F0QRbpSBCkCCoSeQEhIL5vs/P6YZGFNgASSLAnf9+s1r92dPTPzbA4JT06eOcdiGIaBiIiIiEgNZPV1ACIiIiIiJ0vJrIiIiIjUWEpmRURERKTGUjIrIiIiIjWWklkRERERqbGUzIqIiIhIjaVkVkRERERqLCWzIiIiIlJjKZkVERERkRpLyayICNCoUSNuuummkzq2V69e9OrVq1Ljqekee+wxLBaL177yfo1nzpyJxWIhMTGx0uJJTEzEYrEwc+bMSjuniJwelMyK1AAl/7mvWrWqQseV/Ad+9BYaGkrbtm155ZVXKCoq8mrfq1evUu1Lts2bN3u1PXDgAOPHj+e8884jODgYf39/mjVrxsiRI1myZEmpWNavX89VV11Fw4YN8ff3JyEhgT59+vDyyy8fM/6FCxceM56/b2eytLQ0brvtNhISEggKCqJNmzY899xz5Tr2wIED2O12rr/++mO2yczMJCAggCuuuKKyQq4ys2bNYurUqb4Ow8tNN91EcHCwr8MQqbXsvg5ARKre0KFDGThwIADp6el888033HnnnezcubNU0lOvXj0mT55c6hx169b1PP/1118ZNGgQmZmZXHfdddx22204nU527NjBZ599xsyZM/n555+58MILAVi2bBkXXXQRDRo04JZbbiEuLo7du3ezYsUKXnzxRe68884y427ZsiX/+9//vPZNmDCB4OBgHnrooVP6mvzdli1bsFpP7vf7efPmVWosFXXTTTfxzTffMGbMGFq0aMG6det4//33ue+++054bExMDH369OHzzz8nJyeHwMDAUm0++eQT8vLyjpvwlsepfI3La9asWWzYsIG7777ba3/Dhg3Jzc3F4XBU6fVFpPopmRU5A5x//vleicgdd9xB586dmTVrVqlkNiws7LhJS1paGpdddhl2u521a9fSokULr/effPJJZs+eTUBAgGffU089RVhYGCtXriQ8PNyr/YEDB455rdjY2FKxPPPMM0RFRR03RrfbTUFBAf7+/sds83dOp7Pcbf/Oz8/vpI89VdnZ2Xz11VfcdtttvPDCC579+fn55T7H8OHD+e677/jiiy+47rrrSr0/a9YswsLCGDRo0CnFeipf41NlsVgq9O9BRGoOlRmInIEsFguxsbHY7RX/ffb1119n//79TJ06tVQiW3LuoUOH0rFjR8++bdu2cc4555RKZMEcGTxVFouFMWPG8P7773POOefgdDr57rvvAPj3v/9Nt27dqFOnDgEBAbRv356PPvqo1Dn+Xs9ZUtqxdOlSxo0bR3R0NEFBQVx++eUcPHjQ69i/18yWlEd8+OGHPPXUU9SrVw9/f38uueQS/vrrr1LXnjZtGk2aNCEgIIBOnTqxePHictfhlpRZGIbhtb8iiePll19OUFAQs2bNKvXegQMHWLBgAVdddRVOp5PFixdz9dVX06BBA5xOJ/Xr1+eee+4hNzf3hNcpq2Z248aNXHzxxQQEBFCvXj2efPJJ3G53qWM///xzBg0aRN26dXE6nTRt2pQnnnjCq1SmV69efP311+zcudPzdWnUqBFw7JrZH3/8kR49ehAUFER4eDhDhgzhjz/+8GpTUv/7119/cdNNNxEeHk5YWBgjR44kJyfnhJ+7vObOnUv79u0JCAjw/MK2d+9erzZJSUmMHDmSevXq4XQ6iY+PZ8iQIV71xatWraJfv35ERUUREBBA48aNGTVqVKXFKXK60cisyBkgJyeHlJQUADIyMvj222/57rvvmDBhQqm2RUVFnrYl/P39PTV/X375ZYXrJxs2bMjy5cvZsGED55577il8kmP78ccf+fDDDxkzZgxRUVGeJObFF1/kH//4B8OHD6egoIDZs2dz9dVX89VXX5VrpPHOO+8kIiKCiRMnkpiYyNSpUxkzZgxz5sw54bHPPPMMVquVe++9l/T0dKZMmcLw4cP55ZdfPG1ee+01xowZQ48ePbjnnntITEzksssuIyIignr16p3wGoGBgVxzzTXMnDmTW265hXbt2p3wmL8LCgpiyJAhfPTRR6SmphIZGel5b86cORQVFTF8+HDATLhycnK4/fbbqVOnDr/++isvv/wye/bsYe7cuRW6blJSEhdddBGFhYWMHz+eoKAg3nzzTa9R/RIzZ84kODiYcePGERwczI8//sijjz5KRkaG568LDz30EOnp6ezZs8czSn28WtUffviBAQMG0KRJEx577DFyc3N5+eWX6d69O2vWrPH8GypxzTXX0LhxYyZPnsyaNWv473//S0xMDM8++2yFPndZZs6cyciRI+nYsSOTJ08mOTmZF198kaVLl/Lbb795fhG88sor2bhxI3feeSeNGjXiwIEDzJ8/n127dnle9+3bl+joaMaPH094eDiJiYl88sknpxyjyGnLEJHT3owZMwzAWLlyZYWO27FjhwGUud1+++2G2+32at+zZ88y244YMcLTJiIiwmjbtm2pa2VkZBgHDx70bFlZWZ735s2bZ9hsNsNmsxldu3Y17r//fuP77783CgoKKvaFMAzjnHPOMXr27Om1DzCsVquxcePGUu1zcnK8XhcUFBjnnnuucfHFF3vtb9iwodfnLPma9+7d2+vrdM899xg2m804fPiwZ1/Pnj29Yvrpp58MwGjZsqWRn5/v2f/iiy8agLF+/XrDMAwjPz/fqFOnjtGxY0fD5XJ52s2cOdMASn3OsmRmZhq9e/c2/Pz8jNjYWGPr1q0nPKYsX3/9tQEYb7zxhtf+Ll26GAkJCUZRUZFhGKW/noZhGJMnTzYsFouxc+dOz76JEycaf/8v5u9f47vvvtsAjF9++cWz78CBA0ZYWJgBGDt27PDsL+u6//rXv4zAwEAjLy/Ps2/QoEFGw4YNS7Ut+V6YMWOGZ1/btm2NmJgY49ChQ55969atM6xWq3HjjTeW+iyjRo3yOufll19u1KlTp9S1/m7EiBFGUFDQMd8vKCgwYmJijHPPPdfIzc317P/qq68MwHj00UcNwzCMtLQ0AzCee+65Y57r008/PamfFSI1mcoMRM4At956K/Pnz2f+/Pl8/PHHjB49mjfeeINx48aVatuoUSNP25Lt/vvv97yfkZFR5mjXDTfcQHR0tGd74IEHPO/16dOH5cuX849//IN169YxZcoU+vXrR0JCAl988UWlfMaePXvSqlWrUvuPHuVLS0sjPT2dHj16sGbNmnKd99Zbb/WaLaFHjx4UFRWxc+fOEx47cuRIr3raHj16ALB9+3bA/HPwoUOHuOWWW7xKPoYPH05ERES54rvxxhtJTExk8+bNREdH07t3b3bt2uV5f/ny5VgsFhYsWHDc85SM5h1darBjxw5WrFjB0KFDPTduHf31zM7OJiUlhW7dumEYBr/99lu5Yi7xzTff0KVLFzp16uTZFx0d7RkFPtrR183MzCQlJYUePXqQk5NTaqaN8ti/fz9r167lpptu8hqJbt26NX369OGbb74pdcxtt93m9bpHjx4cOnSIjIyMCl//aKtWreLAgQPccccdXnW9gwYNokWLFnz99deA+TXw8/Nj4cKFpKWllXmukhHcr776CpfLdUpxidQUSmZFaoGCggKSkpK8tqNrCZs3b07v3r3p3bs3V1xxBa+88gp33HEHU6dOZf369V7nCgoK8rQt2Y5OEkNCQsjKyioVw+OPP+5JfsvSsWNHPvnkE9LS0vj111+ZMGECmZmZXHXVVWzatOmUvwaNGzcuc/9XX31Fly5d8Pf3JzIykujoaF577TXS09PLdd4GDRp4vS5JMo+VTFTk2JKEuFmzZl7t7HZ7qT9xl2XFihV8+umnPP300zRu3NhTJ9y7d2+Sk5MB2LBhA3a7nfbt2x/3XHa7nWuvvZbFixd76jRLEtujk8tdu3Z5EsDg4GCio6Pp2bMnQLm/piV27txJ8+bNS+0/++yzS+3buHEjl19+OWFhYYSGhhIdHe25CbCi1y259rGu1bJlS1JSUsjOzvbafyr/Fk42lhYtWnjedzqdPPvss3z77bfExsZy4YUXMmXKFJKSkjzte/bsyZVXXsmkSZOIiopiyJAhzJgxo0I3BIrUNEpmRWqBZcuWER8f77Xt3r37uMdccsklACxatKhC12rRogVbtmwpNerTunVrT/J7PH5+fnTs2JGnn36a1157DZfLVeFay7KUVWe5ePFi/vGPf+Dv78+rr77KN998w/z58xk2bFipG6aOxWazlbm/PMefyrHlsWzZMgC6dOkCQEJCAt9//z2pqan06dOH1NRU3nzzTQYOHFjmzXd/d/311+N2u/nggw8A+OCDD2jVqhVt27YFzHrqPn368PXXX/PAAw/w2WefMX/+fM9NVWXduFUZDh8+TM+ePVm3bh2PP/44X375JfPnz/fUqlbVdf+uqvuzPO6++262bt3K5MmT8ff355FHHqFly5aeUXGLxcJHH33E8uXLGTNmDHv37mXUqFG0b9++zF9CRWoDJbMitUCbNm1KlQbExcUd95jCwkKACv8Hd+mll5Kbm8unn3560vGW6NChA2D+ybcqfPzxx/j7+/P9998zatQoBgwYcMJkuzo1bNgQoNQMB4WFheVa/aqk/OHoX1xK/iy9fft22rdvz5o1a5g4cWK54uncuTNNmzZl1qxZrFu3jo0bN3qNyq5fv56tW7fyn//8hwceeIAhQ4bQu3dvrzmIK6Jhw4b8+eefpfZv2bLF6/XChQs5dOgQM2fOZOzYsVx66aX07t27zFKM8i6gUfK1//u1ADZv3kxUVBRBQUHlOtepOl4sW7Zs8bxfomnTpvzf//0f8+bNY8OGDRQUFPCf//zHq02XLl146qmnWLVqFe+//z4bN25k9uzZVfchRHxIyaxILRAREVGqNOBEc2p++eWXgJkIV8Ttt99ObGws99xzD1u3bi31flmjVD/99FOZ+0vqEsv682plsNlsWCwWr5KLxMREPvvssyq5XkV16NCBOnXq8NZbb3l+uQB4//33y/Wn65LR9ccff9zr+M6dO/Pwww+TmJhI8+bNKzSDxPDhw/ntt9+YOHEiFouFYcOGed4rGZk8ui8Nw+DFF18s9/mPNnDgQFasWMGvv/7q2Xfw4EHef/99r3ZlXbegoIBXX3211DmDgoLKVXYQHx9P27Zteeeddzh8+LBn/4YNG5g3b55nkZHq0KFDB2JiYnj99de9ygG+/fZb/vjjD8+sGzk5OeTl5Xkd27RpU0JCQjzHpaWllfpeKxlZV6mB1FaamkukBpk+fbqnLvJoY8eOJSQk5JjHrVmzhvfeew8wb55ZsGABH3/8Md26daNv374ViiEyMpJPP/2UwYMH06ZNG6677jo6duyIw+Fg9+7dnpKBo+sL77zzTnJycrj88stp0aIFBQUFLFu2jDlz5tCoUSNGjhxZoRjKa9CgQTz//PP079+fYcOGceDAAaZNm0azZs34/fffq+SaFeHn58djjz3GnXfeycUXX8w111xDYmIiM2fOpGnTpiccZWzdujV33XUXL730Eh07dmTo0KGEh4ezePFiZs+eTY8ePViyZAm33HIL77zzTrliuv7663n88cf5/PPP6d69u1ftbosWLWjatCn33nsve/fuJTQ0lI8//vika0bvv/9+/ve//9G/f3/Gjh3rmZqrYcOGXv3TrVs3IiIiGDFiBHfddRcWi4X//e9/Zf6C1L59e+bMmcO4cePo2LEjwcHBDB48uMzrP/fccwwYMICuXbty8803e6bmCgsL47HHHjupz3QsLpeLJ598stT+yMhI7rjjDp599llGjhxJz549GTp0qGdqrkaNGnHPPfcAsHXrVi655BKuueYaWrVqhd1u59NPPyU5Odmz2MU777zDq6++yuWXX07Tpk3JzMzkrbfeIjQ0tFoTdJFq5ZtJFESkIkqmiTrWtnv37jKPK2tqLrvdbjRp0sS47777jMzMTK/2PXv2NM4555xyxbR//37jvvvuM1q1amUEBAQYTqfTaNKkiXHjjTcaixYt8mr77bffGqNGjTJatGhhBAcHG35+fkazZs2MO++800hOTq7Q1+JYU3ONHj26zPZvv/220bx5c8PpdBotWrQwZsyYUa5po441HVrJtFs//fSTZ9+xpuaaO3eu17FlTQ9lGIbx0ksvGQ0bNjScTqfRqVMnY+nSpUb79u2N/v37H/+LcdRnbN++veHv728EBwcbPXr0MGbPnm0YhmE8+OCDBmBMmjSpXOcyDMPo2LGjARivvvpqqfc2bdpk9O7d2wgODjaioqKMW265xVi3bl2pz1Wer7FhGMbvv/9u9OzZ0/D39zcSEhKMJ554wnj77bdLTc21dOlSo0uXLkZAQIBRt25dz/Ruf++LrKwsY9iwYUZ4eLgBeKbpOtbX/ocffjC6d+9uBAQEGKGhocbgwYONTZs2ebUp+SwHDx702l/yb+ToOMsyYsSIY37vNm3a1NNuzpw5Rrt27Qyn02lERkYaw4cPN/bs2eN5PyUlxRg9erTRokULIygoyAgLCzM6d+5sfPjhh542a9asMYYOHWo0aNDAcDqdRkxMjHHppZcaq1atOm6MIjWZxTCqsXJdREROyO12Ex0dzRVXXMFbb73l63BERE5rqpkVEfGhvLy8Un8uf/fdd0lNTS3XcrYiImc6jcyKiPjQwoULueeee7j66qupU6cOa9as4e2336Zly5asXr3aa9EFEREpTTeAiYj4UKNGjahfvz4vvfQSqampREZGcuONN/LMM88okRURKQeNzIqIiIhIjaWaWRERERGpsZTMioiIiEiNdcbVzLrdbvbt20dISEi5lz0UERERkepjGAaZmZnUrVsXq/X4Y69nXDK7b98+6tev7+swREREROQEdu/eTb169Y7b5oxLZkuW/Ny9ezehoaFVfj2Xy8W8efPo27cvDoejyq8nVUP9WDuoH2sH9WPtoH6sHaqqHzMyMqhfv/5xl2ovccYlsyWlBaGhodWWzAYGBhIaGqpv1hpM/Vg7qB9rB/Vj7aB+rB2quh/LUxKqG8BEREREpMZSMisiIiIiNZaSWRERERGpsc64mlkRERGpuQzDoLCwkKKiIl+HIpg1s3a7nby8vAr3icPhwGaznXIMSmZFRESkRigoKGD//v3k5OT4OhQpZhgGcXFx7N69u8Lz91ssFurVq0dwcPApxaBkVkRERE57brebHTt2YLPZqFu3Ln5+flr86DTgdrvJysoiODj4hIsbHM0wDA4ePMiePXto3rz5KY3QKpkVERGR015BQQFut5v69esTGBjo63CkmNvtpqCgAH9//wolswDR0dEkJibicrlOKZnVDWAiIiJSY1Q0YZLTV2WNrOtfhIiIiIjUWEpmq9jmpEzWHrLw54EsX4ciIiIiUusoma1i7/2ymxlbbXy/MdnXoYiIiEgt0KhRI6ZOnerrME4bSmarWLDTLGjOyi/0cSQiIiJSnSwWy3G3xx577KTOu3LlSm699dZTiq1Xr15YLBaeeeaZUu8NGjSoVHw7duxg2LBh1K1bF39/f+rVq8eQIUPYvHmzp82xPufs2bNPKdYT8Wkyu2jRIgYPHkzdunWxWCx89tln5T526dKl2O122rZtW2XxVYYQfwcAWfma3FlERORMsn//fs82depUQkNDvfbde++9nrYli0GUR3R0dKXM6FC/fn1mzpzptW/v3r0sWLCA+Ph4zz6Xy0WfPn1IT0/nk08+YcuWLcyZM4fzzjuPw4cPex0/Y8YMr8+4f/9+LrvsslOO9Xh8msxmZ2fTpk0bpk2bVqHjDh8+zI033sgll1xSRZFVHo3MioiIVA3DMMgpKKz2zTCMcsUXFxfn2cLCwrBYLJ7XmzdvJiQkhG+//Zb27dvjdDpZsmQJ27ZtY8iQIcTGxhIcHEzHjh354YcfvM779zIDi8XCf//7Xy6//HICAwNp3rw5X3zxxQnju/TSS0lJSWHp0qWefe+88w59+/YlJibGs2/jxo1s27aNV199lS5dutCwYUO6d+/Ok08+SZcuXbzOGR4e7vW54+Li8Pf3L9fX62T5dJ7ZAQMGMGDAgAofd9tttzFs2DBsNluFRnN9Idhpfomz8pTMioiIVKZcVxGtHv2+2q+76fF+BPpVTgo1fvx4/v3vf9OkSRMiIiLYvXs3AwcO5KmnnsLpdPLuu+8yePBgtmzZQoMGDY55nkmTJjFlyhSee+45Xn75ZYYPH87OnTuJjIw85jF+fn4MHz6cGTNm0L17dwBmzpzJlClTvEoMoqOjsVqtfPTRR9x9992VsgRtZapxiybMmDGD7du389577/Hkk0+esH1+fj75+fme1xkZGYA5ZO5yuaoszhIBdnMOtcy86rmeVI2SvlMf1mzqx9pB/Vg7VLQfXS4XhmHgdrtxu90AnsfqdnQMFTmmrMfHHnvM6y/N4eHhnHfeeZ7XkyZN4tNPP+Xzzz9n9OjRnv0lX4sSI0aM4NprrwXgySef5KWXXmLFihX079//mDEZhsFNN91Ez549eeGFF1i9ejXp6ekMHDiQxx57zHON+Ph4XnzxRR544AEmTZpEhw4d6NWrF8OGDaNx48aecwEMHTq0VLK7YcOGMhNxt9uNYRhlLppQke/vGpXM/vnnn4wfP57Fixdjt5cv9MmTJzNp0qRS++fNm1ctK4hsOWwBbOw/dJhvvvmmyq8nVWv+/Pm+DkEqgfqxdlA/1g7l7Ue73U5cXBxZWVkUFBQAZgK1fFyXExxZ+Vy52WTkVWzC/7y8PAzD8Ayq5eTkAHD22Wd79gFkZWXx7LPPMm/ePJKSkigqKiI3N5c///zT087tdpOXl+d1XLNmzbxeh4SEsGvXLq99RyssLKSgoIDGjRvTpEkT3nvvPRYvXsw111xDTk4ORUVF5Ofne46//vrrGTJkCEuWLGHVqlXMmTOHyZMnM2vWLC666CIyMzMBeOqpp+jVq5fXtYKDg8uMo6CggNzcXBYtWlSqXrjk61MeNSaZLSoqYtiwYUyaNImzzjqr3MdNmDCBcePGeV5nZGRQv359+vbtS2hoaFWE6iU28RCv/rEaiyOAgQMvrPLrSdVwuVzMnz+fPn364HA4fB2OnCT1Y+2gfqwdKtqPeXl57N69m+DgYK8azLCqDLIS+fv7Y7FYPLlHyYBaXFycVz7ywAMP8MMPPzBlyhSaNWtGQEAA11xzjdexVqsVf39/r+NCQ0O9XlutVvz8/I6Z69jtds/7//znP5kxYwabNm1ixYoVhIaGYrPZcDqdpa5x7bXXcu211zJlyhT69+/PCy+8wEUXXURISAhg1vOW9+b8vLw8AgICuPDCC0vV1R4rCS/zs5S7pY9lZmayatUqfvvtN8aMGQMcGZ622+3MmzePiy++uNRxTqcTp9NZar/D4aiWH4LhQWbnZBcU6oduLVBd/26kaqkfawf1Y+1Q3n4sKirCYrFgtVpr5JK2JTGX9Xj051m2bBk33XQTV155JWCO1CYmJtKrVy+vdiVfi6PP//evy4m+ViXnGD58OPfddx9t2rTh3HPPPeY1/q5ly5YsW7bM07Y81/x7fBaLpcx/AxX53q4xyWxoaCjr16/32vfqq6/y448/8tFHH3lqNk43If7FN4DlF2EYRqWtQywiIiK1T/Pmzfnkk08YPHgwFouFRx55pMprgyMiIti/f/8xE8i1a9cyceJEbrjhBlq1aoWfnx8///wz06dP5/777/dqe/jwYZKSkrz2hYSEEBQUVGXx+zSZzcrK4q+//vK83rFjB2vXriUyMpIGDRowYcIE9u7dy7vvvovVavX6bQEgJiYGf3//UvtPJyVTcxW5DXJdRZV296OIiIjUPs8//zyjRo2iW7duREVF8cADD1ToT+4nKzw8/Jjv1atXj0aNGjFp0iQSExOxWCye12PHjiUrK8vTduTIkaWOnzx5MuPHj6+KsAEfJ7OrVq3ioosu8rwuqW0dMWIEM2fOZP/+/ezatctX4VWKAIcNCwYGFrLyCpXMioiInIFuuukmbrrpJs/rXr16lTlfbaNGjfjxxx+99h09iwFAYmKi1+uyzvP3xQz+buHChcd9f+3atZ7nUVFRvPjii2W2O3rUuLzz71Y2n2ZWx+rIEn9fleLvHnvssZNeCq66WCwW/G2QWwSZ+YXEnPgQERERESmnmldBXQP5F0+dpoUTRERERCqXktlq4ElmtaStiIiISKVSMlsNiic0IFMjsyIiIiKVSslsNfC3mXXBmXlaelFERESkMimZrQYqMxARERGpGkpmq4FuABMRERGpGkpmq4FGZkVERESqhpLZauCpmVUyKyIiIlKplMxWg5LZDFRmICIiIhXVq1cv7r77bl+HcdpSMlsNVGYgIiJy5hk8eDD9+/cv873FixdjsVj4/fffqyWWm266CYvFwm233VbqvdGjR2OxWLyW2z148CC33347DRo0wOl0EhcXR79+/Vi6dKmnTaNGjbDZbERERGCz2bBYLFgsFp555pnq+EgePl3O9kxRksxqai4REZEzx80338yVV17Jnj17qFevntd7M2bMoEOHDrRu3bra4qlfvz6zZ8/mhRdeICAgAIC8vDxmzZpFgwYNvNpeeeWVFBQU8M4779CkSROSk5NZsGABhw4d8mo3adIkrr32WkJCQrBazTHSkJCQ6vlAxZTMVoMjyaxGZkVERCqNYYArp/qv6wgEi+WEzS699FKio6OZOXMmDz/8sGd/VlYWc+fO5bnnnuPQoUOMGTOGRYsWkZaWRtOmTXnwwQcZOnRopYd9/vnns23bNj755BOGDx8OwCeffEKDBg1o3Lixp93hw4dZvHgxCxcupGfPngA0bNiQTp06lTpnSEgIsbGxhIaGepLZ6qZkthqU3ACmMgMREZFK5MqBp+tW/3Uf3Ad+QSdsZrfbufHGG5k5cyYPPfQQluIEeO7cuRQVFTF06FCysrJo3749DzzwAKGhoXz99dfccMMNNG3atMzk8VSNGjWKGTNmeJLZ6dOnM3LkSBYuXOhpExwcTHBwMJ999hldunTB6XRWehyVSTWz1UA1syIiImemUaNGsW3bNn7++WfPvhkzZnDllVcSFhZGQkIC9957L23btqVJkybceeed9O/fnw8//LBK4rn++utZsmQJO3fuZOfOnSxdupTrr7/eq43dbmfmzJm88847hIeH0717dx588MEy63vHjx9PvXr1CA0N9STBixcvrpLYj0Ujs9Xg6EUTDMPw/GYmIiIip8ARaI6S+uK65dSiRQu6devG9OnT6dWrF3/99ReLFy/m8ccfB6CoqIinn36aDz/8kL1791JQUEB+fj6BgeW/RkVER0czaNAgZs6ciWEYDBo0iKioqFLtrrzySgYNGsTixYtZsWIF3377LVOmTOG///2v141i9957L1deeSXBwcGeMoOEhIQqif1YlMxWg4Dir3Kh2yC/0I2/w+bbgERERGoDi6Vcf+73tZtvvpk777yTadOmMWPGDJo2beqpRX3uued48cUXmTp1Kueddx5BQUHcfffdFBQUVFk8o0aNYsyYMQBMmzbtmO38/f3p06cPffr04ZFHHuGf//wnEydO9Epmo6KiaNKkiU9rZlVmUA38rEfqxDM0o4GIiMgZ5ZprrsFqtTJr1izeffddRo0a5fkr7dKlSxkyZAjXX389bdq0oUmTJmzdurVK4+nfvz8FBQW4XC769etX7uNatWpFdnZ2FUZ2cjQyWw0sFgh22snMKyQrr5CY6p2xQkRERHwoODiYa6+9lgkTJpCRkeE1stm8eXM++ugjli1bRkREBM8//zzJycm0atWqyuKx2Wz88ccfnud/d+jQIa6++mpGjRpF69atCQkJYdWqVUyZMoUhQ4Z4tc3MzCQ5OZmcnBzPyGxgYCChoaFVFv/faWS2mgQ7zd8bdBOYiIjImefmm28mLS2Nfv36UbfukRkYHn74Yc4//3z69etHr169iIuL47LLLqvyeEJDQ4+ZcAYHB9O5c2deeOEFLrzwQs4991weeeQRbrnlFl555RWvthMnTqRFixYkJCQQHx9PfHw8999/f5XHfzSNzFaTYKf5m4+WtBURETnzdO3aFcMwSu2PjIzks88+O+6xR0+bdbJmzpx53PePjsHpdDJ58mQmT5583GMSExNxu91kZGSoZvZMUDIym6mRWREREZFKo2S2mnjKDDQyKyIiIlJplMxWkxD/4pFZzWYgIiIiUmmUzFYT3QAmIiIiUvmUzFYT1cyKiIicurJuopKaqbL6UslsNVHNrIiIyMlzOBwA5OTk+DgSqSwlq5yVNddtRWhqrmoS7K8yAxERkZNls9kIDw/nwIEDgDkxf8kqWuI7brebgoIC8vLyKjQ1l9vt5uDBgwQGBmK3n1o6qmS2mmieWRERkVMTFxcH4EloxfcMwyA3N5eAgIAK/3JhtVpp0KDBKf9SomS2mqhmVkRE5NRYLBbi4+OJiYnB5dLsQKcDl8vFokWLuPDCCz2lIOXl5+dXKQstKJmtJsGeqbmUzIqIiJwKm812ynWWUjlsNhuFhYX4+/tXOJmtLLoBrJocmZpLv0mKiIiIVBYls9VEsxmIiIiIVD4ls9Xk6EUTNEeeiIiISOVQMltNSpJZV5FBfqHbx9GIiIiI1A5KZqtJkJ+NkpknNNesiIiISOVQMltNrFYLwX6a0UBERESkMimZrUaeVcCUzIqIiIhUCp8ms4sWLWLw4MHUrVsXi8XCZ599dtz2n3zyCX369CE6OprQ0FC6du3K999/Xz3BVoIjCydoei4RERGRyuDTZDY7O5s2bdowbdq0crVftGgRffr04ZtvvmH16tVcdNFFDB48mN9++62KI60cGpkVERERqVw+XQFswIABDBgwoNztp06d6vX66aef5vPPP+fLL7+kXbt2lRxd5Tt6ei4REREROXU1ejlbt9tNZmYmkZGRx2yTn59Pfn6+53VGRgZgriVcHes6l1zD5XIR5GcuvZeek681pWuYo/tRai71Y+2gfqwd1I+1Q1X1Y0XOV6OT2X//+99kZWVxzTXXHLPN5MmTmTRpUqn98+bNIzAwsCrD8zJ//nwOH7QCVlat20jkoQ3Vdm2pPPPnz/d1CFIJ1I+1g/qxdlA/1g6V3Y85OTnlbltjk9lZs2YxadIkPv/8c2JiYo7ZbsKECYwbN87zOiMjg/r169O3b19CQ0OrPE6Xy8X8+fPp06cPay3bWXFgJwmNmjKw71lVfm2pPEf3o8Ph8HU4cpLUj7WD+rF2UD/WDlXVjyV/SS+PGpnMzp49m3/+85/MnTuX3r17H7et0+nE6XSW2u9wOKr1m8fhcBAa4AdAjsutb9waqrr/3UjVUD/WDurH2kH9WDtUdj9W5Fw1bp7ZDz74gJEjR/LBBx8waNAgX4dTISGazUBERESkUvl0ZDYrK4u//vrL83rHjh2sXbuWyMhIGjRowIQJE9i7dy/vvvsuYJYWjBgxghdffJHOnTuTlJQEQEBAAGFhYT75DBWh2QxEREREKpdPR2ZXrVpFu3btPNNqjRs3jnbt2vHoo48CsH//fnbt2uVp/+abb1JYWMjo0aOJj4/3bGPHjvVJ/BVVMs+slrMVERERqRw+HZnt1asXhmEc8/2ZM2d6vV64cGHVBlTFQvzN+g+NzIqIiIhUjhpXM1uTeZaz1cisiIiISKVQMluNPDeAaWRWREREpFIoma1GnhvANDIrIiIiUimUzFajkhvACorc5BcW+TgaERERkZpPyWw1CvI7cr+dRmdFRERETp2S2Wpks1oI8rMBqpsVERERqQxKZqtZyfRcmtFARERE5NQpma1mWjhBREREpPIoma1mWtJWREREpPIoma1mR+aadfk4EhEREZGaT8lsNdNcsyIiIiKVR8lsNfMsaasyAxEREZFTpmS2mmk2AxEREZHKo2S2mpXMZqAyAxEREZFTp2S2moVoNgMRERGRSqNktpppnlkRERGRyqNktpodmWdWU3OJiIiInCols9XMUzOrMgMRERGRU6ZktpqV1MyqzEBERETk1CmZrWYlU3NpNgMRERGRU6dktpp5bgBTmYGIiIjIKVMyW81KbgArKHSTX1jk42hEREREajYls9WsJJkFyM5XMisiIiJyKpTMVjOb1UKgnw1Q3ayIiIjIqVIy6wMlo7OZmmtWRERE5JQomfWBEK0CJiIiIlIplMz6QLCm5xIRERGpFEpmfSDEqVXARERERCqDklkfOFIzq2RWRERE5FQomfWBkoUTVGYgIiIicmqUzPpAsKfMQLMZiIiIiJwKJbM+EKrZDEREREQqhZJZH1CZgYiIiEjlUDLrA8FOc2ou3QAmIiIicmqUzPqARmZFREREKoeSWR/QPLMiIiIilUPJrA94RmaVzIqIiIicEp8ms4sWLWLw4MHUrVsXi8XCZ599dsJjFi5cyPnnn4/T6aRZs2bMnDmzyuOsbJ5FE/I0NZeIiIjIqfBpMpudnU2bNm2YNm1audrv2LGDQYMGcdFFF7F27Vruvvtu/vnPf/L9999XcaSVK0RTc4mIiIhUCrsvLz5gwAAGDBhQ7vavv/46jRs35j//+Q8ALVu2ZMmSJbzwwgv069evqsKsdCHFsxnkF7opKHTjZ1e1h4iIiMjJ8GkyW1HLly+nd+/eXvv69evH3Xfffcxj8vPzyc/P97zOyMgAwOVy4XJV/Z/5S65x9LX8rG7P88PZuUQE+lV5HHJqyupHqXnUj7WD+rF2UD/WDlXVjxU5X41KZpOSkoiNjfXaFxsbS0ZGBrm5uQQEBJQ6ZvLkyUyaNKnU/nnz5hEYGFhlsXoYbuLT1zB/ngEWi2e3n9VGgdvCV9/9QB3/qg9DKsf8+fN9HYJUAvVj7aB+rB3Uj7VDZfdjTk5OudvWqGT2ZEyYMIFx48Z5XmdkZFC/fn369u1LaGho1V7cMODb+3DsmImrw63Q90mwmCUFT2/8meSMfD5JjuT/+jSne9NILEclu3J6cblczJ8/nz59+uBwOHwdjpwk9WPtoH6sHdSPtUNV9WPJX9LLo0Yls3FxcSQnJ3vtS05OJjQ0tMxRWQCn04nT6Sy13+FwVMs3T1GdJub1Vr0JBekwZBrYHIy95Cye+noTG/ZlMPKd1XRtUof7+59NuwYRVR6TnLzq+ncjVUv9WDuoH2sH9WPtUNn9WJFz1ag7j7p27cqCBQu89s2fP5+uXbv6KKITc3e+g9UN/4VhtcPvc+CDoVCQzbDODfj5/osY2b0RfjYry7cf4vJXl3HLu6v4MznT12GLiIiI1Ag+TWazsrJYu3Yta9euBcypt9auXcuuXbsAs0Tgxhtv9LS/7bbb2L59O/fffz+bN2/m1Vdf5cMPP+See+7xRfjltieyO0VX/w/sAfDXfHh3COSkEhXsZOLgc/jx3p5c3b4eVgvM35RM/xcX8+jnG0jNLvB16CIiIiKnNZ8ms6tWraJdu3a0a9cOgHHjxtGuXTseffRRAPbv3+9JbAEaN27M119/zfz582nTpg3/+c9/+O9//1sjpuUymvWBEV+AfzjsWQkzBkD6XgDqRQTy3NVt+P7uC+nTKpYit8G7y3fS67mfeHvJDgoK3cc/uYiIiMgZyqc1s7169cIwjGO+X9bqXr169eK3336rwqiqUP1OMOo7+N8VcHAzvN0HLn8dGl8IQPPYEN66sQPLtqXwxFd/8Mf+DJ74ahPvr9jJQ4NacnGLGN0kJiIiInKUGlUzWyvEtISbv4c6zSFjL7wzGL65HwqyPU26NY3iqzsv4JkrziMq2I/tKdnc/M4q7vzgNy2BKyIiInIUJbO+EN4AbvkR2t9kvv71DXitO+xc7mlis1q4rlMDfrq3F7f1bIrdauGr3/dz6ctL2LA33Tdxi4iIiJxmlMz6in8oDH4Rrv8EQhMgbYdZR/vdg+DK9TQL8XcwfkALPrytKwnhAew8lMMVry7jnWWJxy3REBERETkTKJn1tWaXwB3Lod31gAErpsGMgV4JLcD5DSL45q4e9G0VS0GRm4lfbOT299aQnquyAxERETlzKZk9HfiHmYspDJsLARGwbw18OdZcQewoYYEO3rihPRMHt8Jhs/DdxiQGvbSYZdtSfBS4iIiIiG8pmT2dnNUXrnkXLDZzgYUVr5ZqYrFYGNm9MR/d1o36kQHsSctl2Fu/8NCn63VzmIiIiJxxlMyebhpfCP2eNp/Pexi2/VRmszb1w/nmrh4M79wAgPd/2UW/FxaxcMuB6opURERExOeUzJ6OOv8L2gwDww0fjYS0xDKbhfg7eOry85h1S2caRAayLz2Pm2as5N6560jP0SitiIiI1H5KZk9HFgtc+gLUPR9y02D2cK95aP+uW9Movru7ByO7N8JigY9W76H3Cz/zxbp9mvFAREREajUls6crhz9c+x4ERUPyBvh8dKkbwo4W6Gdn4uBzmPuvrjSJDuJgZj53ffAbN07/lZ2Hjp0Ii4iIiNRkSmZPZ2EJcM3/wGqHjZ/CD48dN6EF6NAokm/H9uCe3mfhZ7ey+M8U+r6wiFd+/JOCQnf1xC0iIiJSTZTMnu4adoWB/zafL50K340H9/GTUqfdxtjezfn+7gvp3qwO+YVu/j1vKwNfWsy63YerPGQRERGR6qJktiboMBIG/cd8/svr8MWd4C464WGNo4J47+bOTL22LVHBfvx1IIur31jOZ7/treKARURERKqHktmaouM/4bLXwWKFte/BxzdDYcEJD7NYLFzWLoEF43rRu2UsBYVu7p6zlme/24zbrZvDREREpGZTMluTtB0KV88Eq8OsoZ1zfallb48lLNDBmze0545eTQF4beE2bv3fKrLyC6swYBEREZGqpWS2pmk1BIbOBrs//Pk9vH81FOSU61Cr1cL9/Vvw4nVt8bNb+eGPA1zx6lJ2HSrf8SIiIiKnGyWzNVHz3nD9J+AXAomLzZKDctTQlhjSNoEP/9WVmBAnW5OzGDJtCVuTM6swYBEREZGqoWS2pmrUHa7/CGxO2PINfHPfCaftOlrb+uF8MeYCzksIIy3Hxe3vrSZbJQciIiJSwyiZrckadIEr3wIssOptWPJChQ6PC/Nn5siOxIY62XYwm4c/26AVw0RERKRGUTJb07UaAv2fMZ8vmATr5lTo8DrBTl4eej5WC3z6214+XLW7CoIUERERqRpKZmuDLrdB1zHm889Hw/aFFTq8U+NI/q/v2QA8+vlGNidlVHKAIiIiIlWjQsnslClTyM09MhXU0qVLyc/P97zOzMzkjjvuqLzopPz6PAHnXA5uF8y5AZI2VOjw23s2pedZ0eQXurnj/TWasktERERqhAolsxMmTCAz88hd7wMGDGDv3iOrSeXk5PDGG29UXnRSflaruahCw+6QnwGzroW89AocbuGFa9sSF+rP9oPZPPTpetXPioiIyGmvQsns35MbJTunGYc/XPc+RDSGjD3w/YMVOjwyyI+Xh7XDZrXw+dp9zF6p+lkRERE5valmtrYJiIDLXgUs8Nt78Of8Ch3esVEk9xbXzz72xUbNPysiIiKnNSWztVHDbtD5NvP5F3dB7uEKHf6vC5t46mfv+uA38lzlX5BBREREpDrZK3rAf//7X4KDgwEoLCxk5syZREVFAXjV04qPXfKoudxt6nb4/iG4bFq5D7VaLfz76jYMeHERm5Myefa7zUwcfE4VBisiIiJyciqUzDZo0IC33nrL8zouLo7//e9/pdrIacAvEC57Dab3h7XvQat/wFn9yn14dIiT565qw8iZK5mxNJELz4rmorNjqjBgERERkYqrUDKbmJhYRWFIlWjQBbrcASumwZdj4Y7lZk1tOV3UIoabujVi5rJE7pu7jm/HXkh0iLMKAxYRERGpGNXM1nYXPwx1mkHmfviuYrMbAIwf0IKzY0NIySrg/o/WaQYLEREROa1UKJldvnw5X331lde+d999l8aNGxMTE8Ott97qtYiCnAb8AmFI8ewG62bB1u8rdLi/w8ZLQ9vhZ7fy05aDvLMssUrCFBERETkZFUpmH3/8cTZu3Oh5vX79em6++WZ69+7N+PHj+fLLL5k8eXKlBymnqEFn6DrafP7T0xU+/Oy4EB4a2BKAp7/dzB/7tdytiIiInB4qlMyuXbuWSy65xPN69uzZdO7cmbfeeotx48bx0ksv8eGHH1Z6kFIJLhgHNifsXwt711T48Bu7NuTiFjEUFLoZ/t9fWPJnSuXHKCIiIlJBFUpm09LSiI2N9bz++eefGTBggOd1x44d2b1bq0adloLqwDmXmc9XvV3hwy0WC89d1Zpz6oaSml3AjdN/YdpPf+F2q4ZWREREfKdCyWxsbCw7duwAoKCggDVr1tClSxfP+5mZmTgcjsqNUCpPh1Hm4/qPK7yQAkCdYCcf396NazrUw23Ac99v4db/rSY911W5cYqIiIiUU4WS2YEDBzJ+/HgWL17MhAkTCAwMpEePHp73f//9d5o2bVrpQUolqd8ZYs6Bwlz4fc5JncLfYWPKVW145orz8LNb+eGPZP7xyhLV0YqIiIhPVCiZfeKJJ7Db7fTs2ZO33nqLN998Ez8/P8/706dPp2/fvpUepFQSiwU6jDSfr3wbTmGares6NeDj27pRLyKAnYdyuPzVpcxYukNlByIiIlKtKpTMRkVFsWjRItLS0khLS+OKK67wen/u3Lk89thjlRmfVLbW14IjCFK2wM5lp3Sq8+qF8dWdF9Dr7GjyXG4mfbmJq99Yzl8HsiopWBEREZHjq9AKYKNGjSpXu+nTp5f7nNOmTeO5554jKSmJNm3a8PLLL9OpU6djtp86dSqvvfYau3btIioqiquuuorJkyfj7+9f7mue0fxDofXVsHomrJoOjbqf0unCA/2YPqIjs37dxTPfbmb1zjQGvrSYsZc059YLm+CwaV0OERERqToVyjRmzpzJTz/9xOHDhz2js2Vt5TVnzhzGjRvHxIkTWbNmDW3atKFfv34cOHCgzPazZs1i/PjxTJw4kT/++IO3336bOXPm8OCDFV/Z6oxWciPYps8h6+Apn85qtXB9l4bMu+dCep0dTUGhm+e+38Jl05ayYW/6KZ9fRERE5FgqNDJ7++2388EHH7Bjxw5GjhzJ9ddfT2Rk5Elf/Pnnn+eWW25h5EizjvP111/n66+/Zvr06YwfP75U+2XLltG9e3eGDRsGQKNGjRg6dCi//PLLScdwRopvAwkdYO8qWPseXHBPpZy2bngAM27qyKe/7eXxrzaxcV8G/3hlCVeeX49xfc8iPiygUq4jIiIiUqJCyey0adN4/vnn+eSTT5g+fToTJkxg0KBB3HzzzfTt2xeLxVLucxUUFLB69WomTJjg2We1WunduzfLly8v85hu3brx3nvv8euvv9KpUye2b9/ON998ww033HDM6+Tn53stsZuRYd5173K5cLmqfkqpkmtUx7UqwtJuBPa9qzBWzaCw0x1gqbxygMHnxdK1cThPfr2FrzckMXf1Hr5Yt48RXRvwrx6NCQ2oedO3na79KBWjfqwd1I+1g/qxdqiqfqzI+SyGcfK3tO/cuZOZM2fy7rvvUlhYyMaNGwkODi7Xsfv27SMhIYFly5bRtWtXz/7777+fn3/++ZijrS+99BL33nsvhmFQWFjIbbfdxmuvvXbM6zz22GNMmjSp1P5Zs2YRGBhYrlhrI6u7gH4b7sKvKIflTe/lQGjrKrlOYiZ8sdPGtkzzF51Au0HfBDcXxBk4VE4rIiIiZcjJyWHYsGGkp6cTGhp63LYVGpn9O6vVisViwTAMioqKTuVU5bJw4UKefvppXn31VTp37sxff/3F2LFjeeKJJ3jkkUfKPGbChAmMGzfO8zojI4P69evTt2/fE35xKoPL5WL+/Pn06dPntFtQwupcCb++QWfbRooGli7rqCy3GwY/bjnIv+f9yV8Hs/lsp43lqU5uvqAR13ZIINDvlP4ZVovTuR+l/NSPtYP6sXZQP9YOVdWPJX9JL48KZxH5+fmeMoMlS5Zw6aWX8sorr9C/f3+s1vIPtUVFRWGz2UhOTvban5ycTFxcXJnHPPLII9xwww3885//BOC8884jOzubW2+9lYceeqjM6zudTpxOZ6n9DoejWr95qvt65dLxn/DrG1j//B5rTjKE1auyS/U/L4HereL5ZM1eXvhhK/vT83j62y28vmgHo7o34oaujQirAeUHp2U/SoWpH2sH9WPtoH6sHSq7Hytyrgr9ofeOO+4gPj6eZ555hksvvZTdu3czd+5cBg4cWKFEFsDPz4/27duzYMECzz63282CBQu8yg6OlpOTU+o6NpsNgFOoljhzRZ8FjXqA4Ya1s6r8cnablWs61mfhfb2YfMV5NIgMJDW7gH/P28oFz/zIlO82czinoMrjEBERkdqjQiOzr7/+Og0aNKBJkyb8/PPP/Pzzz2W2++STT8p1vnHjxjFixAg6dOhAp06dmDp1KtnZ2Z7ZDW688UYSEhKYPHkyAIMHD+b555+nXbt2njKDRx55hMGDB3uSWqmgdjdA4mJY+z5ceJ+5SlgVc9ptDO3UgKvb1+Pr9fuZ9tNfbE3O4tWF25j16y7uvqQ5w7s01By1IiIickIVSmZvvPHGCs1YcCLXXnstBw8e5NFHHyUpKYm2bdvy3XffERsbC8CuXbu8RmIffvhhLBYLDz/8MHv37iU6OprBgwfz1FNPVVpMZ5yWl8LXIZCWCLuWQ8Nu1XZpu83KkLYJDG5dl/l/JPP8vK1sSc7ksS838b8VO3l4UCt6nR1dqf/mREREpHapUDI7c+bMSg9gzJgxjBkzpsz3Fi5c6PXabrczceJEJk6cWOlxnLH8guCcy+C3/5mjs9WYzJawWi30OyeOS1rEMHvlbp6fv5VtB7MZOXMlPZpH8fCgVpwdF1LtcYmIiMjpT3/HFWg73Hzc+BkUZPssDLvNyvVdGrLwvl7868ImOGwWFv+ZQv8XF/HPd1bxy/ZDqo0WERERL0pmBRp0gcgmUJAFm77wdTSE+juYMLAlP4zryYBz4zAM+OGPZK59cwVDpi3l87V7cRW5fR2miIiInAaUzIp501dbc4lg1r7v21iO0rBOEK9d354fxvVkWOcGOO1Wft+TztjZa+k55Sde/OFPNidlaLRWRETkDKZkVkytrwMs5swGaTt9HY2XZjHBPH35eSwbfzH39D6LqGA/9qXn8cIPW+k/dTG9/r2Qp77exMrEVIrcSmxFRETOJKf/0ktSPcLrQ5OesH0hrPsAelXdimAnq06wk7G9m/Ovnk34+vf9fLthP4v+TGHnoRzeWryDtxbvoE6QH73OjuHiFjH0OCuKUH9NxC0iIlKbKZmVI9oON5PZtbPgwvuhggthVBd/h40r29fjyvb1yM4vZNHWg8zblMyCP5I5lF3Ax2v28PGaPditFto3jODiFjFc1CKG5jHBmuZLRESkllEyK0e0uBScoXB4J+xaBo0u8HVEJxTktDPgvHgGnBePq8jNrztS+WnzAX7ccoDtB7P5ZUcqv+xIZfK3m4kJcdK9WRTdmtahe7Mo6oYH+Dp8EREROUVKZuUIv0A453JY8445OlsDktmjOWxWujeLonuzKB6+tBU7D2WzcMtBftx8gBXbD3EgM59Pf9vLp7/tBaBJVBC9zo7h8nYJnJsQqlFbERGRGkjJrHhrO9xMZjd+BgOmgDPY1xGdtIZ1ghjRLYgR3RqR5ypiza40lv11iCV/pfD7nsNsT8lme8oOpi/dQbOYYC5vl8CQtnWpFxHo69BFRESknJTMirf6nSCyKaRug02fQ7vhvo6oUvg7bHRrGkW3plHc2+9sMvJcLN92iC/X7WP+pmT+OpDFc99v4bnvt9CpcST/vKAxfVrFarRWRETkNHd63uEjvnOazjlb2UL9HfQ7J45Xhp3Pyod7M+Wq1nRtUgeLBX7dkcqt/1vNNW8sZ82uNF+HKiIiIsehZFZKazMULFbYuRTWzfZ1NFUu1N/BNR3q88GtXVj6wMXc3qspTruVlYlpXPHqMm5/bzU7Uny3zK+IiIgcm5JZKS0sAXr8n/n8y7Gwb61Pw6lOdcMDeKB/Cxbe14trOtTDaoFvNyQx8OVlfLjdyvaDSmpFREROJ0pmpWy9JkDzvlCYB3Ouh+xDvo6oWsWHBTDlqjZ8O/ZCLm4RQ6HbYGmylX4vLeWq15bx4ard5BQU+jpMERGRM56SWSmb1QZXvAWRTSB9N3x0ExSdecnb2XEhTL+pI++N6sA5EW6sFli1M437P/qdTk8tYMInv7N6ZypuLaMrIiLiE0pm5dgCwuG6WeAXDDsWwQ8TfR2Rz3RuHMmtLdwsuvdC7ut3Ng3rBJKVX8gHv+7myteW0/3ZH5n05UZWJiqxFRERqU6amkuOL6YlXPYafHgDLH8F4ttC66t9HZXPxIb6M/qiZtzesym/7Ejlw1W7mbcxif3pecxYmsiMpYnEhDgZcG4cF7eMpWOjCAL99G0mIiJSVfS/rJxYq39Aj3th8b/hizsh+iyIb+PrqHzKarXQtWkdujatQ56riMV/pvDt+v3M35TMgcx83lm+k3eW78Rhs3B+gwguaBZF9+ZRtE4Iw27TH0REREQqi5JZKZ+LHoT96+Cv+fDBULh5vjnrgeDvsNGnVSx9WsWSX1jE0r9S+G5DEkv+TGFfeh6/7Ejllx2p/Gf+VoKddlrGh9AqPpSW8aG0qhvKWbEh+Dtsvv4YIiIiNZKSWSkfqw2u/C+83RdStsD7V8Oob8E/zNeRnVacdhsXt4jl4haxGIZB4qEclvyVwrK/Uli27RDpuS5WJqaxMvHIYgxWCzSOCuKs2BCax4ZwVmwwZ8WG0KhOEH52jeKKiIgcj5JZKb+AcLj+I/hvbziw0Zyya/jHYPfzdWSnJYvFQuOoIBpHBXFDl4YUuQ22Jmfyx/6M4i2TTfszSM0uYNvBbLYdzObbDUme4+1WC3XDA0gIDyAh4shjvfAA6oYHEBfmrxFdERE54ymZlYoJbwDD58KMgeYMB5+PhiveNJfBleOyWS20LC4vKGEYBgcy89mclMmfyZn8mZzF1gPmY1Z+IbtSc9iVmnPMc0YFO6kb7k/dMDPRbRAZSIPIQOpHBlIvIkDJroiI1HpKZqXi4tvANe/CrGtg/YcQVg96n7nTdp0Ki8VCbKg/saH+9Dwr2rPfMAySMvLYnZrL3sM57E3LZe/hXPYUP+47nEuey01KVj4pWfn8vie9zPPHhfqbCW6dQBrVCaRBnSAaRgbSsE4gYQEOLPolREREajgls3Jyml0Cg1+Cz++AJc+bN4N1/Kevo6o1LBYL8WEBxIcFAJGl3jcMg8M5Lk9iuz89j92pOexOy2FXai67DmWTXVBEUkYeSRl5/JqYWuocwU67OapbXLaQEB5AfJg/caH+xIaZCXawUz8iRETk9Kb/qeTktRsOGXvhp6fgm/vA6oD2I3wd1RnBYrEQEeRHRJAf5yaUvgnPMAzSclzsPJTNrtQcdh4yt12p2ew8lMOBzHyy8gvZmpzF1uSsY14n2GknJtRJXOiRJDeueCQ5rvh5dIgTm1UjvCIi4htKZuXUXHifmdCunglf3gVJv0O/ybopzMcsFguRQX5EBvnRrkFEqfdzC4rYl26O6u47nMvew3me58kZeSRnmMluVn4hWQcL2X4w+5jXslogOsTpSXJjQ/2JCPIjMtBhJtyB5lYn2NycdtXxiohI5VEyK6fGYoFLp5p1sz8+BSv/C8mbzJra4OgTHi6+EeBno2l0ME2jg4/ZJiu/0Exs0/M85QpHnueTnJ7Hwax8itwGyRn5JGfkA2XX7h4tLMBBdIiT6GAn0SFOT9IdUZz8RgaaI851gs3nWmRCRESOR8msnDqLxRyhjT0XPrkVdi2DN3vCde9D3Xa+jk5OUrDTTvAJEt4it0FKVj7JGXkkpeeRnJHHgcx80nIKSMt2kZZTQGp2gefRVWSQnusiPdfFXweOXd5wtIhAB3WCndQJ8iM21J96xbM21I8MpH5EIPHh/jiU8IqInLGUzErlOXsA/HMBzB4Gh/6E6f3Nm8TaXOvryKSK2KxHZmNoXe/4bQ3DTGQPZuabW5b5aCa7LtKyC0jNKeBwceKbml2A28B8L8fFX8eJoV5EQPFIcxBNo4NpFhNMk+hgIgI1Y4OISG2nZFYqV/RZcMsC+PgW+PN7+PRWSN8NPf5Pc9Ge4SwWC+GBfoQH+tE8NuSE7YvcBodzCjiUXVA8BVkBSem57E7NZXdaDrtTc9iTlkt+odtzg9uPm73PEeJvp2GdQBpGBplz74Y7SU6HwzkuosMcVfRJRUSkOimZlcrnHwZDZ8OCSbB0Kvz4BGSnQL+nwao/B0v52KwWs7wg2MlZx0h+3W5z0YkdKdlsO5jFXwey2HYwi+0Hs9l7OJfMvEI27M1gw96Mo46y8/Kmn4gP8y9exCKEFnGhNI8NpkFkIIF++rEoIlKT6Ke2VA2rFfpMgpA4+G48/PIa5KTAkFc104FUGqvVYk4RFuZP16Z1vN7LLShid1rJtGTmFGWJKVls2JVCar6F/el57E/P48fNB7yOiwlx0rBOIA0ig2hYJ5BGUUE0iQqiUVSQ5t0VETkN6SezVK0ut0NgFHx2G6yfCzmp5kwHzmPfVCRSGQL8bJwVG+I1qutyufjmm2/ocXEfth3K44/9GfyxP5M/9meQeCibwzkuDmTmcyAzn5WJaaXOGR3ipHFUEI3rmMlt4ygz2W1UJ0hLB4uI+IiSWal6ra+GgAj48AbYtgDe/QcMmwtBdU58rEgVCPF30LFRIB0bea+ulp7jYmdqNomHcth1yHxMTMlmR0o2h7ILPDev/bqj9Ipq8WH+ZqJbvDWNDqZxVBD1IgI0vZiISBVSMivVo3lvGPElvH817F0N/70YrvkfxLf2dWQiHmGBDloHhtO6Xnip99JzXSSmZJN4KJvtB83HkkQ3I6/QU7awbNshr+PsxaUQ8WH+xIUFUDfsyOppMaFOYkLMVdQ0sisicnKUzEr1qdcBRn0P718JaYnwdh8Y9B9od72vIxM5obAAB23qh9OmfrjX/pKlg3ekZLEjJaf40Ux4d6Rkk1/oZk9aLnvScoHSpQslQv3txIT6Ex3sJDLYj6ggPyKDnObKaUF+xTfD+REV5CQ0wK4px0REiimZleoVfRbc+jN8eps5ddfno2HXChj4HDgCfB2dSIUdWTo4kvYNvcsW3G6DpIw89qfnsj/dXFii5DEpI48DmebSwQWFbjLyCsnIyyrXYhIOm3nNOkFOoo5aTS0q2M+ztHCDOoHEhvhjtSrpFZHazefJ7LRp03juuedISkqiTZs2vPzyy3Tq1OmY7Q8fPsxDDz3EJ598QmpqKg0bNmTq1KkMHDiwGqOWUxIYaU7dteR5+Okp+O1/sH+deWNYZGNfRydSaaxWC3XDA6gbfuxf1AzDICO3kAOZ5uppKVn5HMoq4FC2uaBESlYBh7LM54eyCsjML8RVdNQSwvuPfX2n3Ur9yEAaRgbSoE6guXJaRPHqaZEBmoZMRGoFn/4kmzNnDuPGjeP111+nc+fOTJ06lX79+rFlyxZiYmJKtS8oKKBPnz7ExMTw0UcfkZCQwM6dOwkPD6/+4OXUWK1w4b2Q0B4+vhmSfoc3esLAKdD6Wi2wIGcMi8VCWKCDsEBHuRaTyHMVeVZIO5iVT0rxamopmQXFq6qZo78lC0r8deDYo711gvyoFxlIbIg5suvZgp3EhPoTG+okKtip5YJF5LTm02T2+eef55ZbbmHkyJEAvP7663z99ddMnz6d8ePHl2o/ffp0UlNTWbZsGQ6HuXpPo0aNqjNkqWxNL4J/LYa5I2DPSvj0X/DbezDoebMkQUS8+DtsJxztBSgscrPvcB47U7PZeSiHXanmqmnm6mm5pOe6OJRtrrB2PBYLRAU7iQ11Elt8s1pEkFnHG1m81Qky63zrBPnpRjYRqXY+S2YLCgpYvXo1EyZM8OyzWq307t2b5cuXl3nMF198QdeuXRk9ejSff/450dHRDBs2jAceeACbrewfoPn5+eTn53teZ2SYKwG5XC5cLlclfqKylVyjOq5VYwXGwPWfY13xKtYl/8aSuBjjtW64u96Fu/vdp0UtrfqxdjjT+jE+1EF8aDhdGoWXei8zz8XutFz2puUVj+jmczDLXDr4YFY+BzLMJYQL3YZnSrINZJS+yN8EOW3mDWvFiW5YgIOIQAfhAeboc3iAg4hAP8IDHYQHOogIcOCsYAJ8pvVjbaV+rB2qqh8rcj6LYRhGpV69nPbt20dCQgLLli2ja9eunv33338/P//8M7/88kupY1q0aEFiYiLDhw/njjvu4K+//uKOO+7grrvuYuLEiWVe57HHHmPSpEml9s+aNYvAwMDK+0BSKQLzD9B6z7vEZvwOQJZfDL/Xv5GDoZrCS6S6uQ3IckGGC9ILLKQXQKYLslwWslyQXVj8vNBsV2ScXHmQn9UgyA6hfhDqMAj3g1A/gzA/CPeDCKdBhBMcqnYQOWPk5OQwbNgw0tPTCQ0NPW7bGpXMnnXWWeTl5bFjxw7PSOzzzz/Pc889x/79Zd8FUdbIbP369UlJSTnhF6cyuFwu5s+fT58+fTylEXIChoFly1fYvp+AJSsJAHezvhRdMgmimvskJPVj7aB+rDqGYZCVX2iWLmQVkJptljGk57pIyyngcK6LwznFW/G+9NxCitzl/y8oJsRJ3XB/6oY6KUhL4oJ2LWkYFUz9iADiw/zxsyvbrUn0/Vg7VFU/ZmRkEBUVVa5k1mdlBlFRUdhsNpKTk732JycnExcXV+Yx8fHxOBwOr5KCli1bkpSUREFBAX5+fqWOcTqdOJ3OUvsdDke1fvNU9/VqvPOuMBdaWDgZfn0T61/zsG5bAB1vhl4TzBkRfED9WDuoH6tGpJ8fkSGBlPdXTrfbIDO/kMM55g1tBzLzOZBhTleWlJFHcoY5jdnew7nkFBR5lhpeC4CVH/Zt8ZzLaoH4MDOpjQ0t2Zye5/HFi1Wopvf0o+/H2qGy+7Ei5/JZMuvn50f79u1ZsGABl112GQBut5sFCxYwZsyYMo/p3r07s2bNwu12Y7Wav4Fv3bqV+Pj4MhNZqeH8Q6H/ZOgwCuY9Alu/hV/fhN/nQM8HoOMtYFe/i9RUVquFsAAHYQEOGtYJOma7koUp9qblsicth52Hsli6djO20Bj2HM5jd2oO+YVu9h7OZe/h3ONeMyLQQVxx0hsf5k9CRAD1IgJJCA+gfkQAUcFOzc0rUsP4dDaDcePGMWLECDp06ECnTp2YOnUq2dnZntkNbrzxRhISEpg8eTIAt99+O6+88gpjx47lzjvv5M8//+Tpp5/mrrvu8uXHkKoW1RyGzYbtC+H7hyB5A3z/ICx7Gc6/0dzC6vk6ShGpIkcWpvDjvHphuFwu4tM3MXDg+TgcDgzD4GBWPrtTc0hKzyc5I4/kzDwOZOSTlG6O8O5PzyPXVURajou0HBd/7C/7ZjY/u5XoYHMBisiSldeC/MzV14K9py+LCPRT4ityGvBpMnvttddy8OBBHn30UZKSkmjbti3fffcdsbGxAOzatcszAgtQv359vv/+e+655x5at25NQkICY8eO5YEHHvDVR5Dq1KQX/GsRrJ0FPz4Jmfvh52dh0XNwVn9zBLfpxWDVnxFFziQWi4WYEH9iQvyP2cYwDDLyCotXYcv1lC/sTctlT/Hj/vRcCso5wgtgs1qICPQjItCcoaFktobwQAcRQX5EBvp5pjEreQz1dygBFqlkPl/+ZcyYMccsK1i4cGGpfV27dmXFihVVHJWctqw2OP8Gc2GFzV/CqhmQuBi2fGNu4Q3gwvug7fXmwgwiIhQvTlFc0nB2XNmLU7iK3CSlm1OVmTexmdOTmauvmc8PlqzSll1AkdsgJct8XV4lCXBUsDnaGxnk9ExlVifYSZ3gI88jg/wI9bdj0SIyIsfl82RW5KTY/eDcK83t4BZYPRPWvg+Hd8EXd8LKt2HAFGjQ2deRikgN4bBZi5f6PfG0ja4id/Fyw/mkHzVDgzljQ4FZzlC8KEVa8Q1umXmFFU6AHTYz+Y0MMsscIoP8CA2wE+x0EOJvJ9hpbiH+diKCjholDnBg18ptcoZQMis1X/TZ5o1iFz8Cq96Ghc/C/rUwvS+0vg56Pwah8b6OUkRqEYfN6pkpobwKCt2k5ZhTlx3Kzi9+NBPi1OLnh7LzzSQ5M5/sgiJcRYZnFgfIrFCMof52ooKdRBXX+HrV+xYnvuElC1go+ZUaTMms1B5+gdDtTrMEYcEkc1nc32fD5q/gwnuh6xiwafoXEfENP3vFEuA8VxGp2cVlDtlm2cOhLHOENyu/kKzix8z8QjJyXZ4pzjLyCgHIyCskI6+Q7SnZ5bpeiL/dq8QhKthcqthrxLf4MSLQQbDTrgRYTgtKZqX2CY6BIdPMG8K+fQD2rIQfHoNNn8Nlr0NMC19HKCJyQv4OG3XDA6gbXrElvQuL3GbZQ3ESXLIc8cGsfFKKR3lLSiEO5xxJfjPzCsnMKyTxUE4FYrQS7LQT5LQT5GeWPAQ6bQT52Qn0s5mb006Qn81sU9wuyGnD3wZ7s2FXag5hQf4EO+047VbVCEuFKZmV2iuhPYyaB+s+MKfy2vcbvHEhXPIodLlDN4iJSK1kt1nN8oJgZ7kWsCgschev1Oby3Ox2ZCW3fO+V3IpXccvKNxPgPJebPFcBKVkFJxstU35f4nlltUCQnx1/Pxv+DisBDhv+Dhv+dhv+fjYCHWaCHOBX8mgvbnNUW4cVp8OG027FaS95tOJX/Nphs+Bnt+KwWfGzWTW7RC2gZFZqN6sV2g03p+z6Ygz89QPMe8ic+WDINIhs7OsIRUR8ym6zFs+kUHq1zGMpKHSTnV9c7pBf6HmenV9ETkEhOQVFZBcUkltQ5NlX0i47v6j4OBdpmTkUYiPX5QbAbUBmcelEdbFbjyS3ZoJrwWE3E12HzVr83LuNw2bBbrVit1lwWK047MWvrRZsNov5WPzabrN4zmW3WTxJtM1a0s5sU9Lea7/VbGezWrBazIVGbBYLVosFqxWsFvM9i8V8brWY71usJa/Nx5L3bcXPa9vot5JZOTOExsPwj8xZD75/CHYuhde6Q9/Hof0ojdKKiFSAn92Kn92cP/dkuVwuvvnmGwYO7IfVZifXVUROcVKc53KT6yoi31VEXmERuQXm69ziRDmnoMhsX1BIboGbvMLiti43eS7zvYJCN/mF7uLHI68L3YZXHIVug8KCIqDoFL8qNceR5NdMbK1HJcNHv2e+Np+bCbWF2bd2KdeMH9VJyaycOSwW6DDSXHzh89FmQvv1/8Fv78Og/0DC+b6OUETkjGSzWjzTjMVU8bXcboOCIjeuIjPRdRUZ5qO75LW55Re/5yp0e9rnF79fWGSYj26DwiKzXaHbTZEbitzm/iK3gavIoMhdfI0iNy7P+UvaG6XaFhW/LnQbuN0GLreBYRie/YYBRcaR527DoMgwn5eX5xzmqwp9/SpyneqiZFbOPJGNYcRX8Oub5kpi+9bAWxebie7Fj0BgpK8jFBGRKmK1WvC3mvW1tYlhGLgNzCSXI4muu/jRcJe8NvcZRyXBbsO7fZHbAI48P/r92LDyl6NUFyWzcmayWqHLbXDOZTDvEVj/Iayabs540OdxaDNMpQciIlJjWCwWbMXlAGca/W8tZ7aQOLjyLXOkNroF5BwySxDeuRQObfN1dCIiInICSmZFABr3gNuWQN8nwRFUfINYN1gyFYqq765aERERqRglsyIlbA5zBbE7lkOTi6AwD36YCP+9BJI3+jo6ERERKYOSWZG/i2gIN3wKQ14F/zDYvxb79Etosf9jcOX6OjoRERE5ipJZkbJYLOZiC6N/hRaXYnEXcnbS59jf6AYbPzs95yYRERE5AymZFTmekDi49j0Kr5hOriMSS/pumDsCZg6C/b/7OjoREZEznpJZkROxWDBa/oMFLZ+l6IJ7we5v3iD2xoXwxV2QddDXEYqIiJyxlMyKlFORzYm753gYswrOvRIwYM078GIbc4ncjH2+DlFEROSMo2RWpKLC68NV02HktxDfFlzZsPwVM6n94i7NTysiIlKNlMyKnKyG3eDWhTD8I2jQDYoKzJHaVzrA3JGw+1fdKCYiIlLFtJytyKmwWKB5H3PbtQIWPw9/fg8bPzG3yCbQZii0vgYiGvk6WhERkVpHyaxIZWnQBYZ/CEkbYPk02PQ5pG6Hn54ytwbd4NwrICACLFaw2sBiM58HRUF8G7A7ff0pREREahQlsyKVLe5cuPw1GPRv+OMrWPcBbF8Iu5aZ27HYnJDQ3kyKG3SF+p0gILy6ohYREamRlMyKVBW/IGhzrbml74X1cyFxsVlb63aDUQTuIvMxbSfkpPwt4bWYN5tFNIbIxkceI5tCTEtzZFdEROQMp2RWpDqEJcAFd5tbWQzDnAVh13Kz9nbXckjdBod3mduOn73bB0RAk4ugWW9odom5uIOIiMgZSMmsyOnAYoGoZuZ2/g3mvqyDcOgvSNsBqTuOPKZshdy0IzeZAcSeB016Quw55qht1NngF+i7zyMiIlJNlMyKnK6Co82tYVfv/UWFsHcV/PWDue1bC8nrzc3DYpYkxLSCum2hUQ+oez7Y/arxA4iIiFQ9JbMiNY3NXnyTWBe4+GHIToFtP8KelXDgDziwCXIOmTMppG6HzV+Zx9kDoEFnaHQBNOwOdZpBYBRYNd20iIjUXEpmRWq6oChzHtvW1xzZl3XQTGqTN8LuFZC4xExwty80txJWB4TGQ2gChNYt3uqZj2EJ5vOgaCW8IiJy2lIyK1IbBUdDcE+zjrbrHeYNZgc3m0lt4mLYvRIy94PbdeQms2OxOiC6BTTuAY0vNFc+8w+rvs8iIiJyHEpmRc4EFot5Y1hMS+h0i7mvyAWZSZCxDzL2HnlM33PkeWaSmfCW1OSueNVc5CG+rZnc1mkOwbEQEms+BkVryjAREalWSmZFzlQ2hzmPbXj9Y7cpcpmJ7d7VsGORuaVug31rzO3vLFYIrAMBkRAYaT4GREBgBATHHZkvN6KRZlsQEZFKoWRWRI7N5oCIhuZ27hXmvvS9ZqnCzmVmopuVbG7ZB8Fwm4/ZB0987uA4iGwC0Webq6bFngexrcAZUrWfSUREahUlsyJSMWEJ0OY6czuau8i8ySwrGXJSzblwc1OPPM/Ya86Tm7oD8tMhK8nc/r7Eb0RjiG9tLghx1gCz/ldEROQYlMyKSOWw2iA4xtyOxzDM5DZ1hzl12IGNkLQBkjeYN6WlFS8QselzwAL1O8HZA6HFIIhqXi0fRUREag4lsyJSvSwWs542MBLqtQeuPvJedgokrYfdv8KWb2D/Wtj9i7n9MBHCGkBMC4g6yyxPiDobos8y63JFROSMpGRWRE4fQVHQ9CJz6/WAWZ+75Rtz27EY0neZ25/zvI8LiS9eyrcVxJ5rPo86SyueiYicAU6LZHbatGk899xzJCUl0aZNG15++WU6dep0wuNmz57N0KFDGTJkCJ999lnVByoi1SsswZxKrNMtkJcBSb/DwS2QsvXIY8Zeszwhc7+5vG8Jq8NcCrh5X2jeTyUKIiK1lM+T2Tlz5jBu3Dhef/11OnfuzNSpU+nXrx9btmwhJubYtXeJiYnce++99OjRoxqjFRGf8Q81l+JtdIH3/rwMc0GI5A3mimfJxSuf5acfmU5s3sMQ3hBr097EpYdA5vkQeZwpyUREpMbweTL7/PPPc8sttzBy5EgAXn/9db7++mumT5/O+PHjyzymqKiI4cOHM2nSJBYvXszhw4erMWIROa34h5o3idU/6q85hgGHtpkjtX9+b658dngnttVv0xngpanmIg/xbSCutfmY0N4cCRYRkRrFp8lsQUEBq1evZsKECZ59VquV3r17s3z58mMe9/jjjxMTE8PNN9/M4sWLj3uN/Px88vPzPa8zMjIAcLlcuFyuU/wEJ1Zyjeq4llQd9WMNFNYQ2t9sbgXZWBIXY2z9ntwtPxKSvx9LVrJZe3tU/a0RWg+jXkeMep1x1+9k1uBaff47v/yNvh9rB/Vj7VBV/ViR8/n0p3RKSgpFRUXExsZ67Y+NjWXz5s1lHrNkyRLefvtt1q5dW65rTJ48mUmTJpXaP2/ePAIDq28Fovnz51fbtaTqqB9rOGsfaNkHmzuf0NzdhOUkEp67k7CcHYTl7saSsQfLpj2w6VNsQKHVn4MhrUgKa0dyaBvyHeG+/gRyFH0/1g7qx9qhsvsxJyen3G1r1JBDZmYmN9xwA2+99RZRUVHlOmbChAmMGzfO8zojI4P69evTt29fQkNDqypUD5fLxfz58+nTpw8Oh6PKrydVQ/1YO5T048X9Li3Vj4UFWVj2rsGy5xcse1Zi2bsSe34m8elriE83l+51x7fFaNYXo+klGNEtwC/IFx/jjKfvx9pB/Vg7VFU/lvwlvTx8msxGRUVhs9lITk722p+cnExcXFyp9tu2bSMxMZHBgwd79rndbgDsdjtbtmyhadOmXsc4nU6cTmepczkcjmr95qnu60nVUD/WDmX2oyMCzrrE3MBc0SxpvVmGsPU72Lsa6/615ty3i6eYbcIamPPcRp1dPO9tc3OJ3uBYcz5dqVL6fqwd1I+1Q2X3Y0XO5dNk1s/Pj/bt27NgwQIuu+wywExOFyxYwJgxY0q1b9GiBevXr/fa9/DDD5OZmcmLL75I/fq6O1lEKonVBnXbmlvP+yEz+Uhiu2u5uXRvyby3R08JBuAINJPayMbmY+y5ULcdRDYFq9UXn0ZEpNbyeZnBuHHjGDFiBB06dKBTp05MnTqV7Oxsz+wGN954IwkJCUyePBl/f3/OPfdcr+PDw8MBSu0XEalUIbFw/g3mBpB9CFK2mPPdHtxiPj+0DdJ3gyuneKqwDd7ncIYWz5xwvjmLQlh9CIkzN3vpvyCJiMiJ+TyZvfbaazl48CCPPvooSUlJtG3blu+++85zU9iuXbuwaiRDRE43QXUgqBs07Oa9v7AADu+C1O3mdugv2L/OXPAhPwMSF5vb3wXWMVcyC4kzH0PrFj+vC6HxEFrPXAJY5QsiIl58nswCjBkzpsyyAoCFCxce99iZM2dWfkAiIifL7gdRzcztaEWFcPAP2PebuSVvKl65LAmK8s2yhZxDpUdzj+YIhLB6xVt9CK9v1uvGtISIxmA7LX6ki4hUK/3kExGpDjY7xJ1nbuffeGS/YUBuGmTsO7Isb2ZS8eskyNwHGfsh+4BZvpCy1dxKnd8JUWdBTAuIbwstL4WIRtX16UREfEbJrIiIL1ksZvlAYCTEHaf2vzAf0vcUb7vNx7REOPCHWbNbmAvJ681t/VyY95B501mry6DVEPNmNBGRWkjJrIhITWB3Qp2m5vZ3bjcc3mkmtgc2wfaFsHPpkZKGHyaaN541vADqNCmeaaGpWa5gtVX7RxERqUxKZkVEajqrtXgasMbQYiBceC9kHYA/voRNn5s3nO1fZ25exznMUoSos8w5cqPOOvI8INwXn0REpMKUzIqI1EbBMdDxZnPLToEt35ojt6nbIXWbWaJQVACH/jS3LX87PijavKksopH3VqepFoUQkdOKklkRkdouKOrI/Lgl3EVm3W3qNkj568iNZSl/mjedZR80tz2/lj6ff7g5g0J0C3OLaWEmuqEJYNNKTiJSvZTMioiciaw2iGhobk0v9n4vL6N49HanOYJ7uPgxdYf5PO+wuQraruXex1ms5hy5JdOGhTcwa3PrNDM3zZMrIlVAyayIiHjzDzVnQqjbrvR7rjyzLOHAZnPe3AObzdXPDu8258vN2Gtuu1eUcd4wM6mNbFK6fCGkrpb6FZGTomRWRETKz+F/ZL7co7ndZllC+m5zBbT03eZo7qFtZp1u+m7IS4e9q83t72x+5ihu9NnmFnWWWcJQp5l5TRGRY1AyKyIip85qhZBYc6vXofT7BTmQtsNc3jct8UjZQlqimegWFZgjvQf/8D7OYjOT2rrtoG5bc0GIOmdX+ccRkZpDyayIiFQ9v0CIPcfc/q6oEDL2mDefHdxsLgKRstV8npcOBzaa29r3ALBbbFzkjMdW8Jm50ETMORDbyqzVVU2uyBlHyayIiPiWzX6kdrZ5nyP7DcNc1nf/Wti3tvjxNyzZBwnN2wMb98DGT460d4ZC7LnFI7htzK1Oc/P8IlJr6TtcREROTxYLhCWYW4tB5j7DwJW6i9Vfz6BjgyBsKZvNVc9StkJ+BuxaZm4l7AHm6G3ceWaiG9faHMX1C/LNZxKRSqdkVkREag6LBULrkhzWDnf3gdgcxfPaFhaYCW3S77D/d3O1s6TfoSAL9qw0tyMnMWdUiG4BIXHmFhx75HlgHQiIAEegyhZEagAlsyIiUvPZ/YpHYM+FtsPMfW63OZPC/rWQvAGSNkDSeshKMufRTd12/HNaHeayvgER5hZWz5w717M1NOt0NduCiE8pmRURkdrJaoWoZuZ23lVH9mcdhOT1ZqKbmWwmt5lHbbmp4C4Et+vISmgAu38p4yIWc+WzyMbmaG/JVqepOdWYEl2RKqdkVkREzizB0RB8cemVz0oYBhRkmyud5aZB7mHIOWQu/3t4l7kK2uFd5gpprmxzJoaMPZC4+G8nspgrodVpDlHNixeMaAwRjc0RXbtfFX9QkTODklkREZGjWSzgDDa3sHrHbmcYkJ1izp+buv3IdmibueWnFye/u2Dbgr9fBELrHpnFIbJx8dK/xSO6zuAq/IAitYuSWRERkZNhsRSP8kZD/U7e7xmGWZ6Q8qe5/G/Kn2aCm5Zojuy6co4s/btzaelzB8dCUDRYrGC1mY8WG1jtZhJcMtJb8qjZGeQMpmRWRESkslksEBxjbo26e7/nGdFNNBPb1B3mzWiHim9KyzkEWcnmVl6hCWZiG3U2RJ9lPkadZV5fMzJILadkVkREpDp5jeh2LP1+7mEzqc1LN2dkMIrAXWQ+FhWYZQspf5lLAx/600x+S0Z5ty/0PpfdH/zDwT/M3AKKn4c3NOfbjTnHHNnVwhJSg+lfr4iIyOkkIBwS2pe/fU6qWcaQshVStsDB4se0nVCYZ87WkJV07ONtfuZIbkwLc4T36Hl3g2MhKAr8gs1yB5HTkJJZERGRmiwwEhp0NrejuXIh64A5wpuXbs7OkJduJr+H/oTkTXDgD3NGhuT15nY8jkAzqXUGmzW6jkBz5NcRAHanudqaw99cdCIwyqz5DSp+HhxjvlZCLFVAyayIiEht5AiAiIbHb+N2Q/ouM7FN2XJkrt2s5COPrhyzrSvH3LIPnFw8Fps50hsaDyHxWINiaZachWVzEUQ3N6cs0ywOchKUzIqIiJyprNYj04MxsPT7hmGWKhRkQ36muTxwQTbkZ5mJbWE+FOaCK89s58oxa3izUyAnxXwseW4UQeY+cwNswDkAH885cr2gaDOpDYg4MgLsF2xuDn8z+XYXFtcRF5pbUaFZS1yUD0Uu83lhwZHkuyDHHH0uyDFjBcBSfGNc8aPNWVxPHF684lvxc2cI+AUeGZUueW5zmCvEWe1mvbHVYZZrOPyPjFDbA8x2ugGvyimZFRERkbJZLOYIryPArJ09WUWF5lRlmfs9W9HhvezbtJyEgAKsaTvMldeOXnGtuhUn2ZXKYjNLMKwOM7EtSYJtdjDc5o197qIjCbrhPvZ5rPbizVZ8nqNee54Xb3b/4tKPox5tfuYvL55zFT/aHGYyby/ebE5zH4b5y4zhPvKIAS0GmUn+aUTJrIiIiFQtm90sLwiN9+xyu1ysyfmGuIEDsTocZj1v6g5zyrL8jCMjwAXFmyuvjMTNaiaH9uIEzOZXnIzZwRF01KhqcY2vI8C8uGHgSdbAHLHNPey96lve4SMj0AXZ5lby3F1ojgK7XWai7i4eEXblHTX6i5mglpRp1BZ3rVUyKyIiIlKKfxjUbWtuNVlJaYYr13wsKiguf/hb8utZEMN2JEm3WMs+n+E+UlbhKa9wHZm2zbO/yNxfWFz2UZh/5HlRQfEUb+4jU72VnKekXVFB8WNxfCXxlDy3WM1R3tOMklkRERGRynJ0aYZUizJ+BRARERERqRmUzIqIiIhIjaVkVkRERERqLCWzIiIiIlJjKZkVERERkRpLyayIiIiI1FhKZkVERESkxlIyKyIiIiI1lpJZEREREamxlMyKiIiISI2lZFZEREREaiy7rwOoboZhAJCRkVEt13O5XOTk5JCRkYHD4aiWa0rlUz/WDurH2kH9WDuoH2uHqurHkjytJG87njMumc3MzASgfv36Po5ERERERI4nMzOTsLCw47axGOVJeWsRt9vNvn37CAkJwWKxVPn1MjIyqF+/Prt37yY0NLTKrydVQ/1YO6gfawf1Y+2gfqwdqqofDcMgMzOTunXrYrUevyr2jBuZtVqt1KtXr9qvGxoaqm/WWkD9WDuoH2sH9WPtoH6sHaqiH080IltCN4CJiIiISI2lZFZEREREaiwls1XM6XQyceJEnE6nr0ORU6B+rB3Uj7WD+rF2UD/WDqdDP55xN4CJiIiISO2hkVkRERERqbGUzIqIiIhIjaVkVkRERERqLCWzIiIiIlJjKZmtYtOmTaNRo0b4+/vTuXNnfv31V1+HJMcxefJkOnbsSEhICDExMVx22WVs2bLFq01eXh6jR4+mTp06BAcHc+WVV5KcnOyjiOVEnnnmGSwWC3fffbdnn/qwZti7dy/XX389derUISAggPPOO49Vq1Z53jcMg0cffZT4+HgCAgLo3bs3f/75pw8jlr8rKirikUceoXHjxgQEBNC0aVOeeOIJjr73XP14+lm0aBGDBw+mbt26WCwWPvvsM6/3y9NnqampDB8+nNDQUMLDw7n55pvJysqqkniVzFahOXPmMG7cOCZOnMiaNWto06YN/fr148CBA74OTY7h559/ZvTo0axYsYL58+fjcrno27cv2dnZnjb33HMPX375JXPnzuXnn39m3759XHHFFT6MWo5l5cqVvPHGG7Ru3dprv/rw9JeWlkb37t1xOBx8++23bNq0if/85z9ERER42kyZMoWXXnqJ119/nV9++YWgoCD69etHXl6eDyOXoz377LO89tprvPLKK/zxxx88++yzTJkyhZdfftnTRv14+snOzqZNmzZMmzatzPfL02fDhw9n48aNzJ8/n6+++opFixZx6623Vk3AhlSZTp06GaNHj/a8LioqMurWrWtMnjzZh1FJRRw4cMAAjJ9//tkwDMM4fPiw4XA4jLlz53ra/PHHHwZgLF++3FdhShkyMzON5s2bG/Pnzzd69uxpjB071jAM9WFN8cADDxgXXHDBMd93u91GXFyc8dxzz3n2HT582HA6ncYHH3xQHSFKOQwaNMgYNWqU174rrrjCGD58uGEY6seaADA+/fRTz+vy9NmmTZsMwFi5cqWnzbfffmtYLBZj7969lR6jRmarSEFBAatXr6Z3796efVarld69e7N8+XIfRiYVkZ6eDkBkZCQAq1evxuVyefVrixYtaNCggfr1NDN69GgGDRrk1VegPqwpvvjiCzp06MDVV19NTEwM7dq146233vK8v2PHDpKSkrz6MSwsjM6dO6sfTyPdunVjwYIFbN26FYB169axZMkSBgwYAKgfa6Ly9Nny5csJDw+nQ4cOnja9e/fGarXyyy+/VHpM9ko/owCQkpJCUVERsbGxXvtjY2PZvHmzj6KSinC73dx99910796dc889F4CkpCT8/PwIDw/3ahsbG0tSUpIPopSyzJ49mzVr1rBy5cpS76kPa4bt27fz2muvMW7cOB588EFWrlzJXXfdhZ+fHyNGjPD0VVk/Y9WPp4/x48eTkZFBixYtsNlsFBUV8dRTTzF8+HAA9WMNVJ4+S0pKIiYmxut9u91OZGRklfSrklmRYxg9ejQbNmxgyZIlvg5FKmD37t2MHTuW+fPn4+/v7+tw5CS53W46dOjA008/DUC7du3YsGEDr7/+OiNGjPBxdFJeH374Ie+//z6zZs3inHPOYe3atdx9993UrVtX/SiVRmUGVSQqKgqbzVbqDunk5GTi4uJ8FJWU15gxY/jqq6/46aefqFevnmd/XFwcBQUFHD582Ku9+vX0sXr1ag4cOMD555+P3W7Hbrfz888/89JLL2G324mNjVUf1gDx8fG0atXKa1/Lli3ZtWsXgKev9DP29Hbfffcxfvx4rrvuOs477zxuuOEG7rnnHiZPngyoH2ui8vRZXFxcqZvdCwsLSU1NrZJ+VTJbRfz8/Gjfvj0LFizw7HO73SxYsICuXbv6MDI5HsMwGDNmDJ9++ik//vgjjRs39nq/ffv2OBwOr37dsmULu3btUr+eJi655BLWr1/P2rVrPVuHDh0YPny457n68PTXvXv3UtPibd26lYYNGwLQuHFj4uLivPoxIyODX375Rf14GsnJycFq9U41bDYbbrcbUD/WROXps65du3L48GFWr17tafPjjz/idrvp3Llz5QdV6beUicfs2bMNp9NpzJw509i0aZNx6623GuHh4UZSUpKvQ5NjuP32242wsDBj4cKFxv79+z1bTk6Op81tt91mNGjQwPjxxx+NVatWGV27djW6du3qw6jlRI6ezcAw1Ic1wa+//mrY7XbjqaeeMv7880/j/fffNwIDA4333nvP0+aZZ54xwsPDjc8//9z4/fffjSFDhhiNGzc2cnNzfRi5HG3EiBFGQkKC8dVXXxk7duwwPvnkEyMqKsq4//77PW3Uj6efzMxM47fffjN+++03AzCef/5547fffjN27txpGEb5+qx///5Gu3btjF9++cVYsmSJ0bx5c2Po0KFVEq+S2Sr28ssvGw0aNDD8/PyMTp06GStWrPB1SHIcQJnbjBkzPG1yc3ONO+64w4iIiDACAwONyy+/3Ni/f7/vgpYT+nsyqz6sGb788kvj3HPPNZxOp9GiRQvjzTff9Hrf7XYbjzzyiBEbG2s4nU7jkksuMbZs2eKjaKUsGRkZxtixY40GDRoY/v7+RpMmTYyHHnrIyM/P97RRP55+fvrppzL/LxwxYoRhGOXrs0OHDhlDhw41goODjdDQUGPkyJFGZmZmlcRrMYyjluEQEREREalBVDMrIiIiIjWWklkRERERqbGUzIqIiIhIjaVkVkRERERqLCWzIiIiIlJjKZkVERERkRpLyayIiIiI1FhKZkVERESkxlIyKyJyhrJYLHz22We+DkNE5JQomRUR8YGbbroJi8VSauvfv7+vQxMRqVHsvg5ARORM1b9/f2bMmOG1z+l0+igaEZGaSSOzIiI+4nQ6iYuL89oiIiIAswTgtddeY8CAAQQEBNCkSRM++ugjr+PXr1/PxRdfTEBAAHXq1OHWW28lKyvLq8306dM555xzcDqdxMfHM2bMGK/3U1JSuPzyywkMDKR58+Z88cUXVfuhRUQqmZJZEZHT1COPPMKVV17JunXrGD58ONdddx1//PEHANnZ2fTr14+IiAhWrlzJ3Llz+eGHH7yS1ddee43Ro0dz6623sn79er744guaNWvmdY1JkyZxzTXX8PvvvzNw4ECGDx9OampqtX5OEZFTYTEMw/B1ECIiZ5qbbrqJ9957D39/f6/9Dz74IA8++CAWi4XbbruN1157zfNely5dOP/883n11Vd56623eOCBB9i9ezdBQUEAfPPNNwwePJh9+/YRGxtLQkICI0eO5MknnywzBovFwsMPP8wTTzwBmAlycHAw3377rWp3RaTGUM2siIiPXHTRRV7JKkBkZKTnedeuXb3e69q1K2vXrgXgjz/+oE2bNp5EFqB79+643W62bNmCxWJh3759XHLJJceNoXXr1p7nQUFBhIaGcuDAgZP9SCIi1U7JrIiIjwQFBZX6s39lCQgIKFc7h8Ph9dpiseB2u6siJBGRKqGaWRGR09SKFStKvW7ZsiUALVu2ZN26dWRnZ3veX7p0KVarlbPPPpuQkBAaNWrEggULqjVmEZHqppFZEREfyc/PJykpyWuf3W4nKioKgLlz59KhQwcuuOAC3n//fX799VfefvttAIYPH87EiRMZMWIEjz32GAcPHuTOO+/khhtuIDY2FoDHHnuM2267jZiYGAYMGEBmZiZLly7lzjvvrN4PKiJShZTMioj4yHfffUd8fLzXvrPPPpvNmzcD5kwDs2fP5o477iA+Pp4PPviAVq1aARAYGMj333/P2LFj6dixI4GBgVx55ZU8//zznnONGDGCvLw8XnjhBe69916ioqK46qqrqu8DiohUA81mICJyGrJYLHz66adcdtllvg5FROS0pppZEREREamxlMyKiIiISI2lmlkRkdOQKsBERMpHI7MiIiIiUmMpmRURERGRGkvJrIiIiIjUWEpmRURERKTGUjIrIiIiIjWWklkRERERqbGUzIqIiIhIjaVkVkRERERqrP8HxKgXrfVJX6MAAAAASUVORK5CYII="/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=9a2f126b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="LBFGS-run-using-parameters-found-with-optuna:">LBFGS run using parameters found with optuna:<a class="anchor-link" href="#LBFGS-run-using-parameters-found-with-optuna:"></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=e08bd7b5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Final L-BFGS Training and Checkpointing for Feed-Forward Neural Network</span>
<span class="sd">========================================================================</span>

<span class="sd">This script performs final training of a Feed-Forward Neural Network (FFN)</span>
<span class="sd">using the L-BFGS optimiser with previously tuned hyperparameters, logs</span>
<span class="sd">training and validation losses, and saves the model weights and metadata</span>
<span class="sd">for downstream use.</span>

<span class="sd">Modules Used:</span>
<span class="sd">-------------</span>
<span class="sd">- PyTorch (`torch`, `torch.nn`, `torch.optim`)</span>
<span class="sd">- Matplotlib for loss visualization</span>
<span class="sd">- NumPy and JSON for checkpoint management</span>

<span class="sd">Workflow Overview:</span>
<span class="sd">------------------</span>
<span class="sd">1. Load preprocessed training and validation tensors (`X_train`, `y_train`, `X_val`, `y_val`).</span>
<span class="sd">2. Define and build the FFN architecture using saved best parameters.</span>
<span class="sd">3. Initialise the L-BFGS optimiser with the tuned hyperparameters.</span>
<span class="sd">4. Train the model using full-batch L-BFGS over multiple epochs.</span>
<span class="sd">5. Log epoch duration, train MSE, and validation MSE to a CSV file (`lbfgs_stats.csv`).</span>
<span class="sd">6. Plot loss curves and save figure (optional extension).</span>
<span class="sd">7. Save:</span>
<span class="sd">   - model weights (`.pth`)</span>
<span class="sd">   - final genome as flat parameter vector (`.npy`)</span>
<span class="sd">   - full metadata (`.json`) for reproducibility.</span>

<span class="sd">How to Use:</span>
<span class="sd">-----------</span>
<span class="sd">- Ensure your data tensors (`X_train`, `y_train`, `X_val`, `y_val`) are defined beforehand.</span>
<span class="sd">- Adjust `arch_params` or `best_lbfgs` dictionaries to use different model configurations or hyperparameters.</span>
<span class="sd">- This script automatically logs results and saves model checkpoints to `checkpoints_lbfgs/`.</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- `lbfgs_stats.csv`: MSE log per epoch</span>
<span class="sd">- `checkpoints_lbfgs/lbfgs_final_weights.pth`: Final model weights</span>
<span class="sd">- `checkpoints_lbfgs/lbfgs_final_genome.npy`: Flat parameter vector</span>
<span class="sd">- `checkpoints_lbfgs/lbfgs_model_meta.json`: JSON metadata (architecture + optimiser params)</span>
<span class="sd">- Final loss curves plotted using Matplotlib</span>

<span class="sd">Notes:</span>
<span class="sd">------</span>
<span class="sd">- This script assumes full-batch training (i.e., L-BFGS is called once per epoch).</span>
<span class="sd">- Make sure the GPU is available and accessible via `torch.cuda.is_available()`; otherwise, CPU will be used.</span>
<span class="sd">- Can be extended to include test-set evaluation using the trained model.</span>

<span class="sd">"""</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span><span class="o">,</span><span class="w"> </span><span class="nn">json</span><span class="o">,</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span>

<span class="c1"># 1) Device</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># 2) Best FFN architecture (from Optuna) + your L-BFGS hyperparams</span>
<span class="n">arch_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"n_units"</span><span class="p">:</span>    <span class="mi">24</span><span class="p">,</span>
    <span class="s2">"activation"</span><span class="p">:</span> <span class="s2">"ReLU"</span>
<span class="p">}</span>
<span class="n">best_lbfgs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"lr"</span><span class="p">:</span>       <span class="mf">0.9669461523303573</span><span class="p">,</span>
    <span class="s2">"max_iter"</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">"epochs"</span><span class="p">:</span>   <span class="mi">100</span>
<span class="p">}</span>

<span class="c1"># 3) Move data to device</span>
<span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_v</span><span class="p">,</span>  <span class="n">y_v</span>  <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 4) Build the model</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">in_f</span>   <span class="o">=</span> <span class="n">X_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">Act</span>    <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
    <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
    <span class="n">in_f</span>    <span class="o">=</span> <span class="n">arch_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 5) Optimizer &amp; loss</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">best_lbfgs</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="n">best_lbfgs</span><span class="p">[</span><span class="s2">"max_iter"</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># 6) Prepare L-BFGS log file</span>
<span class="n">LOG_FN</span> <span class="o">=</span> <span class="s2">"lbfgs_stats.csv"</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">"epoch,epoch_time,train_mse,val_mse</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># 6) Train for N full-batch epochs</span>
<span class="n">epochs</span>       <span class="o">=</span> <span class="n">best_lbfgs</span><span class="p">[</span><span class="s2">"epochs"</span><span class="p">]</span>
<span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_losses</span>   <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">out</span>  <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tr</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="c1"># one L-BFGS step</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="c1"># eval on validation set</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X_v</span><span class="p">),</span> <span class="n">y_v</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_mse</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">epoch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="c1"># append to CSV</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">epoch_time</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">train_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">val_mse</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s2">3d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">  train MSE: </span><span class="si">{</span><span class="n">train_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">val_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> L-BFGS training complete!"</span><span class="p">)</span>

<span class="c1"># 7) Plot curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_losses</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Final FFN (L-BFGS) Training &amp; Validation Loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">#  Save final L-BFGStrained model </span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">"checkpoints_lbfgs"</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 1) Save the PyTorch state_dict</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s2">"checkpoints_lbfgs/lbfgs_final_weights.pth"</span>
<span class="p">)</span>

<span class="c1"># 2) Save the flat parameter vector</span>
<span class="n">genome_lbfgs</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
    <span class="s2">"checkpoints_lbfgs/lbfgs_final_genome.npy"</span><span class="p">,</span>
    <span class="n">genome_lbfgs</span>
<span class="p">)</span>

<span class="c1"># 3) Save the architecture &amp; L-BFGS hyperparams so you can rebuild later</span>
<span class="n">meta_lbfgs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"arch"</span><span class="p">:</span> <span class="n">arch_params</span><span class="p">,</span>      <span class="c1"># your {"n_layers":, "n_units":, "activation":}</span>
    <span class="s2">"lbfgs_params"</span><span class="p">:</span> <span class="n">best_lbfgs</span> <span class="c1"># your {"lr":, "max_iter":, "epochs":}</span>
<span class="p">}</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"checkpoints_lbfgs/lbfgs_model_meta.json"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">meta_lbfgs</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> L-BFGStrained model and metadata saved to `checkpoints_lbfgs/`"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Epoch   1/100  train MSE: 1.0225, val MSE: 0.9938
Epoch  20/100  train MSE: 0.7566, val MSE: 0.5818
Epoch  40/100  train MSE: 0.6345, val MSE: 0.4083
Epoch  60/100  train MSE: 0.6128, val MSE: 0.3795
Epoch  80/100  train MSE: 0.5997, val MSE: 0.3634
Epoch 100/100  train MSE: 0.5949, val MSE: 0.3593

 L-BFGS training complete!
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
 L-BFGStrained model and metadata saved to `checkpoints_lbfgs/`
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=f8b502f8">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Vanilla-GA:-First-reasonably-successful-implementation:">Vanilla GA: First reasonably successful implementation:<a class="anchor-link" href="#Vanilla-GA:-First-reasonably-successful-implementation:"></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0fc6cbde">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Genetic Algorithm (GA) for Optimising Feed-Forward Neural Network Weights</span>
<span class="sd">==========================================================================</span>

<span class="sd">This script implements a real-valued Genetic Algorithm (GA) using Blend Crossover (BLX-)</span>
<span class="sd">and Gaussian mutation to optimise the parameters of a fixed-architecture feed-forward</span>
<span class="sd">neural network (FFN). The algorithm is applied to a regression task with MSE loss, and</span>
<span class="sd">performance is tracked across generations.</span>

<span class="sd">Key Features:</span>
<span class="sd">-------------</span>
<span class="sd">- Fixed FFN architecture using PyTorch with Xavier weight initialisation</span>
<span class="sd">- Real-valued genome representation using PyTorch parameter vectors</span>
<span class="sd">- Tournament selection (size = 3) with elitism (top 20% retained each generation)</span>
<span class="sd">- BLX- crossover with  = 0.3 for diversity-preserving recombination</span>
<span class="sd">- Gaussian mutation applied per gene with probability `mutation_p` and std dev `mutation_sd`</span>
<span class="sd">- Reproducible results via fixed seeds for NumPy and PyTorch</span>
<span class="sd">- Final best model reconstructed and evaluated</span>
<span class="sd">- MSE loss curves plotted across generations</span>

<span class="sd">Instructions:</span>
<span class="sd">-------------</span>
<span class="sd">1. Ensure that `X_train`, `y_train`, `X_val`, and `y_val` are defined as PyTorch tensors.</span>
<span class="sd">2. Adjust architecture via the `arch` dictionary.</span>
<span class="sd">3. Modify genetic algorithm hyperparameters (e.g., `pop_size`, `generations`, `mutation_p`) as needed.</span>
<span class="sd">4. Run the script to execute the full evolutionary cycle and view results.</span>

<span class="sd">Hyperparameters:</span>
<span class="sd">----------------</span>
<span class="sd">- `pop_size`: Number of individuals in the population</span>
<span class="sd">- `generations`: Number of generations to evolve</span>
<span class="sd">- `elite_frac`: Proportion of top individuals carried over each generation</span>
<span class="sd">- `tourn_size`: Tournament size for selection</span>
<span class="sd">- `mutation_p`: Probability of mutating each gene</span>
<span class="sd">- `mutation_sd`: Standard deviation of mutation noise</span>
<span class="sd">- `blx_alpha`: BLX- parameter controlling crossover range</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Printed train/validation MSE every 100 generations</span>
<span class="sd">- Final best model with evaluation on training and validation sets</span>
<span class="sd">- Plot of training and validation MSE vs. generation</span>
<span class="sd">- Internal genome evolution stored in memory only (can be extended to save)</span>


<span class="sd">"""</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">skPCA</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>
<span class="c1">#  Repro &amp; Device</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1"># Data to device</span>
<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">#   Fixed Architecture + Xavier init </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"ReLU"</span><span class="p">)</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="n">criterion</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span><span class="p">,</span> <span class="n">in_f</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>

<span class="c1">#  GA Hyperparams</span>
<span class="n">pop_size</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">generations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">elite_frac</span>  <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tourn_size</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">mutation_p</span>  <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">mutation_sd</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">blx_alpha</span>   <span class="o">=</span> <span class="mf">0.3</span>  

<span class="c1">#  Init Population</span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pop_size</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>

<span class="c1"># visualize initial population </span>
<span class="n">pop_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>          
<span class="n">genome_len</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#  5) Tournament Selection</span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pop_size</span><span class="p">,</span> <span class="n">tourn_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>
<span class="c1">#  6) Evolution </span>
<span class="n">train_curve</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_curve</span>   <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># Fitness eval</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="c1"># record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="c1">#Diagnostic code</span>
    <span class="n">best_gen</span>   <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">norm_best</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">best_gen</span><span class="p">)</span>
    <span class="n">best_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_best</span><span class="p">)</span>
    <span class="n">prev_best_gen</span> <span class="o">=</span> <span class="n">best_gen</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">tr_mse</span>   <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">m_best</span>   <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">va_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tr_mse</span><span class="p">)</span>
    <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">va_mse</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gen</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">gen</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2">  train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1">#  Elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elites</span>  <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]]</span>
    <span class="n">pop_size</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
    <span class="n">elite_frac</span> <span class="o">=</span> <span class="mf">0.2</span>            
    <span class="n">elite_n</span>    <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
    <span class="n">elites</span>     <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">]</span>
    <span class="c1"># selection probabilities</span>
    <span class="n">p_elite</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_tourn</span>   <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># p_random = 0.1  # implicit: 1 - (p_elite + p_tourn)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># pure tournament selection</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="c1"># pure elitism</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span><span class="p">:</span>
            <span class="c1"># exploit: uniform from elites</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="k">elif</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span> <span class="o">+</span> <span class="n">p_tourn</span><span class="p">:</span>
            <span class="c1"># competition: standard tournament over full pop</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># pure exploration: uniform from entire pop</span>
            <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">pop_size</span><span class="p">)]</span>
    <span class="c1">#  Reproduce via BLX- + mutation</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">pop_size</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="c1"># BLX- crossover</span>
        <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># mutation</span>
        <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mutation_p</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mutation_sd</span>
        <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1">#  7) Final Best Model </span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))]</span>
<span class="n">best_model_ga</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                     <span class="n">best_model_ga</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">best_model_ga</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> GA done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="c1"># 8) Plot </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA (w/ BLX-) Optimization of FFN Weights"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen  1/2000  train MSE: 1.0086, val MSE: 1.0063
Gen 100/2000  train MSE: 0.9933, val MSE: 0.9912
Gen 200/2000  train MSE: 0.9892, val MSE: 0.9848
Gen 300/2000  train MSE: 0.9840, val MSE: 0.9778
Gen 400/2000  train MSE: 0.9783, val MSE: 0.9704
Gen 500/2000  train MSE: 0.9736, val MSE: 0.9633
Gen 600/2000  train MSE: 0.9697, val MSE: 0.9576
Gen 700/2000  train MSE: 0.9661, val MSE: 0.9520
Gen 800/2000  train MSE: 0.9629, val MSE: 0.9459
Gen 900/2000  train MSE: 0.9605, val MSE: 0.9425
Gen 1000/2000  train MSE: 0.9582, val MSE: 0.9379
Gen 1100/2000  train MSE: 0.9561, val MSE: 0.9344
Gen 1200/2000  train MSE: 0.9541, val MSE: 0.9314
Gen 1300/2000  train MSE: 0.9513, val MSE: 0.9272
Gen 1400/2000  train MSE: 0.9486, val MSE: 0.9223
Gen 1500/2000  train MSE: 0.9459, val MSE: 0.9175
Gen 1600/2000  train MSE: 0.9430, val MSE: 0.9129
Gen 1700/2000  train MSE: 0.9401, val MSE: 0.9082
Gen 1800/2000  train MSE: 0.9375, val MSE: 0.9038
Gen 1900/2000  train MSE: 0.9340, val MSE: 0.8980
Gen 2000/2000  train MSE: 0.9304, val MSE: 0.8929

 GA done!  Final Train MSE: 0.9308, Val MSE: 0.8929
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=c444ac3f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Same-GA-implementation:-Changing-alpha-value-for-blx-crossover-from-0.3-to-0.5:">Same GA implementation: Changing alpha value for blx crossover from 0.3 to 0.5:<a class="anchor-link" href="#Same-GA-implementation:-Changing-alpha-value-for-blx-crossover-from-0.3-to-0.5:"></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=04819981">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Genetic Algorithm (GA) for Optimising Feed-Forward Neural Network Weights</span>
<span class="sd">==========================================================================</span>

<span class="sd">This script implements a real-valued Genetic Algorithm (GA) using Blend Crossover (BLX-)</span>
<span class="sd">and Gaussian mutation to optimise the parameters of a fixed-architecture feed-forward</span>
<span class="sd">neural network (FFN). The algorithm is applied to a regression task with MSE loss, and</span>
<span class="sd">performance is tracked across generations.</span>

<span class="sd">Key Features:</span>
<span class="sd">-------------</span>
<span class="sd">- Fixed FFN architecture using PyTorch with Xavier weight initialisation</span>
<span class="sd">- Real-valued genome representation using PyTorch parameter vectors</span>
<span class="sd">- Tournament selection (size = 3) with elitism (top 20% retained each generation)</span>
<span class="sd">- BLX- crossover with  = 0.5 for diversity-preserving recombination</span>
<span class="sd">- Gaussian mutation applied per gene with probability `mutation_p` and std dev `mutation_sd`</span>
<span class="sd">- Reproducible results via fixed seeds for NumPy and PyTorch</span>
<span class="sd">- Final best model reconstructed and evaluated</span>
<span class="sd">- MSE loss curves plotted across generations</span>

<span class="sd">Instructions:</span>
<span class="sd">-------------</span>
<span class="sd">1. Ensure that `X_train`, `y_train`, `X_val`, and `y_val` are defined as PyTorch tensors.</span>
<span class="sd">2. Adjust architecture via the `arch` dictionary.</span>
<span class="sd">3. Modify genetic algorithm hyperparameters (e.g., `pop_size`, `generations`, `mutation_p`) as needed.</span>
<span class="sd">4. Run the script to execute the full evolutionary cycle and view results.</span>

<span class="sd">Hyperparameters:</span>
<span class="sd">----------------</span>
<span class="sd">- `pop_size`: Number of individuals in the population</span>
<span class="sd">- `generations`: Number of generations to evolve</span>
<span class="sd">- `elite_frac`: Proportion of top individuals carried over each generation</span>
<span class="sd">- `tourn_size`: Tournament size for selection</span>
<span class="sd">- `mutation_p`: Probability of mutating each gene</span>
<span class="sd">- `mutation_sd`: Standard deviation of mutation noise</span>
<span class="sd">- `blx_alpha`: BLX- parameter controlling crossover range</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Printed train/validation MSE every 100 generations</span>
<span class="sd">- Final best model with evaluation on training and validation sets</span>
<span class="sd">- Plot of training and validation MSE vs. generation</span>
<span class="sd">- Internal genome evolution stored in memory only (can be extended to save)</span>


<span class="sd">"""</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">skPCA</span>

<span class="c1">#  0) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1">#  1) Data to device </span>
<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">#  2) Fixed Architecture + Xavier init </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"ReLU"</span><span class="p">)</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="n">criterion</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span><span class="p">,</span> <span class="n">in_f</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>

<span class="c1">#  3) GA Hyperparams </span>
<span class="n">pop_size</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">generations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">elite_frac</span>  <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tourn_size</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">mutation_p</span>  <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">mutation_sd</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">blx_alpha</span>   <span class="o">=</span> <span class="mf">0.5</span>  

<span class="c1">#  4) Init Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pop_size</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>

<span class="c1">#  visualize initial population </span>
<span class="n">pop_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>          
<span class="n">genome_len</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span>

<span class="c1">#  5) Tournament Selection </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pop_size</span><span class="p">,</span> <span class="n">tourn_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>
<span class="c1">#  6) Evolution </span>
<span class="n">train_curve</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_curve</span>   <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># a) Fitness eval</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gen</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">gen</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2">  train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">tr_mse</span>   <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">m_best</span>   <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">va_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tr_mse</span><span class="p">)</span>
    <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">va_mse</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2">  train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># Elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elites</span>  <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]]</span>
    <span class="n">pop_size</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
    <span class="n">elite_frac</span> <span class="o">=</span> <span class="mf">0.2</span>             
    <span class="n">elite_n</span>    <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
    <span class="n">elites</span>     <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">]</span>
    <span class="c1"># selection probabilities</span>
    <span class="n">p_elite</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_tourn</span>   <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># p_random = 0.1  # implicit: 1 - (p_elite + p_tourn)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># pure tournament selection</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="c1"># pure elitism</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span><span class="p">:</span>
            <span class="c1"># exploit: uniform from elites</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="k">elif</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span> <span class="o">+</span> <span class="n">p_tourn</span><span class="p">:</span>
            <span class="c1"># competition: standard tournament over full pop</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># pure exploration: uniform from entire pop</span>
            <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">pop_size</span><span class="p">)]</span>
    <span class="c1"># c) Reproduce via BLX- + mutation</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">pop_size</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="c1"># BLX- crossover</span>
        <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># mutation applied</span>
        <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mutation_p</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mutation_sd</span>
        <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1">#  7) Final Best Model </span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))]</span>
<span class="n">best_model_ga</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                     <span class="n">best_model_ga</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">best_model_ga</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> GA done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  8) Plot </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA (w/ BLX-) Optimization of FFN Weights"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen  1/2000  train MSE: 0.9623, val MSE: 0.9498
Gen  1/2000  train MSE: 1.0086, val MSE: 1.0063
Gen  2/2000  train MSE: 1.0086, val MSE: 1.0063
Gen  3/2000  train MSE: 1.0086, val MSE: 1.0063
Gen  4/2000  train MSE: 1.0086, val MSE: 1.0063
Gen  5/2000  train MSE: 1.0086, val MSE: 1.0063
Gen  6/2000  train MSE: 1.0086, val MSE: 1.0063
Gen  7/2000  train MSE: 1.0086, val MSE: 1.0063
Gen  8/2000  train MSE: 1.0086, val MSE: 1.0063
Gen  9/2000  train MSE: 1.0086, val MSE: 1.0063
Gen 10/2000  train MSE: 1.0086, val MSE: 1.0063
Gen 11/2000  train MSE: 1.0080, val MSE: 1.0096
Gen 12/2000  train MSE: 1.0080, val MSE: 1.0096
Gen 13/2000  train MSE: 1.0080, val MSE: 1.0096
Gen 14/2000  train MSE: 1.0047, val MSE: 1.0029
Gen 15/2000  train MSE: 1.0047, val MSE: 1.0029
Gen 16/2000  train MSE: 1.0047, val MSE: 1.0029
Gen 17/2000  train MSE: 1.0047, val MSE: 1.0029
Gen 18/2000  train MSE: 1.0047, val MSE: 1.0029
Gen 19/2000  train MSE: 1.0047, val MSE: 1.0029
Gen 20/2000  train MSE: 1.0047, val MSE: 1.0029
Gen 21/2000  train MSE: 1.0047, val MSE: 1.0029
Gen 22/2000  train MSE: 1.0047, val MSE: 1.0029
Gen 23/2000  train MSE: 1.0047, val MSE: 1.0029
Gen 24/2000  train MSE: 1.0047, val MSE: 1.0029
Gen 25/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 26/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 27/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 28/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 29/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 30/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 31/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 32/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 33/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 34/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 35/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 36/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 37/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 38/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 39/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 40/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 41/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 42/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 43/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 44/2000  train MSE: 1.0010, val MSE: 1.0012
Gen 45/2000  train MSE: 1.0010, val MSE: 0.9990
Gen 46/2000  train MSE: 1.0008, val MSE: 0.9994
Gen 47/2000  train MSE: 1.0008, val MSE: 0.9994
Gen 48/2000  train MSE: 1.0008, val MSE: 0.9994
Gen 49/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 50/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 51/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 52/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 53/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 54/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 55/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 56/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 57/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 58/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 59/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 60/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 61/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 62/2000  train MSE: 0.9997, val MSE: 1.0002
Gen 63/2000  train MSE: 0.9996, val MSE: 1.0013
Gen 64/2000  train MSE: 0.9996, val MSE: 1.0013
Gen 65/2000  train MSE: 0.9996, val MSE: 1.0013
Gen 66/2000  train MSE: 0.9996, val MSE: 1.0013
Gen 67/2000  train MSE: 0.9996, val MSE: 1.0013
Gen 68/2000  train MSE: 0.9991, val MSE: 0.9998
Gen 69/2000  train MSE: 0.9991, val MSE: 0.9998
Gen 70/2000  train MSE: 0.9991, val MSE: 0.9998
Gen 71/2000  train MSE: 0.9991, val MSE: 0.9998
Gen 72/2000  train MSE: 0.9991, val MSE: 0.9998
Gen 73/2000  train MSE: 0.9991, val MSE: 0.9998
Gen 74/2000  train MSE: 0.9988, val MSE: 0.9994
Gen 75/2000  train MSE: 0.9988, val MSE: 0.9994
Gen 76/2000  train MSE: 0.9985, val MSE: 0.9987
Gen 77/2000  train MSE: 0.9985, val MSE: 0.9987
Gen 78/2000  train MSE: 0.9985, val MSE: 0.9987
Gen 79/2000  train MSE: 0.9985, val MSE: 0.9987
Gen 80/2000  train MSE: 0.9985, val MSE: 0.9987
Gen 81/2000  train MSE: 0.9981, val MSE: 0.9985
Gen 82/2000  train MSE: 0.9981, val MSE: 0.9985
Gen 83/2000  train MSE: 0.9981, val MSE: 0.9985
Gen 84/2000  train MSE: 0.9981, val MSE: 0.9985
Gen 85/2000  train MSE: 0.9981, val MSE: 0.9985
Gen 86/2000  train MSE: 0.9980, val MSE: 0.9980
Gen 87/2000  train MSE: 0.9980, val MSE: 0.9980
Gen 88/2000  train MSE: 0.9980, val MSE: 0.9980
Gen 89/2000  train MSE: 0.9980, val MSE: 0.9980
Gen 90/2000  train MSE: 0.9980, val MSE: 0.9980
Gen 91/2000  train MSE: 0.9980, val MSE: 0.9980
Gen 92/2000  train MSE: 0.9979, val MSE: 0.9982
Gen 93/2000  train MSE: 0.9978, val MSE: 0.9977
Gen 94/2000  train MSE: 0.9975, val MSE: 0.9978
Gen 95/2000  train MSE: 0.9975, val MSE: 0.9978
Gen 96/2000  train MSE: 0.9975, val MSE: 0.9978
Gen 97/2000  train MSE: 0.9974, val MSE: 0.9968
Gen 98/2000  train MSE: 0.9971, val MSE: 0.9972
Gen 99/2000  train MSE: 0.9971, val MSE: 0.9972
Gen 100/2000  train MSE: 0.9971, val MSE: 0.9972
Gen 100/2000  train MSE: 0.9971, val MSE: 0.9967
Gen 101/2000  train MSE: 0.9965, val MSE: 0.9970
Gen 102/2000  train MSE: 0.9965, val MSE: 0.9970
Gen 103/2000  train MSE: 0.9965, val MSE: 0.9970
Gen 104/2000  train MSE: 0.9965, val MSE: 0.9970
Gen 105/2000  train MSE: 0.9963, val MSE: 0.9967
Gen 106/2000  train MSE: 0.9963, val MSE: 0.9959
Gen 107/2000  train MSE: 0.9961, val MSE: 0.9956
Gen 108/2000  train MSE: 0.9961, val MSE: 0.9956
Gen 109/2000  train MSE: 0.9960, val MSE: 0.9970
Gen 110/2000  train MSE: 0.9960, val MSE: 0.9949
Gen 111/2000  train MSE: 0.9958, val MSE: 0.9963
Gen 112/2000  train MSE: 0.9958, val MSE: 0.9963
Gen 113/2000  train MSE: 0.9955, val MSE: 0.9942
Gen 114/2000  train MSE: 0.9955, val MSE: 0.9956
Gen 115/2000  train MSE: 0.9951, val MSE: 0.9937
Gen 116/2000  train MSE: 0.9951, val MSE: 0.9937
Gen 117/2000  train MSE: 0.9951, val MSE: 0.9937
Gen 118/2000  train MSE: 0.9951, val MSE: 0.9937
Gen 119/2000  train MSE: 0.9951, val MSE: 0.9937
Gen 120/2000  train MSE: 0.9949, val MSE: 0.9935
Gen 121/2000  train MSE: 0.9949, val MSE: 0.9935
Gen 122/2000  train MSE: 0.9949, val MSE: 0.9935
Gen 123/2000  train MSE: 0.9947, val MSE: 0.9938
Gen 124/2000  train MSE: 0.9944, val MSE: 0.9932
Gen 125/2000  train MSE: 0.9940, val MSE: 0.9930
Gen 126/2000  train MSE: 0.9940, val MSE: 0.9930
Gen 127/2000  train MSE: 0.9940, val MSE: 0.9930
Gen 128/2000  train MSE: 0.9939, val MSE: 0.9914
Gen 129/2000  train MSE: 0.9939, val MSE: 0.9914
Gen 130/2000  train MSE: 0.9939, val MSE: 0.9914
Gen 131/2000  train MSE: 0.9938, val MSE: 0.9917
Gen 132/2000  train MSE: 0.9937, val MSE: 0.9925
Gen 133/2000  train MSE: 0.9937, val MSE: 0.9925
Gen 134/2000  train MSE: 0.9937, val MSE: 0.9925
Gen 135/2000  train MSE: 0.9935, val MSE: 0.9910
Gen 136/2000  train MSE: 0.9935, val MSE: 0.9910
Gen 137/2000  train MSE: 0.9932, val MSE: 0.9916
Gen 138/2000  train MSE: 0.9932, val MSE: 0.9916
Gen 139/2000  train MSE: 0.9932, val MSE: 0.9916
Gen 140/2000  train MSE: 0.9932, val MSE: 0.9916
Gen 141/2000  train MSE: 0.9932, val MSE: 0.9916
Gen 142/2000  train MSE: 0.9929, val MSE: 0.9902
Gen 143/2000  train MSE: 0.9928, val MSE: 0.9909
Gen 144/2000  train MSE: 0.9928, val MSE: 0.9908
Gen 145/2000  train MSE: 0.9926, val MSE: 0.9900
Gen 146/2000  train MSE: 0.9926, val MSE: 0.9909
Gen 147/2000  train MSE: 0.9926, val MSE: 0.9909
Gen 148/2000  train MSE: 0.9924, val MSE: 0.9905
Gen 149/2000  train MSE: 0.9924, val MSE: 0.9905
Gen 150/2000  train MSE: 0.9924, val MSE: 0.9905
Gen 151/2000  train MSE: 0.9918, val MSE: 0.9898
Gen 152/2000  train MSE: 0.9918, val MSE: 0.9898
Gen 153/2000  train MSE: 0.9918, val MSE: 0.9898
Gen 154/2000  train MSE: 0.9918, val MSE: 0.9898
Gen 155/2000  train MSE: 0.9918, val MSE: 0.9895
Gen 156/2000  train MSE: 0.9918, val MSE: 0.9895
Gen 157/2000  train MSE: 0.9917, val MSE: 0.9894
Gen 158/2000  train MSE: 0.9917, val MSE: 0.9894
Gen 159/2000  train MSE: 0.9913, val MSE: 0.9890
Gen 160/2000  train MSE: 0.9913, val MSE: 0.9890
Gen 161/2000  train MSE: 0.9911, val MSE: 0.9888
Gen 162/2000  train MSE: 0.9911, val MSE: 0.9888
Gen 163/2000  train MSE: 0.9911, val MSE: 0.9888
Gen 164/2000  train MSE: 0.9911, val MSE: 0.9888
Gen 165/2000  train MSE: 0.9911, val MSE: 0.9888
Gen 166/2000  train MSE: 0.9909, val MSE: 0.9879
Gen 167/2000  train MSE: 0.9909, val MSE: 0.9885
Gen 168/2000  train MSE: 0.9908, val MSE: 0.9873
Gen 169/2000  train MSE: 0.9908, val MSE: 0.9883
Gen 170/2000  train MSE: 0.9907, val MSE: 0.9883
Gen 171/2000  train MSE: 0.9904, val MSE: 0.9864
Gen 172/2000  train MSE: 0.9904, val MSE: 0.9864
Gen 173/2000  train MSE: 0.9904, val MSE: 0.9864
Gen 174/2000  train MSE: 0.9903, val MSE: 0.9870
Gen 175/2000  train MSE: 0.9903, val MSE: 0.9870
Gen 176/2000  train MSE: 0.9902, val MSE: 0.9869
Gen 177/2000  train MSE: 0.9901, val MSE: 0.9863
Gen 178/2000  train MSE: 0.9901, val MSE: 0.9863
Gen 179/2000  train MSE: 0.9901, val MSE: 0.9863
Gen 180/2000  train MSE: 0.9900, val MSE: 0.9865
Gen 181/2000  train MSE: 0.9898, val MSE: 0.9854
Gen 182/2000  train MSE: 0.9898, val MSE: 0.9854
Gen 183/2000  train MSE: 0.9898, val MSE: 0.9854
Gen 184/2000  train MSE: 0.9898, val MSE: 0.9854
Gen 185/2000  train MSE: 0.9897, val MSE: 0.9854
Gen 186/2000  train MSE: 0.9897, val MSE: 0.9854
Gen 187/2000  train MSE: 0.9897, val MSE: 0.9854
Gen 188/2000  train MSE: 0.9895, val MSE: 0.9851
Gen 189/2000  train MSE: 0.9895, val MSE: 0.9851
Gen 190/2000  train MSE: 0.9895, val MSE: 0.9851
Gen 191/2000  train MSE: 0.9895, val MSE: 0.9851
Gen 192/2000  train MSE: 0.9889, val MSE: 0.9836
Gen 193/2000  train MSE: 0.9889, val MSE: 0.9836
Gen 194/2000  train MSE: 0.9889, val MSE: 0.9836
Gen 195/2000  train MSE: 0.9889, val MSE: 0.9836
Gen 196/2000  train MSE: 0.9889, val MSE: 0.9836
Gen 197/2000  train MSE: 0.9888, val MSE: 0.9845
Gen 198/2000  train MSE: 0.9887, val MSE: 0.9842
Gen 199/2000  train MSE: 0.9887, val MSE: 0.9842
Gen 200/2000  train MSE: 0.9887, val MSE: 0.9842
Gen 200/2000  train MSE: 0.9887, val MSE: 0.9842
Gen 201/2000  train MSE: 0.9886, val MSE: 0.9854
Gen 202/2000  train MSE: 0.9886, val MSE: 0.9854
Gen 203/2000  train MSE: 0.9882, val MSE: 0.9835
Gen 204/2000  train MSE: 0.9882, val MSE: 0.9835
Gen 205/2000  train MSE: 0.9881, val MSE: 0.9828
Gen 206/2000  train MSE: 0.9881, val MSE: 0.9828
Gen 207/2000  train MSE: 0.9879, val MSE: 0.9830
Gen 208/2000  train MSE: 0.9879, val MSE: 0.9830
Gen 209/2000  train MSE: 0.9879, val MSE: 0.9830
Gen 210/2000  train MSE: 0.9879, val MSE: 0.9830
Gen 211/2000  train MSE: 0.9878, val MSE: 0.9818
Gen 212/2000  train MSE: 0.9877, val MSE: 0.9822
Gen 213/2000  train MSE: 0.9876, val MSE: 0.9823
Gen 214/2000  train MSE: 0.9874, val MSE: 0.9823
Gen 215/2000  train MSE: 0.9871, val MSE: 0.9819
Gen 216/2000  train MSE: 0.9869, val MSE: 0.9823
Gen 217/2000  train MSE: 0.9869, val MSE: 0.9823
Gen 218/2000  train MSE: 0.9869, val MSE: 0.9823
Gen 219/2000  train MSE: 0.9869, val MSE: 0.9814
Gen 220/2000  train MSE: 0.9864, val MSE: 0.9805
Gen 221/2000  train MSE: 0.9864, val MSE: 0.9805
Gen 222/2000  train MSE: 0.9864, val MSE: 0.9805
Gen 223/2000  train MSE: 0.9862, val MSE: 0.9795
Gen 224/2000  train MSE: 0.9862, val MSE: 0.9795
Gen 225/2000  train MSE: 0.9861, val MSE: 0.9802
Gen 226/2000  train MSE: 0.9861, val MSE: 0.9802
Gen 227/2000  train MSE: 0.9855, val MSE: 0.9788
Gen 228/2000  train MSE: 0.9855, val MSE: 0.9788
Gen 229/2000  train MSE: 0.9855, val MSE: 0.9788
Gen 230/2000  train MSE: 0.9855, val MSE: 0.9788
Gen 231/2000  train MSE: 0.9855, val MSE: 0.9788
Gen 232/2000  train MSE: 0.9854, val MSE: 0.9788
Gen 233/2000  train MSE: 0.9854, val MSE: 0.9788
Gen 234/2000  train MSE: 0.9854, val MSE: 0.9788
Gen 235/2000  train MSE: 0.9851, val MSE: 0.9783
Gen 236/2000  train MSE: 0.9851, val MSE: 0.9783
Gen 237/2000  train MSE: 0.9847, val MSE: 0.9772
Gen 238/2000  train MSE: 0.9846, val MSE: 0.9766
Gen 239/2000  train MSE: 0.9846, val MSE: 0.9766
Gen 240/2000  train MSE: 0.9846, val MSE: 0.9774
Gen 241/2000  train MSE: 0.9840, val MSE: 0.9765
Gen 242/2000  train MSE: 0.9840, val MSE: 0.9765
Gen 243/2000  train MSE: 0.9840, val MSE: 0.9765
Gen 244/2000  train MSE: 0.9837, val MSE: 0.9762
Gen 245/2000  train MSE: 0.9837, val MSE: 0.9762
Gen 246/2000  train MSE: 0.9836, val MSE: 0.9768
Gen 247/2000  train MSE: 0.9834, val MSE: 0.9759
Gen 248/2000  train MSE: 0.9834, val MSE: 0.9759
Gen 249/2000  train MSE: 0.9834, val MSE: 0.9759
Gen 250/2000  train MSE: 0.9833, val MSE: 0.9761
Gen 251/2000  train MSE: 0.9833, val MSE: 0.9761
Gen 252/2000  train MSE: 0.9833, val MSE: 0.9754
Gen 253/2000  train MSE: 0.9829, val MSE: 0.9756
Gen 254/2000  train MSE: 0.9828, val MSE: 0.9751
Gen 255/2000  train MSE: 0.9828, val MSE: 0.9751
Gen 256/2000  train MSE: 0.9828, val MSE: 0.9751
Gen 257/2000  train MSE: 0.9824, val MSE: 0.9753
Gen 258/2000  train MSE: 0.9824, val MSE: 0.9753
Gen 259/2000  train MSE: 0.9824, val MSE: 0.9753
Gen 260/2000  train MSE: 0.9824, val MSE: 0.9753
Gen 261/2000  train MSE: 0.9824, val MSE: 0.9734
Gen 262/2000  train MSE: 0.9822, val MSE: 0.9734
Gen 263/2000  train MSE: 0.9822, val MSE: 0.9734
Gen 264/2000  train MSE: 0.9821, val MSE: 0.9722
Gen 265/2000  train MSE: 0.9821, val MSE: 0.9722
Gen 266/2000  train MSE: 0.9819, val MSE: 0.9740
Gen 267/2000  train MSE: 0.9818, val MSE: 0.9726
Gen 268/2000  train MSE: 0.9814, val MSE: 0.9725
Gen 269/2000  train MSE: 0.9814, val MSE: 0.9725
Gen 270/2000  train MSE: 0.9814, val MSE: 0.9725
Gen 271/2000  train MSE: 0.9813, val MSE: 0.9727
Gen 272/2000  train MSE: 0.9812, val MSE: 0.9705
Gen 273/2000  train MSE: 0.9811, val MSE: 0.9725
Gen 274/2000  train MSE: 0.9810, val MSE: 0.9730
Gen 275/2000  train MSE: 0.9810, val MSE: 0.9720
Gen 276/2000  train MSE: 0.9810, val MSE: 0.9720
Gen 277/2000  train MSE: 0.9804, val MSE: 0.9715
Gen 278/2000  train MSE: 0.9804, val MSE: 0.9715
Gen 279/2000  train MSE: 0.9804, val MSE: 0.9715
Gen 280/2000  train MSE: 0.9804, val MSE: 0.9715
Gen 281/2000  train MSE: 0.9800, val MSE: 0.9696
Gen 282/2000  train MSE: 0.9800, val MSE: 0.9696
Gen 283/2000  train MSE: 0.9800, val MSE: 0.9696
Gen 284/2000  train MSE: 0.9800, val MSE: 0.9696
Gen 285/2000  train MSE: 0.9800, val MSE: 0.9696
Gen 286/2000  train MSE: 0.9800, val MSE: 0.9696
Gen 287/2000  train MSE: 0.9798, val MSE: 0.9696
Gen 288/2000  train MSE: 0.9798, val MSE: 0.9696
Gen 289/2000  train MSE: 0.9798, val MSE: 0.9696
Gen 290/2000  train MSE: 0.9798, val MSE: 0.9696
Gen 291/2000  train MSE: 0.9792, val MSE: 0.9690
Gen 292/2000  train MSE: 0.9792, val MSE: 0.9690
Gen 293/2000  train MSE: 0.9789, val MSE: 0.9688
Gen 294/2000  train MSE: 0.9789, val MSE: 0.9688
Gen 295/2000  train MSE: 0.9789, val MSE: 0.9688
Gen 296/2000  train MSE: 0.9789, val MSE: 0.9688
Gen 297/2000  train MSE: 0.9789, val MSE: 0.9688
Gen 298/2000  train MSE: 0.9784, val MSE: 0.9675
Gen 299/2000  train MSE: 0.9784, val MSE: 0.9675
Gen 300/2000  train MSE: 0.9784, val MSE: 0.9675
Gen 300/2000  train MSE: 0.9784, val MSE: 0.9675
Gen 301/2000  train MSE: 0.9784, val MSE: 0.9675
Gen 302/2000  train MSE: 0.9784, val MSE: 0.9675
Gen 303/2000  train MSE: 0.9784, val MSE: 0.9675
Gen 304/2000  train MSE: 0.9784, val MSE: 0.9675
Gen 305/2000  train MSE: 0.9784, val MSE: 0.9675
Gen 306/2000  train MSE: 0.9784, val MSE: 0.9675
Gen 307/2000  train MSE: 0.9783, val MSE: 0.9673
Gen 308/2000  train MSE: 0.9783, val MSE: 0.9673
Gen 309/2000  train MSE: 0.9783, val MSE: 0.9673
Gen 310/2000  train MSE: 0.9783, val MSE: 0.9673
Gen 311/2000  train MSE: 0.9781, val MSE: 0.9673
Gen 312/2000  train MSE: 0.9781, val MSE: 0.9673
Gen 313/2000  train MSE: 0.9781, val MSE: 0.9665
Gen 314/2000  train MSE: 0.9777, val MSE: 0.9673
Gen 315/2000  train MSE: 0.9777, val MSE: 0.9673
Gen 316/2000  train MSE: 0.9777, val MSE: 0.9673
Gen 317/2000  train MSE: 0.9777, val MSE: 0.9673
Gen 318/2000  train MSE: 0.9777, val MSE: 0.9673
Gen 319/2000  train MSE: 0.9777, val MSE: 0.9673
Gen 320/2000  train MSE: 0.9777, val MSE: 0.9673
Gen 321/2000  train MSE: 0.9777, val MSE: 0.9663
Gen 322/2000  train MSE: 0.9777, val MSE: 0.9663
Gen 323/2000  train MSE: 0.9776, val MSE: 0.9648
Gen 324/2000  train MSE: 0.9773, val MSE: 0.9666
Gen 325/2000  train MSE: 0.9773, val MSE: 0.9651
Gen 326/2000  train MSE: 0.9773, val MSE: 0.9651
Gen 327/2000  train MSE: 0.9773, val MSE: 0.9651
Gen 328/2000  train MSE: 0.9773, val MSE: 0.9651
Gen 329/2000  train MSE: 0.9773, val MSE: 0.9661
Gen 330/2000  train MSE: 0.9771, val MSE: 0.9648
Gen 331/2000  train MSE: 0.9769, val MSE: 0.9656
Gen 332/2000  train MSE: 0.9765, val MSE: 0.9645
Gen 333/2000  train MSE: 0.9765, val MSE: 0.9645
Gen 334/2000  train MSE: 0.9765, val MSE: 0.9645
Gen 335/2000  train MSE: 0.9765, val MSE: 0.9645
Gen 336/2000  train MSE: 0.9765, val MSE: 0.9645
Gen 337/2000  train MSE: 0.9765, val MSE: 0.9645
Gen 338/2000  train MSE: 0.9764, val MSE: 0.9638
Gen 339/2000  train MSE: 0.9764, val MSE: 0.9634
Gen 340/2000  train MSE: 0.9762, val MSE: 0.9621
Gen 341/2000  train MSE: 0.9762, val MSE: 0.9621
Gen 342/2000  train MSE: 0.9757, val MSE: 0.9638
Gen 343/2000  train MSE: 0.9757, val MSE: 0.9638
Gen 344/2000  train MSE: 0.9757, val MSE: 0.9638
Gen 345/2000  train MSE: 0.9755, val MSE: 0.9634
Gen 346/2000  train MSE: 0.9755, val MSE: 0.9637
Gen 347/2000  train MSE: 0.9750, val MSE: 0.9629
Gen 348/2000  train MSE: 0.9750, val MSE: 0.9629
Gen 349/2000  train MSE: 0.9750, val MSE: 0.9629
Gen 350/2000  train MSE: 0.9750, val MSE: 0.9629
Gen 351/2000  train MSE: 0.9750, val MSE: 0.9617
Gen 352/2000  train MSE: 0.9750, val MSE: 0.9617
Gen 353/2000  train MSE: 0.9743, val MSE: 0.9602
Gen 354/2000  train MSE: 0.9743, val MSE: 0.9602
Gen 355/2000  train MSE: 0.9738, val MSE: 0.9594
Gen 356/2000  train MSE: 0.9738, val MSE: 0.9594
Gen 357/2000  train MSE: 0.9738, val MSE: 0.9594
Gen 358/2000  train MSE: 0.9738, val MSE: 0.9594
Gen 359/2000  train MSE: 0.9738, val MSE: 0.9594
Gen 360/2000  train MSE: 0.9738, val MSE: 0.9594
Gen 361/2000  train MSE: 0.9725, val MSE: 0.9573
Gen 362/2000  train MSE: 0.9725, val MSE: 0.9573
Gen 363/2000  train MSE: 0.9725, val MSE: 0.9573
Gen 364/2000  train MSE: 0.9724, val MSE: 0.9560
Gen 365/2000  train MSE: 0.9724, val MSE: 0.9560
Gen 366/2000  train MSE: 0.9724, val MSE: 0.9560
Gen 367/2000  train MSE: 0.9724, val MSE: 0.9560
Gen 368/2000  train MSE: 0.9708, val MSE: 0.9558
Gen 369/2000  train MSE: 0.9708, val MSE: 0.9558
Gen 370/2000  train MSE: 0.9708, val MSE: 0.9558
Gen 371/2000  train MSE: 0.9708, val MSE: 0.9558
Gen 372/2000  train MSE: 0.9708, val MSE: 0.9558
Gen 373/2000  train MSE: 0.9708, val MSE: 0.9558
Gen 374/2000  train MSE: 0.9708, val MSE: 0.9558
Gen 375/2000  train MSE: 0.9705, val MSE: 0.9563
Gen 376/2000  train MSE: 0.9705, val MSE: 0.9563
Gen 377/2000  train MSE: 0.9705, val MSE: 0.9563
Gen 378/2000  train MSE: 0.9705, val MSE: 0.9563
Gen 379/2000  train MSE: 0.9700, val MSE: 0.9539
Gen 380/2000  train MSE: 0.9700, val MSE: 0.9539
Gen 381/2000  train MSE: 0.9697, val MSE: 0.9538
Gen 382/2000  train MSE: 0.9693, val MSE: 0.9519
Gen 383/2000  train MSE: 0.9693, val MSE: 0.9519
Gen 384/2000  train MSE: 0.9693, val MSE: 0.9519
Gen 385/2000  train MSE: 0.9693, val MSE: 0.9519
Gen 386/2000  train MSE: 0.9693, val MSE: 0.9519
Gen 387/2000  train MSE: 0.9690, val MSE: 0.9519
Gen 388/2000  train MSE: 0.9686, val MSE: 0.9489
Gen 389/2000  train MSE: 0.9686, val MSE: 0.9489
Gen 390/2000  train MSE: 0.9686, val MSE: 0.9489
Gen 391/2000  train MSE: 0.9686, val MSE: 0.9489
Gen 392/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 393/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 394/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 395/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 396/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 397/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 398/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 399/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 400/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 400/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 401/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 402/2000  train MSE: 0.9678, val MSE: 0.9495
Gen 403/2000  train MSE: 0.9676, val MSE: 0.9502
Gen 404/2000  train MSE: 0.9672, val MSE: 0.9487
Gen 405/2000  train MSE: 0.9672, val MSE: 0.9487
Gen 406/2000  train MSE: 0.9665, val MSE: 0.9484
Gen 407/2000  train MSE: 0.9665, val MSE: 0.9484
Gen 408/2000  train MSE: 0.9665, val MSE: 0.9484
Gen 409/2000  train MSE: 0.9665, val MSE: 0.9484
Gen 410/2000  train MSE: 0.9665, val MSE: 0.9484
Gen 411/2000  train MSE: 0.9665, val MSE: 0.9484
Gen 412/2000  train MSE: 0.9665, val MSE: 0.9484
Gen 413/2000  train MSE: 0.9665, val MSE: 0.9484
Gen 414/2000  train MSE: 0.9665, val MSE: 0.9484
Gen 415/2000  train MSE: 0.9665, val MSE: 0.9484
Gen 416/2000  train MSE: 0.9665, val MSE: 0.9484
Gen 417/2000  train MSE: 0.9664, val MSE: 0.9465
Gen 418/2000  train MSE: 0.9659, val MSE: 0.9463
Gen 419/2000  train MSE: 0.9659, val MSE: 0.9463
Gen 420/2000  train MSE: 0.9659, val MSE: 0.9463
Gen 421/2000  train MSE: 0.9659, val MSE: 0.9463
Gen 422/2000  train MSE: 0.9659, val MSE: 0.9463
Gen 423/2000  train MSE: 0.9652, val MSE: 0.9461
Gen 424/2000  train MSE: 0.9652, val MSE: 0.9461
Gen 425/2000  train MSE: 0.9652, val MSE: 0.9461
Gen 426/2000  train MSE: 0.9652, val MSE: 0.9461
Gen 427/2000  train MSE: 0.9652, val MSE: 0.9461
Gen 428/2000  train MSE: 0.9649, val MSE: 0.9453
Gen 429/2000  train MSE: 0.9649, val MSE: 0.9453
Gen 430/2000  train MSE: 0.9649, val MSE: 0.9453
Gen 431/2000  train MSE: 0.9649, val MSE: 0.9459
Gen 432/2000  train MSE: 0.9649, val MSE: 0.9459
Gen 433/2000  train MSE: 0.9649, val MSE: 0.9459
Gen 434/2000  train MSE: 0.9649, val MSE: 0.9459
Gen 435/2000  train MSE: 0.9649, val MSE: 0.9459
Gen 436/2000  train MSE: 0.9649, val MSE: 0.9459
Gen 437/2000  train MSE: 0.9649, val MSE: 0.9459
Gen 438/2000  train MSE: 0.9649, val MSE: 0.9459
Gen 439/2000  train MSE: 0.9649, val MSE: 0.9436
Gen 440/2000  train MSE: 0.9649, val MSE: 0.9436
Gen 441/2000  train MSE: 0.9648, val MSE: 0.9456
Gen 442/2000  train MSE: 0.9646, val MSE: 0.9444
Gen 443/2000  train MSE: 0.9645, val MSE: 0.9439
Gen 444/2000  train MSE: 0.9645, val MSE: 0.9439
Gen 445/2000  train MSE: 0.9645, val MSE: 0.9439
Gen 446/2000  train MSE: 0.9644, val MSE: 0.9440
Gen 447/2000  train MSE: 0.9644, val MSE: 0.9442
Gen 448/2000  train MSE: 0.9644, val MSE: 0.9442
Gen 449/2000  train MSE: 0.9644, val MSE: 0.9442
Gen 450/2000  train MSE: 0.9641, val MSE: 0.9435
Gen 451/2000  train MSE: 0.9641, val MSE: 0.9435
Gen 452/2000  train MSE: 0.9641, val MSE: 0.9435
Gen 453/2000  train MSE: 0.9641, val MSE: 0.9435
Gen 454/2000  train MSE: 0.9640, val MSE: 0.9436
Gen 455/2000  train MSE: 0.9640, val MSE: 0.9436
Gen 456/2000  train MSE: 0.9640, val MSE: 0.9436
Gen 457/2000  train MSE: 0.9640, val MSE: 0.9436
Gen 458/2000  train MSE: 0.9636, val MSE: 0.9424
Gen 459/2000  train MSE: 0.9633, val MSE: 0.9418
Gen 460/2000  train MSE: 0.9633, val MSE: 0.9418
Gen 461/2000  train MSE: 0.9633, val MSE: 0.9418
Gen 462/2000  train MSE: 0.9633, val MSE: 0.9418
Gen 463/2000  train MSE: 0.9632, val MSE: 0.9420
Gen 464/2000  train MSE: 0.9632, val MSE: 0.9435
Gen 465/2000  train MSE: 0.9632, val MSE: 0.9435
Gen 466/2000  train MSE: 0.9631, val MSE: 0.9431
Gen 467/2000  train MSE: 0.9630, val MSE: 0.9419
Gen 468/2000  train MSE: 0.9628, val MSE: 0.9417
Gen 469/2000  train MSE: 0.9628, val MSE: 0.9419
Gen 470/2000  train MSE: 0.9627, val MSE: 0.9402
Gen 471/2000  train MSE: 0.9627, val MSE: 0.9402
Gen 472/2000  train MSE: 0.9626, val MSE: 0.9403
Gen 473/2000  train MSE: 0.9624, val MSE: 0.9409
Gen 474/2000  train MSE: 0.9624, val MSE: 0.9409
Gen 475/2000  train MSE: 0.9624, val MSE: 0.9409
Gen 476/2000  train MSE: 0.9624, val MSE: 0.9408
Gen 477/2000  train MSE: 0.9624, val MSE: 0.9408
Gen 478/2000  train MSE: 0.9624, val MSE: 0.9399
Gen 479/2000  train MSE: 0.9624, val MSE: 0.9399
Gen 480/2000  train MSE: 0.9623, val MSE: 0.9405
Gen 481/2000  train MSE: 0.9623, val MSE: 0.9404
Gen 482/2000  train MSE: 0.9622, val MSE: 0.9408
Gen 483/2000  train MSE: 0.9621, val MSE: 0.9400
Gen 484/2000  train MSE: 0.9618, val MSE: 0.9398
Gen 485/2000  train MSE: 0.9618, val MSE: 0.9398
Gen 486/2000  train MSE: 0.9618, val MSE: 0.9397
Gen 487/2000  train MSE: 0.9618, val MSE: 0.9397
Gen 488/2000  train MSE: 0.9617, val MSE: 0.9398
Gen 489/2000  train MSE: 0.9617, val MSE: 0.9398
Gen 490/2000  train MSE: 0.9616, val MSE: 0.9406
Gen 491/2000  train MSE: 0.9616, val MSE: 0.9394
Gen 492/2000  train MSE: 0.9616, val MSE: 0.9394
Gen 493/2000  train MSE: 0.9614, val MSE: 0.9402
Gen 494/2000  train MSE: 0.9613, val MSE: 0.9394
Gen 495/2000  train MSE: 0.9613, val MSE: 0.9394
Gen 496/2000  train MSE: 0.9613, val MSE: 0.9394
Gen 497/2000  train MSE: 0.9613, val MSE: 0.9394
Gen 498/2000  train MSE: 0.9613, val MSE: 0.9394
Gen 499/2000  train MSE: 0.9613, val MSE: 0.9394
Gen 500/2000  train MSE: 0.9613, val MSE: 0.9394
Gen 500/2000  train MSE: 0.9613, val MSE: 0.9394
Gen 501/2000  train MSE: 0.9613, val MSE: 0.9405
Gen 502/2000  train MSE: 0.9613, val MSE: 0.9405
Gen 503/2000  train MSE: 0.9612, val MSE: 0.9407
Gen 504/2000  train MSE: 0.9612, val MSE: 0.9398
Gen 505/2000  train MSE: 0.9612, val MSE: 0.9398
Gen 506/2000  train MSE: 0.9611, val MSE: 0.9393
Gen 507/2000  train MSE: 0.9610, val MSE: 0.9399
Gen 508/2000  train MSE: 0.9608, val MSE: 0.9384
Gen 509/2000  train MSE: 0.9607, val MSE: 0.9385
Gen 510/2000  train MSE: 0.9607, val MSE: 0.9385
Gen 511/2000  train MSE: 0.9606, val MSE: 0.9394
Gen 512/2000  train MSE: 0.9606, val MSE: 0.9394
Gen 513/2000  train MSE: 0.9606, val MSE: 0.9394
Gen 514/2000  train MSE: 0.9606, val MSE: 0.9394
Gen 515/2000  train MSE: 0.9605, val MSE: 0.9392
Gen 516/2000  train MSE: 0.9604, val MSE: 0.9389
Gen 517/2000  train MSE: 0.9603, val MSE: 0.9377
Gen 518/2000  train MSE: 0.9603, val MSE: 0.9377
Gen 519/2000  train MSE: 0.9603, val MSE: 0.9377
Gen 520/2000  train MSE: 0.9602, val MSE: 0.9376
Gen 521/2000  train MSE: 0.9602, val MSE: 0.9376
Gen 522/2000  train MSE: 0.9602, val MSE: 0.9376
Gen 523/2000  train MSE: 0.9602, val MSE: 0.9392
Gen 524/2000  train MSE: 0.9600, val MSE: 0.9383
Gen 525/2000  train MSE: 0.9600, val MSE: 0.9387
Gen 526/2000  train MSE: 0.9600, val MSE: 0.9387
Gen 527/2000  train MSE: 0.9599, val MSE: 0.9388
Gen 528/2000  train MSE: 0.9599, val MSE: 0.9388
Gen 529/2000  train MSE: 0.9599, val MSE: 0.9388
Gen 530/2000  train MSE: 0.9599, val MSE: 0.9388
Gen 531/2000  train MSE: 0.9598, val MSE: 0.9384
Gen 532/2000  train MSE: 0.9598, val MSE: 0.9384
Gen 533/2000  train MSE: 0.9597, val MSE: 0.9384
Gen 534/2000  train MSE: 0.9597, val MSE: 0.9384
Gen 535/2000  train MSE: 0.9597, val MSE: 0.9384
Gen 536/2000  train MSE: 0.9597, val MSE: 0.9384
Gen 537/2000  train MSE: 0.9597, val MSE: 0.9381
Gen 538/2000  train MSE: 0.9595, val MSE: 0.9379
Gen 539/2000  train MSE: 0.9595, val MSE: 0.9379
Gen 540/2000  train MSE: 0.9595, val MSE: 0.9390
Gen 541/2000  train MSE: 0.9595, val MSE: 0.9390
Gen 542/2000  train MSE: 0.9595, val MSE: 0.9390
Gen 543/2000  train MSE: 0.9593, val MSE: 0.9363
Gen 544/2000  train MSE: 0.9593, val MSE: 0.9366
Gen 545/2000  train MSE: 0.9593, val MSE: 0.9366
Gen 546/2000  train MSE: 0.9588, val MSE: 0.9356
Gen 547/2000  train MSE: 0.9588, val MSE: 0.9356
Gen 548/2000  train MSE: 0.9588, val MSE: 0.9356
Gen 549/2000  train MSE: 0.9588, val MSE: 0.9356
Gen 550/2000  train MSE: 0.9588, val MSE: 0.9356
Gen 551/2000  train MSE: 0.9588, val MSE: 0.9356
Gen 552/2000  train MSE: 0.9588, val MSE: 0.9356
Gen 553/2000  train MSE: 0.9588, val MSE: 0.9356
Gen 554/2000  train MSE: 0.9588, val MSE: 0.9366
Gen 555/2000  train MSE: 0.9586, val MSE: 0.9350
Gen 556/2000  train MSE: 0.9582, val MSE: 0.9355
Gen 557/2000  train MSE: 0.9582, val MSE: 0.9355
Gen 558/2000  train MSE: 0.9582, val MSE: 0.9355
Gen 559/2000  train MSE: 0.9582, val MSE: 0.9352
Gen 560/2000  train MSE: 0.9582, val MSE: 0.9352
Gen 561/2000  train MSE: 0.9582, val MSE: 0.9352
Gen 562/2000  train MSE: 0.9582, val MSE: 0.9352
Gen 563/2000  train MSE: 0.9580, val MSE: 0.9346
Gen 564/2000  train MSE: 0.9580, val MSE: 0.9346
Gen 565/2000  train MSE: 0.9580, val MSE: 0.9346
Gen 566/2000  train MSE: 0.9576, val MSE: 0.9340
Gen 567/2000  train MSE: 0.9576, val MSE: 0.9340
Gen 568/2000  train MSE: 0.9576, val MSE: 0.9340
Gen 569/2000  train MSE: 0.9576, val MSE: 0.9340
Gen 570/2000  train MSE: 0.9574, val MSE: 0.9339
Gen 571/2000  train MSE: 0.9574, val MSE: 0.9339
Gen 572/2000  train MSE: 0.9574, val MSE: 0.9339
Gen 573/2000  train MSE: 0.9573, val MSE: 0.9342
Gen 574/2000  train MSE: 0.9570, val MSE: 0.9335
Gen 575/2000  train MSE: 0.9570, val MSE: 0.9335
Gen 576/2000  train MSE: 0.9570, val MSE: 0.9335
Gen 577/2000  train MSE: 0.9570, val MSE: 0.9335
Gen 578/2000  train MSE: 0.9570, val MSE: 0.9335
Gen 579/2000  train MSE: 0.9568, val MSE: 0.9317
Gen 580/2000  train MSE: 0.9568, val MSE: 0.9317
Gen 581/2000  train MSE: 0.9568, val MSE: 0.9317
Gen 582/2000  train MSE: 0.9568, val MSE: 0.9317
Gen 583/2000  train MSE: 0.9567, val MSE: 0.9319
Gen 584/2000  train MSE: 0.9567, val MSE: 0.9319
Gen 585/2000  train MSE: 0.9562, val MSE: 0.9312
Gen 586/2000  train MSE: 0.9562, val MSE: 0.9312
Gen 587/2000  train MSE: 0.9562, val MSE: 0.9312
Gen 588/2000  train MSE: 0.9562, val MSE: 0.9312
Gen 589/2000  train MSE: 0.9562, val MSE: 0.9312
Gen 590/2000  train MSE: 0.9559, val MSE: 0.9307
Gen 591/2000  train MSE: 0.9559, val MSE: 0.9307
Gen 592/2000  train MSE: 0.9559, val MSE: 0.9307
Gen 593/2000  train MSE: 0.9559, val MSE: 0.9307
Gen 594/2000  train MSE: 0.9559, val MSE: 0.9310
Gen 595/2000  train MSE: 0.9559, val MSE: 0.9312
Gen 596/2000  train MSE: 0.9559, val MSE: 0.9312
Gen 597/2000  train MSE: 0.9558, val MSE: 0.9316
Gen 598/2000  train MSE: 0.9557, val MSE: 0.9302
Gen 599/2000  train MSE: 0.9557, val MSE: 0.9302
Gen 600/2000  train MSE: 0.9557, val MSE: 0.9302
Gen 600/2000  train MSE: 0.9556, val MSE: 0.9303
Gen 601/2000  train MSE: 0.9553, val MSE: 0.9302
Gen 602/2000  train MSE: 0.9553, val MSE: 0.9302
Gen 603/2000  train MSE: 0.9553, val MSE: 0.9297
Gen 604/2000  train MSE: 0.9552, val MSE: 0.9313
Gen 605/2000  train MSE: 0.9552, val MSE: 0.9313
Gen 606/2000  train MSE: 0.9551, val MSE: 0.9305
Gen 607/2000  train MSE: 0.9549, val MSE: 0.9293
Gen 608/2000  train MSE: 0.9549, val MSE: 0.9293
Gen 609/2000  train MSE: 0.9549, val MSE: 0.9286
Gen 610/2000  train MSE: 0.9549, val MSE: 0.9303
Gen 611/2000  train MSE: 0.9547, val MSE: 0.9298
Gen 612/2000  train MSE: 0.9547, val MSE: 0.9298
Gen 613/2000  train MSE: 0.9547, val MSE: 0.9298
Gen 614/2000  train MSE: 0.9547, val MSE: 0.9292
Gen 615/2000  train MSE: 0.9547, val MSE: 0.9292
Gen 616/2000  train MSE: 0.9545, val MSE: 0.9285
Gen 617/2000  train MSE: 0.9545, val MSE: 0.9285
Gen 618/2000  train MSE: 0.9544, val MSE: 0.9289
Gen 619/2000  train MSE: 0.9542, val MSE: 0.9290
Gen 620/2000  train MSE: 0.9542, val MSE: 0.9290
Gen 621/2000  train MSE: 0.9542, val MSE: 0.9290
Gen 622/2000  train MSE: 0.9542, val MSE: 0.9290
Gen 623/2000  train MSE: 0.9540, val MSE: 0.9286
Gen 624/2000  train MSE: 0.9540, val MSE: 0.9286
Gen 625/2000  train MSE: 0.9540, val MSE: 0.9295
Gen 626/2000  train MSE: 0.9540, val MSE: 0.9290
Gen 627/2000  train MSE: 0.9540, val MSE: 0.9297
Gen 628/2000  train MSE: 0.9538, val MSE: 0.9282
Gen 629/2000  train MSE: 0.9537, val MSE: 0.9285
Gen 630/2000  train MSE: 0.9537, val MSE: 0.9284
Gen 631/2000  train MSE: 0.9536, val MSE: 0.9282
Gen 632/2000  train MSE: 0.9534, val MSE: 0.9287
Gen 633/2000  train MSE: 0.9534, val MSE: 0.9287
Gen 634/2000  train MSE: 0.9534, val MSE: 0.9287
Gen 635/2000  train MSE: 0.9534, val MSE: 0.9287
Gen 636/2000  train MSE: 0.9534, val MSE: 0.9287
Gen 637/2000  train MSE: 0.9534, val MSE: 0.9287
Gen 638/2000  train MSE: 0.9534, val MSE: 0.9280
Gen 639/2000  train MSE: 0.9534, val MSE: 0.9280
Gen 640/2000  train MSE: 0.9534, val MSE: 0.9280
Gen 641/2000  train MSE: 0.9533, val MSE: 0.9287
Gen 642/2000  train MSE: 0.9532, val MSE: 0.9285
Gen 643/2000  train MSE: 0.9532, val MSE: 0.9285
Gen 644/2000  train MSE: 0.9532, val MSE: 0.9273
Gen 645/2000  train MSE: 0.9532, val MSE: 0.9273
Gen 646/2000  train MSE: 0.9530, val MSE: 0.9287
Gen 647/2000  train MSE: 0.9530, val MSE: 0.9287
Gen 648/2000  train MSE: 0.9530, val MSE: 0.9280
Gen 649/2000  train MSE: 0.9530, val MSE: 0.9280
Gen 650/2000  train MSE: 0.9530, val MSE: 0.9278
Gen 651/2000  train MSE: 0.9530, val MSE: 0.9278
Gen 652/2000  train MSE: 0.9529, val MSE: 0.9279
Gen 653/2000  train MSE: 0.9529, val MSE: 0.9279
Gen 654/2000  train MSE: 0.9527, val MSE: 0.9267
Gen 655/2000  train MSE: 0.9527, val MSE: 0.9267
Gen 656/2000  train MSE: 0.9527, val MSE: 0.9267
Gen 657/2000  train MSE: 0.9527, val MSE: 0.9277
Gen 658/2000  train MSE: 0.9527, val MSE: 0.9277
Gen 659/2000  train MSE: 0.9525, val MSE: 0.9282
Gen 660/2000  train MSE: 0.9525, val MSE: 0.9282
Gen 661/2000  train MSE: 0.9525, val MSE: 0.9282
Gen 662/2000  train MSE: 0.9525, val MSE: 0.9282
Gen 663/2000  train MSE: 0.9525, val MSE: 0.9282
Gen 664/2000  train MSE: 0.9524, val MSE: 0.9273
Gen 665/2000  train MSE: 0.9524, val MSE: 0.9273
Gen 666/2000  train MSE: 0.9524, val MSE: 0.9267
Gen 667/2000  train MSE: 0.9524, val MSE: 0.9267
Gen 668/2000  train MSE: 0.9524, val MSE: 0.9267
Gen 669/2000  train MSE: 0.9519, val MSE: 0.9259
Gen 670/2000  train MSE: 0.9519, val MSE: 0.9259
Gen 671/2000  train MSE: 0.9519, val MSE: 0.9259
Gen 672/2000  train MSE: 0.9519, val MSE: 0.9259
Gen 673/2000  train MSE: 0.9519, val MSE: 0.9259
Gen 674/2000  train MSE: 0.9519, val MSE: 0.9259
Gen 675/2000  train MSE: 0.9519, val MSE: 0.9259
Gen 676/2000  train MSE: 0.9519, val MSE: 0.9259
Gen 677/2000  train MSE: 0.9518, val MSE: 0.9258
Gen 678/2000  train MSE: 0.9518, val MSE: 0.9258
Gen 679/2000  train MSE: 0.9518, val MSE: 0.9258
Gen 680/2000  train MSE: 0.9518, val MSE: 0.9258
Gen 681/2000  train MSE: 0.9518, val MSE: 0.9258
Gen 682/2000  train MSE: 0.9518, val MSE: 0.9258
Gen 683/2000  train MSE: 0.9518, val MSE: 0.9258
Gen 684/2000  train MSE: 0.9518, val MSE: 0.9258
Gen 685/2000  train MSE: 0.9518, val MSE: 0.9258
Gen 686/2000  train MSE: 0.9518, val MSE: 0.9258
Gen 687/2000  train MSE: 0.9518, val MSE: 0.9258
Gen 688/2000  train MSE: 0.9517, val MSE: 0.9271
Gen 689/2000  train MSE: 0.9517, val MSE: 0.9271
Gen 690/2000  train MSE: 0.9517, val MSE: 0.9271
Gen 691/2000  train MSE: 0.9516, val MSE: 0.9244
Gen 692/2000  train MSE: 0.9516, val MSE: 0.9254
Gen 693/2000  train MSE: 0.9515, val MSE: 0.9258
Gen 694/2000  train MSE: 0.9515, val MSE: 0.9258
Gen 695/2000  train MSE: 0.9513, val MSE: 0.9248
Gen 696/2000  train MSE: 0.9512, val MSE: 0.9240
Gen 697/2000  train MSE: 0.9512, val MSE: 0.9254
Gen 698/2000  train MSE: 0.9512, val MSE: 0.9254
Gen 699/2000  train MSE: 0.9510, val MSE: 0.9240
Gen 700/2000  train MSE: 0.9510, val MSE: 0.9240
Gen 700/2000  train MSE: 0.9509, val MSE: 0.9241
Gen 701/2000  train MSE: 0.9509, val MSE: 0.9248
Gen 702/2000  train MSE: 0.9507, val MSE: 0.9250
Gen 703/2000  train MSE: 0.9505, val MSE: 0.9237
Gen 704/2000  train MSE: 0.9504, val MSE: 0.9240
Gen 705/2000  train MSE: 0.9502, val MSE: 0.9234
Gen 706/2000  train MSE: 0.9502, val MSE: 0.9234
Gen 707/2000  train MSE: 0.9501, val MSE: 0.9232
Gen 708/2000  train MSE: 0.9501, val MSE: 0.9232
Gen 709/2000  train MSE: 0.9501, val MSE: 0.9232
Gen 710/2000  train MSE: 0.9501, val MSE: 0.9232
Gen 711/2000  train MSE: 0.9497, val MSE: 0.9220
Gen 712/2000  train MSE: 0.9497, val MSE: 0.9220
Gen 713/2000  train MSE: 0.9497, val MSE: 0.9220
Gen 714/2000  train MSE: 0.9497, val MSE: 0.9222
Gen 715/2000  train MSE: 0.9497, val MSE: 0.9222
Gen 716/2000  train MSE: 0.9497, val MSE: 0.9222
Gen 717/2000  train MSE: 0.9496, val MSE: 0.9223
Gen 718/2000  train MSE: 0.9495, val MSE: 0.9210
Gen 719/2000  train MSE: 0.9495, val MSE: 0.9202
Gen 720/2000  train MSE: 0.9495, val MSE: 0.9208
Gen 721/2000  train MSE: 0.9495, val MSE: 0.9208
Gen 722/2000  train MSE: 0.9494, val MSE: 0.9215
Gen 723/2000  train MSE: 0.9494, val MSE: 0.9215
Gen 724/2000  train MSE: 0.9494, val MSE: 0.9215
Gen 725/2000  train MSE: 0.9492, val MSE: 0.9212
Gen 726/2000  train MSE: 0.9492, val MSE: 0.9212
Gen 727/2000  train MSE: 0.9491, val MSE: 0.9214
Gen 728/2000  train MSE: 0.9491, val MSE: 0.9214
Gen 729/2000  train MSE: 0.9491, val MSE: 0.9214
Gen 730/2000  train MSE: 0.9488, val MSE: 0.9204
Gen 731/2000  train MSE: 0.9488, val MSE: 0.9204
Gen 732/2000  train MSE: 0.9488, val MSE: 0.9204
Gen 733/2000  train MSE: 0.9488, val MSE: 0.9204
Gen 734/2000  train MSE: 0.9488, val MSE: 0.9204
Gen 735/2000  train MSE: 0.9487, val MSE: 0.9210
Gen 736/2000  train MSE: 0.9487, val MSE: 0.9210
Gen 737/2000  train MSE: 0.9487, val MSE: 0.9210
Gen 738/2000  train MSE: 0.9485, val MSE: 0.9206
Gen 739/2000  train MSE: 0.9484, val MSE: 0.9198
Gen 740/2000  train MSE: 0.9484, val MSE: 0.9198
Gen 741/2000  train MSE: 0.9484, val MSE: 0.9198
Gen 742/2000  train MSE: 0.9484, val MSE: 0.9198
Gen 743/2000  train MSE: 0.9484, val MSE: 0.9198
Gen 744/2000  train MSE: 0.9480, val MSE: 0.9201
Gen 745/2000  train MSE: 0.9480, val MSE: 0.9201
Gen 746/2000  train MSE: 0.9480, val MSE: 0.9201
Gen 747/2000  train MSE: 0.9480, val MSE: 0.9201
Gen 748/2000  train MSE: 0.9480, val MSE: 0.9201
Gen 749/2000  train MSE: 0.9480, val MSE: 0.9201
Gen 750/2000  train MSE: 0.9480, val MSE: 0.9201
Gen 751/2000  train MSE: 0.9476, val MSE: 0.9196
Gen 752/2000  train MSE: 0.9476, val MSE: 0.9196
Gen 753/2000  train MSE: 0.9476, val MSE: 0.9196
Gen 754/2000  train MSE: 0.9476, val MSE: 0.9196
Gen 755/2000  train MSE: 0.9476, val MSE: 0.9196
Gen 756/2000  train MSE: 0.9476, val MSE: 0.9196
Gen 757/2000  train MSE: 0.9476, val MSE: 0.9196
Gen 758/2000  train MSE: 0.9476, val MSE: 0.9196
Gen 759/2000  train MSE: 0.9476, val MSE: 0.9196
Gen 760/2000  train MSE: 0.9475, val MSE: 0.9196
Gen 761/2000  train MSE: 0.9475, val MSE: 0.9199
Gen 762/2000  train MSE: 0.9474, val MSE: 0.9196
Gen 763/2000  train MSE: 0.9472, val MSE: 0.9194
Gen 764/2000  train MSE: 0.9472, val MSE: 0.9194
Gen 765/2000  train MSE: 0.9472, val MSE: 0.9194
Gen 766/2000  train MSE: 0.9472, val MSE: 0.9194
Gen 767/2000  train MSE: 0.9472, val MSE: 0.9194
Gen 768/2000  train MSE: 0.9472, val MSE: 0.9194
Gen 769/2000  train MSE: 0.9472, val MSE: 0.9194
Gen 770/2000  train MSE: 0.9472, val MSE: 0.9194
Gen 771/2000  train MSE: 0.9471, val MSE: 0.9190
Gen 772/2000  train MSE: 0.9471, val MSE: 0.9189
Gen 773/2000  train MSE: 0.9471, val MSE: 0.9194
Gen 774/2000  train MSE: 0.9471, val MSE: 0.9194
Gen 775/2000  train MSE: 0.9471, val MSE: 0.9194
Gen 776/2000  train MSE: 0.9471, val MSE: 0.9194
Gen 777/2000  train MSE: 0.9470, val MSE: 0.9183
Gen 778/2000  train MSE: 0.9470, val MSE: 0.9177
Gen 779/2000  train MSE: 0.9470, val MSE: 0.9190
Gen 780/2000  train MSE: 0.9468, val MSE: 0.9183
Gen 781/2000  train MSE: 0.9468, val MSE: 0.9183
Gen 782/2000  train MSE: 0.9468, val MSE: 0.9183
Gen 783/2000  train MSE: 0.9468, val MSE: 0.9183
Gen 784/2000  train MSE: 0.9468, val MSE: 0.9173
Gen 785/2000  train MSE: 0.9467, val MSE: 0.9175
Gen 786/2000  train MSE: 0.9465, val MSE: 0.9176
Gen 787/2000  train MSE: 0.9465, val MSE: 0.9176
Gen 788/2000  train MSE: 0.9465, val MSE: 0.9176
Gen 789/2000  train MSE: 0.9465, val MSE: 0.9176
Gen 790/2000  train MSE: 0.9465, val MSE: 0.9176
Gen 791/2000  train MSE: 0.9464, val MSE: 0.9183
Gen 792/2000  train MSE: 0.9464, val MSE: 0.9183
Gen 793/2000  train MSE: 0.9464, val MSE: 0.9183
Gen 794/2000  train MSE: 0.9464, val MSE: 0.9183
Gen 795/2000  train MSE: 0.9463, val MSE: 0.9174
Gen 796/2000  train MSE: 0.9463, val MSE: 0.9174
Gen 797/2000  train MSE: 0.9463, val MSE: 0.9174
Gen 798/2000  train MSE: 0.9463, val MSE: 0.9174
Gen 799/2000  train MSE: 0.9463, val MSE: 0.9174
Gen 800/2000  train MSE: 0.9463, val MSE: 0.9174
Gen 800/2000  train MSE: 0.9462, val MSE: 0.9182
Gen 801/2000  train MSE: 0.9462, val MSE: 0.9182
Gen 802/2000  train MSE: 0.9462, val MSE: 0.9188
Gen 803/2000  train MSE: 0.9462, val MSE: 0.9169
Gen 804/2000  train MSE: 0.9462, val MSE: 0.9167
Gen 805/2000  train MSE: 0.9461, val MSE: 0.9161
Gen 806/2000  train MSE: 0.9461, val MSE: 0.9180
Gen 807/2000  train MSE: 0.9461, val MSE: 0.9180
Gen 808/2000  train MSE: 0.9460, val MSE: 0.9168
Gen 809/2000  train MSE: 0.9460, val MSE: 0.9169
Gen 810/2000  train MSE: 0.9458, val MSE: 0.9162
Gen 811/2000  train MSE: 0.9458, val MSE: 0.9162
Gen 812/2000  train MSE: 0.9458, val MSE: 0.9162
Gen 813/2000  train MSE: 0.9458, val MSE: 0.9162
Gen 814/2000  train MSE: 0.9458, val MSE: 0.9168
Gen 815/2000  train MSE: 0.9458, val MSE: 0.9168
Gen 816/2000  train MSE: 0.9458, val MSE: 0.9168
Gen 817/2000  train MSE: 0.9458, val MSE: 0.9168
Gen 818/2000  train MSE: 0.9458, val MSE: 0.9165
Gen 819/2000  train MSE: 0.9458, val MSE: 0.9165
Gen 820/2000  train MSE: 0.9458, val MSE: 0.9174
Gen 821/2000  train MSE: 0.9458, val MSE: 0.9174
Gen 822/2000  train MSE: 0.9458, val MSE: 0.9174
Gen 823/2000  train MSE: 0.9458, val MSE: 0.9174
Gen 824/2000  train MSE: 0.9457, val MSE: 0.9162
Gen 825/2000  train MSE: 0.9457, val MSE: 0.9164
Gen 826/2000  train MSE: 0.9456, val MSE: 0.9174
Gen 827/2000  train MSE: 0.9456, val MSE: 0.9174
Gen 828/2000  train MSE: 0.9456, val MSE: 0.9174
Gen 829/2000  train MSE: 0.9456, val MSE: 0.9174
Gen 830/2000  train MSE: 0.9455, val MSE: 0.9172
Gen 831/2000  train MSE: 0.9455, val MSE: 0.9172
Gen 832/2000  train MSE: 0.9455, val MSE: 0.9167
Gen 833/2000  train MSE: 0.9453, val MSE: 0.9164
Gen 834/2000  train MSE: 0.9453, val MSE: 0.9164
Gen 835/2000  train MSE: 0.9453, val MSE: 0.9164
Gen 836/2000  train MSE: 0.9452, val MSE: 0.9151
Gen 837/2000  train MSE: 0.9452, val MSE: 0.9151
Gen 838/2000  train MSE: 0.9452, val MSE: 0.9164
Gen 839/2000  train MSE: 0.9452, val MSE: 0.9164
Gen 840/2000  train MSE: 0.9450, val MSE: 0.9159
Gen 841/2000  train MSE: 0.9450, val MSE: 0.9159
Gen 842/2000  train MSE: 0.9449, val MSE: 0.9153
Gen 843/2000  train MSE: 0.9449, val MSE: 0.9153
Gen 844/2000  train MSE: 0.9448, val MSE: 0.9157
Gen 845/2000  train MSE: 0.9448, val MSE: 0.9149
Gen 846/2000  train MSE: 0.9446, val MSE: 0.9145
Gen 847/2000  train MSE: 0.9446, val MSE: 0.9145
Gen 848/2000  train MSE: 0.9446, val MSE: 0.9145
Gen 849/2000  train MSE: 0.9446, val MSE: 0.9142
Gen 850/2000  train MSE: 0.9446, val MSE: 0.9149
Gen 851/2000  train MSE: 0.9446, val MSE: 0.9149
Gen 852/2000  train MSE: 0.9444, val MSE: 0.9138
Gen 853/2000  train MSE: 0.9443, val MSE: 0.9134
Gen 854/2000  train MSE: 0.9443, val MSE: 0.9134
Gen 855/2000  train MSE: 0.9443, val MSE: 0.9134
Gen 856/2000  train MSE: 0.9443, val MSE: 0.9134
Gen 857/2000  train MSE: 0.9443, val MSE: 0.9134
Gen 858/2000  train MSE: 0.9443, val MSE: 0.9134
Gen 859/2000  train MSE: 0.9443, val MSE: 0.9134
Gen 860/2000  train MSE: 0.9442, val MSE: 0.9137
Gen 861/2000  train MSE: 0.9440, val MSE: 0.9135
Gen 862/2000  train MSE: 0.9440, val MSE: 0.9135
Gen 863/2000  train MSE: 0.9440, val MSE: 0.9135
Gen 864/2000  train MSE: 0.9440, val MSE: 0.9135
Gen 865/2000  train MSE: 0.9440, val MSE: 0.9135
Gen 866/2000  train MSE: 0.9440, val MSE: 0.9135
Gen 867/2000  train MSE: 0.9438, val MSE: 0.9138
Gen 868/2000  train MSE: 0.9438, val MSE: 0.9138
Gen 869/2000  train MSE: 0.9437, val MSE: 0.9123
Gen 870/2000  train MSE: 0.9437, val MSE: 0.9123
Gen 871/2000  train MSE: 0.9437, val MSE: 0.9130
Gen 872/2000  train MSE: 0.9437, val MSE: 0.9127
Gen 873/2000  train MSE: 0.9437, val MSE: 0.9127
Gen 874/2000  train MSE: 0.9437, val MSE: 0.9127
Gen 875/2000  train MSE: 0.9436, val MSE: 0.9125
Gen 876/2000  train MSE: 0.9436, val MSE: 0.9125
Gen 877/2000  train MSE: 0.9435, val MSE: 0.9124
Gen 878/2000  train MSE: 0.9434, val MSE: 0.9130
Gen 879/2000  train MSE: 0.9434, val MSE: 0.9132
Gen 880/2000  train MSE: 0.9434, val MSE: 0.9132
Gen 881/2000  train MSE: 0.9434, val MSE: 0.9132
Gen 882/2000  train MSE: 0.9434, val MSE: 0.9132
Gen 883/2000  train MSE: 0.9434, val MSE: 0.9132
Gen 884/2000  train MSE: 0.9434, val MSE: 0.9132
Gen 885/2000  train MSE: 0.9434, val MSE: 0.9132
Gen 886/2000  train MSE: 0.9432, val MSE: 0.9131
Gen 887/2000  train MSE: 0.9432, val MSE: 0.9124
Gen 888/2000  train MSE: 0.9430, val MSE: 0.9118
Gen 889/2000  train MSE: 0.9430, val MSE: 0.9118
Gen 890/2000  train MSE: 0.9430, val MSE: 0.9118
Gen 891/2000  train MSE: 0.9429, val MSE: 0.9116
Gen 892/2000  train MSE: 0.9428, val MSE: 0.9108
Gen 893/2000  train MSE: 0.9428, val MSE: 0.9108
Gen 894/2000  train MSE: 0.9428, val MSE: 0.9108
Gen 895/2000  train MSE: 0.9427, val MSE: 0.9116
Gen 896/2000  train MSE: 0.9427, val MSE: 0.9116
Gen 897/2000  train MSE: 0.9427, val MSE: 0.9116
Gen 898/2000  train MSE: 0.9427, val MSE: 0.9116
Gen 899/2000  train MSE: 0.9427, val MSE: 0.9116
Gen 900/2000  train MSE: 0.9427, val MSE: 0.9116
Gen 900/2000  train MSE: 0.9427, val MSE: 0.9116
Gen 901/2000  train MSE: 0.9426, val MSE: 0.9110
Gen 902/2000  train MSE: 0.9426, val MSE: 0.9110
Gen 903/2000  train MSE: 0.9426, val MSE: 0.9107
Gen 904/2000  train MSE: 0.9426, val MSE: 0.9107
Gen 905/2000  train MSE: 0.9425, val MSE: 0.9109
Gen 906/2000  train MSE: 0.9423, val MSE: 0.9097
Gen 907/2000  train MSE: 0.9423, val MSE: 0.9097
Gen 908/2000  train MSE: 0.9423, val MSE: 0.9097
Gen 909/2000  train MSE: 0.9423, val MSE: 0.9097
Gen 910/2000  train MSE: 0.9423, val MSE: 0.9097
Gen 911/2000  train MSE: 0.9422, val MSE: 0.9107
Gen 912/2000  train MSE: 0.9422, val MSE: 0.9107
Gen 913/2000  train MSE: 0.9422, val MSE: 0.9107
Gen 914/2000  train MSE: 0.9422, val MSE: 0.9107
Gen 915/2000  train MSE: 0.9422, val MSE: 0.9107
Gen 916/2000  train MSE: 0.9422, val MSE: 0.9107
Gen 917/2000  train MSE: 0.9422, val MSE: 0.9107
Gen 918/2000  train MSE: 0.9422, val MSE: 0.9107
Gen 919/2000  train MSE: 0.9422, val MSE: 0.9107
Gen 920/2000  train MSE: 0.9420, val MSE: 0.9093
Gen 921/2000  train MSE: 0.9420, val MSE: 0.9093
Gen 922/2000  train MSE: 0.9420, val MSE: 0.9093
Gen 923/2000  train MSE: 0.9420, val MSE: 0.9093
Gen 924/2000  train MSE: 0.9418, val MSE: 0.9097
Gen 925/2000  train MSE: 0.9418, val MSE: 0.9097
Gen 926/2000  train MSE: 0.9418, val MSE: 0.9097
Gen 927/2000  train MSE: 0.9418, val MSE: 0.9097
Gen 928/2000  train MSE: 0.9418, val MSE: 0.9097
Gen 929/2000  train MSE: 0.9418, val MSE: 0.9087
Gen 930/2000  train MSE: 0.9418, val MSE: 0.9087
Gen 931/2000  train MSE: 0.9416, val MSE: 0.9095
Gen 932/2000  train MSE: 0.9416, val MSE: 0.9095
Gen 933/2000  train MSE: 0.9416, val MSE: 0.9095
Gen 934/2000  train MSE: 0.9415, val MSE: 0.9093
Gen 935/2000  train MSE: 0.9415, val MSE: 0.9092
Gen 936/2000  train MSE: 0.9415, val MSE: 0.9092
Gen 937/2000  train MSE: 0.9415, val MSE: 0.9092
Gen 938/2000  train MSE: 0.9413, val MSE: 0.9077
Gen 939/2000  train MSE: 0.9413, val MSE: 0.9077
Gen 940/2000  train MSE: 0.9413, val MSE: 0.9077
Gen 941/2000  train MSE: 0.9413, val MSE: 0.9077
Gen 942/2000  train MSE: 0.9413, val MSE: 0.9077
Gen 943/2000  train MSE: 0.9413, val MSE: 0.9077
Gen 944/2000  train MSE: 0.9413, val MSE: 0.9077
Gen 945/2000  train MSE: 0.9413, val MSE: 0.9077
Gen 946/2000  train MSE: 0.9413, val MSE: 0.9067
Gen 947/2000  train MSE: 0.9411, val MSE: 0.9068
Gen 948/2000  train MSE: 0.9411, val MSE: 0.9068
Gen 949/2000  train MSE: 0.9411, val MSE: 0.9068
Gen 950/2000  train MSE: 0.9411, val MSE: 0.9078
Gen 951/2000  train MSE: 0.9408, val MSE: 0.9080
Gen 952/2000  train MSE: 0.9408, val MSE: 0.9080
Gen 953/2000  train MSE: 0.9408, val MSE: 0.9080
Gen 954/2000  train MSE: 0.9408, val MSE: 0.9080
Gen 955/2000  train MSE: 0.9408, val MSE: 0.9080
Gen 956/2000  train MSE: 0.9408, val MSE: 0.9080
Gen 957/2000  train MSE: 0.9408, val MSE: 0.9080
Gen 958/2000  train MSE: 0.9408, val MSE: 0.9067
Gen 959/2000  train MSE: 0.9408, val MSE: 0.9067
Gen 960/2000  train MSE: 0.9408, val MSE: 0.9067
Gen 961/2000  train MSE: 0.9408, val MSE: 0.9067
Gen 962/2000  train MSE: 0.9408, val MSE: 0.9067
Gen 963/2000  train MSE: 0.9406, val MSE: 0.9063
Gen 964/2000  train MSE: 0.9406, val MSE: 0.9063
Gen 965/2000  train MSE: 0.9406, val MSE: 0.9063
Gen 966/2000  train MSE: 0.9406, val MSE: 0.9063
Gen 967/2000  train MSE: 0.9406, val MSE: 0.9063
Gen 968/2000  train MSE: 0.9406, val MSE: 0.9063
Gen 969/2000  train MSE: 0.9405, val MSE: 0.9069
Gen 970/2000  train MSE: 0.9405, val MSE: 0.9069
Gen 971/2000  train MSE: 0.9405, val MSE: 0.9061
Gen 972/2000  train MSE: 0.9405, val MSE: 0.9061
Gen 973/2000  train MSE: 0.9405, val MSE: 0.9061
Gen 974/2000  train MSE: 0.9405, val MSE: 0.9061
Gen 975/2000  train MSE: 0.9404, val MSE: 0.9061
Gen 976/2000  train MSE: 0.9404, val MSE: 0.9061
Gen 977/2000  train MSE: 0.9403, val MSE: 0.9070
Gen 978/2000  train MSE: 0.9402, val MSE: 0.9061
Gen 979/2000  train MSE: 0.9401, val MSE: 0.9052
Gen 980/2000  train MSE: 0.9401, val MSE: 0.9052
Gen 981/2000  train MSE: 0.9401, val MSE: 0.9052
Gen 982/2000  train MSE: 0.9401, val MSE: 0.9052
Gen 983/2000  train MSE: 0.9401, val MSE: 0.9052
Gen 984/2000  train MSE: 0.9401, val MSE: 0.9052
Gen 985/2000  train MSE: 0.9399, val MSE: 0.9058
Gen 986/2000  train MSE: 0.9399, val MSE: 0.9058
Gen 987/2000  train MSE: 0.9399, val MSE: 0.9058
Gen 988/2000  train MSE: 0.9399, val MSE: 0.9058
Gen 989/2000  train MSE: 0.9399, val MSE: 0.9058
Gen 990/2000  train MSE: 0.9399, val MSE: 0.9058
Gen 991/2000  train MSE: 0.9399, val MSE: 0.9063
Gen 992/2000  train MSE: 0.9399, val MSE: 0.9050
Gen 993/2000  train MSE: 0.9399, val MSE: 0.9050
Gen 994/2000  train MSE: 0.9399, val MSE: 0.9050
Gen 995/2000  train MSE: 0.9398, val MSE: 0.9070
Gen 996/2000  train MSE: 0.9398, val MSE: 0.9070
Gen 997/2000  train MSE: 0.9398, val MSE: 0.9070
Gen 998/2000  train MSE: 0.9398, val MSE: 0.9070
Gen 999/2000  train MSE: 0.9398, val MSE: 0.9059
Gen 1000/2000  train MSE: 0.9398, val MSE: 0.9059
Gen 1000/2000  train MSE: 0.9396, val MSE: 0.9058
Gen 1001/2000  train MSE: 0.9396, val MSE: 0.9058
Gen 1002/2000  train MSE: 0.9396, val MSE: 0.9058
Gen 1003/2000  train MSE: 0.9396, val MSE: 0.9058
Gen 1004/2000  train MSE: 0.9396, val MSE: 0.9058
Gen 1005/2000  train MSE: 0.9396, val MSE: 0.9058
Gen 1006/2000  train MSE: 0.9396, val MSE: 0.9058
Gen 1007/2000  train MSE: 0.9395, val MSE: 0.9055
Gen 1008/2000  train MSE: 0.9395, val MSE: 0.9055
Gen 1009/2000  train MSE: 0.9395, val MSE: 0.9055
Gen 1010/2000  train MSE: 0.9395, val MSE: 0.9055
Gen 1011/2000  train MSE: 0.9395, val MSE: 0.9055
Gen 1012/2000  train MSE: 0.9395, val MSE: 0.9055
Gen 1013/2000  train MSE: 0.9395, val MSE: 0.9055
Gen 1014/2000  train MSE: 0.9394, val MSE: 0.9048
Gen 1015/2000  train MSE: 0.9394, val MSE: 0.9048
Gen 1016/2000  train MSE: 0.9394, val MSE: 0.9048
Gen 1017/2000  train MSE: 0.9394, val MSE: 0.9052
Gen 1018/2000  train MSE: 0.9394, val MSE: 0.9052
Gen 1019/2000  train MSE: 0.9393, val MSE: 0.9042
Gen 1020/2000  train MSE: 0.9393, val MSE: 0.9041
Gen 1021/2000  train MSE: 0.9393, val MSE: 0.9041
Gen 1022/2000  train MSE: 0.9393, val MSE: 0.9041
Gen 1023/2000  train MSE: 0.9393, val MSE: 0.9041
Gen 1024/2000  train MSE: 0.9393, val MSE: 0.9041
Gen 1025/2000  train MSE: 0.9393, val MSE: 0.9041
Gen 1026/2000  train MSE: 0.9390, val MSE: 0.9059
Gen 1027/2000  train MSE: 0.9390, val MSE: 0.9059
Gen 1028/2000  train MSE: 0.9390, val MSE: 0.9059
Gen 1029/2000  train MSE: 0.9390, val MSE: 0.9047
Gen 1030/2000  train MSE: 0.9390, val MSE: 0.9047
Gen 1031/2000  train MSE: 0.9390, val MSE: 0.9047
Gen 1032/2000  train MSE: 0.9390, val MSE: 0.9056
Gen 1033/2000  train MSE: 0.9390, val MSE: 0.9056
Gen 1034/2000  train MSE: 0.9390, val MSE: 0.9056
Gen 1035/2000  train MSE: 0.9390, val MSE: 0.9057
Gen 1036/2000  train MSE: 0.9389, val MSE: 0.9057
Gen 1037/2000  train MSE: 0.9389, val MSE: 0.9057
Gen 1038/2000  train MSE: 0.9388, val MSE: 0.9052
Gen 1039/2000  train MSE: 0.9388, val MSE: 0.9053
Gen 1040/2000  train MSE: 0.9388, val MSE: 0.9053
Gen 1041/2000  train MSE: 0.9388, val MSE: 0.9053
Gen 1042/2000  train MSE: 0.9386, val MSE: 0.9042
Gen 1043/2000  train MSE: 0.9386, val MSE: 0.9042
Gen 1044/2000  train MSE: 0.9386, val MSE: 0.9049
Gen 1045/2000  train MSE: 0.9384, val MSE: 0.9050
Gen 1046/2000  train MSE: 0.9383, val MSE: 0.9041
Gen 1047/2000  train MSE: 0.9383, val MSE: 0.9041
Gen 1048/2000  train MSE: 0.9383, val MSE: 0.9041
Gen 1049/2000  train MSE: 0.9383, val MSE: 0.9041
Gen 1050/2000  train MSE: 0.9383, val MSE: 0.9041
Gen 1051/2000  train MSE: 0.9383, val MSE: 0.9046
Gen 1052/2000  train MSE: 0.9383, val MSE: 0.9046
Gen 1053/2000  train MSE: 0.9383, val MSE: 0.9046
Gen 1054/2000  train MSE: 0.9382, val MSE: 0.9042
Gen 1055/2000  train MSE: 0.9381, val MSE: 0.9042
Gen 1056/2000  train MSE: 0.9381, val MSE: 0.9042
Gen 1057/2000  train MSE: 0.9381, val MSE: 0.9042
Gen 1058/2000  train MSE: 0.9381, val MSE: 0.9042
Gen 1059/2000  train MSE: 0.9381, val MSE: 0.9042
Gen 1060/2000  train MSE: 0.9381, val MSE: 0.9042
Gen 1061/2000  train MSE: 0.9381, val MSE: 0.9042
Gen 1062/2000  train MSE: 0.9381, val MSE: 0.9042
Gen 1063/2000  train MSE: 0.9381, val MSE: 0.9042
Gen 1064/2000  train MSE: 0.9380, val MSE: 0.9033
Gen 1065/2000  train MSE: 0.9380, val MSE: 0.9033
Gen 1066/2000  train MSE: 0.9380, val MSE: 0.9033
Gen 1067/2000  train MSE: 0.9379, val MSE: 0.9034
Gen 1068/2000  train MSE: 0.9379, val MSE: 0.9034
Gen 1069/2000  train MSE: 0.9379, val MSE: 0.9034
Gen 1070/2000  train MSE: 0.9378, val MSE: 0.9027
Gen 1071/2000  train MSE: 0.9378, val MSE: 0.9027
Gen 1072/2000  train MSE: 0.9378, val MSE: 0.9027
Gen 1073/2000  train MSE: 0.9378, val MSE: 0.9032
Gen 1074/2000  train MSE: 0.9375, val MSE: 0.9025
Gen 1075/2000  train MSE: 0.9375, val MSE: 0.9025
Gen 1076/2000  train MSE: 0.9375, val MSE: 0.9025
Gen 1077/2000  train MSE: 0.9375, val MSE: 0.9025
Gen 1078/2000  train MSE: 0.9375, val MSE: 0.9025
Gen 1079/2000  train MSE: 0.9375, val MSE: 0.9025
Gen 1080/2000  train MSE: 0.9375, val MSE: 0.9025
Gen 1081/2000  train MSE: 0.9374, val MSE: 0.9021
Gen 1082/2000  train MSE: 0.9374, val MSE: 0.9021
Gen 1083/2000  train MSE: 0.9374, val MSE: 0.9021
Gen 1084/2000  train MSE: 0.9374, val MSE: 0.9024
Gen 1085/2000  train MSE: 0.9374, val MSE: 0.9024
Gen 1086/2000  train MSE: 0.9374, val MSE: 0.9024
Gen 1087/2000  train MSE: 0.9374, val MSE: 0.9024
Gen 1088/2000  train MSE: 0.9374, val MSE: 0.9024
Gen 1089/2000  train MSE: 0.9374, val MSE: 0.9024
Gen 1090/2000  train MSE: 0.9372, val MSE: 0.9015
Gen 1091/2000  train MSE: 0.9372, val MSE: 0.9015
Gen 1092/2000  train MSE: 0.9372, val MSE: 0.9015
Gen 1093/2000  train MSE: 0.9372, val MSE: 0.9017
Gen 1094/2000  train MSE: 0.9372, val MSE: 0.9018
Gen 1095/2000  train MSE: 0.9371, val MSE: 0.9018
Gen 1096/2000  train MSE: 0.9371, val MSE: 0.9023
Gen 1097/2000  train MSE: 0.9371, val MSE: 0.9023
Gen 1098/2000  train MSE: 0.9371, val MSE: 0.9023
Gen 1099/2000  train MSE: 0.9370, val MSE: 0.9020
Gen 1100/2000  train MSE: 0.9370, val MSE: 0.9020
Gen 1100/2000  train MSE: 0.9370, val MSE: 0.9025
Gen 1101/2000  train MSE: 0.9370, val MSE: 0.9025
Gen 1102/2000  train MSE: 0.9369, val MSE: 0.9010
Gen 1103/2000  train MSE: 0.9369, val MSE: 0.9010
Gen 1104/2000  train MSE: 0.9368, val MSE: 0.9018
Gen 1105/2000  train MSE: 0.9368, val MSE: 0.9018
Gen 1106/2000  train MSE: 0.9368, val MSE: 0.9018
Gen 1107/2000  train MSE: 0.9368, val MSE: 0.9015
Gen 1108/2000  train MSE: 0.9368, val MSE: 0.9015
Gen 1109/2000  train MSE: 0.9368, val MSE: 0.9015
Gen 1110/2000  train MSE: 0.9367, val MSE: 0.9008
Gen 1111/2000  train MSE: 0.9367, val MSE: 0.9008
Gen 1112/2000  train MSE: 0.9367, val MSE: 0.9008
Gen 1113/2000  train MSE: 0.9366, val MSE: 0.9008
Gen 1114/2000  train MSE: 0.9366, val MSE: 0.9008
Gen 1115/2000  train MSE: 0.9366, val MSE: 0.9007
Gen 1116/2000  train MSE: 0.9366, val MSE: 0.9007
Gen 1117/2000  train MSE: 0.9365, val MSE: 0.9011
Gen 1118/2000  train MSE: 0.9365, val MSE: 0.9011
Gen 1119/2000  train MSE: 0.9365, val MSE: 0.9011
Gen 1120/2000  train MSE: 0.9365, val MSE: 0.8998
Gen 1121/2000  train MSE: 0.9364, val MSE: 0.9007
Gen 1122/2000  train MSE: 0.9363, val MSE: 0.9012
Gen 1123/2000  train MSE: 0.9363, val MSE: 0.9012
Gen 1124/2000  train MSE: 0.9363, val MSE: 0.9012
Gen 1125/2000  train MSE: 0.9363, val MSE: 0.9012
Gen 1126/2000  train MSE: 0.9363, val MSE: 0.9012
Gen 1127/2000  train MSE: 0.9363, val MSE: 0.9004
Gen 1128/2000  train MSE: 0.9362, val MSE: 0.9004
Gen 1129/2000  train MSE: 0.9362, val MSE: 0.9006
Gen 1130/2000  train MSE: 0.9362, val MSE: 0.9006
Gen 1131/2000  train MSE: 0.9360, val MSE: 0.9011
Gen 1132/2000  train MSE: 0.9360, val MSE: 0.9002
Gen 1133/2000  train MSE: 0.9359, val MSE: 0.9013
Gen 1134/2000  train MSE: 0.9359, val MSE: 0.9013
Gen 1135/2000  train MSE: 0.9359, val MSE: 0.9013
Gen 1136/2000  train MSE: 0.9359, val MSE: 0.9013
Gen 1137/2000  train MSE: 0.9359, val MSE: 0.9013
Gen 1138/2000  train MSE: 0.9359, val MSE: 0.9013
Gen 1139/2000  train MSE: 0.9359, val MSE: 0.8999
Gen 1140/2000  train MSE: 0.9358, val MSE: 0.9010
Gen 1141/2000  train MSE: 0.9358, val MSE: 0.9010
Gen 1142/2000  train MSE: 0.9357, val MSE: 0.8998
Gen 1143/2000  train MSE: 0.9357, val MSE: 0.8998
Gen 1144/2000  train MSE: 0.9357, val MSE: 0.8998
Gen 1145/2000  train MSE: 0.9357, val MSE: 0.8998
Gen 1146/2000  train MSE: 0.9357, val MSE: 0.8998
Gen 1147/2000  train MSE: 0.9356, val MSE: 0.8985
Gen 1148/2000  train MSE: 0.9354, val MSE: 0.8997
Gen 1149/2000  train MSE: 0.9354, val MSE: 0.8997
Gen 1150/2000  train MSE: 0.9354, val MSE: 0.8997
Gen 1151/2000  train MSE: 0.9354, val MSE: 0.8997
Gen 1152/2000  train MSE: 0.9354, val MSE: 0.8997
Gen 1153/2000  train MSE: 0.9354, val MSE: 0.8997
Gen 1154/2000  train MSE: 0.9354, val MSE: 0.8997
Gen 1155/2000  train MSE: 0.9354, val MSE: 0.8997
Gen 1156/2000  train MSE: 0.9354, val MSE: 0.8997
Gen 1157/2000  train MSE: 0.9354, val MSE: 0.9001
Gen 1158/2000  train MSE: 0.9354, val MSE: 0.9001
Gen 1159/2000  train MSE: 0.9352, val MSE: 0.8989
Gen 1160/2000  train MSE: 0.9352, val MSE: 0.8989
Gen 1161/2000  train MSE: 0.9352, val MSE: 0.8989
Gen 1162/2000  train MSE: 0.9352, val MSE: 0.8989
Gen 1163/2000  train MSE: 0.9348, val MSE: 0.8983
Gen 1164/2000  train MSE: 0.9348, val MSE: 0.8983
Gen 1165/2000  train MSE: 0.9348, val MSE: 0.8983
Gen 1166/2000  train MSE: 0.9348, val MSE: 0.8983
Gen 1167/2000  train MSE: 0.9348, val MSE: 0.8983
Gen 1168/2000  train MSE: 0.9348, val MSE: 0.8983
Gen 1169/2000  train MSE: 0.9348, val MSE: 0.8983
Gen 1170/2000  train MSE: 0.9347, val MSE: 0.8992
Gen 1171/2000  train MSE: 0.9347, val MSE: 0.8992
Gen 1172/2000  train MSE: 0.9347, val MSE: 0.8992
Gen 1173/2000  train MSE: 0.9345, val MSE: 0.8979
Gen 1174/2000  train MSE: 0.9345, val MSE: 0.8979
Gen 1175/2000  train MSE: 0.9345, val MSE: 0.8979
Gen 1176/2000  train MSE: 0.9345, val MSE: 0.8979
Gen 1177/2000  train MSE: 0.9343, val MSE: 0.8988
Gen 1178/2000  train MSE: 0.9343, val MSE: 0.8988
Gen 1179/2000  train MSE: 0.9343, val MSE: 0.8988
Gen 1180/2000  train MSE: 0.9343, val MSE: 0.8988
Gen 1181/2000  train MSE: 0.9341, val MSE: 0.8978
Gen 1182/2000  train MSE: 0.9341, val MSE: 0.8978
Gen 1183/2000  train MSE: 0.9341, val MSE: 0.8978
Gen 1184/2000  train MSE: 0.9341, val MSE: 0.8978
Gen 1185/2000  train MSE: 0.9341, val MSE: 0.8978
Gen 1186/2000  train MSE: 0.9341, val MSE: 0.8978
Gen 1187/2000  train MSE: 0.9341, val MSE: 0.8978
Gen 1188/2000  train MSE: 0.9341, val MSE: 0.8980
Gen 1189/2000  train MSE: 0.9341, val MSE: 0.8980
Gen 1190/2000  train MSE: 0.9341, val MSE: 0.8980
Gen 1191/2000  train MSE: 0.9341, val MSE: 0.8980
Gen 1192/2000  train MSE: 0.9340, val MSE: 0.8963
Gen 1193/2000  train MSE: 0.9340, val MSE: 0.8980
Gen 1194/2000  train MSE: 0.9340, val MSE: 0.8980
Gen 1195/2000  train MSE: 0.9338, val MSE: 0.8981
Gen 1196/2000  train MSE: 0.9338, val MSE: 0.8981
Gen 1197/2000  train MSE: 0.9338, val MSE: 0.8981
Gen 1198/2000  train MSE: 0.9338, val MSE: 0.8981
Gen 1199/2000  train MSE: 0.9338, val MSE: 0.8981
Gen 1200/2000  train MSE: 0.9338, val MSE: 0.8981
Gen 1200/2000  train MSE: 0.9337, val MSE: 0.8975
Gen 1201/2000  train MSE: 0.9335, val MSE: 0.8974
Gen 1202/2000  train MSE: 0.9335, val MSE: 0.8974
Gen 1203/2000  train MSE: 0.9335, val MSE: 0.8974
Gen 1204/2000  train MSE: 0.9335, val MSE: 0.8974
Gen 1205/2000  train MSE: 0.9335, val MSE: 0.8974
Gen 1206/2000  train MSE: 0.9335, val MSE: 0.8970
Gen 1207/2000  train MSE: 0.9335, val MSE: 0.8970
Gen 1208/2000  train MSE: 0.9335, val MSE: 0.8970
Gen 1209/2000  train MSE: 0.9335, val MSE: 0.8972
Gen 1210/2000  train MSE: 0.9332, val MSE: 0.8965
Gen 1211/2000  train MSE: 0.9332, val MSE: 0.8965
Gen 1212/2000  train MSE: 0.9332, val MSE: 0.8965
Gen 1213/2000  train MSE: 0.9331, val MSE: 0.8965
Gen 1214/2000  train MSE: 0.9331, val MSE: 0.8965
Gen 1215/2000  train MSE: 0.9330, val MSE: 0.8958
Gen 1216/2000  train MSE: 0.9330, val MSE: 0.8958
Gen 1217/2000  train MSE: 0.9329, val MSE: 0.8963
Gen 1218/2000  train MSE: 0.9328, val MSE: 0.8963
Gen 1219/2000  train MSE: 0.9328, val MSE: 0.8963
Gen 1220/2000  train MSE: 0.9328, val MSE: 0.8963
Gen 1221/2000  train MSE: 0.9325, val MSE: 0.8949
Gen 1222/2000  train MSE: 0.9325, val MSE: 0.8949
Gen 1223/2000  train MSE: 0.9324, val MSE: 0.8955
Gen 1224/2000  train MSE: 0.9324, val MSE: 0.8952
Gen 1225/2000  train MSE: 0.9323, val MSE: 0.8953
Gen 1226/2000  train MSE: 0.9323, val MSE: 0.8953
Gen 1227/2000  train MSE: 0.9323, val MSE: 0.8953
Gen 1228/2000  train MSE: 0.9322, val MSE: 0.8945
Gen 1229/2000  train MSE: 0.9322, val MSE: 0.8945
Gen 1230/2000  train MSE: 0.9322, val MSE: 0.8946
Gen 1231/2000  train MSE: 0.9322, val MSE: 0.8946
Gen 1232/2000  train MSE: 0.9322, val MSE: 0.8946
Gen 1233/2000  train MSE: 0.9320, val MSE: 0.8934
Gen 1234/2000  train MSE: 0.9320, val MSE: 0.8934
Gen 1235/2000  train MSE: 0.9320, val MSE: 0.8934
Gen 1236/2000  train MSE: 0.9320, val MSE: 0.8934
Gen 1237/2000  train MSE: 0.9320, val MSE: 0.8934
Gen 1238/2000  train MSE: 0.9319, val MSE: 0.8938
Gen 1239/2000  train MSE: 0.9317, val MSE: 0.8933
Gen 1240/2000  train MSE: 0.9317, val MSE: 0.8933
Gen 1241/2000  train MSE: 0.9317, val MSE: 0.8933
Gen 1242/2000  train MSE: 0.9317, val MSE: 0.8933
Gen 1243/2000  train MSE: 0.9317, val MSE: 0.8933
Gen 1244/2000  train MSE: 0.9317, val MSE: 0.8933
Gen 1245/2000  train MSE: 0.9317, val MSE: 0.8933
Gen 1246/2000  train MSE: 0.9317, val MSE: 0.8933
Gen 1247/2000  train MSE: 0.9316, val MSE: 0.8932
Gen 1248/2000  train MSE: 0.9316, val MSE: 0.8932
Gen 1249/2000  train MSE: 0.9315, val MSE: 0.8934
Gen 1250/2000  train MSE: 0.9315, val MSE: 0.8934
Gen 1251/2000  train MSE: 0.9315, val MSE: 0.8934
Gen 1252/2000  train MSE: 0.9315, val MSE: 0.8934
Gen 1253/2000  train MSE: 0.9315, val MSE: 0.8934
Gen 1254/2000  train MSE: 0.9314, val MSE: 0.8930
Gen 1255/2000  train MSE: 0.9314, val MSE: 0.8930
Gen 1256/2000  train MSE: 0.9314, val MSE: 0.8930
Gen 1257/2000  train MSE: 0.9314, val MSE: 0.8930
Gen 1258/2000  train MSE: 0.9314, val MSE: 0.8930
Gen 1259/2000  train MSE: 0.9314, val MSE: 0.8927
Gen 1260/2000  train MSE: 0.9314, val MSE: 0.8929
Gen 1261/2000  train MSE: 0.9314, val MSE: 0.8929
Gen 1262/2000  train MSE: 0.9313, val MSE: 0.8922
Gen 1263/2000  train MSE: 0.9313, val MSE: 0.8922
Gen 1264/2000  train MSE: 0.9312, val MSE: 0.8926
Gen 1265/2000  train MSE: 0.9312, val MSE: 0.8926
Gen 1266/2000  train MSE: 0.9311, val MSE: 0.8928
Gen 1267/2000  train MSE: 0.9311, val MSE: 0.8928
Gen 1268/2000  train MSE: 0.9311, val MSE: 0.8928
Gen 1269/2000  train MSE: 0.9311, val MSE: 0.8928
Gen 1270/2000  train MSE: 0.9309, val MSE: 0.8927
Gen 1271/2000  train MSE: 0.9309, val MSE: 0.8918
Gen 1272/2000  train MSE: 0.9309, val MSE: 0.8918
Gen 1273/2000  train MSE: 0.9308, val MSE: 0.8919
Gen 1274/2000  train MSE: 0.9308, val MSE: 0.8919
Gen 1275/2000  train MSE: 0.9308, val MSE: 0.8915
Gen 1276/2000  train MSE: 0.9308, val MSE: 0.8915
Gen 1277/2000  train MSE: 0.9308, val MSE: 0.8915
Gen 1278/2000  train MSE: 0.9308, val MSE: 0.8915
Gen 1279/2000  train MSE: 0.9307, val MSE: 0.8922
Gen 1280/2000  train MSE: 0.9307, val MSE: 0.8908
Gen 1281/2000  train MSE: 0.9306, val MSE: 0.8913
Gen 1282/2000  train MSE: 0.9306, val MSE: 0.8913
Gen 1283/2000  train MSE: 0.9306, val MSE: 0.8913
Gen 1284/2000  train MSE: 0.9306, val MSE: 0.8910
Gen 1285/2000  train MSE: 0.9306, val MSE: 0.8910
Gen 1286/2000  train MSE: 0.9305, val MSE: 0.8920
Gen 1287/2000  train MSE: 0.9305, val MSE: 0.8920
Gen 1288/2000  train MSE: 0.9305, val MSE: 0.8920
Gen 1289/2000  train MSE: 0.9304, val MSE: 0.8915
Gen 1290/2000  train MSE: 0.9304, val MSE: 0.8915
Gen 1291/2000  train MSE: 0.9304, val MSE: 0.8915
Gen 1292/2000  train MSE: 0.9303, val MSE: 0.8904
Gen 1293/2000  train MSE: 0.9303, val MSE: 0.8904
Gen 1294/2000  train MSE: 0.9303, val MSE: 0.8904
Gen 1295/2000  train MSE: 0.9302, val MSE: 0.8904
Gen 1296/2000  train MSE: 0.9302, val MSE: 0.8904
Gen 1297/2000  train MSE: 0.9301, val MSE: 0.8902
Gen 1298/2000  train MSE: 0.9301, val MSE: 0.8902
Gen 1299/2000  train MSE: 0.9301, val MSE: 0.8902
Gen 1300/2000  train MSE: 0.9301, val MSE: 0.8902
Gen 1300/2000  train MSE: 0.9301, val MSE: 0.8902
Gen 1301/2000  train MSE: 0.9300, val MSE: 0.8909
Gen 1302/2000  train MSE: 0.9300, val MSE: 0.8909
Gen 1303/2000  train MSE: 0.9300, val MSE: 0.8909
Gen 1304/2000  train MSE: 0.9300, val MSE: 0.8909
Gen 1305/2000  train MSE: 0.9300, val MSE: 0.8909
Gen 1306/2000  train MSE: 0.9300, val MSE: 0.8909
Gen 1307/2000  train MSE: 0.9300, val MSE: 0.8903
Gen 1308/2000  train MSE: 0.9300, val MSE: 0.8903
Gen 1309/2000  train MSE: 0.9300, val MSE: 0.8903
Gen 1310/2000  train MSE: 0.9300, val MSE: 0.8903
Gen 1311/2000  train MSE: 0.9300, val MSE: 0.8907
Gen 1312/2000  train MSE: 0.9300, val MSE: 0.8907
Gen 1313/2000  train MSE: 0.9300, val MSE: 0.8907
Gen 1314/2000  train MSE: 0.9299, val MSE: 0.8899
Gen 1315/2000  train MSE: 0.9298, val MSE: 0.8901
Gen 1316/2000  train MSE: 0.9298, val MSE: 0.8901
Gen 1317/2000  train MSE: 0.9298, val MSE: 0.8901
Gen 1318/2000  train MSE: 0.9298, val MSE: 0.8911
Gen 1319/2000  train MSE: 0.9298, val MSE: 0.8911
Gen 1320/2000  train MSE: 0.9298, val MSE: 0.8911
Gen 1321/2000  train MSE: 0.9298, val MSE: 0.8911
Gen 1322/2000  train MSE: 0.9298, val MSE: 0.8895
Gen 1323/2000  train MSE: 0.9297, val MSE: 0.8891
Gen 1324/2000  train MSE: 0.9297, val MSE: 0.8891
Gen 1325/2000  train MSE: 0.9297, val MSE: 0.8891
Gen 1326/2000  train MSE: 0.9297, val MSE: 0.8891
Gen 1327/2000  train MSE: 0.9297, val MSE: 0.8891
Gen 1328/2000  train MSE: 0.9297, val MSE: 0.8891
Gen 1329/2000  train MSE: 0.9296, val MSE: 0.8906
Gen 1330/2000  train MSE: 0.9296, val MSE: 0.8900
Gen 1331/2000  train MSE: 0.9294, val MSE: 0.8891
Gen 1332/2000  train MSE: 0.9294, val MSE: 0.8891
Gen 1333/2000  train MSE: 0.9294, val MSE: 0.8891
Gen 1334/2000  train MSE: 0.9294, val MSE: 0.8891
Gen 1335/2000  train MSE: 0.9294, val MSE: 0.8891
Gen 1336/2000  train MSE: 0.9294, val MSE: 0.8891
Gen 1337/2000  train MSE: 0.9294, val MSE: 0.8891
Gen 1338/2000  train MSE: 0.9293, val MSE: 0.8889
Gen 1339/2000  train MSE: 0.9293, val MSE: 0.8889
Gen 1340/2000  train MSE: 0.9293, val MSE: 0.8889
Gen 1341/2000  train MSE: 0.9293, val MSE: 0.8889
Gen 1342/2000  train MSE: 0.9293, val MSE: 0.8889
Gen 1343/2000  train MSE: 0.9293, val MSE: 0.8889
Gen 1344/2000  train MSE: 0.9293, val MSE: 0.8899
Gen 1345/2000  train MSE: 0.9293, val MSE: 0.8899
Gen 1346/2000  train MSE: 0.9293, val MSE: 0.8899
Gen 1347/2000  train MSE: 0.9293, val MSE: 0.8899
Gen 1348/2000  train MSE: 0.9293, val MSE: 0.8899
Gen 1349/2000  train MSE: 0.9291, val MSE: 0.8892
Gen 1350/2000  train MSE: 0.9291, val MSE: 0.8892
Gen 1351/2000  train MSE: 0.9291, val MSE: 0.8892
Gen 1352/2000  train MSE: 0.9291, val MSE: 0.8892
Gen 1353/2000  train MSE: 0.9291, val MSE: 0.8897
Gen 1354/2000  train MSE: 0.9291, val MSE: 0.8897
Gen 1355/2000  train MSE: 0.9291, val MSE: 0.8897
Gen 1356/2000  train MSE: 0.9291, val MSE: 0.8897
Gen 1357/2000  train MSE: 0.9291, val MSE: 0.8897
Gen 1358/2000  train MSE: 0.9290, val MSE: 0.8884
Gen 1359/2000  train MSE: 0.9290, val MSE: 0.8884
Gen 1360/2000  train MSE: 0.9290, val MSE: 0.8874
Gen 1361/2000  train MSE: 0.9290, val MSE: 0.8874
Gen 1362/2000  train MSE: 0.9290, val MSE: 0.8874
Gen 1363/2000  train MSE: 0.9290, val MSE: 0.8874
Gen 1364/2000  train MSE: 0.9288, val MSE: 0.8890
Gen 1365/2000  train MSE: 0.9287, val MSE: 0.8891
Gen 1366/2000  train MSE: 0.9287, val MSE: 0.8891
Gen 1367/2000  train MSE: 0.9287, val MSE: 0.8891
Gen 1368/2000  train MSE: 0.9287, val MSE: 0.8891
Gen 1369/2000  train MSE: 0.9287, val MSE: 0.8884
Gen 1370/2000  train MSE: 0.9286, val MSE: 0.8882
Gen 1371/2000  train MSE: 0.9286, val MSE: 0.8882
Gen 1372/2000  train MSE: 0.9286, val MSE: 0.8885
Gen 1373/2000  train MSE: 0.9284, val MSE: 0.8880
Gen 1374/2000  train MSE: 0.9284, val MSE: 0.8880
Gen 1375/2000  train MSE: 0.9284, val MSE: 0.8880
Gen 1376/2000  train MSE: 0.9284, val MSE: 0.8880
Gen 1377/2000  train MSE: 0.9284, val MSE: 0.8880
Gen 1378/2000  train MSE: 0.9284, val MSE: 0.8880
Gen 1379/2000  train MSE: 0.9284, val MSE: 0.8880
Gen 1380/2000  train MSE: 0.9283, val MSE: 0.8879
Gen 1381/2000  train MSE: 0.9283, val MSE: 0.8879
Gen 1382/2000  train MSE: 0.9283, val MSE: 0.8879
Gen 1383/2000  train MSE: 0.9283, val MSE: 0.8879
Gen 1384/2000  train MSE: 0.9283, val MSE: 0.8879
Gen 1385/2000  train MSE: 0.9283, val MSE: 0.8879
Gen 1386/2000  train MSE: 0.9282, val MSE: 0.8884
Gen 1387/2000  train MSE: 0.9280, val MSE: 0.8878
Gen 1388/2000  train MSE: 0.9280, val MSE: 0.8878
Gen 1389/2000  train MSE: 0.9280, val MSE: 0.8878
Gen 1390/2000  train MSE: 0.9280, val MSE: 0.8878
Gen 1391/2000  train MSE: 0.9280, val MSE: 0.8878
Gen 1392/2000  train MSE: 0.9277, val MSE: 0.8870
Gen 1393/2000  train MSE: 0.9277, val MSE: 0.8870
Gen 1394/2000  train MSE: 0.9277, val MSE: 0.8870
Gen 1395/2000  train MSE: 0.9277, val MSE: 0.8870
Gen 1396/2000  train MSE: 0.9277, val MSE: 0.8870
Gen 1397/2000  train MSE: 0.9277, val MSE: 0.8870
Gen 1398/2000  train MSE: 0.9277, val MSE: 0.8870
Gen 1399/2000  train MSE: 0.9277, val MSE: 0.8870
Gen 1400/2000  train MSE: 0.9277, val MSE: 0.8870
Gen 1400/2000  train MSE: 0.9276, val MSE: 0.8872
Gen 1401/2000  train MSE: 0.9275, val MSE: 0.8862
Gen 1402/2000  train MSE: 0.9275, val MSE: 0.8862
Gen 1403/2000  train MSE: 0.9275, val MSE: 0.8862
Gen 1404/2000  train MSE: 0.9275, val MSE: 0.8862
Gen 1405/2000  train MSE: 0.9275, val MSE: 0.8862
Gen 1406/2000  train MSE: 0.9275, val MSE: 0.8862
Gen 1407/2000  train MSE: 0.9275, val MSE: 0.8862
Gen 1408/2000  train MSE: 0.9275, val MSE: 0.8862
Gen 1409/2000  train MSE: 0.9275, val MSE: 0.8863
Gen 1410/2000  train MSE: 0.9275, val MSE: 0.8866
Gen 1411/2000  train MSE: 0.9274, val MSE: 0.8858
Gen 1412/2000  train MSE: 0.9274, val MSE: 0.8858
Gen 1413/2000  train MSE: 0.9274, val MSE: 0.8869
Gen 1414/2000  train MSE: 0.9274, val MSE: 0.8869
Gen 1415/2000  train MSE: 0.9274, val MSE: 0.8869
Gen 1416/2000  train MSE: 0.9273, val MSE: 0.8862
Gen 1417/2000  train MSE: 0.9273, val MSE: 0.8862
Gen 1418/2000  train MSE: 0.9273, val MSE: 0.8862
Gen 1419/2000  train MSE: 0.9273, val MSE: 0.8863
Gen 1420/2000  train MSE: 0.9273, val MSE: 0.8862
Gen 1421/2000  train MSE: 0.9273, val MSE: 0.8862
Gen 1422/2000  train MSE: 0.9273, val MSE: 0.8862
Gen 1423/2000  train MSE: 0.9273, val MSE: 0.8862
Gen 1424/2000  train MSE: 0.9273, val MSE: 0.8862
Gen 1425/2000  train MSE: 0.9273, val MSE: 0.8862
Gen 1426/2000  train MSE: 0.9272, val MSE: 0.8858
Gen 1427/2000  train MSE: 0.9272, val MSE: 0.8858
Gen 1428/2000  train MSE: 0.9272, val MSE: 0.8858
Gen 1429/2000  train MSE: 0.9272, val MSE: 0.8858
Gen 1430/2000  train MSE: 0.9272, val MSE: 0.8858
Gen 1431/2000  train MSE: 0.9270, val MSE: 0.8857
Gen 1432/2000  train MSE: 0.9270, val MSE: 0.8857
Gen 1433/2000  train MSE: 0.9270, val MSE: 0.8857
Gen 1434/2000  train MSE: 0.9270, val MSE: 0.8857
Gen 1435/2000  train MSE: 0.9270, val MSE: 0.8857
Gen 1436/2000  train MSE: 0.9270, val MSE: 0.8857
Gen 1437/2000  train MSE: 0.9269, val MSE: 0.8859
Gen 1438/2000  train MSE: 0.9269, val MSE: 0.8859
Gen 1439/2000  train MSE: 0.9269, val MSE: 0.8859
Gen 1440/2000  train MSE: 0.9269, val MSE: 0.8859
Gen 1441/2000  train MSE: 0.9269, val MSE: 0.8859
Gen 1442/2000  train MSE: 0.9269, val MSE: 0.8859
Gen 1443/2000  train MSE: 0.9269, val MSE: 0.8859
Gen 1444/2000  train MSE: 0.9269, val MSE: 0.8859
Gen 1445/2000  train MSE: 0.9269, val MSE: 0.8861
Gen 1446/2000  train MSE: 0.9268, val MSE: 0.8851
Gen 1447/2000  train MSE: 0.9268, val MSE: 0.8851
Gen 1448/2000  train MSE: 0.9268, val MSE: 0.8851
Gen 1449/2000  train MSE: 0.9268, val MSE: 0.8851
Gen 1450/2000  train MSE: 0.9268, val MSE: 0.8851
Gen 1451/2000  train MSE: 0.9268, val MSE: 0.8851
Gen 1452/2000  train MSE: 0.9267, val MSE: 0.8859
Gen 1453/2000  train MSE: 0.9267, val MSE: 0.8859
Gen 1454/2000  train MSE: 0.9267, val MSE: 0.8854
Gen 1455/2000  train MSE: 0.9267, val MSE: 0.8854
Gen 1456/2000  train MSE: 0.9267, val MSE: 0.8854
Gen 1457/2000  train MSE: 0.9267, val MSE: 0.8854
Gen 1458/2000  train MSE: 0.9267, val MSE: 0.8854
Gen 1459/2000  train MSE: 0.9267, val MSE: 0.8854
Gen 1460/2000  train MSE: 0.9267, val MSE: 0.8854
Gen 1461/2000  train MSE: 0.9266, val MSE: 0.8857
Gen 1462/2000  train MSE: 0.9266, val MSE: 0.8857
Gen 1463/2000  train MSE: 0.9266, val MSE: 0.8851
Gen 1464/2000  train MSE: 0.9266, val MSE: 0.8851
Gen 1465/2000  train MSE: 0.9265, val MSE: 0.8845
Gen 1466/2000  train MSE: 0.9265, val MSE: 0.8845
Gen 1467/2000  train MSE: 0.9265, val MSE: 0.8845
Gen 1468/2000  train MSE: 0.9265, val MSE: 0.8845
Gen 1469/2000  train MSE: 0.9265, val MSE: 0.8845
Gen 1470/2000  train MSE: 0.9265, val MSE: 0.8845
Gen 1471/2000  train MSE: 0.9265, val MSE: 0.8845
Gen 1472/2000  train MSE: 0.9265, val MSE: 0.8845
Gen 1473/2000  train MSE: 0.9265, val MSE: 0.8845
Gen 1474/2000  train MSE: 0.9265, val MSE: 0.8845
Gen 1475/2000  train MSE: 0.9265, val MSE: 0.8856
Gen 1476/2000  train MSE: 0.9265, val MSE: 0.8856
Gen 1477/2000  train MSE: 0.9265, val MSE: 0.8856
Gen 1478/2000  train MSE: 0.9265, val MSE: 0.8856
Gen 1479/2000  train MSE: 0.9264, val MSE: 0.8848
Gen 1480/2000  train MSE: 0.9264, val MSE: 0.8848
Gen 1481/2000  train MSE: 0.9264, val MSE: 0.8848
Gen 1482/2000  train MSE: 0.9264, val MSE: 0.8848
Gen 1483/2000  train MSE: 0.9264, val MSE: 0.8855
Gen 1484/2000  train MSE: 0.9264, val MSE: 0.8855
Gen 1485/2000  train MSE: 0.9264, val MSE: 0.8855
Gen 1486/2000  train MSE: 0.9263, val MSE: 0.8846
Gen 1487/2000  train MSE: 0.9263, val MSE: 0.8846
Gen 1488/2000  train MSE: 0.9263, val MSE: 0.8846
Gen 1489/2000  train MSE: 0.9263, val MSE: 0.8846
Gen 1490/2000  train MSE: 0.9263, val MSE: 0.8846
Gen 1491/2000  train MSE: 0.9262, val MSE: 0.8849
Gen 1492/2000  train MSE: 0.9262, val MSE: 0.8849
Gen 1493/2000  train MSE: 0.9262, val MSE: 0.8849
Gen 1494/2000  train MSE: 0.9262, val MSE: 0.8849
Gen 1495/2000  train MSE: 0.9262, val MSE: 0.8849
Gen 1496/2000  train MSE: 0.9262, val MSE: 0.8845
Gen 1497/2000  train MSE: 0.9262, val MSE: 0.8845
Gen 1498/2000  train MSE: 0.9262, val MSE: 0.8849
Gen 1499/2000  train MSE: 0.9262, val MSE: 0.8849
Gen 1500/2000  train MSE: 0.9262, val MSE: 0.8849
Gen 1500/2000  train MSE: 0.9261, val MSE: 0.8840
Gen 1501/2000  train MSE: 0.9261, val MSE: 0.8840
Gen 1502/2000  train MSE: 0.9261, val MSE: 0.8840
Gen 1503/2000  train MSE: 0.9261, val MSE: 0.8851
Gen 1504/2000  train MSE: 0.9261, val MSE: 0.8851
Gen 1505/2000  train MSE: 0.9261, val MSE: 0.8847
Gen 1506/2000  train MSE: 0.9260, val MSE: 0.8846
Gen 1507/2000  train MSE: 0.9260, val MSE: 0.8846
Gen 1508/2000  train MSE: 0.9260, val MSE: 0.8846
Gen 1509/2000  train MSE: 0.9260, val MSE: 0.8846
Gen 1510/2000  train MSE: 0.9260, val MSE: 0.8846
Gen 1511/2000  train MSE: 0.9260, val MSE: 0.8841
Gen 1512/2000  train MSE: 0.9260, val MSE: 0.8841
Gen 1513/2000  train MSE: 0.9259, val MSE: 0.8834
Gen 1514/2000  train MSE: 0.9259, val MSE: 0.8834
Gen 1515/2000  train MSE: 0.9259, val MSE: 0.8834
Gen 1516/2000  train MSE: 0.9259, val MSE: 0.8834
Gen 1517/2000  train MSE: 0.9259, val MSE: 0.8834
Gen 1518/2000  train MSE: 0.9259, val MSE: 0.8837
Gen 1519/2000  train MSE: 0.9259, val MSE: 0.8845
Gen 1520/2000  train MSE: 0.9258, val MSE: 0.8843
Gen 1521/2000  train MSE: 0.9258, val MSE: 0.8843
Gen 1522/2000  train MSE: 0.9258, val MSE: 0.8843
Gen 1523/2000  train MSE: 0.9258, val MSE: 0.8843
Gen 1524/2000  train MSE: 0.9258, val MSE: 0.8843
Gen 1525/2000  train MSE: 0.9258, val MSE: 0.8843
Gen 1526/2000  train MSE: 0.9257, val MSE: 0.8839
Gen 1527/2000  train MSE: 0.9257, val MSE: 0.8839
Gen 1528/2000  train MSE: 0.9257, val MSE: 0.8839
Gen 1529/2000  train MSE: 0.9257, val MSE: 0.8839
Gen 1530/2000  train MSE: 0.9257, val MSE: 0.8843
Gen 1531/2000  train MSE: 0.9257, val MSE: 0.8843
Gen 1532/2000  train MSE: 0.9257, val MSE: 0.8843
Gen 1533/2000  train MSE: 0.9257, val MSE: 0.8842
Gen 1534/2000  train MSE: 0.9256, val MSE: 0.8841
Gen 1535/2000  train MSE: 0.9256, val MSE: 0.8841
Gen 1536/2000  train MSE: 0.9256, val MSE: 0.8841
Gen 1537/2000  train MSE: 0.9256, val MSE: 0.8841
Gen 1538/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1539/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1540/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1541/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1542/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1543/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1544/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1545/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1546/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1547/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1548/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1549/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1550/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1551/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1552/2000  train MSE: 0.9254, val MSE: 0.8840
Gen 1553/2000  train MSE: 0.9254, val MSE: 0.8830
Gen 1554/2000  train MSE: 0.9254, val MSE: 0.8830
Gen 1555/2000  train MSE: 0.9253, val MSE: 0.8838
Gen 1556/2000  train MSE: 0.9253, val MSE: 0.8838
Gen 1557/2000  train MSE: 0.9253, val MSE: 0.8838
Gen 1558/2000  train MSE: 0.9253, val MSE: 0.8838
Gen 1559/2000  train MSE: 0.9253, val MSE: 0.8838
Gen 1560/2000  train MSE: 0.9253, val MSE: 0.8844
Gen 1561/2000  train MSE: 0.9253, val MSE: 0.8834
Gen 1562/2000  train MSE: 0.9253, val MSE: 0.8834
Gen 1563/2000  train MSE: 0.9253, val MSE: 0.8834
Gen 1564/2000  train MSE: 0.9253, val MSE: 0.8834
Gen 1565/2000  train MSE: 0.9253, val MSE: 0.8834
Gen 1566/2000  train MSE: 0.9253, val MSE: 0.8834
Gen 1567/2000  train MSE: 0.9251, val MSE: 0.8835
Gen 1568/2000  train MSE: 0.9251, val MSE: 0.8835
Gen 1569/2000  train MSE: 0.9251, val MSE: 0.8835
Gen 1570/2000  train MSE: 0.9251, val MSE: 0.8835
Gen 1571/2000  train MSE: 0.9251, val MSE: 0.8829
Gen 1572/2000  train MSE: 0.9251, val MSE: 0.8829
Gen 1573/2000  train MSE: 0.9251, val MSE: 0.8829
Gen 1574/2000  train MSE: 0.9251, val MSE: 0.8829
Gen 1575/2000  train MSE: 0.9251, val MSE: 0.8829
Gen 1576/2000  train MSE: 0.9251, val MSE: 0.8829
Gen 1577/2000  train MSE: 0.9250, val MSE: 0.8826
Gen 1578/2000  train MSE: 0.9250, val MSE: 0.8832
Gen 1579/2000  train MSE: 0.9249, val MSE: 0.8827
Gen 1580/2000  train MSE: 0.9249, val MSE: 0.8827
Gen 1581/2000  train MSE: 0.9249, val MSE: 0.8827
Gen 1582/2000  train MSE: 0.9249, val MSE: 0.8827
Gen 1583/2000  train MSE: 0.9249, val MSE: 0.8827
Gen 1584/2000  train MSE: 0.9248, val MSE: 0.8826
Gen 1585/2000  train MSE: 0.9248, val MSE: 0.8826
Gen 1586/2000  train MSE: 0.9248, val MSE: 0.8826
Gen 1587/2000  train MSE: 0.9247, val MSE: 0.8826
Gen 1588/2000  train MSE: 0.9247, val MSE: 0.8821
Gen 1589/2000  train MSE: 0.9247, val MSE: 0.8821
Gen 1590/2000  train MSE: 0.9247, val MSE: 0.8817
Gen 1591/2000  train MSE: 0.9247, val MSE: 0.8817
Gen 1592/2000  train MSE: 0.9247, val MSE: 0.8817
Gen 1593/2000  train MSE: 0.9246, val MSE: 0.8822
Gen 1594/2000  train MSE: 0.9246, val MSE: 0.8822
Gen 1595/2000  train MSE: 0.9246, val MSE: 0.8822
Gen 1596/2000  train MSE: 0.9246, val MSE: 0.8822
Gen 1597/2000  train MSE: 0.9246, val MSE: 0.8819
Gen 1598/2000  train MSE: 0.9246, val MSE: 0.8819
Gen 1599/2000  train MSE: 0.9245, val MSE: 0.8819
Gen 1600/2000  train MSE: 0.9245, val MSE: 0.8819
Gen 1600/2000  train MSE: 0.9243, val MSE: 0.8824
Gen 1601/2000  train MSE: 0.9243, val MSE: 0.8824
Gen 1602/2000  train MSE: 0.9243, val MSE: 0.8824
Gen 1603/2000  train MSE: 0.9243, val MSE: 0.8824
Gen 1604/2000  train MSE: 0.9243, val MSE: 0.8824
Gen 1605/2000  train MSE: 0.9243, val MSE: 0.8825
Gen 1606/2000  train MSE: 0.9243, val MSE: 0.8825
Gen 1607/2000  train MSE: 0.9243, val MSE: 0.8825
Gen 1608/2000  train MSE: 0.9243, val MSE: 0.8825
Gen 1609/2000  train MSE: 0.9243, val MSE: 0.8825
Gen 1610/2000  train MSE: 0.9243, val MSE: 0.8825
Gen 1611/2000  train MSE: 0.9242, val MSE: 0.8821
Gen 1612/2000  train MSE: 0.9242, val MSE: 0.8815
Gen 1613/2000  train MSE: 0.9242, val MSE: 0.8815
Gen 1614/2000  train MSE: 0.9240, val MSE: 0.8822
Gen 1615/2000  train MSE: 0.9240, val MSE: 0.8822
Gen 1616/2000  train MSE: 0.9240, val MSE: 0.8822
Gen 1617/2000  train MSE: 0.9240, val MSE: 0.8822
Gen 1618/2000  train MSE: 0.9240, val MSE: 0.8820
Gen 1619/2000  train MSE: 0.9240, val MSE: 0.8820
Gen 1620/2000  train MSE: 0.9240, val MSE: 0.8820
Gen 1621/2000  train MSE: 0.9240, val MSE: 0.8820
Gen 1622/2000  train MSE: 0.9240, val MSE: 0.8816
Gen 1623/2000  train MSE: 0.9240, val MSE: 0.8816
Gen 1624/2000  train MSE: 0.9238, val MSE: 0.8817
Gen 1625/2000  train MSE: 0.9238, val MSE: 0.8817
Gen 1626/2000  train MSE: 0.9238, val MSE: 0.8817
Gen 1627/2000  train MSE: 0.9236, val MSE: 0.8816
Gen 1628/2000  train MSE: 0.9236, val MSE: 0.8816
Gen 1629/2000  train MSE: 0.9236, val MSE: 0.8816
Gen 1630/2000  train MSE: 0.9236, val MSE: 0.8816
Gen 1631/2000  train MSE: 0.9236, val MSE: 0.8816
Gen 1632/2000  train MSE: 0.9236, val MSE: 0.8816
Gen 1633/2000  train MSE: 0.9236, val MSE: 0.8816
Gen 1634/2000  train MSE: 0.9236, val MSE: 0.8799
Gen 1635/2000  train MSE: 0.9236, val MSE: 0.8799
Gen 1636/2000  train MSE: 0.9236, val MSE: 0.8800
Gen 1637/2000  train MSE: 0.9236, val MSE: 0.8800
Gen 1638/2000  train MSE: 0.9236, val MSE: 0.8800
Gen 1639/2000  train MSE: 0.9235, val MSE: 0.8813
Gen 1640/2000  train MSE: 0.9235, val MSE: 0.8813
Gen 1641/2000  train MSE: 0.9235, val MSE: 0.8808
Gen 1642/2000  train MSE: 0.9235, val MSE: 0.8808
Gen 1643/2000  train MSE: 0.9234, val MSE: 0.8804
Gen 1644/2000  train MSE: 0.9234, val MSE: 0.8804
Gen 1645/2000  train MSE: 0.9234, val MSE: 0.8804
Gen 1646/2000  train MSE: 0.9233, val MSE: 0.8806
Gen 1647/2000  train MSE: 0.9233, val MSE: 0.8806
Gen 1648/2000  train MSE: 0.9233, val MSE: 0.8806
Gen 1649/2000  train MSE: 0.9233, val MSE: 0.8806
Gen 1650/2000  train MSE: 0.9233, val MSE: 0.8801
Gen 1651/2000  train MSE: 0.9232, val MSE: 0.8809
Gen 1652/2000  train MSE: 0.9232, val MSE: 0.8809
Gen 1653/2000  train MSE: 0.9231, val MSE: 0.8803
Gen 1654/2000  train MSE: 0.9231, val MSE: 0.8803
Gen 1655/2000  train MSE: 0.9231, val MSE: 0.8803
Gen 1656/2000  train MSE: 0.9231, val MSE: 0.8803
Gen 1657/2000  train MSE: 0.9231, val MSE: 0.8798
Gen 1658/2000  train MSE: 0.9231, val MSE: 0.8798
Gen 1659/2000  train MSE: 0.9231, val MSE: 0.8802
Gen 1660/2000  train MSE: 0.9230, val MSE: 0.8799
Gen 1661/2000  train MSE: 0.9230, val MSE: 0.8799
Gen 1662/2000  train MSE: 0.9229, val MSE: 0.8804
Gen 1663/2000  train MSE: 0.9229, val MSE: 0.8804
Gen 1664/2000  train MSE: 0.9229, val MSE: 0.8804
Gen 1665/2000  train MSE: 0.9229, val MSE: 0.8791
Gen 1666/2000  train MSE: 0.9229, val MSE: 0.8791
Gen 1667/2000  train MSE: 0.9229, val MSE: 0.8791
Gen 1668/2000  train MSE: 0.9229, val MSE: 0.8789
Gen 1669/2000  train MSE: 0.9228, val MSE: 0.8798
Gen 1670/2000  train MSE: 0.9228, val MSE: 0.8798
Gen 1671/2000  train MSE: 0.9228, val MSE: 0.8798
Gen 1672/2000  train MSE: 0.9228, val MSE: 0.8798
Gen 1673/2000  train MSE: 0.9228, val MSE: 0.8798
Gen 1674/2000  train MSE: 0.9228, val MSE: 0.8782
Gen 1675/2000  train MSE: 0.9228, val MSE: 0.8782
Gen 1676/2000  train MSE: 0.9228, val MSE: 0.8782
Gen 1677/2000  train MSE: 0.9226, val MSE: 0.8788
Gen 1678/2000  train MSE: 0.9226, val MSE: 0.8788
Gen 1679/2000  train MSE: 0.9225, val MSE: 0.8786
Gen 1680/2000  train MSE: 0.9225, val MSE: 0.8786
Gen 1681/2000  train MSE: 0.9225, val MSE: 0.8786
Gen 1682/2000  train MSE: 0.9225, val MSE: 0.8786
Gen 1683/2000  train MSE: 0.9225, val MSE: 0.8786
Gen 1684/2000  train MSE: 0.9225, val MSE: 0.8786
Gen 1685/2000  train MSE: 0.9225, val MSE: 0.8792
Gen 1686/2000  train MSE: 0.9225, val MSE: 0.8792
Gen 1687/2000  train MSE: 0.9225, val MSE: 0.8792
Gen 1688/2000  train MSE: 0.9225, val MSE: 0.8792
Gen 1689/2000  train MSE: 0.9225, val MSE: 0.8792
Gen 1690/2000  train MSE: 0.9225, val MSE: 0.8784
Gen 1691/2000  train MSE: 0.9225, val MSE: 0.8784
Gen 1692/2000  train MSE: 0.9224, val MSE: 0.8785
Gen 1693/2000  train MSE: 0.9224, val MSE: 0.8785
Gen 1694/2000  train MSE: 0.9224, val MSE: 0.8784
Gen 1695/2000  train MSE: 0.9223, val MSE: 0.8788
Gen 1696/2000  train MSE: 0.9223, val MSE: 0.8788
Gen 1697/2000  train MSE: 0.9223, val MSE: 0.8788
Gen 1698/2000  train MSE: 0.9223, val MSE: 0.8788
Gen 1699/2000  train MSE: 0.9223, val MSE: 0.8788
Gen 1700/2000  train MSE: 0.9223, val MSE: 0.8788
Gen 1700/2000  train MSE: 0.9223, val MSE: 0.8788
Gen 1701/2000  train MSE: 0.9223, val MSE: 0.8789
Gen 1702/2000  train MSE: 0.9223, val MSE: 0.8789
Gen 1703/2000  train MSE: 0.9222, val MSE: 0.8786
Gen 1704/2000  train MSE: 0.9222, val MSE: 0.8786
Gen 1705/2000  train MSE: 0.9222, val MSE: 0.8786
Gen 1706/2000  train MSE: 0.9222, val MSE: 0.8786
Gen 1707/2000  train MSE: 0.9222, val MSE: 0.8786
Gen 1708/2000  train MSE: 0.9222, val MSE: 0.8786
Gen 1709/2000  train MSE: 0.9222, val MSE: 0.8785
Gen 1710/2000  train MSE: 0.9222, val MSE: 0.8785
Gen 1711/2000  train MSE: 0.9222, val MSE: 0.8785
Gen 1712/2000  train MSE: 0.9222, val MSE: 0.8785
Gen 1713/2000  train MSE: 0.9222, val MSE: 0.8785
Gen 1714/2000  train MSE: 0.9220, val MSE: 0.8783
Gen 1715/2000  train MSE: 0.9220, val MSE: 0.8783
Gen 1716/2000  train MSE: 0.9220, val MSE: 0.8786
Gen 1717/2000  train MSE: 0.9220, val MSE: 0.8786
Gen 1718/2000  train MSE: 0.9220, val MSE: 0.8786
Gen 1719/2000  train MSE: 0.9220, val MSE: 0.8786
Gen 1720/2000  train MSE: 0.9220, val MSE: 0.8786
Gen 1721/2000  train MSE: 0.9220, val MSE: 0.8786
Gen 1722/2000  train MSE: 0.9220, val MSE: 0.8786
Gen 1723/2000  train MSE: 0.9220, val MSE: 0.8786
Gen 1724/2000  train MSE: 0.9220, val MSE: 0.8782
Gen 1725/2000  train MSE: 0.9220, val MSE: 0.8781
Gen 1726/2000  train MSE: 0.9220, val MSE: 0.8781
Gen 1727/2000  train MSE: 0.9220, val MSE: 0.8781
Gen 1728/2000  train MSE: 0.9220, val MSE: 0.8781
Gen 1729/2000  train MSE: 0.9220, val MSE: 0.8781
Gen 1730/2000  train MSE: 0.9220, val MSE: 0.8781
Gen 1731/2000  train MSE: 0.9220, val MSE: 0.8781
Gen 1732/2000  train MSE: 0.9220, val MSE: 0.8781
Gen 1733/2000  train MSE: 0.9219, val MSE: 0.8790
Gen 1734/2000  train MSE: 0.9219, val MSE: 0.8790
Gen 1735/2000  train MSE: 0.9219, val MSE: 0.8790
Gen 1736/2000  train MSE: 0.9219, val MSE: 0.8790
Gen 1737/2000  train MSE: 0.9219, val MSE: 0.8790
Gen 1738/2000  train MSE: 0.9219, val MSE: 0.8777
Gen 1739/2000  train MSE: 0.9218, val MSE: 0.8779
Gen 1740/2000  train MSE: 0.9218, val MSE: 0.8779
Gen 1741/2000  train MSE: 0.9218, val MSE: 0.8779
Gen 1742/2000  train MSE: 0.9218, val MSE: 0.8779
Gen 1743/2000  train MSE: 0.9218, val MSE: 0.8789
Gen 1744/2000  train MSE: 0.9217, val MSE: 0.8775
Gen 1745/2000  train MSE: 0.9216, val MSE: 0.8776
Gen 1746/2000  train MSE: 0.9216, val MSE: 0.8776
Gen 1747/2000  train MSE: 0.9216, val MSE: 0.8782
Gen 1748/2000  train MSE: 0.9216, val MSE: 0.8782
Gen 1749/2000  train MSE: 0.9216, val MSE: 0.8782
Gen 1750/2000  train MSE: 0.9216, val MSE: 0.8782
Gen 1751/2000  train MSE: 0.9216, val MSE: 0.8782
Gen 1752/2000  train MSE: 0.9216, val MSE: 0.8778
Gen 1753/2000  train MSE: 0.9215, val MSE: 0.8779
Gen 1754/2000  train MSE: 0.9215, val MSE: 0.8779
Gen 1755/2000  train MSE: 0.9215, val MSE: 0.8779
Gen 1756/2000  train MSE: 0.9215, val MSE: 0.8769
Gen 1757/2000  train MSE: 0.9214, val MSE: 0.8777
Gen 1758/2000  train MSE: 0.9214, val MSE: 0.8778
Gen 1759/2000  train MSE: 0.9214, val MSE: 0.8778
Gen 1760/2000  train MSE: 0.9214, val MSE: 0.8778
Gen 1761/2000  train MSE: 0.9213, val MSE: 0.8775
Gen 1762/2000  train MSE: 0.9213, val MSE: 0.8775
Gen 1763/2000  train MSE: 0.9213, val MSE: 0.8775
Gen 1764/2000  train MSE: 0.9213, val MSE: 0.8775
Gen 1765/2000  train MSE: 0.9211, val MSE: 0.8769
Gen 1766/2000  train MSE: 0.9211, val MSE: 0.8769
Gen 1767/2000  train MSE: 0.9211, val MSE: 0.8769
Gen 1768/2000  train MSE: 0.9211, val MSE: 0.8765
Gen 1769/2000  train MSE: 0.9210, val MSE: 0.8763
Gen 1770/2000  train MSE: 0.9210, val MSE: 0.8763
Gen 1771/2000  train MSE: 0.9210, val MSE: 0.8763
Gen 1772/2000  train MSE: 0.9210, val MSE: 0.8763
Gen 1773/2000  train MSE: 0.9210, val MSE: 0.8763
Gen 1774/2000  train MSE: 0.9210, val MSE: 0.8763
Gen 1775/2000  train MSE: 0.9209, val MSE: 0.8763
Gen 1776/2000  train MSE: 0.9209, val MSE: 0.8763
Gen 1777/2000  train MSE: 0.9209, val MSE: 0.8763
Gen 1778/2000  train MSE: 0.9209, val MSE: 0.8763
Gen 1779/2000  train MSE: 0.9209, val MSE: 0.8760
Gen 1780/2000  train MSE: 0.9209, val MSE: 0.8760
Gen 1781/2000  train MSE: 0.9209, val MSE: 0.8767
Gen 1782/2000  train MSE: 0.9209, val MSE: 0.8767
Gen 1783/2000  train MSE: 0.9209, val MSE: 0.8767
Gen 1784/2000  train MSE: 0.9209, val MSE: 0.8767
Gen 1785/2000  train MSE: 0.9209, val MSE: 0.8767
Gen 1786/2000  train MSE: 0.9209, val MSE: 0.8767
Gen 1787/2000  train MSE: 0.9209, val MSE: 0.8762
Gen 1788/2000  train MSE: 0.9208, val MSE: 0.8762
Gen 1789/2000  train MSE: 0.9208, val MSE: 0.8762
Gen 1790/2000  train MSE: 0.9208, val MSE: 0.8759
Gen 1791/2000  train MSE: 0.9208, val MSE: 0.8766
Gen 1792/2000  train MSE: 0.9208, val MSE: 0.8766
Gen 1793/2000  train MSE: 0.9208, val MSE: 0.8766
Gen 1794/2000  train MSE: 0.9208, val MSE: 0.8761
Gen 1795/2000  train MSE: 0.9208, val MSE: 0.8761
Gen 1796/2000  train MSE: 0.9207, val MSE: 0.8762
Gen 1797/2000  train MSE: 0.9207, val MSE: 0.8762
Gen 1798/2000  train MSE: 0.9207, val MSE: 0.8762
Gen 1799/2000  train MSE: 0.9207, val MSE: 0.8762
Gen 1800/2000  train MSE: 0.9207, val MSE: 0.8762
Gen 1800/2000  train MSE: 0.9206, val MSE: 0.8757
Gen 1801/2000  train MSE: 0.9205, val MSE: 0.8752
Gen 1802/2000  train MSE: 0.9205, val MSE: 0.8752
Gen 1803/2000  train MSE: 0.9205, val MSE: 0.8758
Gen 1804/2000  train MSE: 0.9205, val MSE: 0.8756
Gen 1805/2000  train MSE: 0.9205, val MSE: 0.8756
Gen 1806/2000  train MSE: 0.9204, val MSE: 0.8752
Gen 1807/2000  train MSE: 0.9204, val MSE: 0.8752
Gen 1808/2000  train MSE: 0.9204, val MSE: 0.8752
Gen 1809/2000  train MSE: 0.9203, val MSE: 0.8753
Gen 1810/2000  train MSE: 0.9203, val MSE: 0.8753
Gen 1811/2000  train MSE: 0.9203, val MSE: 0.8758
Gen 1812/2000  train MSE: 0.9202, val MSE: 0.8757
Gen 1813/2000  train MSE: 0.9202, val MSE: 0.8757
Gen 1814/2000  train MSE: 0.9202, val MSE: 0.8757
Gen 1815/2000  train MSE: 0.9202, val MSE: 0.8757
Gen 1816/2000  train MSE: 0.9202, val MSE: 0.8757
Gen 1817/2000  train MSE: 0.9202, val MSE: 0.8757
Gen 1818/2000  train MSE: 0.9202, val MSE: 0.8757
Gen 1819/2000  train MSE: 0.9202, val MSE: 0.8749
Gen 1820/2000  train MSE: 0.9202, val MSE: 0.8751
Gen 1821/2000  train MSE: 0.9202, val MSE: 0.8751
Gen 1822/2000  train MSE: 0.9202, val MSE: 0.8751
Gen 1823/2000  train MSE: 0.9201, val MSE: 0.8753
Gen 1824/2000  train MSE: 0.9201, val MSE: 0.8753
Gen 1825/2000  train MSE: 0.9201, val MSE: 0.8753
Gen 1826/2000  train MSE: 0.9201, val MSE: 0.8741
Gen 1827/2000  train MSE: 0.9201, val MSE: 0.8741
Gen 1828/2000  train MSE: 0.9201, val MSE: 0.8741
Gen 1829/2000  train MSE: 0.9201, val MSE: 0.8748
Gen 1830/2000  train MSE: 0.9201, val MSE: 0.8748
Gen 1831/2000  train MSE: 0.9200, val MSE: 0.8746
Gen 1832/2000  train MSE: 0.9200, val MSE: 0.8746
Gen 1833/2000  train MSE: 0.9200, val MSE: 0.8742
Gen 1834/2000  train MSE: 0.9199, val MSE: 0.8741
Gen 1835/2000  train MSE: 0.9199, val MSE: 0.8741
Gen 1836/2000  train MSE: 0.9199, val MSE: 0.8741
Gen 1837/2000  train MSE: 0.9199, val MSE: 0.8751
Gen 1838/2000  train MSE: 0.9199, val MSE: 0.8749
Gen 1839/2000  train MSE: 0.9199, val MSE: 0.8749
Gen 1840/2000  train MSE: 0.9199, val MSE: 0.8735
Gen 1841/2000  train MSE: 0.9198, val MSE: 0.8744
Gen 1842/2000  train MSE: 0.9198, val MSE: 0.8744
Gen 1843/2000  train MSE: 0.9198, val MSE: 0.8744
Gen 1844/2000  train MSE: 0.9198, val MSE: 0.8741
Gen 1845/2000  train MSE: 0.9197, val MSE: 0.8744
Gen 1846/2000  train MSE: 0.9197, val MSE: 0.8744
Gen 1847/2000  train MSE: 0.9197, val MSE: 0.8744
Gen 1848/2000  train MSE: 0.9197, val MSE: 0.8744
Gen 1849/2000  train MSE: 0.9197, val MSE: 0.8735
Gen 1850/2000  train MSE: 0.9197, val MSE: 0.8735
Gen 1851/2000  train MSE: 0.9197, val MSE: 0.8735
Gen 1852/2000  train MSE: 0.9197, val MSE: 0.8735
Gen 1853/2000  train MSE: 0.9197, val MSE: 0.8735
Gen 1854/2000  train MSE: 0.9197, val MSE: 0.8735
Gen 1855/2000  train MSE: 0.9197, val MSE: 0.8735
Gen 1856/2000  train MSE: 0.9197, val MSE: 0.8735
Gen 1857/2000  train MSE: 0.9197, val MSE: 0.8735
Gen 1858/2000  train MSE: 0.9196, val MSE: 0.8738
Gen 1859/2000  train MSE: 0.9196, val MSE: 0.8738
Gen 1860/2000  train MSE: 0.9196, val MSE: 0.8738
Gen 1861/2000  train MSE: 0.9196, val MSE: 0.8738
Gen 1862/2000  train MSE: 0.9196, val MSE: 0.8737
Gen 1863/2000  train MSE: 0.9196, val MSE: 0.8737
Gen 1864/2000  train MSE: 0.9196, val MSE: 0.8737
Gen 1865/2000  train MSE: 0.9196, val MSE: 0.8737
Gen 1866/2000  train MSE: 0.9196, val MSE: 0.8737
Gen 1867/2000  train MSE: 0.9196, val MSE: 0.8737
Gen 1868/2000  train MSE: 0.9196, val MSE: 0.8737
Gen 1869/2000  train MSE: 0.9194, val MSE: 0.8738
Gen 1870/2000  train MSE: 0.9194, val MSE: 0.8738
Gen 1871/2000  train MSE: 0.9194, val MSE: 0.8738
Gen 1872/2000  train MSE: 0.9194, val MSE: 0.8738
Gen 1873/2000  train MSE: 0.9194, val MSE: 0.8738
Gen 1874/2000  train MSE: 0.9194, val MSE: 0.8738
Gen 1875/2000  train MSE: 0.9193, val MSE: 0.8723
Gen 1876/2000  train MSE: 0.9193, val MSE: 0.8723
Gen 1877/2000  train MSE: 0.9193, val MSE: 0.8723
Gen 1878/2000  train MSE: 0.9193, val MSE: 0.8723
Gen 1879/2000  train MSE: 0.9193, val MSE: 0.8723
Gen 1880/2000  train MSE: 0.9193, val MSE: 0.8723
Gen 1881/2000  train MSE: 0.9193, val MSE: 0.8723
Gen 1882/2000  train MSE: 0.9192, val MSE: 0.8720
Gen 1883/2000  train MSE: 0.9192, val MSE: 0.8720
Gen 1884/2000  train MSE: 0.9192, val MSE: 0.8720
Gen 1885/2000  train MSE: 0.9192, val MSE: 0.8727
Gen 1886/2000  train MSE: 0.9192, val MSE: 0.8727
Gen 1887/2000  train MSE: 0.9192, val MSE: 0.8727
Gen 1888/2000  train MSE: 0.9190, val MSE: 0.8714
Gen 1889/2000  train MSE: 0.9190, val MSE: 0.8714
Gen 1890/2000  train MSE: 0.9190, val MSE: 0.8714
Gen 1891/2000  train MSE: 0.9190, val MSE: 0.8714
Gen 1892/2000  train MSE: 0.9190, val MSE: 0.8714
Gen 1893/2000  train MSE: 0.9188, val MSE: 0.8730
Gen 1894/2000  train MSE: 0.9188, val MSE: 0.8730
Gen 1895/2000  train MSE: 0.9188, val MSE: 0.8730
Gen 1896/2000  train MSE: 0.9187, val MSE: 0.8727
Gen 1897/2000  train MSE: 0.9187, val MSE: 0.8727
Gen 1898/2000  train MSE: 0.9187, val MSE: 0.8727
Gen 1899/2000  train MSE: 0.9187, val MSE: 0.8729
Gen 1900/2000  train MSE: 0.9187, val MSE: 0.8729
Gen 1900/2000  train MSE: 0.9187, val MSE: 0.8729
Gen 1901/2000  train MSE: 0.9187, val MSE: 0.8729
Gen 1902/2000  train MSE: 0.9187, val MSE: 0.8729
Gen 1903/2000  train MSE: 0.9186, val MSE: 0.8727
Gen 1904/2000  train MSE: 0.9186, val MSE: 0.8724
Gen 1905/2000  train MSE: 0.9186, val MSE: 0.8724
Gen 1906/2000  train MSE: 0.9186, val MSE: 0.8724
Gen 1907/2000  train MSE: 0.9186, val MSE: 0.8724
Gen 1908/2000  train MSE: 0.9186, val MSE: 0.8724
Gen 1909/2000  train MSE: 0.9185, val MSE: 0.8718
Gen 1910/2000  train MSE: 0.9185, val MSE: 0.8718
Gen 1911/2000  train MSE: 0.9185, val MSE: 0.8718
Gen 1912/2000  train MSE: 0.9183, val MSE: 0.8719
Gen 1913/2000  train MSE: 0.9183, val MSE: 0.8719
Gen 1914/2000  train MSE: 0.9183, val MSE: 0.8719
Gen 1915/2000  train MSE: 0.9183, val MSE: 0.8719
Gen 1916/2000  train MSE: 0.9183, val MSE: 0.8719
Gen 1917/2000  train MSE: 0.9182, val MSE: 0.8719
Gen 1918/2000  train MSE: 0.9182, val MSE: 0.8719
Gen 1919/2000  train MSE: 0.9181, val MSE: 0.8715
Gen 1920/2000  train MSE: 0.9181, val MSE: 0.8715
Gen 1921/2000  train MSE: 0.9181, val MSE: 0.8715
Gen 1922/2000  train MSE: 0.9181, val MSE: 0.8715
Gen 1923/2000  train MSE: 0.9181, val MSE: 0.8715
Gen 1924/2000  train MSE: 0.9181, val MSE: 0.8709
Gen 1925/2000  train MSE: 0.9180, val MSE: 0.8714
Gen 1926/2000  train MSE: 0.9180, val MSE: 0.8714
Gen 1927/2000  train MSE: 0.9180, val MSE: 0.8714
Gen 1928/2000  train MSE: 0.9180, val MSE: 0.8714
Gen 1929/2000  train MSE: 0.9180, val MSE: 0.8717
Gen 1930/2000  train MSE: 0.9179, val MSE: 0.8713
Gen 1931/2000  train MSE: 0.9178, val MSE: 0.8718
Gen 1932/2000  train MSE: 0.9178, val MSE: 0.8712
Gen 1933/2000  train MSE: 0.9178, val MSE: 0.8712
Gen 1934/2000  train MSE: 0.9178, val MSE: 0.8712
Gen 1935/2000  train MSE: 0.9178, val MSE: 0.8712
Gen 1936/2000  train MSE: 0.9178, val MSE: 0.8712
Gen 1937/2000  train MSE: 0.9178, val MSE: 0.8715
Gen 1938/2000  train MSE: 0.9177, val MSE: 0.8712
Gen 1939/2000  train MSE: 0.9176, val MSE: 0.8712
Gen 1940/2000  train MSE: 0.9176, val MSE: 0.8711
Gen 1941/2000  train MSE: 0.9176, val MSE: 0.8711
Gen 1942/2000  train MSE: 0.9176, val MSE: 0.8711
Gen 1943/2000  train MSE: 0.9176, val MSE: 0.8711
Gen 1944/2000  train MSE: 0.9176, val MSE: 0.8715
Gen 1945/2000  train MSE: 0.9176, val MSE: 0.8715
Gen 1946/2000  train MSE: 0.9175, val MSE: 0.8706
Gen 1947/2000  train MSE: 0.9175, val MSE: 0.8706
Gen 1948/2000  train MSE: 0.9175, val MSE: 0.8706
Gen 1949/2000  train MSE: 0.9175, val MSE: 0.8707
Gen 1950/2000  train MSE: 0.9175, val MSE: 0.8707
Gen 1951/2000  train MSE: 0.9175, val MSE: 0.8718
Gen 1952/2000  train MSE: 0.9175, val MSE: 0.8718
Gen 1953/2000  train MSE: 0.9174, val MSE: 0.8711
Gen 1954/2000  train MSE: 0.9174, val MSE: 0.8711
Gen 1955/2000  train MSE: 0.9174, val MSE: 0.8711
Gen 1956/2000  train MSE: 0.9174, val MSE: 0.8711
Gen 1957/2000  train MSE: 0.9174, val MSE: 0.8711
Gen 1958/2000  train MSE: 0.9174, val MSE: 0.8711
Gen 1959/2000  train MSE: 0.9173, val MSE: 0.8713
Gen 1960/2000  train MSE: 0.9173, val MSE: 0.8713
Gen 1961/2000  train MSE: 0.9173, val MSE: 0.8713
Gen 1962/2000  train MSE: 0.9173, val MSE: 0.8713
Gen 1963/2000  train MSE: 0.9173, val MSE: 0.8713
Gen 1964/2000  train MSE: 0.9173, val MSE: 0.8713
Gen 1965/2000  train MSE: 0.9173, val MSE: 0.8713
Gen 1966/2000  train MSE: 0.9173, val MSE: 0.8713
Gen 1967/2000  train MSE: 0.9173, val MSE: 0.8713
Gen 1968/2000  train MSE: 0.9172, val MSE: 0.8709
Gen 1969/2000  train MSE: 0.9172, val MSE: 0.8709
Gen 1970/2000  train MSE: 0.9172, val MSE: 0.8709
Gen 1971/2000  train MSE: 0.9172, val MSE: 0.8709
Gen 1972/2000  train MSE: 0.9172, val MSE: 0.8709
Gen 1973/2000  train MSE: 0.9172, val MSE: 0.8716
Gen 1974/2000  train MSE: 0.9171, val MSE: 0.8708
Gen 1975/2000  train MSE: 0.9171, val MSE: 0.8709
Gen 1976/2000  train MSE: 0.9171, val MSE: 0.8709
Gen 1977/2000  train MSE: 0.9171, val MSE: 0.8709
Gen 1978/2000  train MSE: 0.9171, val MSE: 0.8709
Gen 1979/2000  train MSE: 0.9171, val MSE: 0.8709
Gen 1980/2000  train MSE: 0.9171, val MSE: 0.8709
Gen 1981/2000  train MSE: 0.9170, val MSE: 0.8713
Gen 1982/2000  train MSE: 0.9170, val MSE: 0.8713
Gen 1983/2000  train MSE: 0.9170, val MSE: 0.8713
Gen 1984/2000  train MSE: 0.9170, val MSE: 0.8711
Gen 1985/2000  train MSE: 0.9170, val MSE: 0.8711
Gen 1986/2000  train MSE: 0.9169, val MSE: 0.8704
Gen 1987/2000  train MSE: 0.9169, val MSE: 0.8704
Gen 1988/2000  train MSE: 0.9169, val MSE: 0.8704
Gen 1989/2000  train MSE: 0.9169, val MSE: 0.8704
Gen 1990/2000  train MSE: 0.9169, val MSE: 0.8704
Gen 1991/2000  train MSE: 0.9169, val MSE: 0.8704
Gen 1992/2000  train MSE: 0.9169, val MSE: 0.8704
Gen 1993/2000  train MSE: 0.9169, val MSE: 0.8704
Gen 1994/2000  train MSE: 0.9169, val MSE: 0.8704
Gen 1995/2000  train MSE: 0.9169, val MSE: 0.8714
Gen 1996/2000  train MSE: 0.9169, val MSE: 0.8714
Gen 1997/2000  train MSE: 0.9168, val MSE: 0.8702
Gen 1998/2000  train MSE: 0.9168, val MSE: 0.8704
Gen 1999/2000  train MSE: 0.9166, val MSE: 0.8696
Gen 2000/2000  train MSE: 0.9166, val MSE: 0.8696
Gen 2000/2000  train MSE: 0.9166, val MSE: 0.8696

 GA done!  Final Train MSE: 0.9166, Val MSE: 0.8696
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=35cc6e16">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Same-implementation,-increasing-blx-alpha-to-0.6:">Same implementation, increasing blx alpha to 0.6:<a class="anchor-link" href="#Same-implementation,-increasing-blx-alpha-to-0.6:"></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=7c527a0d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Genetic Algorithm (GA) for Optimising Feed-Forward Neural Network Weights</span>
<span class="sd">==========================================================================</span>

<span class="sd">This script implements a real-valued Genetic Algorithm (GA) using Blend Crossover (BLX-)</span>
<span class="sd">and Gaussian mutation to optimise the parameters of a fixed-architecture feed-forward</span>
<span class="sd">neural network (FFN). The algorithm is applied to a regression task with MSE loss, and</span>
<span class="sd">performance is tracked across generations.</span>

<span class="sd">Key Features:</span>
<span class="sd">-------------</span>
<span class="sd">- Fixed FFN architecture using PyTorch with Xavier weight initialisation</span>
<span class="sd">- Real-valued genome representation using PyTorch parameter vectors</span>
<span class="sd">- Tournament selection (size = 3) with elitism (top 10% retained each generation)</span>
<span class="sd">- BLX- crossover with  = 0.6 for diversity-preserving recombination</span>
<span class="sd">- Gaussian mutation applied per gene with probability `mutation_p` and std dev `mutation_sd`</span>
<span class="sd">- Reproducible results via fixed seeds for NumPy and PyTorch</span>
<span class="sd">- Final best model reconstructed and evaluated</span>
<span class="sd">- MSE loss curves plotted across generations</span>

<span class="sd">Instructions:</span>
<span class="sd">-------------</span>
<span class="sd">1. Ensure that `X_train`, `y_train`, `X_val`, and `y_val` are defined as PyTorch tensors.</span>
<span class="sd">2. Adjust architecture via the `arch` dictionary.</span>
<span class="sd">3. Modify genetic algorithm hyperparameters (e.g., `pop_size`, `generations`, `mutation_p`) as needed.</span>
<span class="sd">4. Run the script to execute the full evolutionary cycle and view results.</span>

<span class="sd">Hyperparameters:</span>
<span class="sd">----------------</span>
<span class="sd">- `pop_size`: Number of individuals in the population</span>
<span class="sd">- `generations`: Number of generations to evolve</span>
<span class="sd">- `elite_frac`: Proportion of top individuals carried over each generation</span>
<span class="sd">- `tourn_size`: Tournament size for selection</span>
<span class="sd">- `mutation_p`: Probability of mutating each gene</span>
<span class="sd">- `mutation_sd`: Standard deviation of mutation noise</span>
<span class="sd">- `blx_alpha`: BLX- parameter controlling crossover range</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Printed train/validation MSE every 100 generations</span>
<span class="sd">- Final best model with evaluation on training and validation sets</span>
<span class="sd">- Plot of training and validation MSE vs. generation</span>
<span class="sd">- Internal genome evolution stored in memory only (can be extended to save)</span>

<span class="sd">"""</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">skPCA</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>

<span class="c1">#  0) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  1) Data to device </span>
<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">#  2) Fixed Architecture + Xavier init </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"ReLU"</span><span class="p">)</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="n">criterion</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span><span class="p">,</span> <span class="n">in_f</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
<span class="c1">#  3) GA Hyperparams </span>
<span class="n">pop_size</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">generations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">elite_frac</span>  <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tourn_size</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">mutation_p</span>  <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">mutation_sd</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">blx_alpha</span>   <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1">#  4) Init Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pop_size</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="c1">#  visualize initial population </span>
<span class="n">pop_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>           
<span class="n">genome_len</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#  5) Tournament Selection </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pop_size</span><span class="p">,</span> <span class="n">tourn_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>
<span class="c1">#  6) Evolution </span>
<span class="n">train_curve</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_curve</span>   <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1">#  Fitness eval</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gen</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">gen</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2">  train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">tr_mse</span>   <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">m_best</span>   <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">va_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tr_mse</span><span class="p">)</span>
    <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">va_mse</span><span class="p">)</span>
    <span class="c1"># b) Elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elites</span>  <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]]</span>
    <span class="n">pop_size</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
    <span class="n">elite_frac</span> <span class="o">=</span> <span class="mf">0.2</span>           
    <span class="n">elite_n</span>    <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
    <span class="n">elites</span>     <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">]</span>
    <span class="c1"># selection probabilities</span>
    <span class="n">p_elite</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_tourn</span>   <span class="o">=</span> <span class="mi">1</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># pure tournament selection</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="c1"># pure elitism</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span><span class="p">:</span>
            <span class="c1"># exploit: uniform from elites</span>
            <span class="k">return</span> <span class="n">elites</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
        <span class="k">elif</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">p_elite</span> <span class="o">+</span> <span class="n">p_tourn</span><span class="p">:</span>
            <span class="c1"># competition: standard tournament over full pop</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># pure exploration: uniform from entire pop</span>
            <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">pop_size</span><span class="p">)]</span>
    <span class="c1"># c) Reproduce via BLX- + mutation</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">pop_size</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="c1"># BLX- crossover</span>
        <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># mutation</span>
        <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mutation_p</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mutation_sd</span>
        <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1">#  7) Final Best Model </span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))]</span>
<span class="n">best_model_ga</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                     <span class="n">best_model_ga</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">best_model_ga</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> GA done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1">#  8) Plot </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA (w/ BLX-) Optimization of FFN Weights"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen  1/2000  train MSE: 0.9968, val MSE: 0.9961
Gen 100/2000  train MSE: 0.9953, val MSE: 0.9951
Gen 200/2000  train MSE: 0.9900, val MSE: 0.9884
Gen 300/2000  train MSE: 0.9811, val MSE: 0.9770
Gen 400/2000  train MSE: 0.9740, val MSE: 0.9669
Gen 500/2000  train MSE: 0.9679, val MSE: 0.9572
Gen 600/2000  train MSE: 0.9633, val MSE: 0.9498
Gen 700/2000  train MSE: 0.9552, val MSE: 0.9367
Gen 800/2000  train MSE: 0.9498, val MSE: 0.9307
Gen 900/2000  train MSE: 0.9447, val MSE: 0.9225
Gen 1000/2000  train MSE: 0.9416, val MSE: 0.9181
Gen 1100/2000  train MSE: 0.9356, val MSE: 0.9080
Gen 1200/2000  train MSE: 0.9291, val MSE: 0.8976
Gen 1300/2000  train MSE: 0.9222, val MSE: 0.8861
Gen 1400/2000  train MSE: 0.9191, val MSE: 0.8826
Gen 1500/2000  train MSE: 0.9120, val MSE: 0.8672
Gen 1600/2000  train MSE: 0.9067, val MSE: 0.8601
Gen 1700/2000  train MSE: 0.9005, val MSE: 0.8491
Gen 1800/2000  train MSE: 0.8963, val MSE: 0.8416
Gen 1900/2000  train MSE: 0.8935, val MSE: 0.8368
Gen 2000/2000  train MSE: 0.8908, val MSE: 0.8316

 GA done!  Final Train MSE: 0.8915, Val MSE: 0.8320
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=c8a713b1">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Same-implementation,-with-0.6-blx-alpha,-now-increasing-elitism-percentage:">Same implementation, with 0.6 blx alpha, now increasing elitism percentage:<a class="anchor-link" href="#Same-implementation,-with-0.6-blx-alpha,-now-increasing-elitism-percentage:"></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bf2f8472">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Genetic Algorithm (GA) with Increased BLX- for Feed-Forward Neural Network Optimisation</span>
<span class="sd">=========================================================================================</span>

<span class="sd">This script implements a real-valued Genetic Algorithm (GA) to optimise the weights of a</span>
<span class="sd">Feed-Forward Neural Network (FFN) using BLX- crossover and Gaussian mutation. The key</span>
<span class="sd">modification in this version is an increased elitism percentage.</span>

<span class="sd">Core Features:</span>
<span class="sd">--------------</span>
<span class="sd">- FFN with fixed architecture: 2 hidden layers, 24 units each, ReLU activations</span>
<span class="sd">- Xavier normal initialisation</span>
<span class="sd">- Real-valued GA using:</span>
<span class="sd">    * Tournament selection (size 3)</span>
<span class="sd">    * Elitism (top 20% retained)</span>
<span class="sd">    * BLX- crossover with  = 0.6</span>
<span class="sd">    * Gaussian mutation (=0, =0.01) with mutation rate 1%</span>
<span class="sd">- Reproducibility via fixed seeds</span>
<span class="sd">- Evaluation on both training and validation sets using MSE loss</span>
<span class="sd">- Plot of MSE over generations</span>

<span class="sd">Key Hyperparameters:</span>
<span class="sd">--------------------</span>
<span class="sd">- `pop_size`: 200 (population size)</span>
<span class="sd">- `generations`: 2000 (evolution length)</span>
<span class="sd">- `elite_frac`: 0.2 (elitism retention)</span>
<span class="sd">- `tourn_size`: 3 (selection pressure)</span>
<span class="sd">- `blx_alpha`: 0.6 (exploration during crossover )</span>
<span class="sd">- `mutation_p`: 0.01 (probability of mutation per gene)</span>
<span class="sd">- `mutation_sd`: 0.01 (standard deviation of Gaussian mutation)</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Prints MSE every 100 generations</span>
<span class="sd">- Final training and validation MSE of the best individual</span>
<span class="sd">- Matplotlib plot showing training and validation MSE curves</span>

<span class="sd">Usage Instructions:</span>
<span class="sd">-------------------</span>
<span class="sd">- Ensure that `X_train`, `y_train`, `X_val`, and `y_val` are defined as PyTorch tensors.</span>
<span class="sd">- Adjust the architecture or GA hyperparameters if needed.</span>
<span class="sd">- Run the script to evolve the FFN weights and observe performance.</span>

<span class="sd">Remarks:</span>
<span class="sd">--------</span>
<span class="sd">This version increases the BLX- crossover range, allowing offspring to explore further</span>
<span class="sd">beyond the bounds of the parent genes. This may help escape local minima in multimodal</span>
<span class="sd">landscapes but could also introduce more variance in early training.</span>

<span class="sd">"""</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">skPCA</span>
<span class="c1">#  0) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  1) Data to device </span>
<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">#  2) Fixed Architecture + Xavier init </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"ReLU"</span><span class="p">)</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="n">criterion</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span><span class="p">,</span> <span class="n">in_f</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
<span class="c1">#  3) GA Hyperparams </span>
<span class="n">pop_size</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">generations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">elite_frac</span>  <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">tourn_size</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">mutation_p</span>  <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">mutation_sd</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">blx_alpha</span>   <span class="o">=</span> <span class="mf">0.6</span>
<span class="c1">#  4) Init Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pop_size</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="c1">#  visualize initial population </span>
<span class="n">pop_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>           
<span class="n">genome_len</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#  5) Tournament Selection </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pop_size</span><span class="p">,</span> <span class="n">tourn_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>
<span class="c1">#  6) Evolution </span>
<span class="n">train_curve</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_curve</span>   <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># a) Fitness eval</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gen</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">gen</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2">  train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">tr_mse</span>   <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">m_best</span>   <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">va_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tr_mse</span><span class="p">)</span>
    <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">va_mse</span><span class="p">)</span>
    <span class="c1"># b) Elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elites</span>  <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]]</span>
    <span class="n">pop_size</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
    <span class="c1"># selection probabilities</span>
    <span class="n">p_elite</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_tourn</span>   <span class="o">=</span> <span class="mi">1</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># pure tournament selection</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        
    <span class="c1"># c) Reproduce via BLX- + mutation</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">pop_size</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="c1"># BLX- crossover</span>
        <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># mutation</span>
        <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mutation_p</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mutation_sd</span>
        <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>

    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1">#  7) Final Best Model </span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))]</span>
<span class="n">best_model_ga</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                     <span class="n">best_model_ga</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">best_model_ga</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> GA done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  8) Plot </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA (w/ BLX-) Optimization of FFN Weights"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen  1/2000  train MSE: 0.9953, val MSE: 0.9951
Gen 100/2000  train MSE: 0.9953, val MSE: 0.9951
Gen 200/2000  train MSE: 0.9900, val MSE: 0.9884
Gen 300/2000  train MSE: 0.9811, val MSE: 0.9770
Gen 400/2000  train MSE: 0.9740, val MSE: 0.9669
Gen 500/2000  train MSE: 0.9679, val MSE: 0.9572
Gen 600/2000  train MSE: 0.9633, val MSE: 0.9498
Gen 700/2000  train MSE: 0.9552, val MSE: 0.9367
Gen 800/2000  train MSE: 0.9498, val MSE: 0.9307
Gen 900/2000  train MSE: 0.9447, val MSE: 0.9225
Gen 1000/2000  train MSE: 0.9416, val MSE: 0.9181
Gen 1100/2000  train MSE: 0.9356, val MSE: 0.9080
Gen 1200/2000  train MSE: 0.9291, val MSE: 0.8976
Gen 1300/2000  train MSE: 0.9222, val MSE: 0.8861
Gen 1400/2000  train MSE: 0.9191, val MSE: 0.8826
Gen 1500/2000  train MSE: 0.9120, val MSE: 0.8672
Gen 1600/2000  train MSE: 0.9067, val MSE: 0.8601
Gen 1700/2000  train MSE: 0.9005, val MSE: 0.8491
Gen 1800/2000  train MSE: 0.8963, val MSE: 0.8416
Gen 1900/2000  train MSE: 0.8935, val MSE: 0.8368
Gen 2000/2000  train MSE: 0.8908, val MSE: 0.8316

 GA done!  Final Train MSE: 0.8915, Val MSE: 0.8320
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAsQAAAGJCAYAAACNeyWsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAqbFJREFUeJzs3Xd4FNXXwPHv7maTTS+kkZAQEnoLECD0IiUIIiAIgkoVFcECP0RQpFnwtVBUFEQpUgQRRBGkC4Teew+BQCAhlPS2yc77x5oNSxIImGQJnM/zzJOdO3dmztxdwsnsnXtViqIoCCGEEEII8YRSWzoAIYQQQgghLEkSYiGEEEII8USThFgIIYQQQjzRJCEWQgghhBBPNEmIhRBCCCHEE00SYiGEEEII8USThFgIIYQQQjzRJCEWQgghhBBPNEmIhRBCCCHEE00SYiEeE8nJyXh6erJo0SJLh1JqnDx5EisrK44fP27pUO5p3rx5qFQqLl68WGTHnDBhAiqVqsiO96if90FkZWUxatQo/Pz8UKvVdO3a1dIhPXK2bNmCSqViy5YtD73vb7/9VvSBCfGQJCEWohhERkYybNgwKleujJ2dHXZ2dlSvXp2hQ4dy9OjRAvcbNWoUKpWKXr16PfA5p0+fjqOjIy+88MJ/Cd3MN998g7OzM3q9vsA6KpXKbLG3t6d69ep8/PHHpKammtXt378/Dg4O9zznSy+9hE6n4+zZs3m2ffbZZ6hUKv7666+Hu6C7VK9enU6dOjFu3LgH2u/EiRO89NJL+Pr6YmNjg4+PDy+++CInTpz4T/F8+umnrFy58j8d41GQmprKhAkTHipZehTMmTOHL774gh49ejB//nyGDx9eYN1WrVrl+TeQs5w+fRrITQDzW+7895pzrM6dO+c5z8WLF1GpVHz55ZcFxpKdnY2TkxNdunTJs23q1KmoVCr69euXZ9u4ceNQqVT5/puztMWLFzNt2jRLhyGeBIoQokitWrVKsbOzU5ycnJQhQ4YoM2fOVH744QdlxIgRSkBAgKJSqZSLFy/m2c9gMCjlypVTAgICFFtbWyUxMbHQ58zMzFQ8PDyUTz/9tCgvRQkLC1N69OhxzzqA0q5dO2XBggXKggULlO+//17p06ePAuTZt1+/foq9vf09jxcbG6u4uroqrVu3Niu/cOGCYmtrq3Tv3v3hLqYAa9asUQDl/Pnzhaq/fPlyxdraWvH29lY++OAD5ccff1TGjh2rlC1bVrG2tlZWrFjx0LHY29sr/fr1y1OelZWlpKWlKQaD4aGPfTe9Xq+kpaUV2fHuFBcXpwDK+PHjS/S8RaVXr16Kr69voeq2bNlSKVeunOnzf+eSkJCgKIqi/PPPPwqgvPXWW3nqhIeHmx0LUABl//79ZueJjIxUAOWLL764Zzzt2rVT3N3d85R3795dsbKyUoKCgvJse+qppxRPT89CXW+O7OxsJS0tTcnOzn6g/RQltz2WLVt237qdOnVSypcv/8DnEOJBSUIsRBE6f/68Ym9vr1SrVk25evVqnu16vV6ZPn26EhUVlWfb5s2bFUDZvHmzotVqlXnz5hX6vCtWrHigpK4wUlJSFJ1Op8ydO/ee9QBl6NChecp79OihqNVqs+SnMAmxoijKDz/8oABmbdChQwfFyclJuXLlSuEvohAyMzMVV1dX5cMPP7xv3fPnzyt2dnZK1apVlevXr5tti4uLU6pWrarY29srERERDxVLQQlxaXOvhLg0aN26tVKjRo1C1W3ZsuV96xY2AWzZsqXi7++vuLq6Kp07dzbbVtiEeOLEiQqgnDx50qzc29vb9IfqtWvXTOV6vV6xt7dXunXrds/jFiVJiMWjSLpMCFGEPv/8c1JSUpg7dy5ly5bNs93Kyoq33noLPz+/PNsWLVpE9erVad26NW3btn2gvsArV64kICCAoKAgU9mff/6JSqUy66KxfPlyVCoVzz33nNn+1apVy9NNY9OmTWRkZPD0008XOo47eXt7o1KpsLKyeuB9X3nlFZo2bcrIkSO5efMmS5YsYe3atXz88cf4+vred/9bt24xcOBAXF1dcXV1pXfv3ty+fZuVK1ei0+lITk421dVqtbRq1Yo//vjjvsf94osvSE1N5YcffsDDw8Nsm7u7O7NmzSIlJYXPP//cVJ7TZ/b06dP07NkTJycnypQpw9tvv016erqpnkqlIiUlhfnz55u+Tu/fvz+Qfx/igIAAnnnmGbZs2UL9+vWxtbWlVq1apm4KK1asoFatWuh0OkJCQjh06JBZvHf35e3fv3+BX+tPmDABgMzMTMaNG0dISAjOzs7Y29vTvHlz/vnnH9NxLl68aGqbiRMn5jlGfn2Is7Ky+OijjwgKCsLGxoaAgADef/99MjIyzOrlXPP27dtp2LAhOp2OwMBAfv755/u8c0YpKSn873//w8/PDxsbG6pUqcKXX36Joiim2FUqFf/88w8nTpwwxV6SXT8cHR0ZPnw4q1at4uDBgw+8f7NmzQDYsWOHqezChQvExMQwbNgwdDqd2bbDhw+TkpJi2g/g9OnT9OjRAzc3N3Q6HfXr1+fPP/80O09BfYhnzJhBYGAgtra2NGzYkPDwcFq1akWrVq3yxGowGPjkk08oV64cOp2ONm3acP78edP2Vq1asXr1ai5dumR6LwICAkzbv/nmG2rUqIGdnR2urq7Ur1+fxYsXP3CbCQHw4P9TCSEK9Ndff1GxYkVCQ0MfaL+MjAyWL1/O//73PwB69+7NgAEDiImJwdvb+77779y5k3r16pmVNWvWDJVKxbZt26hduzYA4eHhqNVqtm/fbqoXFxfH6dOnGTZsmNn+a9asISQkBC8vr/uePz09nRs3bgDGpGPHjh3Mnz+fPn36PFRCrFKpmDVrFnXr1mXIkCGEh4dTv359hg4det99MzMzadeuHWfOnGHUqFFotVomT57MG2+8gbW1Na1atcrTjzkkJIQ//viDxMREnJycCjz2qlWrCAgIoHnz5vlub9GiBQEBAaxevTrPtp49exIQEMDkyZPZvXs3X3/9Nbdv3zYlcwsWLOCVV16hYcOGvPrqqwBmf+Dk5/z58/Tp04fXXnuNl156iS+//JLOnTszc+ZM3n//fd544w0AJk+eTM+ePTlz5gxqdf73QV577TXatm1rVrZ27VoWLVqEp6cnAImJifz444/07t2bwYMHk5SUxE8//URYWBh79+6lTp06eHh48P333zNkyBC6detm+uMr5zOYn1deeYX58+fTo0cP/ve//7Fnzx4mT57MqVOn+P333/Ncc48ePRg0aBD9+vVjzpw59O/fn5CQEGrUqFHgORRF4dlnn+Wff/5h0KBB1KlTh3Xr1vHuu+8SHR3N1KlT8fDwYMGCBXzyySckJyczefJkwPgH471kZ2ebPv85dDpdns9ZUlJSnnpubm553pO3336bqVOnMmHChDyJ6P00atQIKysrtm/fziuvvAIYk2N7e3saNGhA/fr12bFjB927dzdtg9xE+sSJEzRt2hRfX19Gjx6Nvb09v/76K127dmX58uV069atwHN///33DBs2jObNmzN8+HAuXrxI165dcXV1pVy5cnnqf/bZZ6jVakaOHElCQgKff/45L774Inv27AHggw8+ICEhgStXrjB16lQAU5vOnj2bt956ix49epj+uDx69Ch79uyhT58+D9RmQgDSh1iIopKQkKAASteuXfNsu337thIXF2daUlNTzbb/9ttvCqCcO3dOURRFSUxMVHQ6nTJ16tT7nlev1ysqlUr53//+l2dbjRo1lJ49e5rW69Wrpzz//PMKoJw6dUpRlNzuFkeOHDHb19/fv1BfefNvn8e7l65duyrp6elmdQvbZSLHmDFjFEDRaDTKgQMHCrXPzz//rADK7NmzTWVTp05VbGxsFFdXV+Xbb7/Ns8/ixYsVQNmzZ0+Bx42Pj1cApUuXLvc8/7PPPqsApj7g48ePVwDl2WefNav3xhtv5Gn3grpMzJ07VwGUyMhIU1n58uUVQNm5c6epbN26dQqg2NraKpcuXTKVz5o1SwGUf/75x1SWE1dBzp07pzg7Oyvt2rVTsrKyFEUx9mXOyMgwq3f79m3Fy8tLGThwoKnsXl0m7j7v4cOHFUB55ZVXzOqNHDnS1IXo7mvetm2bqez69euKjY1Nvp//O61cuVIBlI8//tisvEePHopKpTLrblSYbhB31s3v83/n+5jTRSC/5c739M7z5nR9yPncF7bLhKIoSoMGDcz6Cr/22mumPvmjRo1SGjRoYHb9dnZ2il6vVxRFUdq0aaPUqlXL7N+uwWBQmjRpolSqVCnPNeV8pjIyMpQyZcooDRo0MB1LURRl3rx5CqC0bNkyz77VqlUz+zxNnz5dAZRjx46ZygrqMtGlS5dCv0dCFIZ0mRCiiCQmJgLkO4pCq1at8PDwMC0zZsww275o0SLq169PxYoVAePXpp06dSpUt4lbt26hKAqurq55tjVv3pzw8HDAeHfqyJEjvPrqq7i7u5vKw8PDcXFxoWbNmqb9jh8/TlRUFJ06dSrUtXfp0oUNGzawYcMG/vjjD8aMGcPatWvp06eP6evoh+Hu7g6Aj4+PWXz3snnzZqysrOjdu7eprHPnzmRkZHD79u18n+DPabu7797dKSkpCTC+N/eSsz3n85Dj7rvbb775JmC8E/+wqlevTuPGjU3rOd9MPPXUU/j7++cpv3DhQqGOm5KSQrdu3XB1deWXX35Bo9EAoNFosLa2Boxfd9+6dYusrCzq16//UF/vQ+71jxgxwqw859uSu++2V69e3ewOvYeHB1WqVLnvta1ZswaNRsNbb72V5zyKovD3338/VPxg7MqR8/nPWUaNGpWn3rhx4/LUK+gboLfffhtXV1cmTpz4wPE0a9aMiIgIYmJiAONd4CZNmgDQtGlTDh06ZBoBZseOHYSGhmJlZcWtW7fYvHkzPXv2NN3NvnHjBjdv3iQsLIxz584RHR2d7zn379/PzZs3GTx4sNm3Qi+++GK+v5sABgwYYPo8Aab3tTCfUxcXF65cucK+ffsK0SJC3J90mRCiiOQkQnf2T80xa9YskpKSiI2N5aWXXjLbFh8fz5o1axg2bJhZ/7mmTZuyfPlyzp49S+XKle97/vwSz+bNmzNz5kzOnz9PREQEKpWKxo0bmxLlwYMHEx4eTtOmTc2+tl29ejVeXl7Ur1+/UNderlw5s6/bn332WcqUKcPIkSP566+/8k1C7+fy5cuMHz+emjVrcvz4cT7//HPGjh1r2n7r1i0yMzNN67a2tjg7O3P16lV8fHywt7c3bQsMDMTJyYmAgACzRDFHTtvda3zcnPc3JzEuSEGJc6VKlczWg4KCUKvV/2ls4buvxdnZGSBPH/Wc8tu3bxfquIMHDyYiIoKdO3dSpkwZs23z58/nq6++4vTp02bD8VWoUOGB4we4dOkSarXa9MdgDm9vb1xcXLh06ZJZeX7vn6ur632v7dKlS/j4+OR5X3K6Q9x9ngdhb2+fp7tJfmrVqlWoemB8z9555x3Gjx/PoUOHCkwq89OsWTOmTp3Kjh07aNOmDSdOnDD1a2/SpAlZWVns3buX8uXLc+3aNVPXivPnz6MoCh9++CEffvhhvse+fv16vv34c9rv7vfRysrKrN/vne5+L3OusTCf0/fee4+NGzfSsGFDKlasSPv27enTpw9Nmza9775C5EfuEAtRRJydnSlbtmy+kzyEhobStm3bfH9ZL1u2jIyMDL766isqVapkWnLumN3vLrGbmxsqlSrf/0Ry+gVu27aN8PBw6tWrZ3oQKjw8nOTkZA4dOpSnT+yaNWvo0KHDf5pAoU2bNqZzP4ycPs1///03zz//PJ988onZnaPnnnuOsmXLmpa3334bMPbdvDtulUqFs7MzLVq0yPdcOW2Xc0c6Pznv773GkQY4evQovr6+9+yLnBPTf5Vz57aw5YW5Wz99+nR++eUXZs+eTZ06dcy2LVy4kP79+xMUFMRPP/3E2rVr2bBhA0899RQGg+GB479TYdvjv1xbafP222/j4uLywHeJc/7db9++nV27dgGYvklwd3enUqVKbN++3fQsQU79nPdw5MiRee5k5yx3J7z/xX95L6tVq8aZM2dYsmQJzZo1Y/ny5TRr1ozx48cXWXziySIJsRBFqFOnTpw/f569e/cWep9FixZRs2ZNli1blmdp27btfZ+atrKyIigoiMjIyDzb/P398ff3Jzw8nPDwcFPi26JFCy5evMiyZcvIzs42SxTj4+PZuXNnobtLFCQrKwvI/475/fz+++/8+eeffPTRR5QrV45p06ZhbW1t1u3gq6++yvcraj8/P2JiYszuXh45coTLly8X+HVvZGQkarX6vnfin3nmGSIjI80eSrxTeHg4Fy9e5Jlnnsmz7dy5c2br58+fx2AwmN09s/QMbuHh4YwcOZJ33nmHF198Mc/23377jcDAQFasWMHLL79MWFgYbdu2NRstAx7sOsqXL4/BYMjTPrGxscTHx1O+fPmHu5h8znP16tU8d/hzJs8oqvMUpZy7xH/88UeeUULuxdPT05T07tixg+rVq+Pi4mLa3qRJE3bs2MGOHTvQaDSmZDkwMBAwjrzStm3bfJeCugzltN+d33KB8ffAf/kW5F6fJXt7e3r16sXcuXNNXbw++eSTPJ9HIQpDEmIhitCoUaOws7Nj4MCBxMbG5tl+952Py5cvs23bNnr27EmPHj3yLAMGDOD8+fOmp64L0rhxY/bv35/vtubNm7N582b27t1rSojr1KmDo6Mjn332Gba2toSEhJjqr1+/HoD27ds/0LXfbdWqVQAEBwc/0H5JSUm89dZb1K1b19TP1sfHh48++oi1a9eybNkywDgyxJ3/UVevXh2Ali1bkpGRwZIlS0zHnDVrFmDs55jfncwDBw5Qo0YNU9eCgrz77rvY2try2muvcfPmTbNtt27d4vXXX8fOzo533303z7539xv/5ptvAMyGtbO3tyc+Pv6eMRSXa9eu0bNnT5o1a8YXX3yRb52cO3p3fo737NljuguZw87ODqBQ19KxY0eAPLORTZkyBeA//2F253mys7P59ttvzcpzZnB72OEFi9s777yDi4sLkyZNeqD9mjVrxuHDh1m/fr2p/3COJk2asGvXLsLDw6ldu7YpyfX09KRVq1bMmjWLa9eu5TlmXFxcgeerX78+ZcqUYfbs2aY/hsH4B39hu+rkx97enoSEhDzld//7s7a2pnr16iiKcs+ZNYUoiPQhFqIIVapUicWLF9O7d2+qVKnCiy++SHBwMIqiEBkZyeLFi1Gr1aYhiBYvXmwaDio/HTt2xMrKikWLFt1zKLcuXbqwYMGCfPsbN2/enEWLFqFSqUxfjWo0Gpo0acK6deto1aqV2YMtq1evplmzZvdNDu909uxZFi5cCBin7d29ezfz58+nYsWKvPzyy2Z19Xo9H3/8cZ5juLm58cYbbzB27FiuXr3KihUrzL5SHTp0KPPnz+edd96hQ4cOBd6peu6556hUqRKvv/46ERERZGVlMWvWLLp3787y5csZPnw4gwYNMg0Dptfr2bp1q2mIsnupVKkS8+fP58UXX6RWrVoMGjSIChUqcPHiRX766Sdu3LjBL7/8ku9waZGRkTz77LN06NCBXbt2sXDhQvr06WP2B0NISAgbN25kypQp+Pj4UKFChQcewu9hvfXWW8TFxTFq1CizPybAOGRa7dq1eeaZZ1ixYgXdunWjU6dOREZGMnPmTKpXr272TYCtrS3Vq1dn6dKlVK5cGTc3N2rWrJnvg5HBwcH069ePH374gfj4eFq2bMnevXuZP38+Xbt2pXXr1kVyfZ07d6Z169Z88MEHXLx4keDgYNavX88ff/zBO++8c98h7izF2dmZt99++6G6TcydO5d9+/bleaCzSZMmJCQkkJCQYPqjM8eMGTNo1qwZtWrVYvDgwQQGBhIbG8uuXbu4cuUKR44cyfd81tbWTJgwgTfffJOnnnqKnj17cvHiRebNm0dQUNBDf/sREhLC0qVLGTFiBA0aNMDBwYHOnTvTvn17vL29adq0KV5eXpw6dYpvv/2WTp063ffBVyHyZYmhLYR43J0/f14ZMmSIUrFiRUWn0ym2trZK1apVlddff105fPiwqV6tWrUUf3//ex6rVatWiqenp9lQRnfLyMhQ3N3dlY8++ijPthMnTpiGOLrTxx9/rABmM7QZDAbF09NT+fzzzwt7qXmGkdJoNEq5cuWUV199VYmNjTWr269fvwKHnwoKClL279+vaDQaZdiwYfmea+/evYparVbeeuute8YUERGhdO7cWXFwcFDs7OyUfv36KVlZWcoHH3yg2Nvbmw0H9vfff5sNeVcYR48eVXr37q2ULVtW0Wq1ire3t9K7d2+z4aJy5AwzdvLkSaVHjx6Ko6Oj4urqqgwbNizPFManT59WWrRoodja2poN3VXQsGudOnXKcz7ymTkwvyG77h7+rKDhw7hj+DSDwaB8+umnSvny5RUbGxulbt26yl9//aX069cvz9BYO3fuVEJCQhRra2uzY+Q33Jter1cmTpyoVKhQQdFqtYqfn58yZsyYPMP2FXTNLVu2NBvWqyBJSUnK8OHDFR8fH0Wr1SqVKlVSvvjiizxTYj/osGtFOVNdfse6ffu24uzsXOhh1xRFUc6cOWN6/86ePWu2zWAwKC4uLgqgLF26NM++ERERSt++fRVvb29Fq9Uqvr6+yjPPPKP89ttvea7pzqH8FEVRvv76a9Pno2HDhsqOHTuUkJAQpUOHDnn2vbs9cj6nd86OmZycrPTp08cUb87nbNasWUqLFi2UMmXKKDY2NkpQUJDy7rvvmqbLFuJBqRTlMXwSQYgn0EcffcTcuXM5d+5cgQ+r3M/evXsJDQ3lxIkTpi4Ij7uuXbuiUqnyTABRVCZMmMDEiROJi4u750N7QjyODAYDHh4ePPfcc8yePdvS4QhRIOlDLMRjYvjw4SQnJ+f5uvtBffrpp09MMnzq1Cn++usvPvroI0uHIkSpl56enuc5iZ9//plbt27lO3WzEI8S6UMsxGPCwcGB69ev/6djNGzYkIYNGxZRRI++atWqmT0AJIR4eLt372b48OE8//zzlClThoMHD/LTTz9Rs2ZNnn/+eUuHJ8Q9SUIshBBCiP8sICAAPz8/vv76a27duoWbmxt9+/bls88+M3twV4hHkfQhFkIIIYQQTzTpQyyEEEIIIZ5okhALIYQQQognmvQhfkgGg4GrV6/i6Oho8elWhRBCCCFEXoqikJSUhI+PD2p1wfeBJSF+SFevXsXPz8/SYQghhBBCiPu4fPmyaZbY/EhC/JBypoa8fPkyTk5OxX4+vV7P+vXrad++PVqtttjPV1pIuxRM2iZ/0i4Fk7bJn7RLwaRt8iftUrCSbpvExET8/PzuO6W3JMQPKaebhJOTU4klxHZ2djg5Ock/rjtIuxRM2iZ/0i4Fk7bJn7RLwaRt8iftUjBLtc39urfKQ3VCCCGEEOKJJgmxEEIIIYR4oklCLIQQQgghnmjSh1gIIYQQTxRFUcjKyiI7O7tYjq/X67GysiI9Pb3YzlFaFXXbaDQarKys/vMQuJIQCyGEEOKJkZmZybVr10hNTS22cyiKgre3N5cvX5a5Cu5SHG1jZ2dH2bJlsba2fuhjSEIshBBCiCeCwWAgMjISjUaDj48P1tbWxZKwGgwGkpOTcXBwuOdkEE+iomwbRVHIzMwkLi6OyMhIKlWq9NDHlIRYCCGEEE+EzMxMDAYDfn5+2NnZFdt5DAYDmZmZ6HQ6SYjvUtRtY2tri1ar5dKlS6bjPgx5l4QQQgjxRJEk9fFSFO+nfCKEEEIIIcQTTRLi0uzqIUiLt3QUQgghhBClmiTEpVVkOPzQyrgIIYQQQjyggIAApk2bZukwHgmSEJcC6fpsFu29zPYYFYv2Xmbh7ktE7/zFuPF2pGWDE0IIIUSxUqlU91wmTJjwUMfdt28fr7766n+KrVWrVqhUKj777LM82zp16pQnvsjISF555RXKlSuHTqejXLlydOnShdOnT5vqFHSdS5Ys+U+x3ouMMlEKpGRkMWHVKUDDsshTAIy3usEAefeEEEKIx961a9dMr5cuXcq4ceM4c+aMqczBwcH0WlEUsrOzsbK6f5Lg4eFRJPH5+fkxb948Ro8ebSqLjo5m06ZNlC1b1lSm1+sJCwsjMDCQ3377DV9fX65cucLff/9NfHy82THnzp1Lhw4dzMpcXFyKJN78yB3iUkBrpaZ9dU9quxloX92Ttv5qGqlP5lZIvWW54IQQQohSTFEUUjOzinxJy8y+bx1FUQoVo7e3t2lxdnZGpVKZ1k+fPo2joyN///03ISEh2NjYsH37diIiIujSpQteXl44ODjQoEEDNm7caHbcu7tMqFQqfvzxR7p164adnR2VKlXizz//vG98zzzzDDdu3GDHjh2msvnz59O+fXs8PT1NZSdOnCAiIoIvv/ySRo0aUb58eZo2bcrHH39Mo0aNzI7p4uJidt3e3t4PPaRaYcg9xlLASadlRu86rFmzho5P10Y7oz6oL+dWOLcegl+wXIBCCCFEKZWmz6b6uHUWOffJSWHYWRdNKjZ69Gi+/PJLAgMDcXV15fLly3Ts2JFPPvkEGxsbfv75Zzp37syZM2fw9/cv8DgTJ07k888/54svvuCbb77hxRdf5NKlS7i5uRW4j7W1NS+++CJz586ladOmAMybN4/PP//crLuEh4cHarWaP//8k2rVqj1Sw989OpGIgqXewur7hrQ5+S5W3zWEhCgUlSZ3e0aS5WITQgghhMVNmjSJdu3aERQUhJubG8HBwbz22mvUrFmTSpUq8dFHHxEUFHTfO779+/end+/eVKxYkU8//ZTk5GT27t173/MPHDiQX3/9lZSUFLZt20ZCQgLPPPOMWR1fX1+mT5/O5MmTKVOmDE899RQfffQRFy5cyHO83r174+DgYLZERUU9WKM8ALlDXBooBlS3LuAAkGEsygxozR/n9PS02oqSnojMlC6EEEI8OFuthpOTwor0mAaDgaTEJBydHO95F9RWqylw24OqX7++2XpycjITJkxg9erVXLt2jaysLNLS0u6bVNauXdv02t7eHicnJ65fv37f8wcHB1OpUiV+++03/vnnH15++eV8+zG/8cYbdOnShYMHD7J3716WLVvGp59+yp9//km7du1M9aZOnUrbtm3N9vXx8blvHA9LEuLSQOdMVt/V7Nq1i8aNG2OltUYpU53kTwcBcOlaLAGWjVAIIYQolVQqVZF1W8hhMBjIstZgZ21VYt0C7O3tzdZHjhzJhg0b+PLLL6lYsSK2trb06NGDzMzMex5Hq9WaratUKgwGQ6FiGDhwIDNmzODkyZP3vKvs6OhI586d6dKlCx9//DFhYWF8/PHHZgmxt7c3FStWLNR5i4J0mSgNNFoUv1BuOVRG8QsFv4bo7BzQa4zzsN88sZlbKff+gAshhBDiybFjxw769+9Pt27dqFWrFt7e3ly8eLFYz9mnTx+OHTtGzZo1qV69eqH2UalUVK1alZSUlGKN7X7kDnEp1jm0GuyFEPU5jlw4hVutYEuHJIQQQohHQKVKlVixYgWdO3dGpVLx4YcfFvpO78NydXXl2rVree4y5zh8+DDjxo2je/fuhISEoNPp2Lp1K3PmzOG9994zqxsfH09MTIxZmaOjY5474UVFEuJSzKfpi7D3EwBUcacBSYiFEEIIAVOmTGHgwIE0adIEd3d33nvvPRITE4v9vPcaK7hcuXIEBATwf//3f1y+fBmVSkVAQAATJ05k+PDhZnUHDBiQZ//JkyebjXVclCQhLs2cfTlgE0pIxh7KHf8e4jdDhRZQ90VLRyaEEEKIYtC/f3/69+9vWm/VqlW+4xkHBASwefNms7KhQ4eard/dhSK/49w9YcbdtmzZcs/thw8fNr12d3dn2rRpJCYm4uTkVGD/6sKOz1yULNqHeNu2bXTu3BkfHx9UKhUrV6687z5btmyhXr162NjYULFiRebNm5enzowZMwgICECn0xEaGpqnY3d6ejpDhw6lTJkyODg40L17d2JjY4voqkrWDRvjWIJutw7B0SXw55uQmWrhqIQQQgghSg+LJsQpKSkEBwczY8aMQtWPjIykU6dOtG7dmsOHD/POO+/wyiuvsG5d7oDaS5cuZcSIEYwfP56DBw8SHBxMWFiY2ZAhw4cPZ9WqVSxbtoytW7dy9epVnnvuuSK/vpLwj+fLjNUPYL7jK+hV1qBkw+2LkHbb0qEJIYQQQpQKFu0y8fTTT/P0008Xuv7MmTOpUKECX331FQDVqlVj+/btTJ06lbAw4xiCU6ZMYfDgwaa+JzNnzmT16tXMmTOH0aNHk5CQwE8//cTixYt56qmnAON82dWqVWP37t15pg581Dm4ePBjdjuIg5bWqwhQx8L3jQGIq9QTjxdnWzhCIYQQQohHW6nqQ7xr1648gzSHhYXxzjvvAJCZmcmBAwcYM2aMabtaraZt27bs2rULgAMHDqDX682OU7VqVfz9/dm1a1eBCXFGRgYZGRmm9ZyO6Xq9Hr1eXyTXdy8557j7XK809cfVzop0fTZndlUggDu6fpzfXCKxWVJB7SKkbQoi7VIwaZv8SbsUrLS1jV6vR1EUDAZDsY64kNMHNudcIldxtI3BYEBRFPR6PRqN+WQnhf1slqqEOCYmBi8vL7MyLy8vEhMTSUtL4/bt22RnZ+db5/Tp06ZjWFtb53kK0svLK8/wHneaPHkyEydOzFO+fv167OzsHvKKHtyGDRvylJX792dG7VdYE9+aU9dT+V/6dBwNCaxZvRpUj/88dvm1izCStsmftEvBpG3yJ+1SsNLSNlZWVnh7e5OcnHzfCSqKQlJSUrGfo7QqyrbJzMwkLS2Nbdu2kZWVZbYtNbVwz1WVqoTYksaMGcOIESNM64mJifj5+dG+fXucnJyK/fx6vZ4NGzbQrl27Asf3y9Eg/jbMmI5Opaee6hgqjRUqtQa3el2xKlOh2GMtSQ/SLk8aaZv8SbsUTNomf9IuBSttbZOens7ly5dxcHBAp9MV23kURSEpKQlHR0dUT8BNqQdRHG2Tnp6Ora0tLVq0yPO+FnaouVKVEHt7e+cZDSI2NhYnJydsbW3RaDRoNJp863h7e5uOkZmZSXx8vNld4jvr5MfGxgYbG5s85VqttkR/CRTmfGXc3ElSbHFUpeF36EtT+Zlja6jy3pZijtAySvp9KE2kbfIn7VIwaZv8SbsUrLS0TXZ2NiqVCrVaXaxTKud0Bcg5l8hVHG2jVqtRqVT5fg4L+7ksVQlx48aNWbNmjVnZhg0baNzY+BCZtbU1ISEhbNq0ia5duwLGht+0aRPDhg0DICQkBK1Wy6ZNm+jevTsAZ86cISoqynSc0k6tUbOh0ofYXlgPgKOSRDPlAA5p0eyNvGWq56nEUe7mLqystJCRBOWbgK0rOPmAWlPQ4YUQQgghHisWTYiTk5M5f/68aT0yMpLDhw/j5uaGv78/Y8aMITo6mp9//hmA119/nW+//ZZRo0YxcOBANm/ezK+//srq1atNxxgxYgT9+vWjfv36NGzYkGnTppGSkmIadcLZ2ZlBgwYxYsQI3NzccHJy4s0336Rx48alboSJe3nupaGAcQDupOgzMLshLkoCHWcZ+3m9Z/ULDa3+yX/n8s1gwOr8twkhhBBCPGYsmhDv37+f1q1bm9Zz+uj269ePefPmce3aNaKiokzbK1SowOrVqxk+fDjTp0+nXLly/Pjjj6Yh1wB69epFXFwc48aNIyYmhjp16rB27VqzB+2mTp2KWq2me/fuZGRkEBYWxnfffVcCV2wZjmWMXUHsVRkc0b16/x0ubefiwmEEvPjNE/FAnhBCCPEkaNWqFXXq1GHatGmWDuWRY9GOLTnTDd695Mw+N2/evDxTArZq1YpDhw6RkZFBRESE2fSFOYYNG8alS5fIyMhgz549hIaGmm3X6XTMmDGDW7dukZKSwooVK+7Zf7jUs3GCoKfyFCs+dTn88glWdNiTZ1vA+QVs/Lgzq2ZP4Pb1KyURpRBCCCHy0blzZzp06JDvtvDwcFQqFUePHi2RWPr3749KpeL111/Ps23o0KGoVCqz3CwuLo4hQ4bg7++PjY0NPj4+dO/enR07dpjqBAQEoFKp8iyfffZZSVwSUMr6EIuHpFLBSyvAkG1erNZQR6WiThAoB6qhijtFVJ3/4X/YOPFJ2+xwiA7n2Le/s/aZVagAfzc7yrvb4+OskydnhRBCiBIwaNAgunfvzpUrVyhXrpzZtrlz51K/fn1q165dYvH4+fmxZMkSpk6diq2tLWAc6WHx4sX4+/ub1e3evTuZmZnMnz+fwMBArl27xpo1a7h586ZZvUmTJjF48GCzMkdHx+K9kDtIQvykUKlAU/DbrRq0DpJi8PeoQna7YaQsGYTT5c0A1FJfpPOKY2b1rTVqGgWVIayGFzV8nAku5ywJshBCiNJHUUBfuLFqC81gMB4zUwP3GklBa1eoronPPPMMHh4ezJs3j7Fjx5rKk5OTWbZsGV988QU3b95k2LBhbNu2jdu3bxMUFMT7779P7969i+KKzNSrV4+IiAhWrFjBiy++CMCKFSvw9/enQoXc4V3j4+MJDw9ny5YttGzZEjAm01WrVs0zZK2jo6NFv62XhFgY6ZyNC6Cxd8Op3xL41BcMxhleOlZxxj7lIp1vziEgO4ovs3qx4Ww9dp+9SiZavJxs6NOwPM/XL4ePi60lr0QIIYQoPH0qfOpTpIdUAy6Fqfj+VbC2v281Kysr+vbty7x58/jggw9MN6CWLVtGdnY2vXv3Jjk5mZCQEN577z2cnJxYvXo1L7/8MkFBQTRs2PC/XE6+Bg4cyNy5c00J8Zw5cxgwYIBZV1cHBwccHBxYuXIljRo1ynf42keFDI4n8mdlA28fMa1+Fz+EL24MpYVyAH91HF9bf8sp3UDO6vrxrfZrHJIuMHXjGZp8tplvN58zTc0ohBBCiP9u4MCBREREsHXrVlPZ3Llz6d69O87Ozvj6+jJy5Ejq1KlDYGAgb775Jh06dODXX38tlnheeukltm/fzqVLl7h06RI7duzgpZdeMqtjZWXFvHnzmD9/Pi4uLjRt2pQPPviA48eP5znee++9Z0qgc5bw8PBiiT0/codYFMzZF3zrQ/R+SIgqsNozmt08o9kNwPLs5vxv/RCmbDiLm7011co6Ud3HCTc7a/o3DcDGSsY3FkII8QjR2hnv1BYhg8FAYlISTo6O9558QmtX6GNWrVqVJk2aMGfOHFq1asX58+cJDw9n0qRJgHHSkU8//ZRff/2V6OhoMjMzycjIwM6u8Od4EB4eHnTq1Il58+ahKAqdOnXC3d09T73u3bvTqVMnwsPD2b17N3///TdffPEFP/zwAwMHDjTVe/fdd/MMlODr61sssedHEmJxb/1WQewdf8m5+MPPXSHuFJSpBAFN4cA80+bumnASFTsmZvXjRnIm4eduEH7uBgAatYr21XP7B9nZaHB3eHS/PhFCCPEEUKkK1W3hgRgMoM02HrcIZ6obNGgQb775JjNmzGDu3LkEBQWZ+uZ+8cUXTJ8+nWnTplGrVi3s7e155513yMzMLLLz323gwIGmic9mzJhRYD2dTke7du1o164dH3zwAf3792fixIlmCbG7uzsVK1YstljvRxJicW/WduB3V9+jN3ZBth6srI3rT38O6YnwywsQvZ8BVuvoa7eTDI0j0dYBrKIF++I0fLxa4ePVp8wO1aGGN1/1DMbeRj6KQgghxL307NmTt99+m8WLF/Pzzz8zZMgQU3/iHTt20KVLF1O3BYPBwNmzZ6levXqxxdOhQwcyMzNRqVRmc0LcT5UqVfLMPGxpkoWIB6dS5SbDYOxv7OABr2yEiS4AaDKTsCOJSmlXGcFOsIbXDKPZTl0AUjKNQ8CtPRHD2vExeDvpCKvhRVgNb5pUzPuVixBCCPGkc3BwoFevXowZM4bExESzLgaVKlXit99+Y+fOnbi6ujJlyhRiY2OLNSHWaDScOnXK9PpuN2/e5Pnnn2fgwIHUrl0bR0dH9u7dy9dff82zzz5rVjcpKYmYmBizMjs7uzyjURQXSYhF0VGpoHpXOLkS7D2gWme4cc64JMcwK8wOmhoHFk/P1PP3khncunAQj+zrpKba8O3ubszf5YGVWoWvqy39GgfwcuPyaDXy7KcQQggBxm4TP/30Ex07dsTHJ3d0jLFjx3LhwgXCwsKws7Pj1VdfpWvXriQkJBRrPPdKWB0cHAgNDWXq1KlERESg1+vx8/Ojb9++TJgwwazuuHHjGDdunFnZa6+9xsyZM4sj7DwkIRZF67nZ0PQt8Khm7G4BsP5D2Pk1nFoF6cZ/mLpTq+h246xx+79/VL5gtYXpWd2ZmtWdSzdTmfTXSb5af4b+TQMY2LQCZaS/sRBCiCdc48aN8x3Jyc3NjZUrV95z37tn/30YObMJF+TOGGxsbJg8eTKTJ082lRkMBhITE00TegBcvHjxP8f1X0lCLIqWlTX4hpiXuZY3/ryyz7jcrUJLiDQOI/O21XL6eEZyONOP727W41BmJWb8E8GMfyL4/sV6PF2rbDFfgBBCCCGeNJIQi+JXuxek3IS02+blOidoPAwUAyzuCZf3AOBx6yDtOEg7mz8479qcftd6EI0HQxYd5IOO1RjcItACFyGEEEKIx5UkxKL42ThCq/fuXWfQeriyH7ZPBbWVsR8yUPF2ONt125lq9xZf3wrlkzWn0KhVDGgaIFNFCyGEEKJISEIsHh3l6sMLi4yvs7Ng93ewaRIqg54RqdPB+XW+TmjBpL9O8uv+y7zVphKuthpiUpGZ8YQQQgjx0OTxffFo0lgZH85756ipaETGTFw1GQCcjknijUUH6f3jPiYfsaLD1zu5nphuqWiFEEKUInIT5fFSFO+nJMTi0ebkA0NzH8Tb1VvLe2FVaFbRnarejmg1xm4TF26k0PDTTSzZW/AU00IIIZ5sWq0WgNTUVAtHIopSzvuZ8/4+DOkyIR59HpWh/kDYPwfdby8xxNGHIa9tBQdPMjMzeWPmOjZeNf5tN3rFMebvusSrLSrQsVZZbKzyDhQuhBDiyaTRaHBxceH69euAceKH4ngexWAwkJmZSXp6OuoinLr5cVCUbaMoCqmpqVy/fh0XF5d8JwcpLEmIRekQOgSO/gqZyZB0FXbNgAotUGdnM8jlENPt97MiuSYfXmvKqWuJDF96hDErjtG5tg+VvRzp26S8JMdCCCHw9vYGMCXFxUFRFNLS0rC1tZUHwO9SHG3j4uJiel8fliTEonTwqAxDdsLPXeB2JOyYBjumYQU0/rfKy2yl9nPt+P28wvxjaaTrDSw7cAWAT9ac4t2wKrzSvIIkxkII8QRTqVSULVsWT09P9Hp9sZxDr9ezbds2WrRo8Z++xn8cFXXbaLXa/3RnOIckxKL0cC0PbT6E5a+AooBbINyKMKsSvKYrwagY3XcpS29XYe3xGHZduAnAF+vOsGDXJZa93hg/NztLXIEQQohHhEajKZJEqqBjZ2VlodPpJCG+y6PaNtKxRZQuNbvDexfh/Wh462ABlRR0S3vSL3sFv7zSgK3vtqJ5JXcAYhLTaf75P4z74zgGgzxlLIQQQghJiEVppHMGa3sAFEfjVM4G72DoNAUCmufW2zQRDi2kfPxeFrRMZt2bjU2bft51iZ6zdpGUXjxflwkhhBCi9JCEWJRqWX2WE+nehuznF0KDQdBvFbz4G2hsjBVWvQULusLC56hyaTHHJ4ZRw8cJgP2XblNrwno+/uskGVnZZGRly9iUQgghxBNI+hCL0s29Mkf9+lHOyXinGJUKKrWDQetgbkfQ3zHW5PoPcLh+ktVDv2bhvmhjtwkFftweyY/bIwGo6OnAsNYV8XC0IaS8KzqtPIAnhBBCPO4kIRaPJ5+68N4lSL0BZ9fBX+8Yyw8vgsDWvNToeV5o4Ef3mbs4cjnetNv568m8s/QwAGoVjOpQlX6NA7C1lsRYCCGEeFxJQiweX1bWxpnuAlual694BWo+h5VGwx9Dm5KckYVBUYiMS2HaxrOcjkniWkI6BgU++/s0R6/EM6NPPRlLUgghhHhMSUIsHn//PnhnJvEquPgB4GBj/GcQ7OfC3AENAdh94Sbfbj7P9vM3WHMshkaTN/Fyo/L4utrydM2y0pVCCCGEeIzIQ3Xi8ae1hTbjwaNabtm0mrB8sLE7RT4aBZZh4SuhtK3mBUBsYgZfrj/L8KVHqDZuLcMWHyTyRkpJRC+EEEKIYmbxhHjGjBkEBASg0+kIDQ1l7969BdbV6/VMmjSJoKAgdDodwcHBrF271qxOQEAAKpUqzzJ06FBTnVatWuXZ/vrrrxfbNYpHQPMRMHQ3+DXKLTv2KyzuCQu7F7jbj/3q89lztejd0I921Y3JsaLAX0ev0frLLdQYt5YuM3bwf2tPs3RfFOn67OK+EiGEEEIUMYt2mVi6dCkjRoxg5syZhIaGMm3aNMLCwjhz5gyenp556o8dO5aFCxcye/Zsqlatyrp16+jWrRs7d+6kbt26AOzbt4/s7Nyk5Pjx47Rr147nn3/e7FiDBw9m0qRJpnU7O5m57InQZhzs/AYSrkDsMWPZ+Y0w/1lw8gWvGlCnD2i0YOMIwAsN/XmhoT8At1IyOXz5Nu+vOE5MYjopmdkcuRxvejBv9IpjdKvri5NOy6stAvFxsbXEVQohhBDiAVg0IZ4yZQqDBw9mwIABAMycOZPVq1czZ84cRo8enaf+ggUL+OCDD+jYsSMAQ4YMYePGjXz11VcsXLgQAA8PD7N9PvvsM4KCgmjZ0vzBKjs7O7y9vYvjssSjLKCpcQHIyoRPfcCgh8ituXXWf2D8+fTnEPqa2e5u9tY8VdWLraPcuZ6YwbZzcUTGpXDyWiI7I26iKLDiYDQA83ZeJNjPhRca+NH734RaCCGEEI8eiyXEmZmZHDhwgDFjxpjK1Go1bdu2ZdeuXfnuk5GRgU6nMyuztbVl+/btBZ5j4cKFjBgxIs8IAYsWLWLhwoV4e3vTuXNnPvzww3veJc7IyCAjI8O0npiYCBi7cej1xT/bWc45SuJcpcl/axcVvLYD9dm/QTGg2TzRfPPfo8hy9EUJagtq84fo1IC3o5ae9XxMZZtOXefSrVRSMrOZHR5Jmt5gunu8+VQs374QjFpdciNVyGcmf9IuBZO2yZ+0S8GkbfIn7VKwkm6bwp5HpVhoaq6rV6/i6+vLzp07adw4d0rdUaNGsXXrVvbs2ZNnnz59+nDkyBFWrlxJUFAQmzZtokuXLmRnZ5slqzl+/fVX+vTpQ1RUFD4+uYnLDz/8QPny5fHx8eHo0aO89957NGzYkBUrVhQY74QJE5g4cWKe8sWLF0t3i8dEl0N9AchSW2NlyDSVn/XqzCmf5wvaLV/JeohJgxknNBgwJsF2VgqBjgpWKqjkrNDUS0FGchNCCCGKT2pqKn369CEhIQEnJ6cC65WqhDguLo7BgwezatUqVCoVQUFBtG3bljlz5pCWlpanflhYGNbW1qxateqesWzevJk2bdpw/vx5goKC8q2T3x1iPz8/bty4cc8GLip6vZ4NGzbQrl07tFptsZ+vtCjKdtHM74j6yl6yOk4FtQZN+BeoEi4DkNVxKqrrJ1Cf+pPszt+iBD1VqGOmZmbx/Ky9nL2enGebi62WpkFlePOpIII87P9T7PmRz0z+pF0KJm2TP2mXgknb5E/apWAl3TaJiYm4u7vfNyG2WJcJd3d3NBoNsbGxZuWxsbEF9u318PBg5cqVpKenc/PmTXx8fBg9ejSBgYF56l66dImNGzfe865vjtDQUIB7JsQ2NjbY2NjkKddqtSX6YS/p85UWRdIuL/4KVw9jVaElqNVQ72X4xAuyM7FaM9xUzer4UqgaVqhDOmu1/PlmM45eSSAuKYPEdD17I2/x+6Fo4tP0rD4eg4NOy//1qP3fYr8H+czkT9qlYNI2+ZN2KZi0Tf6kXQpWUm1T2HNYLCG2trYmJCSETZs20bVrVwAMBgObNm1i2LBh99xXp9Ph6+uLXq9n+fLl9OzZM0+duXPn4unpSadOne4by+HDhwEoWzafCRzEk8PWFYJa566r1dBvFcy5K/lNi3+gw+q0GhpWcDOt927oz//aV2bcHyfYfPo6vx28wprj10zbPR1t+PL5YGytNVipVWjUahx1Vrg75P2DTAghhBD/nUVHmRgxYgT9+vWjfv36NGzYkGnTppGSkmIadaJv3774+voyefJkAPbs2UN0dDR16tQhOjqaCRMmYDAYGDVqlNlxDQYDc+fOpV+/flhZmV9iREQEixcvpmPHjpQpU4ajR48yfPhwWrRoQe3axXeXTpRS7pUBFXBHz6KITbBpEoQOAQePgva8p3KudnzarRZPfbWF1MxsktKzTNuS0rPo9t3OPPvUL+9Km2petK3mSSUvx4c6rxBCCCHysmhC3KtXL+Li4hg3bhwxMTHUqVOHtWvX4uVlnAAhKioKtTp37pD09HTGjh3LhQsXcHBwoGPHjixYsAAXFxez427cuJGoqCgGDhyY55zW1tZs3LjRlHz7+fnRvXt3xo4dW6zXKkopOzfoPA2idoMh2ziZB0D4V8alUhi4lgdHb2jwCuicC31ob2cde95vw43k3Af4fj94hZWHr5KZZSDLYCDLoBCfanxCdv+l2+y/dJv/W3saXxdbJnWpQesqniU6coUQQgjxOLJoQgwwbNiwArtIbNmyxWy9ZcuWnDx58r7HbN++PQU9K+jn58fWrVvz3SZEvkL6GxcAz2qw6Y7RRs7dMfXzpV3w4jJQqSAzBazv/6Cco06Loy63f9OI9lUY0b6KWR1FUVhzLIZdF27w24ErpOsNRMenMWj+frrXK8eI9pXxlQlAhBBCiIdm8YRYiFIl+AW4GG68E1yxHaTehPAvIT0Bzm+AiS65dZu+A+3yDtX3oFQqFZ1ql6VT7bJ81KUmOyNu8umaU5y4msjyg1f46+hVXm0RSM/6fvi5yRCAQgghxINS37+KEMLEyQde/h2enwd1X4Smb8E7x4zJ8d12TCvy06tUKppWdOfPYc14vWUQAWXsyMgy8M3m8zT//B8+WX2SC3HJ3EzOOy63EEIIIfInd4iF+K90zvDSb3ArEpKuwdync7cteA66zXroh+8KolGrGP10VUaFVeGL9Wf48/BVouPTmB0eyezwSAC6BJelQjYcioqnpp8rdtbyz10IIYTIj/wPKURRcatgXFqOhq2fGcsiNsG0mjA6CqyKftg0tVrFex2q8l6HqszcGsHPOy+SlJ5FUkYWfxy5Blgx7fheAOr5u1DB3YGxnarham9d5LEIIYQQpZUkxEIUtdZjoNbzsPs72P8TZKXDx57GUSjCJsOi7sYRK8I+BY218UG9IpjD+fWWQbze0jixzIqDV/hh2wVu3E4kIUuNPlvhYFQ8B6PiWX7wCnbWGmy1GuoHuPJayyDq+bv+5/MLIYQQpZUkxEIUB/eK8MwU0NrCrm+NZft+NP6M3Gb8+UNL488On0GjIUV6+ufqlaNzLS/WrFlDhw7tORydxImriXy+7jTpegOpmdmkZmaz7kQs607E4uOso1kld1pV8USnVVPJ0xE3e2vsbeRXhBBCiMef/G8nRHEK+wSaDYevqoJBn5sU3yn2RLGGoFarCA0sQ2hgGfqE+pOQpidDb+DE1QTm7rzI3shbXE1I59f9V/h1/xWzfZ+r60vfJgEEl3NGVQR3sYUQQohHkSTEQhQ3e3d4dQv88ylkpcGlncZuFDn0qSUWik6rQafVAOBfxo6na5UlLimDD1ce51ZKJgoKkTdSufHvKBUrDkWz4lA0Zeyt+bJnMGXsranp4yyTgQghhHisSEIsREnwrgm9F5uXHZgPq94yTuJhQR6ONsx8OcSsLCvbwHdbIthy5joHo+K5mZLJgLn7AHDUWdGljg8V3B3oXLssnk46S4QthBBCFBlJiIWwlJyZ7O5MiCM2w/apkJ1lXLdxMPYxLhNUoqFZadS81aYSb7WpxM+7LvLr/stkZSuciU0iKT2LhbujAPi/tadpVtGdOn4ulC9jR2iFMng7S4IshBCidJGEWAhLyS8h3vYlXNphXu/cetC5QNfvIDIcrh6Een2h7kslEmbfxgH0bRwAQLo+m4W7L3HkSgKrjlwlM8vA5tPX2Xz6uql+r/p+vN+pmmndzlqDViNzAAkhhHh0SUIshKVo/51mOeao8aE7gOR/E8v2n0DqDePdYoD0eFjSJ3ffy3sAlXG2vBKk02p4pXkgAB91qcHWs3GcuJpI1M1U1p6IAWDp/sss3X/ZtI+tVsNHXWvSI6RcicYqhBBCFJYkxEJYintl0NhAdoZxhrsc9h5QfyBY20HTt+HaEfi5S979146BOn2KZAzjh+FiZ02XOr50qeMLGPsdd/9+J0euJJjVS9NnM3LZETafjqVNVS/aVvPC2U5riZCFEEKIfElCLISlOJWFEacgMdq83LW8MRkGsHWFwFbw9Bfw97vGsnr94OB8yEiAv96BNuPBzq0kI8+XlUbNyqFNyTIoprLUzGyCJ64HYM2xGNYcM95FXvRKKE0rulskTiGEEOJukhALYUn2ZYzL/TQYBN61jHeP3StCdiYc+QUOzINLu2DoHovdKb6TSqVCq8mNw9lWzdp3mrPp1HVWHbnK6ZgkAF78cQ+zXg7BUWdFSHlXbKw0lgpZCCGEkIRYiFJBrYHyjXPXn/0Gog/AjbNw4wzEXwLXAIuFdy9VvZ2o6u3E0NYVOR6dwDPfbAfgtQUHAOMwbm+3qcSzwT4yhJsQQgiLkEe/hSiNNFp4Yw+4VzGuTw+GCc6w5wfLxnUfNX2d+aZ3Xer4uVDL1xmApPQsPl59imb/9w/Dlx7mwKVbFo5SCCHEk0buEAtRWqnV4NfAeIc4x9/vGh+0s3GwXFz30TnYh87BPgCkZmbx4coTbD17nRvJmfx+KJrfD0Xzde+6PPtvHSGEEKK4yR1iIUqz6t3ylqXeKPk4HpKdtRVf9Qxm3wdt+fL5YFP5W78coufMXYxefhR9tsGCEQohhHgSSEIsRGlWsQ30XgKuFXLLTv4JE5zRfuJO0PW/Ue+bDTHHLRdjIahUKnqElGP3mDamsr0Xb7Fk32UafLKR9347SlxShgUjFEII8TiTLhNClGYqFVR5Gpx8YFYLY9mGD02ba0b/AtG/GFesdOAWCAPXgs7ZAsHen7ezjmMT2rP7wi0+Xn2SSzdTiU/Vmyb7cLXT4u1sywcdq9GskgzbJoQQomjIHWIhHgdlg43jETv6GCf7+FecQ/XcOlnpcP0kXPx3aujMVNj8CZz6q4SDvTdHnZZ21b1Y904LZr0cwsuNypu23U7Vc+paIn3n7GH/RXn4TgghRNGQO8RCPC6ajzAu+jS4dgS9e3V2bviHLof6mtc7tw62fAoxx3LLPogF7b9DnikKbP7IOJNe8AslF/9ddFoNYTW8CavhzagOVYhP1ZOYrufln/ZyKyWTHjN3ycN3QgghioTcIRbicaO1Bf9Gxp9Adtjn4BcKwb2N2w/MM0+GARbc8XDe5b0Q/hX8/poxOX4EOOq0+LnZUcPHmekv1DGVv/XLId5YdICMrGzLBSeEEKLUk4RYiMecof5AGLQemrwF6ju+FCrXMPd11M7crhPpCbnlSTElE+QDaF7Jg7XvNMfRxngta47FUGvCer5Yd5rVR6+hPCJJvBBCiNJDukwI8aTwqg4jz0HqLeM4xY7eMKU6JEYbt28YB7ci4NyG3H0Wdoc3dlom3nuo6u3E5pGtmLbxLIv2RJGZZWDGPxEAuNlbM6VnMK2qeFo4SiGEEKWFJMRCPEns3IxLjudmw9/vQewxYzK8YZx5/esn4NBCqPtS/sdLTwBrR+MkISXMw9GGT7rVomtdXxbuvsTfx2PIzDJwKyWT/nP34WqnxUqj5rUWgbSs7AGAn5sdmhKPVAghxKNOEmIhnmQBTWHIdtgzC64eNo5EcfWgcQSKlOvGOn8MBVtXqNrJfN+VQ+HwQvCqCa9uBY1lfp00CHCjQYAbX2YbWH30Gu8sPQwYR6QA+Hj1KT5efcpUf0jLCiTFqQhNycTbRWuJkIUQQjxiLN6HeMaMGQQEBKDT6QgNDWXv3r0F1tXr9UyaNImgoCB0Oh3BwcGsXbvWrM6ECRNQqVRmS9WqVc3qpKenM3ToUMqUKYODgwPdu3cnNja2WK5PiFIh9DXo9j08PxfePgLvngPPGrnb/5kMfw03JsETnI3L4YXGbbHHIdnyfY21GjVd6/pyYmIYG4a3YMmrjfBzs8XN3hpXu9zE9/utkSw8r6HRZ1toN2UrU9afkdnwhBDiCWfRO8RLly5lxIgRzJw5k9DQUKZNm0ZYWBhnzpzB0zNv/7+xY8eycOFCZs+eTdWqVVm3bh3dunVj586d1K1b11SvRo0abNy40bRuZWV+mcOHD2f16tUsW7YMZ2dnhg0bxnPPPceOHTuK72KFKG2eGgtL/h2ZIvaYcSlIegI4lyuZuO7D3saKSl6OVALCRz1lKt938RbrjsdwLjaJbefiUFBx7noy5zafZ/eFWzxdyxsAtUpFoIc9jQLLoNVY/J6BEEKIEmDRhHjKlCkMHjyYAQMGADBz5kxWr17NnDlzGD16dJ76CxYs4IMPPqBjx44ADBkyhI0bN/LVV1+xcOFCUz0rKyu8vb3zPWdCQgI//fQTixcv5qmnjP9Zzp07l2rVqrF7924aNWpU1JcpROlUtSN8eAN2fw9pt0HJNvYnTr2Zt+7CHlCnD4S+Dg4eJR9rIeR0rdDr9axZswbboAYM//UoKZnZ7L14i713TfTh4WjDsNYVAbDSqPBxsaV6WSe8nHSWCF8IIUQxslhCnJmZyYEDBxgzZoypTK1W07ZtW3bt2pXvPhkZGeh05v8Z2drasn37drOyc+fO4ePjg06no3HjxkyePBl/f38ADhw4gF6vp23btqb6VatWxd/fn127dhWYEGdkZJCRkWFaT0xMBIzdOPR6/QNc+cPJOUdJnKs0kXYpWJG1TcMhua+bjzbeDbZ3R3VyJVa/v2IsT7oK4V9C+JdkPb8ApfLT/+2cxSinPZoFuvD3W0359p8IUjJzxzE+GBXPtYR04pIyGP/niTz7t63qQf0AV15s6IdO+3g9oif/nvIn7VIwaZv8SbsUrKTbprDnUSkWGrTz6tWr+Pr6snPnTho3bmwqHzVqFFu3bmXPnj159unTpw9Hjhxh5cqVBAUFsWnTJrp06UJ2drYpWf37779JTk6mSpUqXLt2jYkTJxIdHc3x48dxdHRk8eLFDBgwwCy5BWjYsCGtW7fm//7v//KNd8KECUycODFP+eLFi7Gzs/svTSFEqVXv4kz8bpsPy3bbLpBtVSZYJqAioCiw9oqK2DSVqSwxU0VEkipPXS9bBVsNhLgbaFFWxj8WQohHTWpqKn369CEhIQEnJ6cC65WqUSamT5/O4MGDqVq1KiqViqCgIAYMGMCcOXNMdZ5+OvfOVO3atQkNDaV8+fL8+uuvDBo06KHPPWbMGEaMGGFaT0xMxM/Pj/bt29+zgYuKXq9nw4YNtGvXDq1WnozPIe1SsBJpm/SmZEXtRAlojurKfqx+6YFL2kWejcj95gedM9nN30Wp2rl4YnhAhWmXTvmU3UrJZNu5Gxy6HM+SfVcwKJiS5ovJGtZEq6lf3pXKXg74uNjSONANT0cbHGys0KjzJtOPIvn3lD9pl4JJ2+RP2qVgJd02Od/o34/FEmJ3d3c0Gk2e0R1iY2ML7P/r4eHBypUrSU9P5+bNm/j4+DB69GgCAwMLPI+LiwuVK1fm/PnzAHh7e5OZmUl8fDwuLi6FOi+AjY0NNjY2ecq1Wm2JfthL+nylhbRLwYq1bbTuUONZ4+ugFuDgjSo5JneyD4DEaKy2fAK1niueGB7Sg7aLl4uW5xvY83yD8rzVpgoxiemkZGQx6rejRMenkaY3EH7+JuHnzftYq1TQrKI7YztVp7KXAyrVo58cy7+n/Em7FEzaJn/SLgUrqbYp7DkslhBbW1sTEhLCpk2b6Nq1KwAGg4FNmzYxbNiwe+6r0+nw9fVFr9ezfPlyevbsWWDd5ORkIiIiePnllwEICQlBq9WyadMmunfvDsCZM2eIiooy67ohhHhAVjYwbC/cupBbdisSfhsAty/BxonQeCjYu1suxiLi7azD29n4PMO2Ua05HZPI9aQMzscmc+paIvsv3SYmMZ3MLAOKAuHnbhA2bRvuDjb81K8+ZV10uNhaY20lo1gIIcSjwKJdJkaMGEG/fv2oX78+DRs2ZNq0aaSkpJhGnejbty++vr5MnjwZgD179hAdHU2dOnWIjo5mwoQJGAwGRo0aZTrmyJEj6dy5M+XLl+fq1auMHz8ejUZD797G4aOcnZ0ZNGgQI0aMwM3NDScnJ958800aN24sI0wI8V/pnMEndwhEvGqB1h70KbB9inHxqQc954OLv+XiLEIatYoaPs7UAFrfNV10RlY2n64+xc6Im5y7nsyN5Ay6zMgd3nFIqyBaV/HE2kpN9bJOkiALIYSFWDQh7tWrF3FxcYwbN46YmBjq1KnD2rVr8fLyAiAqKgr1HVPCpqenM3bsWC5cuICDgwMdO3ZkwYIFZl0frly5Qu/evbl58yYeHh40a9aM3bt34+GROxTU1KlTUavVdO/enYyMDMLCwvjuu+9K7LqFeGJorKD3YuNMeGfWGMuuHoRlA2DwJsvGVgJsrDRM7FITgFVHrjJrWwTX4tO5mZIJwPdbIvh+SwQA1ho1A5oF8FQVT0LKu2IlYyALIUSJsfhDdcOGDSuwi8SWLVvM1lu2bMnJkyfvebwlS5bc95w6nY4ZM2YwY8aMQscphHhIga2MS1Is7P4OdkyD6P2QkQQ2jhYOruR0Dvahc7APALGJ6byz5DC3UzPJzDYQeSOFzGwDs7ZeYNZWY5cTdwdr+jcJoIavM02D3OXusRBCFCOLJ8RCiCeEoxe0m2ic6CM7wzjZx7YvYMd04/a+fxgT5yeAl5OOX17N7aKVmWVg06lYlh+8ws6Im6RmZnMjOZMv15811dGoVdhYqanr78LTNcui02ooY29NvfKuONvKQztCCPFfSEIshChZOmdIuQ5xZ3KTYYClfaHREKjeBbyqWy4+C7C2UvN0rbI8XassmVkGjlyJZ+aWCJLSszgaHU+63kC2QSE1M5sd52+y466RLDwdbWhfw4saPs4AqAD/MnYEujuYHv4TQghRMEmIhRAly9bFmBAfmGdenpEAWz8zLtW7wnOzwcraAgFalrWV2jjNdH83APTZBhLT9GQZFBbtieLijRQysrJJ0xs4eTWRG8kZXE/KYOHuqHyPF+hhj6Mu9w6yjZWaDzpWI9jPpSQuRwghSgVJiIUQJcvWmOhx+i/jT/cqcOOMeZ2TK6HhqxDQtERDexRpNWrKOBjHQB/RrrLZNkVRuHI7jfUnY9lz4SY5c+XdTM7gYFQ8ABfiUvIcs8uMHbSv7kXLKh4El3Ohho9TqRgfWQghioskxEKIktV8BOz6FgzZYKWDtuPh8l7Y/DGkx+fWS71Z4CGEkUqlws/NjkHNKjCoWQWzbVnZBnZfuEVGVrapLCYxnZlbI7h8y5hErz9pnBipgrs9IeVdqefnRHze/FkIIR57khALIUpW5TDjcqeywdBwsHECj+m1jWXJsWAwgFpGV3gYVho1zSrlnQTlhQb+7Iy4wboTMZy4msjJq4lE3kgh8kYKvx0AsGJx1DaaVfLA21lHcDkXNGoV9jYa6vm7yp1kIcRjSRJiIcSjw7U8BPeBI4thzUjYPs04XvGRXyD1FnjVhOBelo6yVNOoVTSv5EHzSsax2eNTM9l27ga7Im5y6loChy8ncCU+nSX7LufZ11aroZavMz0b+BFczplKXk/OsHlCiMebJMRCiEdLxTZw7FcwZEHiFfiqivl2/1BwDbBIaI8jFztrng324dlgH/R6PfOWryHDoxrJmQbjlNSJGVxNSCM+VU+aPpu9F2+x9+ItALQaFa521oQGluH9jlUp62xr4asRQoiHIwmxEOLRUqsHVH3GOInHpol5t59dB6GvlXxcTwhPW+jYogJarfnYxkevxLP1TBz7L93mRnIGJ64mos9WuJ6UwaojVzl46TYfd61J66qeBRxZCCEeXZIQCyEePVqd8eG7+gNAnw5WNrDyDTj7N/w9ChoMlr7FJax2ORdql3MxrSdnZJGQpuf0tUQ+XHmc6Pg0Bszbx7thVRjauqLlAhVCiIcg/6MIIR5dtq7gVBbs3KDZO7nli3pYLCRh5GBjha+LLW2qebF+REvKuRq7S3yx7gyfrjnF2uPXuJWSaeEohRCicOQOsRCidPBvBC7+EB8FEZtg9f+g3UdgbWfpyJ54DjZWbBzRkqofrgXgh20XTNsGNA3A2VZLs4ru1A9ws1SIQghxT3KHWAhRenS+Y6rnfT8aR6JIT4DsLMvFJADQaTXsfb8NQ1sH0SSoDFZq4/Bsc3dcZNrGc/SYuYstZ66z8/wNUjLk/RJCPFrkDrEQovQIegreOQbT64CSDYcXGReASu3BwROe/hys7S0a5pPK00nHu2FVATgXm8SiPVFkZhtYvMc4rXT/ufsAsNaomdSlBl5OOppXcsdKI/dmhBCWJQmxEKJ0cfGHD2Lg8wqQmZxbfm698aezP7QcBXdPIHFuIxxbhtquDGpD3ZKL9wlVycuRCc/WAKCatyOL917mdkomMYnpZGYbGL3iGGAcuq2Cuz39mgTQtY4v9jby35IQouTJbx4hROljZQ1DdsD04Nwyt0C4dQG2fAo7vwbncqC1g05fgm+IsXvF7Ug0gHfAG0BXCwX/5Hm5cQAvNw4AYNGeS+yMuElaZjaHom5zO1XP2dhkPvj9OB+uPM7UXnUIq+GNTquxbNBCiCeKJMRCiNLJNQBe2wY3zkHN7nBlPyzrb5zMIzMZ4k4b6x1aZEyIk2JMu9pm3rJIyAJeDC3Pi6HlAcjIyubUtSQmrznFnshbGBR4e8lhXO20tKriSeOgMjwfUk6mixZCFDvpuCWEKL3KBhsn8lCpwK8BjDgB716Afn9B42HGOrcj4cZ5yEoz7Vbz6hJUx36FjOQCDixKgo2Vhjp+Lix9rTHf9K7Lc3V9cbbVcjtVz++Hohn121FafPEPq45ctXSoQojHnCTEQojHi30ZqNAcyjc1rkdshm9D8lSz+vMNmOwLP7aFlJslHKS4W+dgH6b0qsOe99vwUZcahJR3BeDyrTTe/OUQTT/bzDtLDnHldqqFIxVCPI6ky4QQ4vHk3wjcKxvHLc5RswfZnjW4vXMe7slnjGVX9sHU6lDreeO6Sg0BzYyTgeSwsgW/hqAxn85YFD2dVmPqc/zPmet8sOIYVxPSiY5PI/pwGisPX+Xvt5tT1dvRtI90qRBC/FeSEAshHk92bjBsX55ig17PjjhfOjarg3btSOPoFFnpcGhBbqWD8/Mer9UYaDXa+Do+CpzKyfTRxax1FU92jH6KM7FJzN4WyfKDVwB4enq4qY6jzopAd3vKudrh4WhDTV9n6vq7EOThYKmwhRClkCTEQognk5MPdP8JjvySO3xbth62TDa+tnUzjlSRngDxl4zjHSdfh5MrIfUmdP4aQvpZLPwnhUqloqq3E1/1DKZNNU/e/OUQ2QbFtD0pPYsjVxI4ciXBbL/pL9Th2WAfuXsshCgUSYiFEE8unROEvmZe1vQdSL1hTIYBrh2FWc2Nd4X3/5Rbb/f3khCXsI61ytKysgcZWQYAsrINRMSlEB2fRkxCGqdjkvjr6DXAOFrFO0sPU8XLkbGdqlO1rCNudtao1ZIgCyHykoRYCCHupNXlJsMAZWvDcz/CrQjj+p5ZkHYL9CmwaRKorYx9jiu0sEy8Txh7GyvsbXLXPZ10ZtuHPZXIW78c4mxsMooCp2OSeOmnPQDU9HVi0aBGAFhpVDIJiBDCRH4bCCHE/dR+Pve1WxCseMV4xzj8K2PZ1v8DZz9o+jY0HGyZGAUAVb2dWD+8JVdup7Lnwi1+3nXR1J3ieHQiwZPWm+p2ql2WPg39qeHjhIudtaVCFkI8AiQhFkKIB3Hn6BMAai0Y9JBwGTZ/DLV7wbUjxn7Hga3M7zaLElPO1Y5yIXZ0DzG2/5gVR/ll72WzOquPXmP1v10s2lbzpG/jAFpU9ijxWIUQlicJsRBCPIiAZhDc23iHuOoz0GAQXNgCi3tCejx85pdb17c+DN5kqUjFHSY/V5tJXWoCkJFlYPjSw0TdTOVMbBIAG09dZ+Op68zoU49OtctaMlQhhAVYfMygGTNmEBAQgE6nIzQ0lL179xZYV6/XM2nSJIKCgtDpdAQHB7N27VqzOpMnT6ZBgwY4Ojri6elJ165dOXPmjFmdVq1aoVKpzJbXX3+9WK5PCPGYsbKBbjNhwBpo/IZxvXKYcVSKu0XvhysHICsDdn4DkdtKPl5hotWo0WrUONhYMbtvfdYNb8Hhce1YMKghtXydARi6+CB9Zu9mx/kbKIpynyMKIR4XFk2Ily5dyogRIxg/fjwHDx4kODiYsLAwrl+/nm/9sWPHMmvWLL755htOnjzJ66+/Trdu3Th06JCpztatWxk6dCi7d+9mw4YN6PV62rdvT0pKitmxBg8ezLVr10zL559/XqzXKoR4zLWbZPzp4g/vXzU+bAfw41PwfwGwfizM7wzTgyEzpcDDiJLlYmdN80oeLHu9Mf5udgDsjLjJiz/uoeUXWwg/F2fhCIUQJeGBEuLPP/+ctLQ00/qOHTvIyMgwrSclJfHGG28U+nhTpkxh8ODBDBgwgOrVqzNz5kzs7OyYM2dOvvUXLFjA+++/T8eOHQkMDGTIkCF07NiRr776ylRn7dq19O/fnxo1ahAcHMy8efOIioriwIEDZseys7PD29vbtDg5ORU6biGEyKPeyzDmCryxB6ztcxNkAP0d0w3fvgiHFkJkOKz7ANa+bxytIimmxEMWuXRaDSuHNuWr54NpWrEMAFG3Unn5p710+343SyLUTN90nmN3jXcshHg8PFAf4jFjxtC/f39sbW0BePrppzl8+DCBgYEApKamMmvWLL777rv7HiszM5MDBw4wZswYU5laraZt27bs2rUr330yMjLQ6cyH2LG1tWX79u0FnichwfjLy83N/OvMRYsWsXDhQry9vencuTMffvghdnZ2BR4nIyPDLPlPTEwEjN049Hp9gfsVlZxzlMS5ShNpl4JJ2+SvWNtFrcs5CdR/FZVPfdTrP0CVmQRqLarYY8btf4/Ks6sh4SrZnb8p+pgewJP+mXG0VvFsbS+ere3F5dupPD9rLzdTMjl+NRFQs+v6Bb7dcoGAMnZ0ru1N+TL2NA50w9PR5r7Hflw96Z+Zgki7FKyk26aw51EpD9BJSq1WExMTg6enJwCOjo4cOXLElBDHxsbi4+NDdnb2fY919epVfH192blzJ40bNzaVjxo1iq1bt7Jnz548+/Tp04cjR46wcuVKgoKC2LRpE126dCE7O9ssWc1hMBh49tlniY+PN0uaf/jhB8qXL4+Pjw9Hjx7lvffeo2HDhqxYsaLAeCdMmMDEiRPzlC9evPieibQQQuTwSjhE7cvzsdPfMpVFuzTEN9747MTOoFHEOdYAmV3tkaA3wNkEFfGZcD1NxZkEFddS8743XcpnE+SkUF5mixbikZOamkqfPn1ISEi4Z2+AUjXKxPTp0xk8eDBVq1ZFpVIRFBTEgAEDCuxiMXToUI4fP57nDvKrr75qel2rVi3Kli1LmzZtiIiIICgoKN9jjRkzhhEjRpjWExMT8fPzo3379iXS3UKv17NhwwbatWuHVqst9vOVFtIuBZO2yZ9l26Uj8AH62xdRR25B8ayOp299+NR4k6FJxOcozn4onjVArSG70zSwdS2x6OQzk7872yUqPpNlB65wICqew5eN30D+cUkDwFNVPJjROxgrjcWfVy8x8pnJn7RLwUq6bXK+0b8fiyXE7u7uaDQaYmNjzcpjY2Px9vbOdx8PDw9WrlxJeno6N2/exMfHh9GjR5vuUN9p2LBh/PXXX2zbto1y5e49DmhoaCgA58+fLzAhtrGxwcYm79diWq22RD/sJX2+0kLapWDSNvmzaLt4VjIuOZ79Fv4cBoAq4TKqBON4uepK7aD+AMjKhNhjoFKDVy3QFO+vbvnM5E+r1VLVx44PfVwAWHcihh3nb/DH4askpOnZfCaOz9ado3FQGZ6q6oW11ZOTGMtnJn/SLgUrqbYp7Dke+Lfqjz/+iIOD8XuhrKws5s2bh7u7O2B8qK6wrK2tCQkJYdOmTXTt2hUwdnHYtGkTw4YNu+e+Op0OX19f9Ho9y5cvp2fPnqZtiqLw5ptv8vvvv7NlyxYqVKhw31gOHz4MQNmyMvakEMIC6r4E9h4QsRn2zsot3zAO9v0IWelw87yxrP4geGaKZeIUZsJqeBNWw5sJnWvQb+5ews/dYP6uS8zfdQlnWy2VPB0YGVaFRoFlLB2qEOI+High9vf3Z/bs2aZ1b29vFixYkKdOYY0YMYJ+/fpRv359GjZsyLRp00hJSWHAgAEA9O3bF19fXyZPngzAnj17iI6Opk6dOkRHRzNhwgQMBgOjRuU+oDJ06FAWL17MH3/8gaOjIzExxie3nZ2dsbW1JSIigsWLF9OxY0fKlCnD0aNHGT58OC1atKB27doP0hxCCFE0VCqo0sG41OkNu2fC0SWQkQixx83r7v8JTv8F2XpIuwXNR0LrD0D95NyNfNSo1So+7VaLuTsuEpuUzo7zN4hP1bP/0m1e+GE31co64e9mS5CHA+XL2KFCRYMKblRwt7d06EKIfz1QQnzx4sUiPXmvXr2Ii4tj3LhxxMTEUKdOHdauXYuXlxcAUVFRqO/4JZ+ens7YsWO5cOECDg4OdOzYkQULFuDi4mKq8/333wPGyTfuNHfuXPr374+1tTUbN240Jd9+fn50796dsWPHFum1CSHEQ/GpC8/NglbvGSf1WPFK3jrJd3Q1C/8Sgp6CgKbmdc6uh2X9zId8A+Od6IHroEz+3cPEw/Fzs2Nc5+oA6LMNLN13mfk7L3LuejKnriVy6loiYN5FsG01T6yt1DQKLEP98m64O1jj6aTL5+hCiOJm8Yfqhg0bVmAXiS1btpitt2zZkpMnT97zePcbNMPPz4+tW7c+UIxCCFHi3AKNi3tF49jFZYPh+Aq4sh80Wjj1Z27deR2hcgeo3cu4Hn0Adn2b/3FT4uDidkmIi5FWo+alRuXpEVKOTaeuk5KRRXpWNkevJHAjOYMtZ4yTfWw8ZZyEas2x3DGora3UjO9cnRdDy1skdiGeVA+UEO/atYubN2/yzDPPmMp+/vlnxo8fT0pKCl27duWbb77J9+EzIYQQD8GnrnEBaDEyt3xmc4g5mrt+dq1xuVvvJVCugfH12tFwbBmk3ii+eIWJTquhU+28z6ZExCWzK+Im+mwDi/ZEkZimJzPbQHyqnswsA9M2nmNv5C2s1Gqs1CrcHa0J8nBAozYO+aZWqfB1taWunwsqGaJPiCLxQAnxpEmTaNWqlSkhPnbsGIMGDaJ///5Uq1aNL774Ah8fHyZMmFAcsQohhMjx7NewuBfU6wuXdkJ2Jmj+vRmhUoFXTajyNAS2zN3Hydf4859PwaMaOJeDsvLsREkL8nAgyMP4cPqAprkPfp+JSSJs2jbikjL44/DVQh3LwcaKip4ONK1YBo1KRVkXW15o4CeJshAP6IES4sOHD/PRRx+Z1pcsWUJoaKjpQTs/Pz/Gjx8vCbEQQhQ3n7ow8uyD7eP+71BvhixY0tv4uv0nsG82pNyEIbuLNkbxQKp4O/LL4EaciUkky6CQbVBISNNzNjaJjCwDBkXBYID4ND3nYpPIMigkZ2Rx+HI8hy/Hm47z0V8nqR/gxptPVaR+eVdJjoUohAdKiG/fvm164A1g69atPP3006b1Bg0acPny5aKLTgghRNGp3Qvio+DQQkiMNpat/8C0WTu9BmFWTlhFeUFmMti5Q58lxjvJokQ0DipD46D7D9OWrs8mPlXPxlOxXIhLwaAonLiawL6Lt0nNzGbb2Ti2nY3Dw9GGZhXd6RPqT10/lydq0hAhHsQDJcReXl5ERkbi5+dHZmYmBw8eNJvOOCkpSQagFkKIR5VGC63fNy5/DYf9eWf51GUlws1/Z3ZKugY/tIYKzcHawdgNQ2sLigECW4FrecjOKvaJQkReOq0Gb2cNLzUyf/juVkomc3dEsv/ibQ5G3SYuKYPfD0Xz+6FodFo1ge4OdKzlzdDWFeXOsRB3eKDfYh07dmT06NH83//9HytXrsTOzo7mzZubth89erTAmd6EEEI8QtqMz02I+/0F/o1RZjREdSsCxSUAVVBrODAXUq7D8eX5H8Pe03gn+ZWN4FWj5GIXBXKzt+Z/7asAxrvI/5y+zmdrT3PpZirpegMnryVy8loiX28+T3A5Z56t40ujCm5U8nK0cORCWNYDJcQfffQRzz33HC1btsTBwYF58+ZhbW1t2j5nzhzat29f5EEKIYQoYrYu8P41SLsNzsaH7bL6/Mbhv2ZTp+cYtDoH8K0Hty6AjSPEHIOMf2cjPb/R+DPFOGwYf75prNfxS6jVo+SvReRLp9XwdK2yPF2rLJlZBk7HJLLuRAw/bLtAZpaBfRdvs+/ibQD6NwlgwrPyR414cj1QQuzu7s62bdtISEjAwcEBjUZjtn3ZsmU4OspfmUIIUSpY2xmXHM5+XHVtRB2trXHmu3p9898vOQ5ij8HKoZB01TjuMcDyQZIQP6KsrdTULudC7XIuDGlVkfCzcfy0PZKj0QlkZhmYt/MiyRlZfPl8sKVDFcIiHighHjhwYKHqzZmTt1+aEEKIx4SDBzg8BfVehq3/Z+loxANysLEy3TlWFIVnv93BsegEfjtwhcQ0PeXL2BHgbk89f1eqlXWydLhClIgHSojnzZtH+fLlqVu37n1nhBNCCPGYK98UVBpQsi0diXhIKpWKP4c1pdHkTcQmZrD+pPn00kNbB/FCA3/83OwKOIIQj4cHSoiHDBnCL7/8QmRkJAMGDOCll17Czc2tuGITQgjxKAtsCe+eh89zJ5fAkA1qTcH7iEeOSqVi8/9a8dP2SDKysrlyO800MciMfyKY8U8Erat40LdJAK2reFo4WiGKxwMNSDhjxgyuXbvGqFGjWLVqFX5+fvTs2ZN169bJHWMhhHgS2bnBC4tz1ye5wdoxkHrLcjGJB2ZvY8VbbSrxblhVpr9Ql9/faEKHGt6m7f+ciWPA3H0ET1zP4J/38+HK41y+lWrBiIUoWg88QreNjQ29e/dmw4YNnDx5kho1avDGG28QEBBAcnJyccQohBDiUVa1E1TukLu++zv4sjJc3me5mMR/UtfflZkvh7DunRZ82q2WqTwhTc+Gk7Es2H2J5p//w6S/ThGfYcFAhSgi/2k0dbVajUqlQlEUsrOlD5kQQjyxei2EyX6QlWZcN+jht4HwzlGQCSBKrSrejlTxdqRXAz9upWRyMOo2uy/cZO6OiwAs2HMZsGJ72mHcHW14vr4f9fxdLRqzEA/jge8QZ2Rk8Msvv9CuXTsqV67MsWPH+Pbbb4mKisLBwaE4YhRCCPGo02hh5BkYvBlajTGWJURBwmXLxiWKhEatwsPRhrAa3ozvXIP9Y9vyXD1f0/YNp67zy97LPPfdTv45fd2CkQrxcB7oDvEbb7zBkiVL8PPzY+DAgfzyyy+4u7sXV2xCCCFKE50z+IYYl5N/wvUTsKw/6FyM3SoaDLJ0hKKIuDvYMKVnHQY18efXddtx9K3Et1suADBg3j6qeufOSeBip6Wevyv1/F0JKe+Kq711QYcVwmIeKCGeOXMm/v7+BAYGsnXrVrZu3ZpvvRUrVhRJcEIIIUqpciHGhDhn0o7IbVC+CWiswS1QulE8Jip7OVLPXaFjm4p0qOXDM99sB+B0TJJZvd0Xch+yrOfvgruDDU62WqqVdaKSpwMh5V2xt/lPvTiF+E8e6NPXt29fVPJLTAghxP20/xgqtIRsPfw9CjIS4btGxm31+sGzX1s2PlHkavo688/IVly5nTv6RGJaFvsu3uLE1QROXE0kNTObg1Hx+e7/dE1vpvaqg04rw/aJkvfAE3MIIYQQ96Vzzp3G+fZF2PuDMTnOTILLe2Dbl3DjLIS+DjumGfsde1azZMSiCFRwt6eCu71ZWafaZU2vj11J4Fh0AvpsAxfikom6lco/Z+IA+Pt4DPsv/cOApgGUsbcmpLwbFT3l2SRRMuT7CSGEEMWr9Rjjcv2U8S5x3GnY/JFx29Glxp83zsEbuywXoygRtco5U6ucs1lZVraBUb8dZcWhaOKSMvh87RnTtuplnXgmuCz9GgdIlwpRrOTTJYQQomQ4eBW8LSG65OIQjxQrjZovnw+mWlknrtxOJSYxnXUnjFNIn7yWyMlrify67zKTutREpYJavs642MmDeaJoSUIshBCiZNi6QpWOcH4TZN81m0NGAmwYb+w6odVZJj5hMWq1isEtAk3rmVkGzsQksXhvFL/sjeLizVT6ztlr2u7lZMOzwT682iIID0cbS4QsHjOSEAshhCgZKhX0/sX4OjMFfu4Kzr5w4ndj2Y5pcGU/dPseXPwtFaV4BFhbqalVzplPfWui1ajYd/E2ANcS0ohP1RObmMHs8Ehmh0fSoYY303vXwcZKHsYTD08SYiGEECXP2h5e2WB83Ww4zG5jnN3u0naYVss4NFvdl6DpO6CWROdJpVKpmNSlpmldURRiEzP437LD7Dh/E4C1J2II+Wgj/9e9ttkDfEI8iAeeqU4IIYQoUmWDYcQpYxKc49YF2DQJlvQxjk4hBMYE2dtZx6JXGnHm4w70CCkHQHJGFkMXH6TXrF1E3Uy9z1GEyEvuEAshhLA8Bw946xCcXQeLe+aWn10L5zdCuYZw8new94Bqz8rEHgIbKw1fPh9M80ruTN90jgtxKeyJvEWLL/7B18WWFpU9qO7jRFknHZW9HPF1tTXtq1Yh8yoIM5IQCyGEeHRUbAvtJkFmKmz9zFiWFAObJsLB+cb1QRvBr4HlYhSPlC51fOlc24dlBy4zOzyS89eTiY5P45e9UQXu4+9mR9/G5anj50IdPxesNPKF+ZNOEmIhhBCPDrUGmr5tfB1/CY78Atu+gMQ7hmWLv1RwQpyVCckx8lDeE0atVtGrgT+9Gvhz6loi60/EEnUrlUs3U4hNSufyrTSz+lG3Uvl49SkA3B1s+PqFOjSp6G6J0MUjwuJ/Es2YMYOAgAB0Oh2hoaHs3bu3wLp6vZ5JkyYRFBSETqcjODiYtWvXPvAx09PTGTp0KGXKlMHBwYHu3bsTGxtb5NcmhBDiP9C5GH8m3jVGcdrtgveZ39n4UN7lfcUWlni0VSvrxNttK/FVz2B+G9KE8FFPcXRCew592I5DH7bjn5GteKVZBaytjCnQjeQM+vy4h5d/2sPa4zFkZGVb+AqEJVg0IV66dCkjRoxg/PjxHDx4kODgYMLCwrh+/Xq+9ceOHcusWbP45ptvOHnyJK+//jrdunXj0KFDD3TM4cOHs2rVKpYtW8bWrVu5evUqzz33XLFfrxBCiAegc86//NQq2PIZrB4JP4XBj+3g4nbjtsu7jT8PLyyZGEWp4KTT4mpvjau9NRXc7Rn7THVOT+rA6Ker4mZvnOQj/NwNXl94gP5z9hGTkG7hiEVJs2hCPGXKFAYPHsyAAQOoXr06M2fOxM7Ojjlz5uRbf8GCBbz//vt07NiRwMBAhgwZQseOHfnqq68KfcyEhAR++uknpkyZwlNPPUVISAhz585l586d7N69u0SuWwghRCFUamd8iM6rFoyJhpajjeWRW2HLZNg325gAX9kL+34031clQ7WJe1OrVbzeMoido5+iV30/vJyME3zsunCTRpM3UX3cWnp8v5Ohiw6y9vg1Dl+Ot2zAolhZrA9xZmYmBw4cYMyYMaYytVpN27Zt2bUr//nsMzIy0OnMZzCytbVl+/bthT7mgQMH0Ov1tG3b1lSnatWq+Pv7s2vXLho1alTguTMycmdWSkxMBIzdOPT64h8SKOccJXGu0kTapWDSNvmTdinYI9c2XsHwzqnc9Tp9UWemQEayqUgduQXV7UgMKTcwnPjT9J9aNioMRXQdj1y7PEIeh7bRAB93qQZUY+GeKCb+dRqA1Mxs9l8yds9ZfewaAC0ruxNW3RNQYatV07qKB/Y2eVOpx6FdiktJt01hz2OxhPjGjRtkZ2fj5WU+t72XlxenT5/Od5+wsDCmTJlCixYtCAoKYtOmTaxYsYLs7OxCHzMmJgZra2tcXFzy1ImJiSkw3smTJzNx4sQ85evXr8fOzu6+11tUNmzYUGLnKk2kXQombZM/aZeCPdptY/4wnadbGRrf/hLDpT1YXQw3lWv2/0jWoSXctg9iT+BwUP33L0Qf7XaxrMelbdyAr0IhNQsSMmFvnJqrqSrOJxqHaNt69gZbz94w26emq4GOfgZ87fMe73Fpl+JQUm2Tmlq4calL1SgT06dPZ/DgwVStWhWVSkVQUBADBgwosItFURozZgwjRowwrScmJuLn50f79u1xcnIq9vPr9Xo2bNhAu3bt0Gq1xX6+0kLapWDSNvmTdilYaWwb1VVviPgSKyUzzzab7GS8E4/QKdCA4lHFOPKElc0Dn6M0tktJedzb5rV/f95MyeTH7ReJS8ogMV2PosDWczdQFDh+W83x22rGdKiMm701KpUKG41CeuRhOnV4PNvlvyjpz0zON/r3Y7GE2N3dHY1Gk2d0h9jYWLy9vfPdx8PDg5UrV5Kens7Nmzfx8fFh9OjRBAYGFvqY3t7eZGZmEh8fb3aX+F7nBbCxscHGJu8vUq1WW6If9pI+X2kh7VIwaZv8SbsUrFS1Tdma4FLeOBRbAaxWDDS+0FhDn6WACrS24FsfNIX/b7BUtUsJe9zbxttFy9hnapiVZWRl88Hvx/ntwBUAJq89a7a9bhk17Qwq7B7jdvkvSuozU9hzWCwhtra2JiQkhE2bNtG1a1cADAYDmzZtYtiwYffcV6fT4evri16vZ/ny5fTs2bPQxwwJCUGr1bJp0ya6d+8OwJkzZ4iKiqJx48bFc7FCCCGKh7W9cYY7fSpY6eBmBPz1bxcJtcb4AF6O7ExY0C13vdX70Oo94+v0BLBxkhnwRKHlzJT3XF1f5u68iD7bQLZBIfycsUvFoZtq6ny8md4N/QjycEClUqFWQQ0fZxpWcLNw9OJuFu0yMWLECPr160f9+vVp2LAh06ZNIyUlhQEDBgDQt29ffH19mTx5MgB79uwhOjqaOnXqEB0dzYQJEzAYDIwaNarQx3R2dmbQoEGMGDECNzc3nJycePPNN2ncuHGBD9QJIYR4hKk1YONofO1ZFQb+bXydFm+c9tm7FkT8A4cWAgqk3oKkq3D9JGz70jgDXnwUBPeGbjMtdRWilGpS0d1sUo+kdD19Zu/mWLTxq/pf9l7Os0+ghz0NyrvRtroXGjWoVSrUKhUatYqavs4428pd5ZJm0YS4V69exMXFMW7cOGJiYqhTpw5r1641PRQXFRWFWp37IER6ejpjx47lwoULODg40LFjRxYsWGDW9eF+xwSYOnUqarWa7t27k5GRQVhYGN99912JXbcQQogSYOsCtXoYX3tUgUavG18f/RVWDIbUm7D1/4x3jgEiNlskTPF4cdRpWfF6Ixb9voYzmgAyshQMioJBgd0XbnI9KYMLcSlciEth6f68ybJWo6JpRXes1GrKudpSvowdDjZWaNQqavk6U9HTeLdZFC2LP1Q3bNiwArtIbNmyxWy9ZcuWnDx58j8dE4xdLmbMmMGMGTMeKFYhhBCPAdt/v66OOZqbDAOk3ACDAdQWn8RVPAZcbWBSx+pmfVgVReFMbBLfbj7PtYR0sg05ybJCtgEibySTrjew5Uxcgcd1sLEiyMOeWuWcaRDgxjO1fdCoJUH+ryyeEAshhBAlyvHfB6jTE4w/nf0g4Qoo2ca7xg4elotNPNZUKhVVvZ34tk+9fLenZWaz8VQsqZlZZGYrnI1JIjo+jWyDwuXbqVyISyE5I4sjVxI4ciWBhbujeH/FMUZ1qGp2HI1aRVVvRyp5OpqNOGhjpcbGSiatyY8kxEIIIZ4sXjWg/cdw4yyggprd4beBkHrDOFqFJMTCQmytNXQO9ilwe1K6nrOxyZy/nsT+i7dZduAKKZnZjP/zROGOr9WwfEgTqvsU/3CxpY0kxEIIIZ4sKhU0edO8TDEYf/7YBkJfh+YjJTEWjxxHnZaQ8q6ElHelVwN/6vq7siPCfKIQFLh4M4VLN1NJzsgy25Smz6bj1+FU8XLM9/ieTja0rOyBSqWirLOOgDL2qNWg1agJdLd/rPsuS0IshBBC3Pkf/Z6ZxuWtQ+AWaLmYhLiPPqH+9An1z3eboijosxXT+l9HrzLi1yMAnIlNynefM7FJpmHj7mZjpcbXxRY7Gw3eTjpq+DgzsFmFx2ZEDEmIhRBCiI5fwG+DwDcEovcby76uC4M2gHddy8YmxENQqVRYW+X+odetri+VPB1JTNfnW//w5XjOxSahALdT9URcTyYz24DBoHAzJZOMLAMXbqQAcDw6kY2nrjN90zkC3e2p7OWIfxk7PB1tqOzlSIvKpe/bFUmIhRBCiJrdjQvAuY2w6N/XP7VD3e5jwB8MWRB3AdwrywQeotRRqVTUKudc4Pamd4ylfLeEVD1nrydhMCjEJmWw49wN05BxF26kmBLlHC83Kk+1sk5o1OBsa42bvTWVPB1wtbcumospBpIQCyGEEHeq1BbaToCNEwDQbBhLoG8frL4eCSnXwbs2dPwS/EMtGqYQJcXZTkuDgNzZ9Z4N9mFS1xrEJKRz4moicUkZRN5IYd7OiwAs2J3/VOp1/FxoGOBChax8N1uUJMRCCCHE3ZoNh3r94PMKANSKXpy7LeYozGlvTJqrdAKPypaJUQgLsrHSUL6MPeXL2JvKejf0Z9bWCNL02WQZFLINCjeTM7h0K5X4VD2HL8dz+HI8VZ3VdFeUexy95ElCLIQQQuTHzg36r8aw9n30Ny5gk3XXg0gbJxiX536EC1vg2hEYuBZsHCwQrBCWV8XbkSm96uS7LTo+jV/2RDF3ZyT1PbIfuRErZDoeIYQQoiABzcgetIm1tWaQ1fnb3HLrO5LeFa/A4YUQewwuboclL8LvQyApFi7uKPmYhXgE+brYMjKsCtvfbUkDj0fr7jDIHWIhhBCiUJSqz8DNM2BlC61Gw6k/YVl/80q/9Mp9fXSpcfa7fqugQosSjVWIR5WDzaOZej6aUQkhhBCPGmsH4wx3OWp0g/JN4eDPsPPr3KmgcyjZxp+n10DEZghsZVyEEI8cSYiFEEKIh+XgCS1GQnBvODgfYo7DmdXmdfbMBBTYPhXGRBuHbLO2z/dwQgjLkIRYCCGE+K+cfaH1+8bXC7vD+Y13bLyjv+RkX+PPMpXg2a9BbQU+dUHzeMz2JURpJQ/VCSGEEEWp60zzdXU+955unoO5T8NP7WD1/0omLiFEgeQOsRBCCFGUHDxg4DoI/woqtoXQ14zlWZlg0MO2L+HUKtCnQeIVuHood9+bEXA7MnddrQX/RmBlU7LXIMQTRhJiIYQQoqj5N4IXl5mXWVkD1tB2vHGJOQYzm8GNc8bRKm6cg9jjeY/lFgTN3oFbdyTKzuWg/kCZQlqIIiIJsRBCCGEJLv6gsYGsNDjxu/k271qQngjxl+BWBPz5Zt79PaoYR77wqAJa25KJWYjHlCTEQgghhCXonKHvSrh62Lhu42h8OM+vEVjbGcs+8QF9ivG1gzfUfA7OrstNkm9dgKrPwAuLLHEFQjw2JCEWQgghLKV8E+NSEP9Q4xjGAGVrQ4fJoBhgT4QxGQY4/RdMqw0ooLWHpz4A/yZgX6bYwxficSEJsRBCCPGosvfMfe3w72uX8nnrxV/Kfb30JeNsej51oXZPqD+geGMU4jEgCbEQQgjxqGr4KiTHGh+ea/CKscw1n4TYzh0yEiE707ielQZRO42LJMRC3JckxEIIIcSjqlyIsZ/xnSp3gOYjwbeecVi3xKvg7Aef+ecmxEKIByIJsRBCCFGaqDXQ5sPcdbcKxp+V2sHJlaCxNk+Mow9A8nXwqQeOXiUaqhClhSTEQgghxOMg7BPwrA41usKMhrnls58y/ixTEd48YJHQhHjUydTNQgghxOPAuRy0es84LnHvpXm33zwPcWdLPi4hSgFJiIUQQojHTZUO0HUmVOkInaeDlc5YPqMBTHCGbD1kJEFCNGRlWDZWIR4B0mVCCCGEeBzV6W1cAE6shAv/5G6bVhtSbxj7Gts4wfAToHOySJhCPAosfod4xowZBAQEoNPpCA0NZe/evfesP23aNKpUqYKtrS1+fn4MHz6c9PR00/aAgABUKlWeZejQoaY6rVq1yrP99ddfL7ZrFEIIISyq9xJ4bnbuetLV3AfvMhLhi4qWiUuIR4RF7xAvXbqUESNGMHPmTEJDQ5k2bRphYWGcOXMGT0/PPPUXL17M6NGjmTNnDk2aNOHs2bP0798flUrFlClTANi3bx/Z2dmmfY4fP067du14/vnnzY41ePBgJk2aZFq3s7MrpqsUQgghLEyrM07S4egNC54Dg958e3YGZKbmThktxBPGoneIp0yZwuDBgxkwYADVq1dn5syZ2NnZMWfOnHzr79y5k6ZNm9KnTx8CAgJo3749vXv3Nrur7OHhgbe3t2n566+/CAoKomXLlmbHsrOzM6vn5CRfFQkhhHjMVWgBY2Ph1S3QbZb5tm2fWyQkIR4FFrtDnJmZyYEDBxgzZoypTK1W07ZtW3bt2pXvPk2aNGHhwoXs3buXhg0bcuHCBdasWcPLL79c4DkWLlzIiBEjUKlUZtsWLVrEwoUL8fb2pnPnznz44Yf3vEuckZFBRkbugweJiYkA6PV69Hp9QbsVmZxzlMS5ShNpl4JJ2+RP2qVg0jb5eyzbxaMmeNRE+/truWXbp6Jv+cEDHeaxbJsiIO1SsJJum8KeR6UoilLMseTr6tWr+Pr6snPnTho3bmwqHzVqFFu3bmXPnj357vf1118zcuRIFEUhKyuL119/ne+//z7fur/++it9+vQhKioKHx8fU/kPP/xA+fLl8fHx4ejRo7z33ns0bNiQFStWFBjvhAkTmDhxYp7yxYsXS3cLIYQQpZL/jS3UvZz7rewfdeYTcGMzTmlR3LavxOUyzSwYnRD/XWpqKn369CEhIeGevQFKVUK8ZcsWXnjhBT7++GNCQ0M5f/48b7/9NoMHD+bDDz/MUz8sLAxra2tWrVp1z1g2b95MmzZtOH/+PEFBQfnWye8OsZ+fHzdu3CiR7hZ6vZ4NGzbQrl07tFptsZ+vtJB2KZi0Tf6kXQombZO/x71dVBGbsVrSE4DssM/RrBsFgIKKrBFnwda1wH0f97Z5WNIuBSvptklMTMTd3f2+CbHFuky4u7uj0WiIjY01K4+NjcXb2zvffT788ENefvllXnnlFQBq1apFSkoKr776Kh988AFqdW6X6EuXLrFx48Z73vXNERoaCnDPhNjGxgYbG5s85VqttkQ/7CV9vtJC2qVg0jb5k3YpmLRN/h7bdqkaZnqpObXS9FqFgjb1Ojjlfcj9bo9t2/xH0i4FK6m2Kew5LPZQnbW1NSEhIWzatMlUZjAY2LRpk9kd4zulpqaaJb0AGo0GgLtvdM+dOxdPT086dep031gOHz4MQNmyZR/kEoQQQojHw1NjjT+jdpqX/zUcdn0HF7bAyT9Bn1bioQlREiw67NqIESPo168f9evXp2HDhkybNo2UlBQGDBgAQN++ffH19WXy5MkAdO7cmSlTplC3bl1Tl4kPP/yQzp07mxJjMCbWc+fOpV+/flhZmV9iREQEixcvpmPHjpQpU4ajR48yfPhwWrRoQe3atUvu4oUQQohHRVAbCJ8C+lRABfx7k+nKXuOSo+VoaD0mvyMIUapZNCHu1asXcXFxjBs3jpiYGOrUqcPatWvx8vICICoqyuyO8NixY1GpVIwdO5bo6Gg8PDzo3Lkzn3zyidlxN27cSFRUFAMHDsxzTmtrazZu3GhKvv38/OjevTtjx44t3osVQgghHlW+9WBUJGSlg9rKOGnH2XWw7QtIuwVpt431bp4DgwHUFp/XS4giZfGpm4cNG8awYcPy3bZlyxazdSsrK8aPH8/48ePvecz27dvn6UKRw8/Pj61btz5UrEIIIcRjS6szLjnunPr5yBL4/TU4vhzO/A0+9aDbTLDP/5kfIUob+RNPCCGEEPdmVyb3tT4VLm2H7xrBrQg8Eo9CYrTlYhOiCEhCLIQQQoh7K5PPCEyZyWi/D6VJxJdYzW5hnPpZiFJKEmIhhBBC3JtbILyyGf6/vfsOi+rK/wf+HsoMoBTpRboGS8SCSrDFb8SaxRJ3NejaYk0g60o0SiJBk1/EX5Ivmk3UmKxlN8ZokrVsYtmgEStKZEWDhQgWolIsoUud8/3jhsErM7YwM8C8X88zD/eee+6953yeI/fj5c65kVuBuFtAG3/ZZkVFkfRIxS8/GqmBRL8PE2IiIiJ6uLYhQNAwwNwSCJ3dcPv5fwNJDV+SRdQcMCEmIiKix2Nb/2W6Iitv1PaZK63kpAAVRUZqFNGTY0JMREREj8f7GcDaEUJhhqvO/wP1M6/WbzuQYLx2ET0ho0+7RkRERM2MnQcw/yJqKstxOekAOlo7AL79pNknMv4FFF+rr2vZChi4CHDw5fzF1GQxISYiIqLHZ24BWFrXr0esBD7uCZQVAOe/ldc9swVw7QzMPiTt98uPwNdTgeoyoF8M0Pcvhmw5UQNMiImIiOj3c24PTNoB3MmuL7uRDpz6XFouOAucWAMU5gC/nKi/i5wUB6R+CihbA9N2AzaOhm45ERNiIiIiaiSB/yN97hXxIbDcF6gqAb5frH2/ol+kn1eOAJ1G6reNRFrwYR4iIiLSHzPzhnd9+88Hhr/XsC7feEdGwoSYiIiI9EtlW7/c4Q/AoDjtcxkXXWtYRmQATIiJiIhIv5St65dbu9Yvd4yQfto4ST8vHQT+NQPY+mfgp28M1z4yeXyGmIiIiPTLzrN+2TGgfnnMp0Dfc9IX7b6ZBuT/JH0A4MpRoMsfDdtOMllMiImIiEi/hr4LtO0FWFoBwePry5U2QNuegJ0XYGUvf8tdRREgBKBQGL69ZHKYEBMREZF+2XkCYa88YLsH8NrPQGWJtP5BO0DUAsfXAP+JlcrMVVJy/MzLQPgSvTeZTAufISYiIiLjs7QCWrsA1m3qy+qSYQCorQRqKoAjK4DEzsCxjwzfRmqxmBATERFR02FuId0Nvt+U7+qXi6/pntOY6AkwISYiIqKmRdlKvt7nL4BTO+O0hUwCE2IiIiJqWqzs65eHvAsMeUc+l3GdsluGaxO1aEyIiYiIqGkZ8v9+e4FHPNAnWiq7/64xAKwdYNh2UYvFWSaIiIioaen4B+lzL4VCemzidlZ9WfF14Nu5wIDXgfTNgF9fwLePtC31M+DcTmm/ni8BnccYrv3U7DAhJiIioubhpe+BG6cA96eB/w2SytI2AleO1CfKnj2A8HggKR6oLpPKSvKZENMDMSEmIiKi5qGVE9A+vGH5vXeNb/wX+Oco+fayAv22i5o9PkNMREREzY+d16PXvfsrUFujv7ZQs8c7xERERNT8jPsc+PtzD66jMAcgAKEGNr0A2LeVyn2eAXpM1nsTqflgQkxERETNT9sQwDkIuJWpu46VHeD2NHDlMHD5YH15+hfAU8OA1q76byc1C0yIiYiIqHlSVz94u9IWGLMWOLsdUP/2yMSJT4CSXCB5OfCHRP23kZoFoz9DvGrVKvj5+cHKygqhoaFITU19YP2VK1ciKCgI1tbW8Pb2xrx581BRUaHZvmTJEigUCtmnQ4cOsmNUVFQgKioKTk5OaN26NcaOHYv8/Hy99I+IiIj0ZOTH0muehyYAbXtJZebK+u1PDQXsvaS5jPv9Vfq4/JYTnFwHrH0WOPy/wKaxwPU0Q7eemhCj3iHeunUrYmJi8MknnyA0NBQrV67E0KFDkZmZCVfXhn/G2Lx5MxYtWoT169ejT58++PnnnzF16lQoFAokJtb/L69z587Yt2+fZt3CQt7NefPmYdeuXfj6669hb2+P6OhovPDCCzh69Kj+OktERESNy68vEHsNsFBKcw1XlQE2jtLdYKEGLFQN9wmdDVw6IC3npksfAKgsBQYuBPz6A+aWhuoBNRFGvUOcmJiImTNnYtq0aejUqRM++eQT2NjYYP369VrrHzt2DH379sWECRPg5+eHIUOGIDIyssFdZQsLC7i7u2s+zs7Omm1FRUVYt24dEhMT8dxzzyEkJAQbNmzAsWPHcPz4cb32l4iIiBqZxW93hC2tpGnZFAopodWWDAOAY4D28l+OA5+PkR6pIJNjtDvEVVVVSEtLQ2xsrKbMzMwM4eHhSElJ0bpPnz59sGnTJqSmpqJ37964dOkSdu/ejUmTJsnqXbx4EZ6enrCyskJYWBgSEhLg4+MDAEhLS0N1dTXCw+vnMezQoQN8fHyQkpKCZ555Ruu5KysrUVlZqVkvLi4GAFRXV6O6+iHPMDWCunMY4lzNCeOiG2OjHeOiG2OjHeOiW7OMjUMAFH/4CGZXDsEs42tNsbBxhqL8FtTX/4va39mfZhkXAzF0bB71PEZLiG/duoXa2lq4ubnJyt3c3HDhwgWt+0yYMAG3bt1Cv379IIRATU0N5syZgzfeeENTJzQ0FBs3bkRQUBByc3OxdOlS9O/fHxkZGbC1tUVeXh6USiUcHBwanDcvL09nexMSErB06dIG5d9//z1sbGweo+e/T1JSksHO1ZwwLroxNtoxLroxNtoxLro1v9jYw9xsMPpZn0Drynzk2XXHDYee6H3lY5id3YZTpa6wqSqA7+1DuNW6I075zJDuPD+m5hcXwzFUbMrLyx+pXrOaZSI5ORnLli3D6tWrERoaiqysLMydOxfvvPMO4uLiAADDhw/X1A8ODkZoaCh8fX3x1VdfYfr06U987tjYWMTExGjWi4uL4e3tjSFDhsDOzu7JO/WIqqurkZSUhMGDB8PSks821WFcdGNstGNcdGNstGNcdGv+sXkBAoAbALe7vwKJHwMAQq7WPzbhc+cwPCZ/BrRy1n4ILZp/XPTH0LGp+4v+wxgtIXZ2doa5uXmD2R3y8/Ph7u6udZ+4uDhMmjQJM2bMAAB06dIFZWVlmDVrFt58802YmTV8JNrBwQFPPfUUsrKk1zq6u7ujqqoKhYWFsrvEDzovAKhUKqhUDZ9HsrS0NOhgN/T5mgvGRTfGRjvGRTfGRjvGRbcWERtLV+C1n4HEjoColW+qKgIcPIDKEunLd4D0zLJ1mwcfsiXERU8MFZtHPYfRvlSnVCoREhKC/fv3a8rUajX279+PsLAwrfuUl5c3SHrNzc0BAEIIrfuUlpYiOzsbHh4eAICQkBBYWlrKzpuZmYmcnByd5yUiIiITYOsGvJkLTN4JeHSrLz/6IbB7AZDQFkjsIH3eCwC+jATuXDJac6nxGPWRiZiYGEyZMgU9e/ZE7969sXLlSpSVlWHatGkAgMmTJ8PLywsJCQkAgIiICCQmJqJ79+6aRybi4uIQERGhSYznz5+PiIgI+Pr64saNG4iPj4e5uTkiIyMBAPb29pg+fTpiYmLg6OgIOzs7vPrqqwgLC9P5hToiIiIyERYqIGAgMPsgsPEP0lvuTm9uWE+ogczdgJU9MIYzUzR3Rk2Ix48fj5s3b+Ktt95CXl4eunXrhr1792q+aJeTkyO7I7x48WIoFAosXrwY169fh4uLCyIiIvDuu+9q6ly7dg2RkZG4ffs2XFxc0K9fPxw/fhwuLi6aOitWrICZmRnGjh2LyspKDB06FKtXrzZcx4mIiKjpGxQvTcOW8Y20buMMzNgHOPoD/5oJ/PQVUFpQXz9tI3DrIszMlLCq9jVKk+nJGP1LddHR0YiOjta6LTk5WbZuYWGB+Ph4xMfH6zzeli1bHnpOKysrrFq1CqtWrXqsthIREZEJ8e4lfcKXALmngQ7P18820Xm0lBBX/valrYILwLdzAQDmANo7hwOYaIRG05MwekJMRERE1KQ5eEufe6lspZ8VvyXExddkmwNu7UODGXBLCwBlK+lDTQoTYiIiIqLHpfptytVbmUDqZ9KzxABg7QjcvQMAsHzXGTD/7U16Qi29UtrOC5h7mq+HbmKYEBMRERE9LlsPAAoAAtg9v748YCBwdlv9em2VfL/i60BJXsM7zmRURpt2jYiIiKjZsnUDJn4NtL7vHQa+faD2vmfWqvAlwLyz0qfubvHZ7UCJ/D0MZFy8Q0xERET0JNoPBl67AJTkAupawMIKaO0CXE2pr+PRFbBvKy07tQMKzgFJccC5ncDM396JoK4FbpwCqkoB92DAxtHwfTFxTIiJiIiInpRCAdh5yopq+83H1fxf4dMxBOb+z9ZvuPf1z9dP1i+nfgbsXSgtWzsCc44ACjPpbXgVxYCtuzQ/MukNE2IiIiKixuTcHme8p6LtgBEwNzOvLx/wOvDrFaAwR1r/7Dnp569X6uvcvQOs6CQ/nmMgEP0j8PN/gMxdUtLcyhnw6gn49dVnT0wGE2IiIiIiQ/DvD0SnAf8bJCW+19Mebb872UBOCvDNNKCmor5cYQ68dbt+bmR6YkyIiYiIiAzFQgnMPgTkn5WX23kCa/tLy/3nS49R1FZKb8crvgZsfL7hsUSt9GKQuinf6IkxISYiIiIyJG0v+gCAPn8Bzn8LhEUBvaZLcxef2wkk/3+gskiqo7KTvoRXXSatF99omBBXVwBfTwV+vSwvb+UCjF0nzZBBMkyIiYiIiJqCIe9In3uFRUmfczuBtI1A10jgqWHA8t8S6tXPSHeU73XlMPDLiYbHv3kByNwN9Jyml+Y3Z0yIiYiIiJq6TqOkT50xa4Hts6Xlwx9o36dtL2DQW9Jyyirg571A2U39trOZYkJMRERE1Nx0fRGwtAGuHJGX38kGsvZJy+7BgP8AafnyISkhTv1MukusjZ0XYN1GWvbuLd2NztoHVJUBAf8DtHLST1+aACbERERERM1Rp5HS536HE6Uv7T3zcn2ZW2fpZ1mB9NHmxqn65VObgPLbwL4l0nqHPwAvftEozW6KmBATERERtST9YxqWdRoNTE8C7v6qfZ/Kkt/mRxbA4RVAVQmQfaB+e9Y+YP0wQNkKGLAA+OlroLIUeGoI0MoVaO0KuATpozcGwYSYiIiIqKVTKKTHIB7FT99Ir5i+kV5fVlMhzYUM1D+SAQBntvx2fHNgbjrg4NMYrTU4M2M3gIiIiIiaEPu20s+6qd7ClwLjPgd6zZSmeLOyr3/WuI6oBb79q0Gb2Zh4h5iIiIiI6oUvAdr4A+pqoLWb9CyyhUp6Xvn5e2a0WHLf/MfZ+4EvJwDjNwFmzeueKxNiIiIiIqrn1hkY8d7D64W+DKSuBRRmgLpGKsvcBeSmA1499NrExsaEmIiIiIge3/DlwNBl0uMSe2OBHz+TyjePAwIGStPCte0F2DgBpflA+8GATdN8Sx4TYiIiIiJ6MmZmAMyA4e8BP/8HKMqRXv7x09fS9v/+Q1bdvN0QWFqPangcI2NCTERERES/j5kZMGk7kPIxINSAnac0r3FpPlBbDeRnSNWyvoe7jy+A8cZt732YEBMRERHR7+fcDohYqX1b2S3g457A3V+hqik2aLMeRfP6CiARERERNT+tnIFuEwEASibERERERGSSbBwBAKqaUiM3pCE+MkFERERE+ucYCLVXL5SpXYzdkgZ4h5iIiIiI9K/zaNRO3YOf3UcbuyUNMCEmIiIiIpNm9IR41apV8PPzg5WVFUJDQ5GamvrA+itXrkRQUBCsra3h7e2NefPmoaKiQrM9ISEBvXr1gq2tLVxdXTF69GhkZmbKjjFw4EAoFArZZ86cOXrpHxERERE1bUZNiLdu3YqYmBjEx8fjv//9L7p27YqhQ4eioKBAa/3Nmzdj0aJFiI+Px/nz57Fu3Tps3boVb7zxhqbOwYMHERUVhePHjyMpKQnV1dUYMmQIysrKZMeaOXMmcnNzNZ/33nuEVxQSERERUYtj1C/VJSYmYubMmZg2bRoA4JNPPsGuXbuwfv16LFq0qEH9Y8eOoW/fvpgwYQIAwM/PD5GRkThx4oSmzt69e2X7bNy4Ea6urkhLS8OAAQM05TY2NnB3d9dHt4iIiIioGTFaQlxVVYW0tDTExsZqyszMzBAeHo6UlBSt+/Tp0webNm1CamoqevfujUuXLmH37t2YNGmSzvMUFRUBABwdHWXlX3zxBTZt2gR3d3dEREQgLi4ONjY2Oo9TWVmJyspKzXpxsTSHXnV1Naqrqx/e4d+p7hyGOFdzwrjoxthox7joxthox7joxthox7joZujYPOp5FEIIoee2aHXjxg14eXnh2LFjCAsL05S//vrrOHjwoOyu773+9re/Yf78+RBCoKamBnPmzMGaNWu01lWr1Rg5ciQKCwtx5MgRTfmnn34KX19feHp64syZM1i4cCF69+6Nbdu26WzvkiVLsHTp0gblmzdvfmAiTURERETGUV5ejgkTJqCoqAh2dnY66zWreYiTk5OxbNkyrF69GqGhocjKysLcuXPxzjvvIC4urkH9qKgoZGRkyJJhAJg1a5ZmuUuXLvDw8MCgQYOQnZ2NwMBAreeOjY1FTEyMZr24uBje3t4YMmTIAwPcWKqrq5GUlITBgwfD0tJS7+drLhgX3Rgb7RgX3Rgb7RgX3Rgb7RgX3Qwdm7q/6D+M0RJiZ2dnmJubIz8/X1aen5+v89neuLg4TJo0CTNmzAAgJbNlZWWYNWsW3nzzTZiZ1X9HMDo6Gt999x0OHTqEtm3bPrAtoaGhAICsrCydCbFKpYJKpWpQbmlpadDBbujzNReMi26MjXaMi26MjXaMi26MjXaMi26Gis2jnsNos0wolUqEhIRg//79mjK1Wo39+/fLHqG4V3l5uSzpBQBzc3MAQN2TH0IIREdHY/v27fjhhx/g7+//0Lakp6cDADw8PJ6kK0RERETUjBn1kYmYmBhMmTIFPXv2RO/evbFy5UqUlZVpZp2YPHkyvLy8kJCQAACIiIhAYmIiunfvrnlkIi4uDhEREZrEOCoqCps3b8bOnTtha2uLvLw8AIC9vT2sra2RnZ2NzZs3Y8SIEXBycsKZM2cwb948DBgwAMHBwcYJBBEREREZjVET4vHjx+PmzZt46623kJeXh27dumHv3r1wc3MDAOTk5MjuCC9evBgKhQKLFy/G9evX4eLigoiICLz77ruaOnVfsBs4cKDsXBs2bMDUqVOhVCqxb98+TfLt7e2NsWPHYvHixfrvMBERERE1OUb/Ul10dDSio6O1bktOTpatW1hYID4+HvHx8TqP97BJM7y9vXHw4MHHbqeu8zzqw9q/V3V1NcrLy1FcXMznke7BuOjG2GjHuOjG2GjHuOjG2GjHuOhm6NjU5WkPyw+NnhA3VyUlJQCkBJuIiIiImq6SkhLY29vr3G60eYibO7VajRs3bsDW1hYKhULv56ub5u2XX34xyDRvzQXjohtjox3johtjox3johtjox3jopuhYyOEQElJCTw9PRtMzHAv3iF+QmZmZg+dzk0f7Ozs+I9LC8ZFN8ZGO8ZFN8ZGO8ZFN8ZGO8ZFN0PG5kF3husYbdo1IiIiIqKmgAkxEREREZk0JsTNhEqlQnx8vNa35ZkyxkU3xkY7xkU3xkY7xkU3xkY7xkW3phobfqmOiIiIiEwa7xATERERkUljQkxEREREJo0JMRERERGZNCbERERERGTSmBA3A6tWrYKfnx+srKwQGhqK1NRUYzdJrxISEtCrVy/Y2trC1dUVo0ePRmZmpqzOwIEDoVAoZJ85c+bI6uTk5OD555+HjY0NXF1dsWDBAtTU1BiyK41uyZIlDfrdoUMHzfaKigpERUXByckJrVu3xtixY5Gfny87RkuMi5+fX4O4KBQKREVFATCt8XLo0CFERETA09MTCoUCO3bskG0XQuCtt96Ch4cHrK2tER4ejosXL8rq3LlzBxMnToSdnR0cHBwwffp0lJaWyuqcOXMG/fv3h5WVFby9vfHee+/pu2u/y4PiUl1djYULF6JLly5o1aoVPD09MXnyZNy4cUN2DG3jbPny5bI6zS0uwMPHzNSpUxv0e9iwYbI6pjZmAGj9naNQKPD+++9r6rTEMfMo1+jGuhYlJyejR48eUKlUaNeuHTZu3Ki/jglq0rZs2SKUSqVYv369OHv2rJg5c6ZwcHAQ+fn5xm6a3gwdOlRs2LBBZGRkiPT0dDFixAjh4+MjSktLNXWeffZZMXPmTJGbm6v5FBUVabbX1NSIp59+WoSHh4tTp06J3bt3C2dnZxEbG2uMLjWa+Ph40blzZ1m/b968qdk+Z84c4e3tLfbv3y9OnjwpnnnmGdGnTx/N9pYal4KCAllMkpKSBABx4MABIYRpjZfdu3eLN998U2zbtk0AENu3b5dtX758ubC3txc7duwQp0+fFiNHjhT+/v7i7t27mjrDhg0TXbt2FcePHxeHDx8W7dq1E5GRkZrtRUVFws3NTUycOFFkZGSIL7/8UlhbW4u1a9caqpuP7UFxKSwsFOHh4WLr1q3iwoULIiUlRfTu3VuEhITIjuHr6yvefvtt2Ti69/dSc4yLEA8fM1OmTBHDhg2T9fvOnTuyOqY2ZoQQsnjk5uaK9evXC4VCIbKzszV1WuKYeZRrdGNciy5duiRsbGxETEyMOHfunPjoo4+Eubm52Lt3r176xYS4ievdu7eIiorSrNfW1gpPT0+RkJBgxFYZVkFBgQAgDh48qCl79tlnxdy5c3Xus3v3bmFmZiby8vI0ZWvWrBF2dnaisrJSn83Vq/j4eNG1a1et2woLC4WlpaX4+uuvNWXnz58XAERKSooQouXG5X5z584VgYGBQq1WCyFMd7zcfxFXq9XC3d1dvP/++5qywsJCoVKpxJdffimEEOLcuXMCgPjxxx81dfbs2SMUCoW4fv26EEKI1atXizZt2shis3DhQhEUFKTnHjUObcnN/VJTUwUAcfXqVU2Zr6+vWLFihc59mntchNAemylTpohRo0bp3IdjRjJq1Cjx3HPPycpMYczcf41urGvR66+/Ljp37iw71/jx48XQoUP10g8+MtGEVVVVIS0tDeHh4ZoyMzMzhIeHIyUlxYgtM6yioiIAgKOjo6z8iy++gLOzM55++mnExsaivLxcsy0lJQVdunSBm5ubpmzo0KEoLi7G2bNnDdNwPbl48SI8PT0REBCAiRMnIicnBwCQlpaG6upq2Xjp0KEDfHx8NOOlJcelTlVVFTZt2oSXXnoJCoVCU26q4+Vely9fRl5enmyM2NvbIzQ0VDZGHBwc0LNnT02d8PBwmJmZ4cSJE5o6AwYMgFKp1NQZOnQoMjMz8euvvxqoN/pVVFQEhUIBBwcHWfny5cvh5OSE7t274/3335f9ibclxyU5ORmurq4ICgrCyy+/jNu3b2u2ccwA+fn52LVrF6ZPn95gW0sfM/dfoxvrWpSSkiI7Rl0dfeU/Fno5KjWKW7duoba2VjZgAMDNzQ0XLlwwUqsMS61W469//Sv69u2Lp59+WlM+YcIE+Pr6wtPTE2fOnMHChQuRmZmJbdu2AQDy8vK0xq1uW3MVGhqKjRs3IigoCLm5uVi6dCn69++PjIwM5OXlQalUNriAu7m5afrcUuNyrx07dqCwsBBTp07VlJnqeLlfXV+09fXeMeLq6irbbmFhAUdHR1kdf3//Bseo29amTRu9tN9QKioqsHDhQkRGRsLOzk5T/pe//AU9evSAo6Mjjh07htjYWOTm5iIxMRFAy43LsGHD8MILL8Df3x/Z2dl44403MHz4cKSkpMDc3JxjBsA//vEP2Nra4oUXXpCVt/Qxo+0a3VjXIl11iouLcffuXVhbWzdqX5gQU5MWFRWFjIwMHDlyRFY+a9YszXKXLl3g4eGBQYMGITs7G4GBgYZupsEMHz5csxwcHIzQ0FD4+vriq6++avRfDs3VunXrMHz4cHh6emrKTHW80OOrrq7GuHHjIITAmjVrZNtiYmI0y8HBwVAqlZg9ezYSEhKa3GtoG9OLL76oWe7SpQuCg4MRGBiI5ORkDBo0yIgtazrWr1+PiRMnwsrKSlbe0seMrmt0c8RHJpowZ2dnmJubN/hmZn5+Ptzd3Y3UKsOJjo7Gd999hwMHDqBt27YPrBsaGgoAyMrKAgC4u7trjVvdtpbCwcEBTz31FLKysuDu7o6qqioUFhbK6tw7Xlp6XK5evYp9+/ZhxowZD6xnquOlri8P+p3i7u6OgoIC2faamhrcuXOnxY+jumT46tWrSEpKkt0d1iY0NBQ1NTW4cuUKgJYbl/sFBATA2dlZ9u/HVMcMABw+fBiZmZkP/b0DtKwxo+sa3VjXIl117Ozs9HIDiAlxE6ZUKhESEoL9+/drytRqNfbv34+wsDAjtky/hBCIjo7G9u3b8cMPPzT4c5I26enpAAAPDw8AQFhYGH766SfZL+m6C1ynTp300m5jKC0tRXZ2Njw8PBASEgJLS0vZeMnMzEROTo5mvLT0uGzYsAGurq54/vnnH1jPVMeLv78/3N3dZWOkuLgYJ06ckI2RwsJCpKWlaer88MMPUKvVmv9IhIWF4dChQ6iurtbUSUpKQlBQUJP/E68udcnwxYsXsW/fPjg5OT10n/T0dJiZmWkeF2iJcdHm2rVruH37tuzfjymOmTrr1q1DSEgIunbt+tC6LWHMPOwa3VjXorCwMNkx6uroLf/Ry1f1qNFs2bJFqFQqsXHjRnHu3Dkxa9Ys4eDgIPtmZkvz8ssvC3t7e5GcnCybqqa8vFwIIURWVpZ4++23xcmTJ8Xly5fFzp07RUBAgBgwYIDmGHVTugwZMkSkp6eLvXv3ChcXl2Y5jda9XnvtNZGcnCwuX74sjh49KsLDw4Wzs7MoKCgQQkhT3fj4+IgffvhBnDx5UoSFhYmwsDDN/i01LkJIM7D4+PiIhQsXyspNbbyUlJSIU6dOiVOnTgkAIjExUZw6dUozW8Ly5cuFg4OD2Llzpzhz5owYNWqU1mnXunfvLk6cOCGOHDki2rdvL5tCq7CwULi5uYlJkyaJjIwMsWXLFmFjY9Okp4p6UFyqqqrEyJEjRdu2bUV6errs907dN96PHTsmVqxYIdLT00V2drbYtGmTcHFxEZMnT9acoznGRYgHx6akpETMnz9fpKSkiMuXL4t9+/aJHj16iPbt24uKigrNMUxtzNQpKioSNjY2Ys2aNQ32b6lj5mHXaCEa51pUN+3aggULxPnz58WqVas47Zqp++ijj4SPj49QKpWid+/e4vjx48Zukl4B0PrZsGGDEEKInJwcMWDAAOHo6ChUKpVo166dWLBggWxeWSGEuHLlihg+fLiwtrYWzs7O4rXXXhPV1dVG6FHjGT9+vPDw8BBKpVJ4eXmJ8ePHi6ysLM32u3fvildeeUW0adNG2NjYiDFjxojc3FzZMVpiXIQQ4j//+Y8AIDIzM2XlpjZeDhw4oPXfz5QpU4QQ0tRrcXFxws3NTahUKjFo0KAGMbt9+7aIjIwUrVu3FnZ2dmLatGmipKREVuf06dOiX79+QqVSCS8vL7F8+XJDdfGJPCguly9f1vl7p24u67S0NBEaGirs7e2FlZWV6Nixo1i2bJksKRSi+cVFiAfHpry8XAwZMkS4uLgIS0tL4evrK2bOnNngpoypjZk6a9euFdbW1qKwsLDB/i11zDzsGi1E412LDhw4ILp16yaUSqUICAiQnaOxKX7rHBERERGRSeIzxERERERk0pgQExEREZFJY0JMRERERCaNCTERERERmTQmxERERERk0pgQExEREZFJY0JMRERERCaNCTERERERmTQmxERE1Og2btwIBwcHYzeDiOiRMCEmIjKivLw8zJ07F+3atYOVlRXc3NzQt29frFmzBuXl5cZu3iPx8/PDypUrZWXjx4/Hzz//bJwGERE9JgtjN4CIyFRdunQJffv2hYODA5YtW4YuXbpApVLhp59+wqeffgovLy+MHDnSKG0TQqC2thYWFk92mbC2toa1tXUjt4qISD94h5iIyEheeeUVWFhY4OTJkxg3bhw6duyIgIAAjBo1Crt27UJERAQAoLCwEDNmzICLiwvs7Ozw3HPP4fTp05rjLFmyBN26dcPnn38OPz8/2Nvb48UXX0RJSYmmjlqtRkJCAvz9/WFtbY2uXbvim2++0WxPTk6GQqHAnj17EBISApVKhSNHjiA7OxujRo2Cm5sbWrdujV69emHfvn2a/QYOHIirV69i3rx5UCgUUCgUALQ/MrFmzRoEBgZCqVQiKCgIn3/+uWy7QqHA3//+d4wZMwY2NjZo3749/v3vfzdavImIdGFCTERkBLdv38b333+PqKgotGrVSmuduuTyT3/6EwoKCrBnzx6kpaWhR48eGDRoEO7cuaOpm52djR07duC7777Dd999h4MHD2L58uWa7QkJCfjnP/+JTz75BGfPnsW8efPw5z//GQcPHpSdc9GiRVi+fDnOnz+P4OBglJaWYsSIEdi/fz9OnTqFYcOGISIiAjk5OQCAbdu2oW3btnj77beRm5uL3NxcrX3Zvn075s6di9deew0ZGRmYPXs2pk2bhgMHDsjqLV26FOPGjcOZM2cwYsQITJw4UdZPIiK9EEREZHDHjx8XAMS2bdtk5U5OTqJVq1aiVatW4vXXXxeHDx8WdnZ2oqKiQlYvMDBQrF27VgghRHx8vLCxsRHFxcWa7QsWLBChoaFCCCEqKiqEjY2NOHbsmOwY06dPF5GRkUIIIQ4cOCAAiB07djy07Z07dxYfffSRZt3X11esWLFCVmfDhg3C3t5es96nTx8xc+ZMWZ0//elPYsSIEZp1AGLx4sWa9dLSUgFA7Nmz56FtIiL6PfgMMRFRE5Kamgq1Wo2JEyeisrISp0+fRmlpKZycnGT17t69i+zsbM26n58fbG1tNeseHh4oKCgAAGRlZaG8vByDBw+WHaOqqgrdu3eXlfXs2VO2XlpaiiVLlmDXrl3Izc1FTU0N7t69q7lD/KjOnz+PWbNmycr69u2LDz/8UFYWHBysWW7VqhXs7Ow0/SAi0hcmxERERtCuXTsoFApkZmbKygMCAgBA84W00tJSeHh4IDk5ucEx7n1G19LSUrZNoVBArVZrjgEAu3btgpeXl6yeSqWSrd//+Mb8+fORlJSEDz74AO3atYO1tTX++Mc/oqqq6hF7+nge1A8iIn1hQkxEZAROTk4YPHgwPv74Y7z66qs6nyPu0aMH8vLyYGFhAT8/vyc6V6dOnaBSqZCTk4Nnn332sfY9evQopk6dijFjxgCQkusrV67I6iiVStTW1j7wOB07dsTRo0cxZcoU2bE7der0WO0hItIHJsREREayevVq9O3bFz179sSSJUsQHBwMMzMz/Pjjj7hw4QJCQkIQHh6OsLAwjB49Gu+99x6eeuop3LhxA7t27cKYMWMaPOKgja2tLebPn4958+ZBrVajX79+KCoqwtGjR2FnZydLUu/Xvn17bNu2DREREVAoFIiLi2twx9bPzw+HDh3Ciy++CJVKBWdn5wbHWbBgAcaNG4fu3bsjPDwc3377LbZt2yabsYKIyFiYEBMRGUlgYCBOnTqFZcuWITY2FteuXYNKpUKnTp0wf/58vPLKK1AoFNi9ezfefPNNTJs2DTdv3oS7uzsGDBgANze3Rz7XO++8AxcXFyQkJODSpUtwcHBAjx498MYbbzxwv8TERLz00kvo06cPnJ2dsXDhQhQXF8vqvP3225g9ezYCAwNRWVkJIUSD44wePRoffvghPvjgA8ydOxf+/v7YsGEDBg4c+Mh9ICLSF4XQ9puLiIiIiMhEcB5iIiIiIjJpTIiJiIiIyKQxISYiIiIik8aEmIiIiIhMGhNiIiIiIjJpTIiJiIiIyKQxISYiIiIik8aEmIiIiIhMGhNiIiIiIjJpTIiJiIiIyKQxISYiIiIik/Z/u9Rv6XXA9osAAAAASUVORK5CYII="/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=c162711b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Best-implementation-from-previous-experiments,-now-increasing-tournament-size:">Best implementation from previous experiments, now increasing tournament size:<a class="anchor-link" href="#Best-implementation-from-previous-experiments,-now-increasing-tournament-size:"></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8ff057af">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Genetic Algorithm (GA) with Larger Tournament Size for FFN Weight Optimisation</span>
<span class="sd">===============================================================================</span>

<span class="sd">This script evolves the weights of a fixed-architecture feed-forward neural network (FFN)</span>
<span class="sd">using a real-valued Genetic Algorithm. The crossover operator is BLX- with  = 0.6,</span>
<span class="sd">and mutation is Gaussian. The key difference in this version is an **increased tournament</span>
<span class="sd">size (tourn_size = 5)** to apply stronger selective pressure during parent selection.</span>

<span class="sd">Key Features:</span>
<span class="sd">-------------</span>
<span class="sd">- Fixed FFN: 2 layers  24 units, ReLU activations</span>
<span class="sd">- Xavier normal initialisation for consistency</span>
<span class="sd">- Real-valued genome (vector of FFN weights)</span>
<span class="sd">- BLX- crossover ( = 0.6) for diversity</span>
<span class="sd">- Gaussian mutation (1% mutation rate,  = 0.01)</span>
<span class="sd">- Tournament selection with size = 5 ( selective pressure)</span>
<span class="sd">- Elitism: top 20% of population preserved each generation</span>

<span class="sd">Hyperparameters:</span>
<span class="sd">----------------</span>
<span class="sd">- `pop_size`: 200</span>
<span class="sd">- `generations`: 2000</span>
<span class="sd">- `elite_frac`: 0.2</span>
<span class="sd">- `tourn_size`: 5   Increased from earlier runs</span>
<span class="sd">- `blx_alpha`: 0.6</span>
<span class="sd">- `mutation_p`: 0.01</span>
<span class="sd">- `mutation_sd`: 0.01</span>

<span class="sd">Outputs:</span>
<span class="sd">--------</span>
<span class="sd">- Printed MSE every 100 generations</span>
<span class="sd">- Final train/val MSE for best individual</span>
<span class="sd">- Plot of training and validation loss curves</span>

<span class="sd">"""</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>

<span class="c1">#  0) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  1) Data to device </span>
<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">#  2) Fixed Architecture + Xavier init </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"ReLU"</span><span class="p">)</span>
<span class="n">init_scheme</span> <span class="o">=</span> <span class="s2">"xavier_normal"</span>
<span class="n">criterion</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
    <span class="n">layers</span><span class="p">,</span> <span class="n">in_f</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>

<span class="c1">#  3) GA Hyperparams </span>
<span class="n">pop_size</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">generations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">elite_frac</span>  <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">tourn_size</span>  <span class="o">=</span> <span class="mi">5</span>
<span class="n">mutation_p</span>  <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">mutation_sd</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">blx_alpha</span>   <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1">#  4) Init Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pop_size</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">skPCA</span>
<span class="c1">#  visualize initial population </span>
<span class="n">pop_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>         
<span class="n">genome_len</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#  5) Tournament Selection </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pop_size</span><span class="p">,</span> <span class="n">tourn_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>
<span class="c1">#  6) Evolution </span>
<span class="n">train_curve</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_curve</span>   <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># a) Fitness eval</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gen</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">gen</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">generations</span><span class="si">}</span><span class="s2">  train MSE: </span><span class="si">{</span><span class="n">tr_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">va_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">tr_mse</span>   <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
    <span class="n">m_best</span>   <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">va_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tr_mse</span><span class="p">)</span>
    <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">va_mse</span><span class="p">)</span>
    <span class="c1"># b) Elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">elite_frac</span> <span class="o">*</span> <span class="n">pop_size</span><span class="p">))</span>
    <span class="n">elites</span>  <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]]</span>
    <span class="n">pop_size</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
    <span class="c1"># selection probabilities</span>
    <span class="n">p_elite</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_tourn</span>   <span class="o">=</span> <span class="mi">1</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p_elite</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p_tourn</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># pure tournament selection</span>
            <span class="k">return</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
    <span class="c1"># c) Reproduce via BLX- + mutation</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">pop_size</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">sample_parent</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="c1"># BLX- crossover</span>
        <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">blx_alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># mutation</span>
        <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mutation_p</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">genome_len</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mutation_sd</span>
        <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1">#  7) Final Best Model </span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))]</span>
<span class="n">best_model_ga</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                     <span class="n">best_model_ga</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">best_model_ga</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model_ga</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> GA done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  8) Plot </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA (w/ BLX-) Optimization of FFN Weights"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen  1/2000  train MSE: 0.8906, val MSE: 0.8294
Gen 100/2000  train MSE: 0.9938, val MSE: 0.9944
Gen 200/2000  train MSE: 0.9901, val MSE: 0.9904
Gen 300/2000  train MSE: 0.9875, val MSE: 0.9864
Gen 400/2000  train MSE: 0.9853, val MSE: 0.9842
Gen 500/2000  train MSE: 0.9835, val MSE: 0.9820
Gen 600/2000  train MSE: 0.9820, val MSE: 0.9805
Gen 700/2000  train MSE: 0.9806, val MSE: 0.9781
Gen 800/2000  train MSE: 0.9789, val MSE: 0.9753
Gen 900/2000  train MSE: 0.9770, val MSE: 0.9723
Gen 1000/2000  train MSE: 0.9755, val MSE: 0.9698
Gen 1100/2000  train MSE: 0.9742, val MSE: 0.9687
Gen 1200/2000  train MSE: 0.9731, val MSE: 0.9669
Gen 1300/2000  train MSE: 0.9710, val MSE: 0.9626
Gen 1400/2000  train MSE: 0.9698, val MSE: 0.9609
Gen 1500/2000  train MSE: 0.9687, val MSE: 0.9593
Gen 1600/2000  train MSE: 0.9671, val MSE: 0.9569
Gen 1700/2000  train MSE: 0.9659, val MSE: 0.9542
Gen 1800/2000  train MSE: 0.9647, val MSE: 0.9531
Gen 1900/2000  train MSE: 0.9637, val MSE: 0.9516
Gen 2000/2000  train MSE: 0.9623, val MSE: 0.9498

 GA done!  Final Train MSE: 0.9623, Val MSE: 0.9498
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e3f12bed">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Hybrid-GA:-First-implementation.-Refine-elites-every-100-generations,-for-10-epochs-using-gradient-descent.-Only-run-for-2000-generations.">Hybrid GA: First implementation. Refine elites every 100 generations, for 10 epochs using gradient descent. Only run for 2000 generations.<a class="anchor-link" href="#Hybrid-GA:-First-implementation.-Refine-elites-every-100-generations,-for-10-epochs-using-gradient-descent.-Only-run-for-2000-generations."></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=df3f3b78">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Hybrid Genetic Algorithm with Periodic SGD Refinement for FFN Training</span>
<span class="sd">=======================================================================</span>

<span class="sd">This script implements a hybrid optimisation strategy that combines a real-valued</span>
<span class="sd">Genetic Algorithm (GA) with periodic Stochastic Gradient Descent (SGD) refinement</span>
<span class="sd">for training a Feed-Forward Neural Network (FFN). The GA uses BLX- crossover,</span>
<span class="sd">Gaussian mutation, and elitism. Every 100 generations, the top-performing elite</span>
<span class="sd">individuals are further fine-tuned using a fixed number of SGD epochs.</span>

<span class="sd">Key Innovations:</span>
<span class="sd">----------------</span>
<span class="sd">- Hybrid Strategy: Combines global GA exploration with local gradient-based exploitation</span>
<span class="sd">- Periodic Refinement: Every 100 generations, elite individuals undergo 10 epochs of SGD</span>
<span class="sd">- Logging: Logs elite MSEs before and after refinement to a CSV file for ablation and analysis</span>

<span class="sd">Core GA Configuration:</span>
<span class="sd">----------------------</span>
<span class="sd">- Population Size: 200</span>
<span class="sd">- Generations: 2000</span>
<span class="sd">- Elitism: top 20% retained per generation</span>
<span class="sd">- Tournament Selection: size 3</span>
<span class="sd">- Crossover: BLX- with  = 0.6</span>
<span class="sd">- Mutation: Gaussian with probability 1%, std 0.01</span>

<span class="sd">SGD Refinement:</span>
<span class="sd">---------------</span>
<span class="sd">- Performed every 100 generations (`REFINE_EVERY`)</span>
<span class="sd">- Runs for 10 epochs (`REFINE_EPOCHS`) on each elite</span>
<span class="sd">- Uses momentum SGD with hyperparameters obtained via Optuna</span>

<span class="sd">Feedforward Network Architecture:</span>
<span class="sd">---------------------------------</span>
<span class="sd">- 2 hidden layers with 24 ReLU units each</span>
<span class="sd">- Xavier weight initialisation</span>
<span class="sd">- Output layer: linear (for regression)</span>

<span class="sd">Output:</span>
<span class="sd">-------</span>
<span class="sd">- Console logs for generation-wise MSE and refinement notifications</span>
<span class="sd">- `elite_mse_log.csv` file containing per-elite MSE before and after refinement</span>
<span class="sd">- Final training and validation MSE</span>
<span class="sd">- Matplotlib plot of training and validation MSE over time</span>

<span class="sd">Usage Notes:</span>
<span class="sd">------------</span>
<span class="sd">- Assumes `X_train`, `y_train`, `X_val`, `y_val` are pre-defined as `torch.Tensor`</span>
<span class="sd">- Designed for regression tasks (uses `nn.MSELoss`)</span>
<span class="sd">- Can be adapted to run refinement more/less frequently or to use other optimisers</span>

<span class="sd">"""</span>


<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1">#  1) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  2) Hyperparameters </span>
<span class="c1"># GA settings</span>
<span class="n">POP_SIZE</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">GENERATIONS</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">ELITE_FRAC</span>  <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">TOURN_SIZE</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">MUT_P</span>       <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">MUT_SD</span>      <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">BLX_ALPHA</span>   <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1"># Local SGD refinement (every REFINE_EVERY gens, on top ELITE_FRAC)</span>
<span class="n">REFINE_EVERY</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">REFINE_EPOCHS</span>  <span class="o">=</span> <span class="mi">10</span> 
<span class="c1"># FFN / SGD settings (from Optuna)</span>
<span class="n">best_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"lr"</span><span class="p">:</span>         <span class="mf">0.013988</span><span class="p">,</span>
    <span class="s2">"momentum"</span><span class="p">:</span>   <span class="mf">0.6346</span><span class="p">,</span>
    <span class="s2">"batch_size"</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"n_units"</span><span class="p">:</span>    <span class="mi">24</span><span class="p">,</span>
    <span class="s2">"activation"</span><span class="p">:</span> <span class="s2">"ReLU"</span>
<span class="p">}</span>

<span class="c1">#  3) DataPrep </span>
<span class="c1"># assume X_train, y_train, X_val, y_val are already in scope as torch.Tensor</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">],</span>
                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1">#  4) Model Builder </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">n_layers</span>   <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">],</span>
    <span class="n">n_units</span>    <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">],</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Constructs a feed-forward neural network (FFN) using the architecture specified in `arch`.</span>
<span class="sd">    </span>
<span class="sd">    The model includes:</span>
<span class="sd">    - `n_layers` hidden layers with `n_units` neurons each</span>
<span class="sd">    - Activation function as defined in `arch["activation"]`</span>
<span class="sd">    - Xavier normal weight initialization and zero-initialized biases</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: A PyTorch sequential model moved to the appropriate device.</span>
<span class="sd">    """</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_f</span> <span class="o">=</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Xavier init</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1">#  5) GA Helpers </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Selects one individual from the population using tournament selection.</span>

<span class="sd">    Args:</span>
<span class="sd">        pop (list of np.ndarray): Population of genome vectors.</span>
<span class="sd">        fitness (list of float): Corresponding fitness values (lower is better).</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Genome of the selected individual.</span>
<span class="sd">    """</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">),</span> <span class="n">TOURN_SIZE</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">crossover_and_mutate</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Produces a new child genome via BLX- crossover and Gaussian mutation.</span>

<span class="sd">    - BLX- expands the gene-wise search space around two parents.</span>
<span class="sd">    - Gaussian noise is added to a subset of genes based on mutation probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        p1 (np.ndarray): Parent 1 genome.</span>
<span class="sd">        p2 (np.ndarray): Parent 2 genome.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Mutated child genome.</span>
<span class="sd">    """</span>
    <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">BLX_ALPHA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
    <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">BLX_ALPHA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
    <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># mutation</span>
    <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">MUT_P</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">MUT_SD</span>
    <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">child</span>

<span class="k">def</span><span class="w"> </span><span class="nf">refine_with_sgd</span><span class="p">(</span><span class="n">genome</span><span class="p">):</span>

<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Produces a new child genome via BLX- crossover and Gaussian mutation.</span>

<span class="sd">    - BLX- expands the gene-wise search space around two parents.</span>
<span class="sd">    - Gaussian noise is added to a subset of genes based on mutation probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        p1 (np.ndarray): Parent 1 genome.</span>
<span class="sd">        p2 (np.ndarray): Parent 2 genome.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Mutated child genome.</span>
<span class="sd">    """</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                         <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="n">optim_sgd</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                          <span class="n">lr</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span>
                          <span class="n">momentum</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"momentum"</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">REFINE_EPOCHS</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"refine step : "</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optim_sgd</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optim_sgd</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># write back</span>
    <span class="k">return</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1">#  6) Initialize Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">POP_SIZE</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="c1">#  7) GA Main Loop </span>
<span class="n">train_curve</span><span class="p">,</span> <span class="n">val_curve</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="c1">###logging of mse values before and after sgd</span>
<span class="n">LOG_FN</span> <span class="o">=</span> <span class="s2">"elite_mse_log.csv"</span>
<span class="c1"># write header: gen,phase,elite0,elite1,</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="c1"># we'll fill in the number of elites after we know ELITE_FRAC and POP_SIZE</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"gen"</span><span class="p">,</span><span class="s2">"phase"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"elite</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">GENERATIONS</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># a) evaluate all fitness</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="c1"># b) stats &amp; record best</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span>
    <span class="c1"># validation mse of best</span>
    <span class="n">m_best</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="c1">#if gen % 100 == 0 or gen == 1:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">GENERATIONS</span><span class="si">}</span><span class="s2">  "</span>
        <span class="sa">f</span><span class="s2">"train MSE: </span><span class="si">{</span><span class="n">fitness</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">val_curve</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># c) elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
    <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
    <span class="n">elites</span> <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">]</span>
    <span class="n">elite_tuned</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">((</span><span class="n">ELITE_FRAC</span><span class="p">)</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
    <span class="n">elite_tuned_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">gen</span> <span class="o">%</span> <span class="n">REFINE_EVERY</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># (re)compute your elite indices</span>
        <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
        <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
        <span class="c1"># 1) capture pre MSEs</span>
        <span class="n">pre_vals</span> <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">elite_idxs</span><span class="p">]</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">line</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">gen</span><span class="p">),</span> <span class="s2">"pre"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">pre_vals</span><span class="p">]</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
        <span class="c1"># 2) do your SGD refine exactly as before</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Refinement @ gen </span><span class="si">{</span><span class="n">gen</span><span class="si">}</span><span class="s2"> ---"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">:</span>
            <span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">refine_with_sgd</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="c1"># 3) capture post MSEs (reevaluate on train set)</span>
        <span class="n">post_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
            <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">post_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">line</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">gen</span><span class="p">),</span> <span class="s2">"post"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">post_vals</span><span class="p">]</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># e) selection + reproduction</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="p">[</span> <span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span> <span class="p">]</span> 
    <span class="n">new_popa</span> <span class="o">=</span> <span class="n">elites</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="c1"># show that new_pop still points at the old arrays A, B, C not your refined ones</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">POP_SIZE</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">crossover_and_mutate</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">)</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
<span class="c1">#  8) Final Eval &amp; Plot </span>
<span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
<span class="n">best_model</span>  <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">best_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> GA+SGD done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA+PeriodicSGD Refinement"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen 1/2000  train MSE: 1.0075, val MSE: 1.0054
Gen 2/2000  train MSE: 1.0075, val MSE: 1.0054
Gen 3/2000  train MSE: 1.0075, val MSE: 1.0054
Gen 4/2000  train MSE: 1.0073, val MSE: 1.0051
Gen 5/2000  train MSE: 1.0073, val MSE: 1.0051
Gen 6/2000  train MSE: 1.0073, val MSE: 1.0051
Gen 7/2000  train MSE: 1.0073, val MSE: 1.0051
Gen 8/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 9/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 10/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 11/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 12/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 13/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 14/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 15/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 16/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 17/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 18/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 19/2000  train MSE: 1.0072, val MSE: 1.0051
Gen 20/2000  train MSE: 1.0023, val MSE: 1.0025
Gen 21/2000  train MSE: 1.0011, val MSE: 1.0026
Gen 22/2000  train MSE: 1.0011, val MSE: 1.0026
Gen 23/2000  train MSE: 1.0011, val MSE: 1.0026
Gen 24/2000  train MSE: 1.0011, val MSE: 1.0026
Gen 25/2000  train MSE: 1.0011, val MSE: 1.0026
Gen 26/2000  train MSE: 1.0011, val MSE: 1.0026
Gen 27/2000  train MSE: 1.0011, val MSE: 1.0026
Gen 28/2000  train MSE: 1.0011, val MSE: 1.0026
Gen 29/2000  train MSE: 1.0011, val MSE: 1.0026
Gen 30/2000  train MSE: 1.0009, val MSE: 1.0004
Gen 31/2000  train MSE: 1.0001, val MSE: 1.0015
Gen 32/2000  train MSE: 1.0001, val MSE: 1.0015
Gen 33/2000  train MSE: 1.0001, val MSE: 1.0015
Gen 34/2000  train MSE: 1.0001, val MSE: 1.0015
Gen 35/2000  train MSE: 1.0001, val MSE: 1.0015
Gen 36/2000  train MSE: 1.0001, val MSE: 1.0013
Gen 37/2000  train MSE: 1.0001, val MSE: 1.0013
Gen 38/2000  train MSE: 1.0001, val MSE: 1.0013
Gen 39/2000  train MSE: 0.9995, val MSE: 1.0004
Gen 40/2000  train MSE: 0.9995, val MSE: 1.0004
Gen 41/2000  train MSE: 0.9995, val MSE: 1.0004
Gen 42/2000  train MSE: 0.9995, val MSE: 1.0004
Gen 43/2000  train MSE: 0.9995, val MSE: 1.0004
Gen 44/2000  train MSE: 0.9991, val MSE: 1.0000
Gen 45/2000  train MSE: 0.9991, val MSE: 1.0000
Gen 46/2000  train MSE: 0.9991, val MSE: 1.0000
Gen 47/2000  train MSE: 0.9991, val MSE: 1.0000
Gen 48/2000  train MSE: 0.9991, val MSE: 1.0000
Gen 49/2000  train MSE: 0.9991, val MSE: 1.0000
Gen 50/2000  train MSE: 0.9987, val MSE: 1.0000
Gen 51/2000  train MSE: 0.9987, val MSE: 1.0000
Gen 52/2000  train MSE: 0.9987, val MSE: 1.0000
Gen 53/2000  train MSE: 0.9987, val MSE: 1.0000
Gen 54/2000  train MSE: 0.9987, val MSE: 1.0000
Gen 55/2000  train MSE: 0.9987, val MSE: 1.0000
Gen 56/2000  train MSE: 0.9987, val MSE: 1.0000
Gen 57/2000  train MSE: 0.9987, val MSE: 1.0000
Gen 58/2000  train MSE: 0.9986, val MSE: 0.9985
Gen 59/2000  train MSE: 0.9981, val MSE: 0.9981
Gen 60/2000  train MSE: 0.9981, val MSE: 0.9981
Gen 61/2000  train MSE: 0.9981, val MSE: 0.9981
Gen 62/2000  train MSE: 0.9981, val MSE: 0.9981
Gen 63/2000  train MSE: 0.9981, val MSE: 0.9981
Gen 64/2000  train MSE: 0.9981, val MSE: 0.9996
Gen 65/2000  train MSE: 0.9981, val MSE: 0.9996
Gen 66/2000  train MSE: 0.9981, val MSE: 0.9996
Gen 67/2000  train MSE: 0.9980, val MSE: 0.9987
Gen 68/2000  train MSE: 0.9980, val MSE: 0.9987
Gen 69/2000  train MSE: 0.9979, val MSE: 0.9974
Gen 70/2000  train MSE: 0.9979, val MSE: 0.9974
Gen 71/2000  train MSE: 0.9977, val MSE: 0.9981
Gen 72/2000  train MSE: 0.9977, val MSE: 0.9981
Gen 73/2000  train MSE: 0.9977, val MSE: 0.9981
Gen 74/2000  train MSE: 0.9977, val MSE: 0.9981
Gen 75/2000  train MSE: 0.9977, val MSE: 0.9981
Gen 76/2000  train MSE: 0.9975, val MSE: 0.9986
Gen 77/2000  train MSE: 0.9975, val MSE: 0.9986
Gen 78/2000  train MSE: 0.9975, val MSE: 0.9986
Gen 79/2000  train MSE: 0.9975, val MSE: 0.9978
Gen 80/2000  train MSE: 0.9975, val MSE: 0.9978
Gen 81/2000  train MSE: 0.9975, val MSE: 0.9978
Gen 82/2000  train MSE: 0.9975, val MSE: 0.9978
Gen 83/2000  train MSE: 0.9975, val MSE: 0.9978
Gen 84/2000  train MSE: 0.9975, val MSE: 0.9978
Gen 85/2000  train MSE: 0.9975, val MSE: 0.9979
Gen 86/2000  train MSE: 0.9975, val MSE: 0.9979
Gen 87/2000  train MSE: 0.9975, val MSE: 0.9979
Gen 88/2000  train MSE: 0.9974, val MSE: 0.9980
Gen 89/2000  train MSE: 0.9973, val MSE: 0.9972
Gen 90/2000  train MSE: 0.9973, val MSE: 0.9972
Gen 91/2000  train MSE: 0.9970, val MSE: 0.9978
Gen 92/2000  train MSE: 0.9970, val MSE: 0.9978
Gen 93/2000  train MSE: 0.9970, val MSE: 0.9978
Gen 94/2000  train MSE: 0.9970, val MSE: 0.9978
Gen 95/2000  train MSE: 0.9970, val MSE: 0.9978
Gen 96/2000  train MSE: 0.9970, val MSE: 0.9978
Gen 97/2000  train MSE: 0.9968, val MSE: 0.9974
Gen 98/2000  train MSE: 0.9968, val MSE: 0.9974
Gen 99/2000  train MSE: 0.9968, val MSE: 0.9974
Gen 100/2000  train MSE: 0.9968, val MSE: 0.9974

--- Refinement @ gen 100 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 101/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 102/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 103/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 104/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 105/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 106/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 107/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 108/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 109/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 110/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 111/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 112/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 113/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 114/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 115/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 116/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 117/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 118/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 119/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 120/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 121/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 122/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 123/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 124/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 125/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 126/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 127/2000  train MSE: 0.9738, val MSE: 0.9631
Gen 128/2000  train MSE: 0.9734, val MSE: 0.9631
Gen 129/2000  train MSE: 0.9734, val MSE: 0.9631
Gen 130/2000  train MSE: 0.9733, val MSE: 0.9629
Gen 131/2000  train MSE: 0.9732, val MSE: 0.9627
Gen 132/2000  train MSE: 0.9729, val MSE: 0.9622
Gen 133/2000  train MSE: 0.9724, val MSE: 0.9623
Gen 134/2000  train MSE: 0.9724, val MSE: 0.9623
Gen 135/2000  train MSE: 0.9724, val MSE: 0.9625
Gen 136/2000  train MSE: 0.9722, val MSE: 0.9625
Gen 137/2000  train MSE: 0.9722, val MSE: 0.9626
Gen 138/2000  train MSE: 0.9721, val MSE: 0.9623
Gen 139/2000  train MSE: 0.9719, val MSE: 0.9620
Gen 140/2000  train MSE: 0.9717, val MSE: 0.9618
Gen 141/2000  train MSE: 0.9716, val MSE: 0.9620
Gen 142/2000  train MSE: 0.9716, val MSE: 0.9624
Gen 143/2000  train MSE: 0.9716, val MSE: 0.9624
Gen 144/2000  train MSE: 0.9715, val MSE: 0.9621
Gen 145/2000  train MSE: 0.9714, val MSE: 0.9623
Gen 146/2000  train MSE: 0.9712, val MSE: 0.9621
Gen 147/2000  train MSE: 0.9712, val MSE: 0.9622
Gen 148/2000  train MSE: 0.9711, val MSE: 0.9622
Gen 149/2000  train MSE: 0.9709, val MSE: 0.9614
Gen 150/2000  train MSE: 0.9709, val MSE: 0.9614
Gen 151/2000  train MSE: 0.9709, val MSE: 0.9614
Gen 152/2000  train MSE: 0.9705, val MSE: 0.9612
Gen 153/2000  train MSE: 0.9705, val MSE: 0.9612
Gen 154/2000  train MSE: 0.9705, val MSE: 0.9612
Gen 155/2000  train MSE: 0.9705, val MSE: 0.9612
Gen 156/2000  train MSE: 0.9705, val MSE: 0.9612
Gen 157/2000  train MSE: 0.9705, val MSE: 0.9612
Gen 158/2000  train MSE: 0.9705, val MSE: 0.9612
Gen 159/2000  train MSE: 0.9705, val MSE: 0.9612
Gen 160/2000  train MSE: 0.9705, val MSE: 0.9612
Gen 161/2000  train MSE: 0.9705, val MSE: 0.9612
Gen 162/2000  train MSE: 0.9705, val MSE: 0.9615
Gen 163/2000  train MSE: 0.9705, val MSE: 0.9615
Gen 164/2000  train MSE: 0.9705, val MSE: 0.9615
Gen 165/2000  train MSE: 0.9705, val MSE: 0.9615
Gen 166/2000  train MSE: 0.9705, val MSE: 0.9615
Gen 167/2000  train MSE: 0.9704, val MSE: 0.9611
Gen 168/2000  train MSE: 0.9704, val MSE: 0.9611
Gen 169/2000  train MSE: 0.9704, val MSE: 0.9611
Gen 170/2000  train MSE: 0.9704, val MSE: 0.9611
Gen 171/2000  train MSE: 0.9704, val MSE: 0.9609
Gen 172/2000  train MSE: 0.9703, val MSE: 0.9614
Gen 173/2000  train MSE: 0.9703, val MSE: 0.9614
Gen 174/2000  train MSE: 0.9703, val MSE: 0.9614
Gen 175/2000  train MSE: 0.9703, val MSE: 0.9614
Gen 176/2000  train MSE: 0.9703, val MSE: 0.9615
Gen 177/2000  train MSE: 0.9702, val MSE: 0.9611
Gen 178/2000  train MSE: 0.9702, val MSE: 0.9611
Gen 179/2000  train MSE: 0.9702, val MSE: 0.9611
Gen 180/2000  train MSE: 0.9702, val MSE: 0.9611
Gen 181/2000  train MSE: 0.9702, val MSE: 0.9611
Gen 182/2000  train MSE: 0.9702, val MSE: 0.9611
Gen 183/2000  train MSE: 0.9700, val MSE: 0.9605
Gen 184/2000  train MSE: 0.9700, val MSE: 0.9605
Gen 185/2000  train MSE: 0.9700, val MSE: 0.9605
Gen 186/2000  train MSE: 0.9700, val MSE: 0.9605
Gen 187/2000  train MSE: 0.9700, val MSE: 0.9605
Gen 188/2000  train MSE: 0.9700, val MSE: 0.9605
Gen 189/2000  train MSE: 0.9700, val MSE: 0.9605
Gen 190/2000  train MSE: 0.9700, val MSE: 0.9605
Gen 191/2000  train MSE: 0.9699, val MSE: 0.9605
Gen 192/2000  train MSE: 0.9699, val MSE: 0.9605
Gen 193/2000  train MSE: 0.9699, val MSE: 0.9605
Gen 194/2000  train MSE: 0.9699, val MSE: 0.9605
Gen 195/2000  train MSE: 0.9698, val MSE: 0.9601
Gen 196/2000  train MSE: 0.9698, val MSE: 0.9601
Gen 197/2000  train MSE: 0.9698, val MSE: 0.9601
Gen 198/2000  train MSE: 0.9698, val MSE: 0.9601
Gen 199/2000  train MSE: 0.9698, val MSE: 0.9601
Gen 200/2000  train MSE: 0.9698, val MSE: 0.9601

--- Refinement @ gen 200 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 201/2000  train MSE: 0.9361, val MSE: 0.9021
Gen 202/2000  train MSE: 0.9361, val MSE: 0.9033
Gen 203/2000  train MSE: 0.9361, val MSE: 0.9033
Gen 204/2000  train MSE: 0.9361, val MSE: 0.9033
Gen 205/2000  train MSE: 0.9361, val MSE: 0.9033
Gen 206/2000  train MSE: 0.9361, val MSE: 0.9033
Gen 207/2000  train MSE: 0.9361, val MSE: 0.9033
Gen 208/2000  train MSE: 0.9361, val MSE: 0.9033
Gen 209/2000  train MSE: 0.9359, val MSE: 0.9019
Gen 210/2000  train MSE: 0.9359, val MSE: 0.9019
Gen 211/2000  train MSE: 0.9359, val MSE: 0.9019
Gen 212/2000  train MSE: 0.9359, val MSE: 0.9019
Gen 213/2000  train MSE: 0.9359, val MSE: 0.9019
Gen 214/2000  train MSE: 0.9359, val MSE: 0.9019
Gen 215/2000  train MSE: 0.9359, val MSE: 0.9019
Gen 216/2000  train MSE: 0.9349, val MSE: 0.9036
Gen 217/2000  train MSE: 0.9349, val MSE: 0.9036
Gen 218/2000  train MSE: 0.9349, val MSE: 0.9036
Gen 219/2000  train MSE: 0.9349, val MSE: 0.9036
Gen 220/2000  train MSE: 0.9349, val MSE: 0.9036
Gen 221/2000  train MSE: 0.9333, val MSE: 0.9006
Gen 222/2000  train MSE: 0.9328, val MSE: 0.8989
Gen 223/2000  train MSE: 0.9320, val MSE: 0.8979
Gen 224/2000  train MSE: 0.9315, val MSE: 0.8979
Gen 225/2000  train MSE: 0.9313, val MSE: 0.8991
Gen 226/2000  train MSE: 0.9299, val MSE: 0.8970
Gen 227/2000  train MSE: 0.9299, val MSE: 0.8970
Gen 228/2000  train MSE: 0.9299, val MSE: 0.8970
Gen 229/2000  train MSE: 0.9281, val MSE: 0.8947
Gen 230/2000  train MSE: 0.9281, val MSE: 0.8947
Gen 231/2000  train MSE: 0.9281, val MSE: 0.8947
Gen 232/2000  train MSE: 0.9280, val MSE: 0.8951
Gen 233/2000  train MSE: 0.9278, val MSE: 0.8946
Gen 234/2000  train MSE: 0.9271, val MSE: 0.8923
Gen 235/2000  train MSE: 0.9271, val MSE: 0.8923
Gen 236/2000  train MSE: 0.9271, val MSE: 0.8923
Gen 237/2000  train MSE: 0.9271, val MSE: 0.8923
Gen 238/2000  train MSE: 0.9271, val MSE: 0.8923
Gen 239/2000  train MSE: 0.9271, val MSE: 0.8923
Gen 240/2000  train MSE: 0.9270, val MSE: 0.8923
Gen 241/2000  train MSE: 0.9270, val MSE: 0.8923
Gen 242/2000  train MSE: 0.9266, val MSE: 0.8931
Gen 243/2000  train MSE: 0.9266, val MSE: 0.8931
Gen 244/2000  train MSE: 0.9266, val MSE: 0.8931
Gen 245/2000  train MSE: 0.9266, val MSE: 0.8931
Gen 246/2000  train MSE: 0.9261, val MSE: 0.8897
Gen 247/2000  train MSE: 0.9261, val MSE: 0.8897
Gen 248/2000  train MSE: 0.9261, val MSE: 0.8897
Gen 249/2000  train MSE: 0.9261, val MSE: 0.8897
Gen 250/2000  train MSE: 0.9259, val MSE: 0.8911
Gen 251/2000  train MSE: 0.9259, val MSE: 0.8911
Gen 252/2000  train MSE: 0.9256, val MSE: 0.8896
Gen 253/2000  train MSE: 0.9256, val MSE: 0.8896
Gen 254/2000  train MSE: 0.9256, val MSE: 0.8896
Gen 255/2000  train MSE: 0.9256, val MSE: 0.8896
Gen 256/2000  train MSE: 0.9256, val MSE: 0.8896
Gen 257/2000  train MSE: 0.9253, val MSE: 0.8893
Gen 258/2000  train MSE: 0.9253, val MSE: 0.8893
Gen 259/2000  train MSE: 0.9253, val MSE: 0.8893
Gen 260/2000  train MSE: 0.9252, val MSE: 0.8902
Gen 261/2000  train MSE: 0.9252, val MSE: 0.8902
Gen 262/2000  train MSE: 0.9252, val MSE: 0.8902
Gen 263/2000  train MSE: 0.9246, val MSE: 0.8900
Gen 264/2000  train MSE: 0.9246, val MSE: 0.8900
Gen 265/2000  train MSE: 0.9246, val MSE: 0.8900
Gen 266/2000  train MSE: 0.9246, val MSE: 0.8900
Gen 267/2000  train MSE: 0.9246, val MSE: 0.8900
Gen 268/2000  train MSE: 0.9246, val MSE: 0.8900
Gen 269/2000  train MSE: 0.9244, val MSE: 0.8896
Gen 270/2000  train MSE: 0.9244, val MSE: 0.8896
Gen 271/2000  train MSE: 0.9244, val MSE: 0.8896
Gen 272/2000  train MSE: 0.9243, val MSE: 0.8905
Gen 273/2000  train MSE: 0.9243, val MSE: 0.8905
Gen 274/2000  train MSE: 0.9243, val MSE: 0.8891
Gen 275/2000  train MSE: 0.9243, val MSE: 0.8891
Gen 276/2000  train MSE: 0.9243, val MSE: 0.8891
Gen 277/2000  train MSE: 0.9242, val MSE: 0.8889
Gen 278/2000  train MSE: 0.9242, val MSE: 0.8882
Gen 279/2000  train MSE: 0.9242, val MSE: 0.8882
Gen 280/2000  train MSE: 0.9242, val MSE: 0.8882
Gen 281/2000  train MSE: 0.9242, val MSE: 0.8882
Gen 282/2000  train MSE: 0.9239, val MSE: 0.8883
Gen 283/2000  train MSE: 0.9239, val MSE: 0.8883
Gen 284/2000  train MSE: 0.9238, val MSE: 0.8885
Gen 285/2000  train MSE: 0.9238, val MSE: 0.8885
Gen 286/2000  train MSE: 0.9238, val MSE: 0.8885
Gen 287/2000  train MSE: 0.9238, val MSE: 0.8885
Gen 288/2000  train MSE: 0.9238, val MSE: 0.8885
Gen 289/2000  train MSE: 0.9238, val MSE: 0.8885
Gen 290/2000  train MSE: 0.9238, val MSE: 0.8886
Gen 291/2000  train MSE: 0.9237, val MSE: 0.8886
Gen 292/2000  train MSE: 0.9233, val MSE: 0.8884
Gen 293/2000  train MSE: 0.9233, val MSE: 0.8884
Gen 294/2000  train MSE: 0.9233, val MSE: 0.8884
Gen 295/2000  train MSE: 0.9233, val MSE: 0.8884
Gen 296/2000  train MSE: 0.9233, val MSE: 0.8884
Gen 297/2000  train MSE: 0.9233, val MSE: 0.8884
Gen 298/2000  train MSE: 0.9233, val MSE: 0.8884
Gen 299/2000  train MSE: 0.9233, val MSE: 0.8884
Gen 300/2000  train MSE: 0.9233, val MSE: 0.8884

--- Refinement @ gen 300 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 301/2000  train MSE: 0.7639, val MSE: 0.6133
Gen 302/2000  train MSE: 0.7639, val MSE: 0.6133
Gen 303/2000  train MSE: 0.7639, val MSE: 0.6133
Gen 304/2000  train MSE: 0.7639, val MSE: 0.6133
Gen 305/2000  train MSE: 0.7639, val MSE: 0.6133
Gen 306/2000  train MSE: 0.7639, val MSE: 0.6133
Gen 307/2000  train MSE: 0.7639, val MSE: 0.6133
Gen 308/2000  train MSE: 0.7635, val MSE: 0.6129
Gen 309/2000  train MSE: 0.7635, val MSE: 0.6129
Gen 310/2000  train MSE: 0.7635, val MSE: 0.6129
Gen 311/2000  train MSE: 0.7635, val MSE: 0.6129
Gen 312/2000  train MSE: 0.7635, val MSE: 0.6129
Gen 313/2000  train MSE: 0.7632, val MSE: 0.6129
Gen 314/2000  train MSE: 0.7632, val MSE: 0.6129
Gen 315/2000  train MSE: 0.7632, val MSE: 0.6129
Gen 316/2000  train MSE: 0.7632, val MSE: 0.6129
Gen 317/2000  train MSE: 0.7632, val MSE: 0.6129
Gen 318/2000  train MSE: 0.7632, val MSE: 0.6129
Gen 319/2000  train MSE: 0.7632, val MSE: 0.6129
Gen 320/2000  train MSE: 0.7629, val MSE: 0.6126
Gen 321/2000  train MSE: 0.7629, val MSE: 0.6126
Gen 322/2000  train MSE: 0.7629, val MSE: 0.6126
Gen 323/2000  train MSE: 0.7629, val MSE: 0.6126
Gen 324/2000  train MSE: 0.7626, val MSE: 0.6120
Gen 325/2000  train MSE: 0.7611, val MSE: 0.6102
Gen 326/2000  train MSE: 0.7611, val MSE: 0.6102
Gen 327/2000  train MSE: 0.7605, val MSE: 0.6106
Gen 328/2000  train MSE: 0.7590, val MSE: 0.6095
Gen 329/2000  train MSE: 0.7579, val MSE: 0.6074
Gen 330/2000  train MSE: 0.7561, val MSE: 0.6075
Gen 331/2000  train MSE: 0.7556, val MSE: 0.6059
Gen 332/2000  train MSE: 0.7512, val MSE: 0.6034
Gen 333/2000  train MSE: 0.7500, val MSE: 0.6040
Gen 334/2000  train MSE: 0.7451, val MSE: 0.5962
Gen 335/2000  train MSE: 0.7420, val MSE: 0.5975
Gen 336/2000  train MSE: 0.7420, val MSE: 0.5975
Gen 337/2000  train MSE: 0.7399, val MSE: 0.5936
Gen 338/2000  train MSE: 0.7397, val MSE: 0.5938
Gen 339/2000  train MSE: 0.7359, val MSE: 0.5941
Gen 340/2000  train MSE: 0.7359, val MSE: 0.5941
Gen 341/2000  train MSE: 0.7344, val MSE: 0.5918
Gen 342/2000  train MSE: 0.7332, val MSE: 0.5886
Gen 343/2000  train MSE: 0.7328, val MSE: 0.5890
Gen 344/2000  train MSE: 0.7328, val MSE: 0.5890
Gen 345/2000  train MSE: 0.7328, val MSE: 0.5890
Gen 346/2000  train MSE: 0.7313, val MSE: 0.5908
Gen 347/2000  train MSE: 0.7311, val MSE: 0.5898
Gen 348/2000  train MSE: 0.7311, val MSE: 0.5898
Gen 349/2000  train MSE: 0.7305, val MSE: 0.5872
Gen 350/2000  train MSE: 0.7305, val MSE: 0.5872
Gen 351/2000  train MSE: 0.7293, val MSE: 0.5833
Gen 352/2000  train MSE: 0.7287, val MSE: 0.5843
Gen 353/2000  train MSE: 0.7287, val MSE: 0.5843
Gen 354/2000  train MSE: 0.7281, val MSE: 0.5812
Gen 355/2000  train MSE: 0.7281, val MSE: 0.5812
Gen 356/2000  train MSE: 0.7279, val MSE: 0.5858
Gen 357/2000  train MSE: 0.7277, val MSE: 0.5857
Gen 358/2000  train MSE: 0.7272, val MSE: 0.5837
Gen 359/2000  train MSE: 0.7260, val MSE: 0.5841
Gen 360/2000  train MSE: 0.7260, val MSE: 0.5841
Gen 361/2000  train MSE: 0.7256, val MSE: 0.5825
Gen 362/2000  train MSE: 0.7253, val MSE: 0.5822
Gen 363/2000  train MSE: 0.7253, val MSE: 0.5822
Gen 364/2000  train MSE: 0.7253, val MSE: 0.5822
Gen 365/2000  train MSE: 0.7253, val MSE: 0.5822
Gen 366/2000  train MSE: 0.7253, val MSE: 0.5822
Gen 367/2000  train MSE: 0.7253, val MSE: 0.5822
Gen 368/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 369/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 370/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 371/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 372/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 373/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 374/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 375/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 376/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 377/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 378/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 379/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 380/2000  train MSE: 0.7209, val MSE: 0.5766
Gen 381/2000  train MSE: 0.7209, val MSE: 0.5773
Gen 382/2000  train MSE: 0.7209, val MSE: 0.5773
Gen 383/2000  train MSE: 0.7195, val MSE: 0.5727
Gen 384/2000  train MSE: 0.7195, val MSE: 0.5727
Gen 385/2000  train MSE: 0.7195, val MSE: 0.5727
Gen 386/2000  train MSE: 0.7195, val MSE: 0.5727
Gen 387/2000  train MSE: 0.7189, val MSE: 0.5724
Gen 388/2000  train MSE: 0.7189, val MSE: 0.5724
Gen 389/2000  train MSE: 0.7189, val MSE: 0.5724
Gen 390/2000  train MSE: 0.7189, val MSE: 0.5724
Gen 391/2000  train MSE: 0.7189, val MSE: 0.5724
Gen 392/2000  train MSE: 0.7181, val MSE: 0.5717
Gen 393/2000  train MSE: 0.7181, val MSE: 0.5717
Gen 394/2000  train MSE: 0.7178, val MSE: 0.5739
Gen 395/2000  train MSE: 0.7178, val MSE: 0.5739
Gen 396/2000  train MSE: 0.7178, val MSE: 0.5739
Gen 397/2000  train MSE: 0.7176, val MSE: 0.5729
Gen 398/2000  train MSE: 0.7176, val MSE: 0.5729
Gen 399/2000  train MSE: 0.7176, val MSE: 0.5729
Gen 400/2000  train MSE: 0.7176, val MSE: 0.5729

--- Refinement @ gen 400 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 401/2000  train MSE: 0.6965, val MSE: 0.5159
Gen 402/2000  train MSE: 0.6965, val MSE: 0.5159
Gen 403/2000  train MSE: 0.6937, val MSE: 0.5026
Gen 404/2000  train MSE: 0.6915, val MSE: 0.5080
Gen 405/2000  train MSE: 0.6915, val MSE: 0.5080
Gen 406/2000  train MSE: 0.6915, val MSE: 0.5080
Gen 407/2000  train MSE: 0.6915, val MSE: 0.5080
Gen 408/2000  train MSE: 0.6915, val MSE: 0.5080
Gen 409/2000  train MSE: 0.6915, val MSE: 0.5080
Gen 410/2000  train MSE: 0.6877, val MSE: 0.5014
Gen 411/2000  train MSE: 0.6877, val MSE: 0.5014
Gen 412/2000  train MSE: 0.6845, val MSE: 0.4985
Gen 413/2000  train MSE: 0.6845, val MSE: 0.4985
Gen 414/2000  train MSE: 0.6838, val MSE: 0.4990
Gen 415/2000  train MSE: 0.6838, val MSE: 0.4990
Gen 416/2000  train MSE: 0.6826, val MSE: 0.4936
Gen 417/2000  train MSE: 0.6826, val MSE: 0.4936
Gen 418/2000  train MSE: 0.6824, val MSE: 0.4930
Gen 419/2000  train MSE: 0.6808, val MSE: 0.4890
Gen 420/2000  train MSE: 0.6796, val MSE: 0.4948
Gen 421/2000  train MSE: 0.6796, val MSE: 0.4948
Gen 422/2000  train MSE: 0.6784, val MSE: 0.4867
Gen 423/2000  train MSE: 0.6784, val MSE: 0.4867
Gen 424/2000  train MSE: 0.6761, val MSE: 0.4887
Gen 425/2000  train MSE: 0.6761, val MSE: 0.4887
Gen 426/2000  train MSE: 0.6761, val MSE: 0.4887
Gen 427/2000  train MSE: 0.6761, val MSE: 0.4887
Gen 428/2000  train MSE: 0.6760, val MSE: 0.4845
Gen 429/2000  train MSE: 0.6747, val MSE: 0.4842
Gen 430/2000  train MSE: 0.6747, val MSE: 0.4842
Gen 431/2000  train MSE: 0.6736, val MSE: 0.4902
Gen 432/2000  train MSE: 0.6736, val MSE: 0.4902
Gen 433/2000  train MSE: 0.6736, val MSE: 0.4902
Gen 434/2000  train MSE: 0.6736, val MSE: 0.4902
Gen 435/2000  train MSE: 0.6736, val MSE: 0.4902
Gen 436/2000  train MSE: 0.6716, val MSE: 0.4791
Gen 437/2000  train MSE: 0.6716, val MSE: 0.4791
Gen 438/2000  train MSE: 0.6716, val MSE: 0.4791
Gen 439/2000  train MSE: 0.6716, val MSE: 0.4791
Gen 440/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 441/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 442/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 443/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 444/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 445/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 446/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 447/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 448/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 449/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 450/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 451/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 452/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 453/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 454/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 455/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 456/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 457/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 458/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 459/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 460/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 461/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 462/2000  train MSE: 0.6668, val MSE: 0.4770
Gen 463/2000  train MSE: 0.6668, val MSE: 0.4816
Gen 464/2000  train MSE: 0.6661, val MSE: 0.4780
Gen 465/2000  train MSE: 0.6650, val MSE: 0.4797
Gen 466/2000  train MSE: 0.6647, val MSE: 0.4781
Gen 467/2000  train MSE: 0.6645, val MSE: 0.4773
Gen 468/2000  train MSE: 0.6645, val MSE: 0.4773
Gen 469/2000  train MSE: 0.6645, val MSE: 0.4773
Gen 470/2000  train MSE: 0.6642, val MSE: 0.4768
Gen 471/2000  train MSE: 0.6632, val MSE: 0.4755
Gen 472/2000  train MSE: 0.6628, val MSE: 0.4745
Gen 473/2000  train MSE: 0.6628, val MSE: 0.4745
Gen 474/2000  train MSE: 0.6627, val MSE: 0.4755
Gen 475/2000  train MSE: 0.6622, val MSE: 0.4749
Gen 476/2000  train MSE: 0.6622, val MSE: 0.4743
Gen 477/2000  train MSE: 0.6622, val MSE: 0.4743
Gen 478/2000  train MSE: 0.6622, val MSE: 0.4743
Gen 479/2000  train MSE: 0.6616, val MSE: 0.4753
Gen 480/2000  train MSE: 0.6609, val MSE: 0.4724
Gen 481/2000  train MSE: 0.6609, val MSE: 0.4724
Gen 482/2000  train MSE: 0.6609, val MSE: 0.4724
Gen 483/2000  train MSE: 0.6609, val MSE: 0.4724
Gen 484/2000  train MSE: 0.6609, val MSE: 0.4724
Gen 485/2000  train MSE: 0.6605, val MSE: 0.4710
Gen 486/2000  train MSE: 0.6605, val MSE: 0.4710
Gen 487/2000  train MSE: 0.6605, val MSE: 0.4710
Gen 488/2000  train MSE: 0.6605, val MSE: 0.4710
Gen 489/2000  train MSE: 0.6600, val MSE: 0.4724
Gen 490/2000  train MSE: 0.6587, val MSE: 0.4677
Gen 491/2000  train MSE: 0.6587, val MSE: 0.4677
Gen 492/2000  train MSE: 0.6587, val MSE: 0.4677
Gen 493/2000  train MSE: 0.6587, val MSE: 0.4677
Gen 494/2000  train MSE: 0.6587, val MSE: 0.4677
Gen 495/2000  train MSE: 0.6582, val MSE: 0.4705
Gen 496/2000  train MSE: 0.6582, val MSE: 0.4705
Gen 497/2000  train MSE: 0.6582, val MSE: 0.4705
Gen 498/2000  train MSE: 0.6582, val MSE: 0.4705
Gen 499/2000  train MSE: 0.6582, val MSE: 0.4705
Gen 500/2000  train MSE: 0.6582, val MSE: 0.4705

--- Refinement @ gen 500 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 501/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 502/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 503/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 504/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 505/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 506/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 507/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 508/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 509/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 510/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 511/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 512/2000  train MSE: 0.6578, val MSE: 0.4677
Gen 513/2000  train MSE: 0.6576, val MSE: 0.4679
Gen 514/2000  train MSE: 0.6576, val MSE: 0.4679
Gen 515/2000  train MSE: 0.6576, val MSE: 0.4679
Gen 516/2000  train MSE: 0.6575, val MSE: 0.4679
Gen 517/2000  train MSE: 0.6574, val MSE: 0.4675
Gen 518/2000  train MSE: 0.6572, val MSE: 0.4658
Gen 519/2000  train MSE: 0.6571, val MSE: 0.4651
Gen 520/2000  train MSE: 0.6571, val MSE: 0.4651
Gen 521/2000  train MSE: 0.6568, val MSE: 0.4677
Gen 522/2000  train MSE: 0.6563, val MSE: 0.4673
Gen 523/2000  train MSE: 0.6560, val MSE: 0.4652
Gen 524/2000  train MSE: 0.6557, val MSE: 0.4665
Gen 525/2000  train MSE: 0.6553, val MSE: 0.4639
Gen 526/2000  train MSE: 0.6548, val MSE: 0.4630
Gen 527/2000  train MSE: 0.6548, val MSE: 0.4630
Gen 528/2000  train MSE: 0.6548, val MSE: 0.4630
Gen 529/2000  train MSE: 0.6548, val MSE: 0.4630
Gen 530/2000  train MSE: 0.6544, val MSE: 0.4647
Gen 531/2000  train MSE: 0.6542, val MSE: 0.4636
Gen 532/2000  train MSE: 0.6534, val MSE: 0.4624
Gen 533/2000  train MSE: 0.6534, val MSE: 0.4624
Gen 534/2000  train MSE: 0.6534, val MSE: 0.4624
Gen 535/2000  train MSE: 0.6528, val MSE: 0.4613
Gen 536/2000  train MSE: 0.6528, val MSE: 0.4613
Gen 537/2000  train MSE: 0.6528, val MSE: 0.4613
Gen 538/2000  train MSE: 0.6528, val MSE: 0.4613
Gen 539/2000  train MSE: 0.6524, val MSE: 0.4631
Gen 540/2000  train MSE: 0.6524, val MSE: 0.4631
Gen 541/2000  train MSE: 0.6522, val MSE: 0.4589
Gen 542/2000  train MSE: 0.6522, val MSE: 0.4609
Gen 543/2000  train MSE: 0.6517, val MSE: 0.4583
Gen 544/2000  train MSE: 0.6511, val MSE: 0.4596
Gen 545/2000  train MSE: 0.6511, val MSE: 0.4596
Gen 546/2000  train MSE: 0.6511, val MSE: 0.4596
Gen 547/2000  train MSE: 0.6505, val MSE: 0.4580
Gen 548/2000  train MSE: 0.6505, val MSE: 0.4580
Gen 549/2000  train MSE: 0.6504, val MSE: 0.4566
Gen 550/2000  train MSE: 0.6504, val MSE: 0.4566
Gen 551/2000  train MSE: 0.6504, val MSE: 0.4566
Gen 552/2000  train MSE: 0.6498, val MSE: 0.4538
Gen 553/2000  train MSE: 0.6498, val MSE: 0.4538
Gen 554/2000  train MSE: 0.6498, val MSE: 0.4538
Gen 555/2000  train MSE: 0.6498, val MSE: 0.4538
Gen 556/2000  train MSE: 0.6498, val MSE: 0.4538
Gen 557/2000  train MSE: 0.6498, val MSE: 0.4538
Gen 558/2000  train MSE: 0.6498, val MSE: 0.4538
Gen 559/2000  train MSE: 0.6498, val MSE: 0.4538
Gen 560/2000  train MSE: 0.6493, val MSE: 0.4531
Gen 561/2000  train MSE: 0.6493, val MSE: 0.4531
Gen 562/2000  train MSE: 0.6493, val MSE: 0.4531
Gen 563/2000  train MSE: 0.6493, val MSE: 0.4531
Gen 564/2000  train MSE: 0.6493, val MSE: 0.4531
Gen 565/2000  train MSE: 0.6493, val MSE: 0.4531
Gen 566/2000  train MSE: 0.6492, val MSE: 0.4575
Gen 567/2000  train MSE: 0.6492, val MSE: 0.4575
Gen 568/2000  train MSE: 0.6492, val MSE: 0.4575
Gen 569/2000  train MSE: 0.6492, val MSE: 0.4575
Gen 570/2000  train MSE: 0.6492, val MSE: 0.4570
Gen 571/2000  train MSE: 0.6487, val MSE: 0.4568
Gen 572/2000  train MSE: 0.6487, val MSE: 0.4568
Gen 573/2000  train MSE: 0.6481, val MSE: 0.4568
Gen 574/2000  train MSE: 0.6480, val MSE: 0.4557
Gen 575/2000  train MSE: 0.6476, val MSE: 0.4545
Gen 576/2000  train MSE: 0.6476, val MSE: 0.4545
Gen 577/2000  train MSE: 0.6475, val MSE: 0.4547
Gen 578/2000  train MSE: 0.6474, val MSE: 0.4541
Gen 579/2000  train MSE: 0.6474, val MSE: 0.4541
Gen 580/2000  train MSE: 0.6467, val MSE: 0.4534
Gen 581/2000  train MSE: 0.6467, val MSE: 0.4534
Gen 582/2000  train MSE: 0.6467, val MSE: 0.4534
Gen 583/2000  train MSE: 0.6467, val MSE: 0.4534
Gen 584/2000  train MSE: 0.6467, val MSE: 0.4534
Gen 585/2000  train MSE: 0.6467, val MSE: 0.4534
Gen 586/2000  train MSE: 0.6467, val MSE: 0.4534
Gen 587/2000  train MSE: 0.6464, val MSE: 0.4543
Gen 588/2000  train MSE: 0.6464, val MSE: 0.4543
Gen 589/2000  train MSE: 0.6464, val MSE: 0.4543
Gen 590/2000  train MSE: 0.6464, val MSE: 0.4543
Gen 591/2000  train MSE: 0.6459, val MSE: 0.4532
Gen 592/2000  train MSE: 0.6458, val MSE: 0.4536
Gen 593/2000  train MSE: 0.6458, val MSE: 0.4536
Gen 594/2000  train MSE: 0.6458, val MSE: 0.4536
Gen 595/2000  train MSE: 0.6458, val MSE: 0.4537
Gen 596/2000  train MSE: 0.6451, val MSE: 0.4542
Gen 597/2000  train MSE: 0.6451, val MSE: 0.4542
Gen 598/2000  train MSE: 0.6451, val MSE: 0.4542
Gen 599/2000  train MSE: 0.6451, val MSE: 0.4542
Gen 600/2000  train MSE: 0.6451, val MSE: 0.4542

--- Refinement @ gen 600 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 601/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 602/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 603/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 604/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 605/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 606/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 607/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 608/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 609/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 610/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 611/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 612/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 613/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 614/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 615/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 616/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 617/2000  train MSE: 0.6453, val MSE: 0.4521
Gen 618/2000  train MSE: 0.6452, val MSE: 0.4497
Gen 619/2000  train MSE: 0.6449, val MSE: 0.4477
Gen 620/2000  train MSE: 0.6449, val MSE: 0.4477
Gen 621/2000  train MSE: 0.6449, val MSE: 0.4477
Gen 622/2000  train MSE: 0.6449, val MSE: 0.4477
Gen 623/2000  train MSE: 0.6449, val MSE: 0.4477
Gen 624/2000  train MSE: 0.6448, val MSE: 0.4475
Gen 625/2000  train MSE: 0.6448, val MSE: 0.4475
Gen 626/2000  train MSE: 0.6448, val MSE: 0.4475
Gen 627/2000  train MSE: 0.6448, val MSE: 0.4475
Gen 628/2000  train MSE: 0.6448, val MSE: 0.4475
Gen 629/2000  train MSE: 0.6448, val MSE: 0.4475
Gen 630/2000  train MSE: 0.6448, val MSE: 0.4473
Gen 631/2000  train MSE: 0.6447, val MSE: 0.4477
Gen 632/2000  train MSE: 0.6446, val MSE: 0.4467
Gen 633/2000  train MSE: 0.6446, val MSE: 0.4467
Gen 634/2000  train MSE: 0.6446, val MSE: 0.4467
Gen 635/2000  train MSE: 0.6446, val MSE: 0.4467
Gen 636/2000  train MSE: 0.6445, val MSE: 0.4476
Gen 637/2000  train MSE: 0.6444, val MSE: 0.4475
Gen 638/2000  train MSE: 0.6436, val MSE: 0.4472
Gen 639/2000  train MSE: 0.6436, val MSE: 0.4472
Gen 640/2000  train MSE: 0.6436, val MSE: 0.4472
Gen 641/2000  train MSE: 0.6429, val MSE: 0.4449
Gen 642/2000  train MSE: 0.6429, val MSE: 0.4449
Gen 643/2000  train MSE: 0.6429, val MSE: 0.4449
Gen 644/2000  train MSE: 0.6429, val MSE: 0.4449
Gen 645/2000  train MSE: 0.6428, val MSE: 0.4467
Gen 646/2000  train MSE: 0.6428, val MSE: 0.4467
Gen 647/2000  train MSE: 0.6428, val MSE: 0.4467
Gen 648/2000  train MSE: 0.6427, val MSE: 0.4456
Gen 649/2000  train MSE: 0.6423, val MSE: 0.4471
Gen 650/2000  train MSE: 0.6422, val MSE: 0.4449
Gen 651/2000  train MSE: 0.6422, val MSE: 0.4449
Gen 652/2000  train MSE: 0.6422, val MSE: 0.4449
Gen 653/2000  train MSE: 0.6422, val MSE: 0.4449
Gen 654/2000  train MSE: 0.6419, val MSE: 0.4441
Gen 655/2000  train MSE: 0.6419, val MSE: 0.4441
Gen 656/2000  train MSE: 0.6419, val MSE: 0.4441
Gen 657/2000  train MSE: 0.6414, val MSE: 0.4408
Gen 658/2000  train MSE: 0.6414, val MSE: 0.4408
Gen 659/2000  train MSE: 0.6414, val MSE: 0.4408
Gen 660/2000  train MSE: 0.6414, val MSE: 0.4408
Gen 661/2000  train MSE: 0.6414, val MSE: 0.4408
Gen 662/2000  train MSE: 0.6414, val MSE: 0.4408
Gen 663/2000  train MSE: 0.6414, val MSE: 0.4408
Gen 664/2000  train MSE: 0.6414, val MSE: 0.4408
Gen 665/2000  train MSE: 0.6414, val MSE: 0.4413
Gen 666/2000  train MSE: 0.6414, val MSE: 0.4413
Gen 667/2000  train MSE: 0.6413, val MSE: 0.4416
Gen 668/2000  train MSE: 0.6413, val MSE: 0.4416
Gen 669/2000  train MSE: 0.6413, val MSE: 0.4416
Gen 670/2000  train MSE: 0.6411, val MSE: 0.4397
Gen 671/2000  train MSE: 0.6408, val MSE: 0.4434
Gen 672/2000  train MSE: 0.6408, val MSE: 0.4434
Gen 673/2000  train MSE: 0.6408, val MSE: 0.4434
Gen 674/2000  train MSE: 0.6402, val MSE: 0.4398
Gen 675/2000  train MSE: 0.6402, val MSE: 0.4398
Gen 676/2000  train MSE: 0.6402, val MSE: 0.4398
Gen 677/2000  train MSE: 0.6402, val MSE: 0.4398
Gen 678/2000  train MSE: 0.6402, val MSE: 0.4398
Gen 679/2000  train MSE: 0.6402, val MSE: 0.4398
Gen 680/2000  train MSE: 0.6402, val MSE: 0.4399
Gen 681/2000  train MSE: 0.6402, val MSE: 0.4399
Gen 682/2000  train MSE: 0.6402, val MSE: 0.4399
Gen 683/2000  train MSE: 0.6402, val MSE: 0.4384
Gen 684/2000  train MSE: 0.6396, val MSE: 0.4385
Gen 685/2000  train MSE: 0.6395, val MSE: 0.4383
Gen 686/2000  train MSE: 0.6395, val MSE: 0.4383
Gen 687/2000  train MSE: 0.6395, val MSE: 0.4383
Gen 688/2000  train MSE: 0.6395, val MSE: 0.4383
Gen 689/2000  train MSE: 0.6395, val MSE: 0.4383
Gen 690/2000  train MSE: 0.6395, val MSE: 0.4383
Gen 691/2000  train MSE: 0.6395, val MSE: 0.4383
Gen 692/2000  train MSE: 0.6395, val MSE: 0.4383
Gen 693/2000  train MSE: 0.6395, val MSE: 0.4373
Gen 694/2000  train MSE: 0.6391, val MSE: 0.4352
Gen 695/2000  train MSE: 0.6391, val MSE: 0.4352
Gen 696/2000  train MSE: 0.6391, val MSE: 0.4352
Gen 697/2000  train MSE: 0.6385, val MSE: 0.4364
Gen 698/2000  train MSE: 0.6385, val MSE: 0.4364
Gen 699/2000  train MSE: 0.6385, val MSE: 0.4364
Gen 700/2000  train MSE: 0.6385, val MSE: 0.4364

--- Refinement @ gen 700 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 701/2000  train MSE: 0.6397, val MSE: 0.4369
Gen 702/2000  train MSE: 0.6397, val MSE: 0.4369
Gen 703/2000  train MSE: 0.6397, val MSE: 0.4369
Gen 704/2000  train MSE: 0.6397, val MSE: 0.4369
Gen 705/2000  train MSE: 0.6397, val MSE: 0.4369
Gen 706/2000  train MSE: 0.6397, val MSE: 0.4369
Gen 707/2000  train MSE: 0.6397, val MSE: 0.4369
Gen 708/2000  train MSE: 0.6397, val MSE: 0.4369
Gen 709/2000  train MSE: 0.6396, val MSE: 0.4361
Gen 710/2000  train MSE: 0.6396, val MSE: 0.4361
Gen 711/2000  train MSE: 0.6393, val MSE: 0.4383
Gen 712/2000  train MSE: 0.6383, val MSE: 0.4364
Gen 713/2000  train MSE: 0.6383, val MSE: 0.4364
Gen 714/2000  train MSE: 0.6383, val MSE: 0.4364
Gen 715/2000  train MSE: 0.6383, val MSE: 0.4364
Gen 716/2000  train MSE: 0.6383, val MSE: 0.4364
Gen 717/2000  train MSE: 0.6379, val MSE: 0.4363
Gen 718/2000  train MSE: 0.6379, val MSE: 0.4363
Gen 719/2000  train MSE: 0.6379, val MSE: 0.4363
Gen 720/2000  train MSE: 0.6379, val MSE: 0.4363
Gen 721/2000  train MSE: 0.6379, val MSE: 0.4363
Gen 722/2000  train MSE: 0.6379, val MSE: 0.4363
Gen 723/2000  train MSE: 0.6379, val MSE: 0.4363
Gen 724/2000  train MSE: 0.6379, val MSE: 0.4363
Gen 725/2000  train MSE: 0.6375, val MSE: 0.4348
Gen 726/2000  train MSE: 0.6368, val MSE: 0.4331
Gen 727/2000  train MSE: 0.6368, val MSE: 0.4331
Gen 728/2000  train MSE: 0.6368, val MSE: 0.4331
Gen 729/2000  train MSE: 0.6367, val MSE: 0.4332
Gen 730/2000  train MSE: 0.6367, val MSE: 0.4332
Gen 731/2000  train MSE: 0.6367, val MSE: 0.4332
Gen 732/2000  train MSE: 0.6367, val MSE: 0.4332
Gen 733/2000  train MSE: 0.6367, val MSE: 0.4332
Gen 734/2000  train MSE: 0.6365, val MSE: 0.4326
Gen 735/2000  train MSE: 0.6365, val MSE: 0.4326
Gen 736/2000  train MSE: 0.6362, val MSE: 0.4327
Gen 737/2000  train MSE: 0.6362, val MSE: 0.4327
Gen 738/2000  train MSE: 0.6362, val MSE: 0.4327
Gen 739/2000  train MSE: 0.6362, val MSE: 0.4327
Gen 740/2000  train MSE: 0.6362, val MSE: 0.4327
Gen 741/2000  train MSE: 0.6353, val MSE: 0.4296
Gen 742/2000  train MSE: 0.6353, val MSE: 0.4296
Gen 743/2000  train MSE: 0.6353, val MSE: 0.4296
Gen 744/2000  train MSE: 0.6353, val MSE: 0.4296
Gen 745/2000  train MSE: 0.6353, val MSE: 0.4296
Gen 746/2000  train MSE: 0.6353, val MSE: 0.4296
Gen 747/2000  train MSE: 0.6353, val MSE: 0.4296
Gen 748/2000  train MSE: 0.6353, val MSE: 0.4296
Gen 749/2000  train MSE: 0.6353, val MSE: 0.4296
Gen 750/2000  train MSE: 0.6351, val MSE: 0.4293
Gen 751/2000  train MSE: 0.6349, val MSE: 0.4310
Gen 752/2000  train MSE: 0.6349, val MSE: 0.4310
Gen 753/2000  train MSE: 0.6349, val MSE: 0.4310
Gen 754/2000  train MSE: 0.6349, val MSE: 0.4310
Gen 755/2000  train MSE: 0.6349, val MSE: 0.4310
Gen 756/2000  train MSE: 0.6349, val MSE: 0.4310
Gen 757/2000  train MSE: 0.6349, val MSE: 0.4310
Gen 758/2000  train MSE: 0.6349, val MSE: 0.4310
Gen 759/2000  train MSE: 0.6345, val MSE: 0.4305
Gen 760/2000  train MSE: 0.6343, val MSE: 0.4315
Gen 761/2000  train MSE: 0.6340, val MSE: 0.4290
Gen 762/2000  train MSE: 0.6340, val MSE: 0.4301
Gen 763/2000  train MSE: 0.6340, val MSE: 0.4301
Gen 764/2000  train MSE: 0.6340, val MSE: 0.4301
Gen 765/2000  train MSE: 0.6339, val MSE: 0.4306
Gen 766/2000  train MSE: 0.6338, val MSE: 0.4289
Gen 767/2000  train MSE: 0.6336, val MSE: 0.4301
Gen 768/2000  train MSE: 0.6332, val MSE: 0.4292
Gen 769/2000  train MSE: 0.6332, val MSE: 0.4292
Gen 770/2000  train MSE: 0.6332, val MSE: 0.4292
Gen 771/2000  train MSE: 0.6332, val MSE: 0.4292
Gen 772/2000  train MSE: 0.6332, val MSE: 0.4292
Gen 773/2000  train MSE: 0.6328, val MSE: 0.4284
Gen 774/2000  train MSE: 0.6328, val MSE: 0.4284
Gen 775/2000  train MSE: 0.6328, val MSE: 0.4284
Gen 776/2000  train MSE: 0.6328, val MSE: 0.4284
Gen 777/2000  train MSE: 0.6328, val MSE: 0.4284
Gen 778/2000  train MSE: 0.6328, val MSE: 0.4284
Gen 779/2000  train MSE: 0.6327, val MSE: 0.4270
Gen 780/2000  train MSE: 0.6327, val MSE: 0.4270
Gen 781/2000  train MSE: 0.6327, val MSE: 0.4270
Gen 782/2000  train MSE: 0.6327, val MSE: 0.4270
Gen 783/2000  train MSE: 0.6327, val MSE: 0.4270
Gen 784/2000  train MSE: 0.6327, val MSE: 0.4270
Gen 785/2000  train MSE: 0.6325, val MSE: 0.4274
Gen 786/2000  train MSE: 0.6325, val MSE: 0.4274
Gen 787/2000  train MSE: 0.6323, val MSE: 0.4264
Gen 788/2000  train MSE: 0.6323, val MSE: 0.4264
Gen 789/2000  train MSE: 0.6323, val MSE: 0.4264
Gen 790/2000  train MSE: 0.6323, val MSE: 0.4264
Gen 791/2000  train MSE: 0.6323, val MSE: 0.4264
Gen 792/2000  train MSE: 0.6323, val MSE: 0.4264
Gen 793/2000  train MSE: 0.6323, val MSE: 0.4264
Gen 794/2000  train MSE: 0.6323, val MSE: 0.4264
Gen 795/2000  train MSE: 0.6323, val MSE: 0.4264
Gen 796/2000  train MSE: 0.6323, val MSE: 0.4264
Gen 797/2000  train MSE: 0.6323, val MSE: 0.4264
Gen 798/2000  train MSE: 0.6321, val MSE: 0.4276
Gen 799/2000  train MSE: 0.6321, val MSE: 0.4276
Gen 800/2000  train MSE: 0.6321, val MSE: 0.4276

--- Refinement @ gen 800 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 801/2000  train MSE: 0.6339, val MSE: 0.4275
Gen 802/2000  train MSE: 0.6330, val MSE: 0.4298
Gen 803/2000  train MSE: 0.6330, val MSE: 0.4298
Gen 804/2000  train MSE: 0.6330, val MSE: 0.4298
Gen 805/2000  train MSE: 0.6330, val MSE: 0.4298
Gen 806/2000  train MSE: 0.6330, val MSE: 0.4298
Gen 807/2000  train MSE: 0.6330, val MSE: 0.4298
Gen 808/2000  train MSE: 0.6330, val MSE: 0.4298
Gen 809/2000  train MSE: 0.6330, val MSE: 0.4298
Gen 810/2000  train MSE: 0.6330, val MSE: 0.4298
Gen 811/2000  train MSE: 0.6330, val MSE: 0.4298
Gen 812/2000  train MSE: 0.6330, val MSE: 0.4269
Gen 813/2000  train MSE: 0.6329, val MSE: 0.4285
Gen 814/2000  train MSE: 0.6329, val MSE: 0.4285
Gen 815/2000  train MSE: 0.6329, val MSE: 0.4285
Gen 816/2000  train MSE: 0.6329, val MSE: 0.4285
Gen 817/2000  train MSE: 0.6329, val MSE: 0.4285
Gen 818/2000  train MSE: 0.6328, val MSE: 0.4273
Gen 819/2000  train MSE: 0.6328, val MSE: 0.4273
Gen 820/2000  train MSE: 0.6328, val MSE: 0.4273
Gen 821/2000  train MSE: 0.6326, val MSE: 0.4282
Gen 822/2000  train MSE: 0.6326, val MSE: 0.4282
Gen 823/2000  train MSE: 0.6326, val MSE: 0.4282
Gen 824/2000  train MSE: 0.6326, val MSE: 0.4282
Gen 825/2000  train MSE: 0.6326, val MSE: 0.4282
Gen 826/2000  train MSE: 0.6317, val MSE: 0.4284
Gen 827/2000  train MSE: 0.6317, val MSE: 0.4284
Gen 828/2000  train MSE: 0.6317, val MSE: 0.4284
Gen 829/2000  train MSE: 0.6317, val MSE: 0.4284
Gen 830/2000  train MSE: 0.6317, val MSE: 0.4284
Gen 831/2000  train MSE: 0.6317, val MSE: 0.4284
Gen 832/2000  train MSE: 0.6317, val MSE: 0.4265
Gen 833/2000  train MSE: 0.6312, val MSE: 0.4274
Gen 834/2000  train MSE: 0.6312, val MSE: 0.4274
Gen 835/2000  train MSE: 0.6312, val MSE: 0.4269
Gen 836/2000  train MSE: 0.6310, val MSE: 0.4254
Gen 837/2000  train MSE: 0.6310, val MSE: 0.4254
Gen 838/2000  train MSE: 0.6310, val MSE: 0.4254
Gen 839/2000  train MSE: 0.6310, val MSE: 0.4254
Gen 840/2000  train MSE: 0.6307, val MSE: 0.4256
Gen 841/2000  train MSE: 0.6306, val MSE: 0.4241
Gen 842/2000  train MSE: 0.6304, val MSE: 0.4269
Gen 843/2000  train MSE: 0.6304, val MSE: 0.4269
Gen 844/2000  train MSE: 0.6304, val MSE: 0.4269
Gen 845/2000  train MSE: 0.6304, val MSE: 0.4269
Gen 846/2000  train MSE: 0.6304, val MSE: 0.4269
Gen 847/2000  train MSE: 0.6304, val MSE: 0.4269
Gen 848/2000  train MSE: 0.6304, val MSE: 0.4269
Gen 849/2000  train MSE: 0.6304, val MSE: 0.4269
Gen 850/2000  train MSE: 0.6304, val MSE: 0.4269
Gen 851/2000  train MSE: 0.6304, val MSE: 0.4269
Gen 852/2000  train MSE: 0.6303, val MSE: 0.4249
Gen 853/2000  train MSE: 0.6303, val MSE: 0.4249
Gen 854/2000  train MSE: 0.6303, val MSE: 0.4249
Gen 855/2000  train MSE: 0.6302, val MSE: 0.4231
Gen 856/2000  train MSE: 0.6302, val MSE: 0.4231
Gen 857/2000  train MSE: 0.6302, val MSE: 0.4244
Gen 858/2000  train MSE: 0.6298, val MSE: 0.4251
Gen 859/2000  train MSE: 0.6298, val MSE: 0.4251
Gen 860/2000  train MSE: 0.6298, val MSE: 0.4251
Gen 861/2000  train MSE: 0.6298, val MSE: 0.4251
Gen 862/2000  train MSE: 0.6298, val MSE: 0.4251
Gen 863/2000  train MSE: 0.6298, val MSE: 0.4251
Gen 864/2000  train MSE: 0.6298, val MSE: 0.4251
Gen 865/2000  train MSE: 0.6296, val MSE: 0.4240
Gen 866/2000  train MSE: 0.6296, val MSE: 0.4240
Gen 867/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 868/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 869/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 870/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 871/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 872/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 873/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 874/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 875/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 876/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 877/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 878/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 879/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 880/2000  train MSE: 0.6291, val MSE: 0.4223
Gen 881/2000  train MSE: 0.6290, val MSE: 0.4241
Gen 882/2000  train MSE: 0.6290, val MSE: 0.4241
Gen 883/2000  train MSE: 0.6290, val MSE: 0.4233
Gen 884/2000  train MSE: 0.6290, val MSE: 0.4233
Gen 885/2000  train MSE: 0.6289, val MSE: 0.4243
Gen 886/2000  train MSE: 0.6286, val MSE: 0.4227
Gen 887/2000  train MSE: 0.6286, val MSE: 0.4215
Gen 888/2000  train MSE: 0.6285, val MSE: 0.4227
Gen 889/2000  train MSE: 0.6283, val MSE: 0.4224
Gen 890/2000  train MSE: 0.6283, val MSE: 0.4234
Gen 891/2000  train MSE: 0.6280, val MSE: 0.4217
Gen 892/2000  train MSE: 0.6280, val MSE: 0.4219
Gen 893/2000  train MSE: 0.6280, val MSE: 0.4219
Gen 894/2000  train MSE: 0.6280, val MSE: 0.4219
Gen 895/2000  train MSE: 0.6277, val MSE: 0.4208
Gen 896/2000  train MSE: 0.6277, val MSE: 0.4208
Gen 897/2000  train MSE: 0.6277, val MSE: 0.4208
Gen 898/2000  train MSE: 0.6276, val MSE: 0.4192
Gen 899/2000  train MSE: 0.6276, val MSE: 0.4192
Gen 900/2000  train MSE: 0.6275, val MSE: 0.4203

--- Refinement @ gen 900 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 901/2000  train MSE: 0.6283, val MSE: 0.4216
Gen 902/2000  train MSE: 0.6283, val MSE: 0.4216
Gen 903/2000  train MSE: 0.6283, val MSE: 0.4216
Gen 904/2000  train MSE: 0.6283, val MSE: 0.4210
Gen 905/2000  train MSE: 0.6281, val MSE: 0.4208
Gen 906/2000  train MSE: 0.6281, val MSE: 0.4208
Gen 907/2000  train MSE: 0.6280, val MSE: 0.4192
Gen 908/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 909/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 910/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 911/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 912/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 913/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 914/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 915/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 916/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 917/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 918/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 919/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 920/2000  train MSE: 0.6274, val MSE: 0.4200
Gen 921/2000  train MSE: 0.6274, val MSE: 0.4201
Gen 922/2000  train MSE: 0.6274, val MSE: 0.4201
Gen 923/2000  train MSE: 0.6274, val MSE: 0.4201
Gen 924/2000  train MSE: 0.6274, val MSE: 0.4201
Gen 925/2000  train MSE: 0.6274, val MSE: 0.4201
Gen 926/2000  train MSE: 0.6268, val MSE: 0.4174
Gen 927/2000  train MSE: 0.6268, val MSE: 0.4174
Gen 928/2000  train MSE: 0.6268, val MSE: 0.4174
Gen 929/2000  train MSE: 0.6268, val MSE: 0.4174
Gen 930/2000  train MSE: 0.6268, val MSE: 0.4174
Gen 931/2000  train MSE: 0.6268, val MSE: 0.4174
Gen 932/2000  train MSE: 0.6268, val MSE: 0.4174
Gen 933/2000  train MSE: 0.6268, val MSE: 0.4174
Gen 934/2000  train MSE: 0.6266, val MSE: 0.4178
Gen 935/2000  train MSE: 0.6266, val MSE: 0.4178
Gen 936/2000  train MSE: 0.6265, val MSE: 0.4169
Gen 937/2000  train MSE: 0.6265, val MSE: 0.4175
Gen 938/2000  train MSE: 0.6265, val MSE: 0.4175
Gen 939/2000  train MSE: 0.6265, val MSE: 0.4175
Gen 940/2000  train MSE: 0.6265, val MSE: 0.4175
Gen 941/2000  train MSE: 0.6259, val MSE: 0.4169
Gen 942/2000  train MSE: 0.6259, val MSE: 0.4169
Gen 943/2000  train MSE: 0.6259, val MSE: 0.4169
Gen 944/2000  train MSE: 0.6259, val MSE: 0.4169
Gen 945/2000  train MSE: 0.6259, val MSE: 0.4169
Gen 946/2000  train MSE: 0.6259, val MSE: 0.4169
Gen 947/2000  train MSE: 0.6259, val MSE: 0.4169
Gen 948/2000  train MSE: 0.6259, val MSE: 0.4169
Gen 949/2000  train MSE: 0.6259, val MSE: 0.4169
Gen 950/2000  train MSE: 0.6259, val MSE: 0.4169
Gen 951/2000  train MSE: 0.6259, val MSE: 0.4169
Gen 952/2000  train MSE: 0.6256, val MSE: 0.4160
Gen 953/2000  train MSE: 0.6256, val MSE: 0.4160
Gen 954/2000  train MSE: 0.6256, val MSE: 0.4160
Gen 955/2000  train MSE: 0.6256, val MSE: 0.4160
Gen 956/2000  train MSE: 0.6256, val MSE: 0.4160
Gen 957/2000  train MSE: 0.6256, val MSE: 0.4160
Gen 958/2000  train MSE: 0.6256, val MSE: 0.4160
Gen 959/2000  train MSE: 0.6256, val MSE: 0.4160
Gen 960/2000  train MSE: 0.6256, val MSE: 0.4160
Gen 961/2000  train MSE: 0.6252, val MSE: 0.4155
Gen 962/2000  train MSE: 0.6252, val MSE: 0.4155
Gen 963/2000  train MSE: 0.6252, val MSE: 0.4155
Gen 964/2000  train MSE: 0.6252, val MSE: 0.4155
Gen 965/2000  train MSE: 0.6252, val MSE: 0.4155
Gen 966/2000  train MSE: 0.6252, val MSE: 0.4155
Gen 967/2000  train MSE: 0.6252, val MSE: 0.4155
Gen 968/2000  train MSE: 0.6252, val MSE: 0.4155
Gen 969/2000  train MSE: 0.6252, val MSE: 0.4155
Gen 970/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 971/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 972/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 973/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 974/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 975/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 976/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 977/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 978/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 979/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 980/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 981/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 982/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 983/2000  train MSE: 0.6250, val MSE: 0.4145
Gen 984/2000  train MSE: 0.6250, val MSE: 0.4136
Gen 985/2000  train MSE: 0.6250, val MSE: 0.4136
Gen 986/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 987/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 988/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 989/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 990/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 991/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 992/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 993/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 994/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 995/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 996/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 997/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 998/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 999/2000  train MSE: 0.6249, val MSE: 0.4141
Gen 1000/2000  train MSE: 0.6249, val MSE: 0.4131

--- Refinement @ gen 1000 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1001/2000  train MSE: 0.6260, val MSE: 0.4131
Gen 1002/2000  train MSE: 0.6260, val MSE: 0.4131
Gen 1003/2000  train MSE: 0.6260, val MSE: 0.4131
Gen 1004/2000  train MSE: 0.6260, val MSE: 0.4131
Gen 1005/2000  train MSE: 0.6260, val MSE: 0.4131
Gen 1006/2000  train MSE: 0.6260, val MSE: 0.4131
Gen 1007/2000  train MSE: 0.6260, val MSE: 0.4131
Gen 1008/2000  train MSE: 0.6260, val MSE: 0.4131
Gen 1009/2000  train MSE: 0.6256, val MSE: 0.4119
Gen 1010/2000  train MSE: 0.6256, val MSE: 0.4119
Gen 1011/2000  train MSE: 0.6256, val MSE: 0.4119
Gen 1012/2000  train MSE: 0.6256, val MSE: 0.4119
Gen 1013/2000  train MSE: 0.6256, val MSE: 0.4119
Gen 1014/2000  train MSE: 0.6256, val MSE: 0.4119
Gen 1015/2000  train MSE: 0.6256, val MSE: 0.4119
Gen 1016/2000  train MSE: 0.6256, val MSE: 0.4119
Gen 1017/2000  train MSE: 0.6256, val MSE: 0.4119
Gen 1018/2000  train MSE: 0.6255, val MSE: 0.4121
Gen 1019/2000  train MSE: 0.6255, val MSE: 0.4121
Gen 1020/2000  train MSE: 0.6255, val MSE: 0.4121
Gen 1021/2000  train MSE: 0.6254, val MSE: 0.4124
Gen 1022/2000  train MSE: 0.6254, val MSE: 0.4124
Gen 1023/2000  train MSE: 0.6254, val MSE: 0.4124
Gen 1024/2000  train MSE: 0.6254, val MSE: 0.4124
Gen 1025/2000  train MSE: 0.6254, val MSE: 0.4124
Gen 1026/2000  train MSE: 0.6254, val MSE: 0.4124
Gen 1027/2000  train MSE: 0.6253, val MSE: 0.4117
Gen 1028/2000  train MSE: 0.6251, val MSE: 0.4133
Gen 1029/2000  train MSE: 0.6251, val MSE: 0.4133
Gen 1030/2000  train MSE: 0.6251, val MSE: 0.4133
Gen 1031/2000  train MSE: 0.6251, val MSE: 0.4133
Gen 1032/2000  train MSE: 0.6251, val MSE: 0.4133
Gen 1033/2000  train MSE: 0.6251, val MSE: 0.4133
Gen 1034/2000  train MSE: 0.6250, val MSE: 0.4124
Gen 1035/2000  train MSE: 0.6250, val MSE: 0.4130
Gen 1036/2000  train MSE: 0.6250, val MSE: 0.4130
Gen 1037/2000  train MSE: 0.6250, val MSE: 0.4130
Gen 1038/2000  train MSE: 0.6250, val MSE: 0.4130
Gen 1039/2000  train MSE: 0.6248, val MSE: 0.4112
Gen 1040/2000  train MSE: 0.6247, val MSE: 0.4108
Gen 1041/2000  train MSE: 0.6247, val MSE: 0.4108
Gen 1042/2000  train MSE: 0.6247, val MSE: 0.4115
Gen 1043/2000  train MSE: 0.6243, val MSE: 0.4112
Gen 1044/2000  train MSE: 0.6242, val MSE: 0.4107
Gen 1045/2000  train MSE: 0.6240, val MSE: 0.4100
Gen 1046/2000  train MSE: 0.6238, val MSE: 0.4101
Gen 1047/2000  train MSE: 0.6238, val MSE: 0.4101
Gen 1048/2000  train MSE: 0.6237, val MSE: 0.4100
Gen 1049/2000  train MSE: 0.6235, val MSE: 0.4103
Gen 1050/2000  train MSE: 0.6234, val MSE: 0.4085
Gen 1051/2000  train MSE: 0.6234, val MSE: 0.4085
Gen 1052/2000  train MSE: 0.6234, val MSE: 0.4085
Gen 1053/2000  train MSE: 0.6234, val MSE: 0.4088
Gen 1054/2000  train MSE: 0.6233, val MSE: 0.4084
Gen 1055/2000  train MSE: 0.6230, val MSE: 0.4085
Gen 1056/2000  train MSE: 0.6229, val MSE: 0.4072
Gen 1057/2000  train MSE: 0.6229, val MSE: 0.4072
Gen 1058/2000  train MSE: 0.6228, val MSE: 0.4087
Gen 1059/2000  train MSE: 0.6228, val MSE: 0.4087
Gen 1060/2000  train MSE: 0.6228, val MSE: 0.4087
Gen 1061/2000  train MSE: 0.6227, val MSE: 0.4087
Gen 1062/2000  train MSE: 0.6227, val MSE: 0.4087
Gen 1063/2000  train MSE: 0.6227, val MSE: 0.4087
Gen 1064/2000  train MSE: 0.6227, val MSE: 0.4087
Gen 1065/2000  train MSE: 0.6227, val MSE: 0.4087
Gen 1066/2000  train MSE: 0.6226, val MSE: 0.4069
Gen 1067/2000  train MSE: 0.6226, val MSE: 0.4069
Gen 1068/2000  train MSE: 0.6226, val MSE: 0.4069
Gen 1069/2000  train MSE: 0.6226, val MSE: 0.4069
Gen 1070/2000  train MSE: 0.6222, val MSE: 0.4069
Gen 1071/2000  train MSE: 0.6222, val MSE: 0.4069
Gen 1072/2000  train MSE: 0.6222, val MSE: 0.4069
Gen 1073/2000  train MSE: 0.6222, val MSE: 0.4069
Gen 1074/2000  train MSE: 0.6222, val MSE: 0.4069
Gen 1075/2000  train MSE: 0.6222, val MSE: 0.4069
Gen 1076/2000  train MSE: 0.6222, val MSE: 0.4069
Gen 1077/2000  train MSE: 0.6222, val MSE: 0.4069
Gen 1078/2000  train MSE: 0.6222, val MSE: 0.4069
Gen 1079/2000  train MSE: 0.6222, val MSE: 0.4069
Gen 1080/2000  train MSE: 0.6222, val MSE: 0.4077
Gen 1081/2000  train MSE: 0.6222, val MSE: 0.4078
Gen 1082/2000  train MSE: 0.6222, val MSE: 0.4078
Gen 1083/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1084/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1085/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1086/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1087/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1088/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1089/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1090/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1091/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1092/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1093/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1094/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1095/2000  train MSE: 0.6218, val MSE: 0.4074
Gen 1096/2000  train MSE: 0.6217, val MSE: 0.4062
Gen 1097/2000  train MSE: 0.6217, val MSE: 0.4062
Gen 1098/2000  train MSE: 0.6217, val MSE: 0.4062
Gen 1099/2000  train MSE: 0.6217, val MSE: 0.4062
Gen 1100/2000  train MSE: 0.6217, val MSE: 0.4062

--- Refinement @ gen 1100 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1101/2000  train MSE: 0.6221, val MSE: 0.4066
Gen 1102/2000  train MSE: 0.6221, val MSE: 0.4066
Gen 1103/2000  train MSE: 0.6221, val MSE: 0.4066
Gen 1104/2000  train MSE: 0.6220, val MSE: 0.4069
Gen 1105/2000  train MSE: 0.6220, val MSE: 0.4069
Gen 1106/2000  train MSE: 0.6220, val MSE: 0.4069
Gen 1107/2000  train MSE: 0.6220, val MSE: 0.4069
Gen 1108/2000  train MSE: 0.6220, val MSE: 0.4069
Gen 1109/2000  train MSE: 0.6219, val MSE: 0.4067
Gen 1110/2000  train MSE: 0.6219, val MSE: 0.4067
Gen 1111/2000  train MSE: 0.6219, val MSE: 0.4067
Gen 1112/2000  train MSE: 0.6219, val MSE: 0.4067
Gen 1113/2000  train MSE: 0.6219, val MSE: 0.4067
Gen 1114/2000  train MSE: 0.6219, val MSE: 0.4067
Gen 1115/2000  train MSE: 0.6219, val MSE: 0.4067
Gen 1116/2000  train MSE: 0.6219, val MSE: 0.4067
Gen 1117/2000  train MSE: 0.6219, val MSE: 0.4075
Gen 1118/2000  train MSE: 0.6219, val MSE: 0.4075
Gen 1119/2000  train MSE: 0.6219, val MSE: 0.4075
Gen 1120/2000  train MSE: 0.6219, val MSE: 0.4075
Gen 1121/2000  train MSE: 0.6219, val MSE: 0.4075
Gen 1122/2000  train MSE: 0.6219, val MSE: 0.4075
Gen 1123/2000  train MSE: 0.6217, val MSE: 0.4068
Gen 1124/2000  train MSE: 0.6217, val MSE: 0.4068
Gen 1125/2000  train MSE: 0.6217, val MSE: 0.4068
Gen 1126/2000  train MSE: 0.6217, val MSE: 0.4068
Gen 1127/2000  train MSE: 0.6217, val MSE: 0.4068
Gen 1128/2000  train MSE: 0.6214, val MSE: 0.4074
Gen 1129/2000  train MSE: 0.6214, val MSE: 0.4074
Gen 1130/2000  train MSE: 0.6214, val MSE: 0.4074
Gen 1131/2000  train MSE: 0.6214, val MSE: 0.4074
Gen 1132/2000  train MSE: 0.6214, val MSE: 0.4074
Gen 1133/2000  train MSE: 0.6214, val MSE: 0.4074
Gen 1134/2000  train MSE: 0.6214, val MSE: 0.4074
Gen 1135/2000  train MSE: 0.6214, val MSE: 0.4074
Gen 1136/2000  train MSE: 0.6214, val MSE: 0.4074
Gen 1137/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1138/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1139/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1140/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1141/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1142/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1143/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1144/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1145/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1146/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1147/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1148/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1149/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1150/2000  train MSE: 0.6212, val MSE: 0.4061
Gen 1151/2000  train MSE: 0.6212, val MSE: 0.4073
Gen 1152/2000  train MSE: 0.6211, val MSE: 0.4066
Gen 1153/2000  train MSE: 0.6211, val MSE: 0.4066
Gen 1154/2000  train MSE: 0.6211, val MSE: 0.4066
Gen 1155/2000  train MSE: 0.6211, val MSE: 0.4066
Gen 1156/2000  train MSE: 0.6211, val MSE: 0.4068
Gen 1157/2000  train MSE: 0.6211, val MSE: 0.4068
Gen 1158/2000  train MSE: 0.6211, val MSE: 0.4068
Gen 1159/2000  train MSE: 0.6211, val MSE: 0.4068
Gen 1160/2000  train MSE: 0.6210, val MSE: 0.4052
Gen 1161/2000  train MSE: 0.6210, val MSE: 0.4052
Gen 1162/2000  train MSE: 0.6209, val MSE: 0.4061
Gen 1163/2000  train MSE: 0.6209, val MSE: 0.4061
Gen 1164/2000  train MSE: 0.6208, val MSE: 0.4067
Gen 1165/2000  train MSE: 0.6208, val MSE: 0.4067
Gen 1166/2000  train MSE: 0.6208, val MSE: 0.4070
Gen 1167/2000  train MSE: 0.6208, val MSE: 0.4070
Gen 1168/2000  train MSE: 0.6208, val MSE: 0.4070
Gen 1169/2000  train MSE: 0.6208, val MSE: 0.4070
Gen 1170/2000  train MSE: 0.6208, val MSE: 0.4062
Gen 1171/2000  train MSE: 0.6208, val MSE: 0.4062
Gen 1172/2000  train MSE: 0.6203, val MSE: 0.4066
Gen 1173/2000  train MSE: 0.6203, val MSE: 0.4066
Gen 1174/2000  train MSE: 0.6203, val MSE: 0.4066
Gen 1175/2000  train MSE: 0.6203, val MSE: 0.4066
Gen 1176/2000  train MSE: 0.6203, val MSE: 0.4066
Gen 1177/2000  train MSE: 0.6203, val MSE: 0.4066
Gen 1178/2000  train MSE: 0.6203, val MSE: 0.4066
Gen 1179/2000  train MSE: 0.6203, val MSE: 0.4052
Gen 1180/2000  train MSE: 0.6203, val MSE: 0.4052
Gen 1181/2000  train MSE: 0.6203, val MSE: 0.4052
Gen 1182/2000  train MSE: 0.6203, val MSE: 0.4052
Gen 1183/2000  train MSE: 0.6203, val MSE: 0.4052
Gen 1184/2000  train MSE: 0.6203, val MSE: 0.4052
Gen 1185/2000  train MSE: 0.6201, val MSE: 0.4041
Gen 1186/2000  train MSE: 0.6201, val MSE: 0.4041
Gen 1187/2000  train MSE: 0.6200, val MSE: 0.4051
Gen 1188/2000  train MSE: 0.6200, val MSE: 0.4051
Gen 1189/2000  train MSE: 0.6200, val MSE: 0.4051
Gen 1190/2000  train MSE: 0.6196, val MSE: 0.4040
Gen 1191/2000  train MSE: 0.6196, val MSE: 0.4040
Gen 1192/2000  train MSE: 0.6196, val MSE: 0.4040
Gen 1193/2000  train MSE: 0.6196, val MSE: 0.4040
Gen 1194/2000  train MSE: 0.6196, val MSE: 0.4040
Gen 1195/2000  train MSE: 0.6196, val MSE: 0.4040
Gen 1196/2000  train MSE: 0.6196, val MSE: 0.4040
Gen 1197/2000  train MSE: 0.6196, val MSE: 0.4040
Gen 1198/2000  train MSE: 0.6196, val MSE: 0.4040
Gen 1199/2000  train MSE: 0.6196, val MSE: 0.4040
Gen 1200/2000  train MSE: 0.6196, val MSE: 0.4040

--- Refinement @ gen 1200 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1201/2000  train MSE: 0.6206, val MSE: 0.4047
Gen 1202/2000  train MSE: 0.6206, val MSE: 0.4047
Gen 1203/2000  train MSE: 0.6205, val MSE: 0.4060
Gen 1204/2000  train MSE: 0.6205, val MSE: 0.4060
Gen 1205/2000  train MSE: 0.6205, val MSE: 0.4060
Gen 1206/2000  train MSE: 0.6205, val MSE: 0.4060
Gen 1207/2000  train MSE: 0.6205, val MSE: 0.4060
Gen 1208/2000  train MSE: 0.6205, val MSE: 0.4060
Gen 1209/2000  train MSE: 0.6205, val MSE: 0.4060
Gen 1210/2000  train MSE: 0.6205, val MSE: 0.4060
Gen 1211/2000  train MSE: 0.6205, val MSE: 0.4060
Gen 1212/2000  train MSE: 0.6205, val MSE: 0.4044
Gen 1213/2000  train MSE: 0.6201, val MSE: 0.4059
Gen 1214/2000  train MSE: 0.6201, val MSE: 0.4059
Gen 1215/2000  train MSE: 0.6201, val MSE: 0.4059
Gen 1216/2000  train MSE: 0.6201, val MSE: 0.4059
Gen 1217/2000  train MSE: 0.6201, val MSE: 0.4059
Gen 1218/2000  train MSE: 0.6201, val MSE: 0.4059
Gen 1219/2000  train MSE: 0.6201, val MSE: 0.4059
Gen 1220/2000  train MSE: 0.6201, val MSE: 0.4059
Gen 1221/2000  train MSE: 0.6201, val MSE: 0.4059
Gen 1222/2000  train MSE: 0.6201, val MSE: 0.4059
Gen 1223/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1224/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1225/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1226/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1227/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1228/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1229/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1230/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1231/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1232/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1233/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1234/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1235/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1236/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1237/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1238/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1239/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1240/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1241/2000  train MSE: 0.6194, val MSE: 0.4031
Gen 1242/2000  train MSE: 0.6192, val MSE: 0.4032
Gen 1243/2000  train MSE: 0.6192, val MSE: 0.4032
Gen 1244/2000  train MSE: 0.6192, val MSE: 0.4032
Gen 1245/2000  train MSE: 0.6188, val MSE: 0.4010
Gen 1246/2000  train MSE: 0.6188, val MSE: 0.4010
Gen 1247/2000  train MSE: 0.6188, val MSE: 0.4010
Gen 1248/2000  train MSE: 0.6186, val MSE: 0.3995
Gen 1249/2000  train MSE: 0.6186, val MSE: 0.3995
Gen 1250/2000  train MSE: 0.6186, val MSE: 0.3995
Gen 1251/2000  train MSE: 0.6186, val MSE: 0.3995
Gen 1252/2000  train MSE: 0.6186, val MSE: 0.3995
Gen 1253/2000  train MSE: 0.6186, val MSE: 0.3995
Gen 1254/2000  train MSE: 0.6186, val MSE: 0.3995
Gen 1255/2000  train MSE: 0.6186, val MSE: 0.3995
Gen 1256/2000  train MSE: 0.6186, val MSE: 0.3995
Gen 1257/2000  train MSE: 0.6186, val MSE: 0.3995
Gen 1258/2000  train MSE: 0.6186, val MSE: 0.3995
Gen 1259/2000  train MSE: 0.6186, val MSE: 0.3993
Gen 1260/2000  train MSE: 0.6186, val MSE: 0.3993
Gen 1261/2000  train MSE: 0.6186, val MSE: 0.3993
Gen 1262/2000  train MSE: 0.6186, val MSE: 0.3993
Gen 1263/2000  train MSE: 0.6186, val MSE: 0.3993
Gen 1264/2000  train MSE: 0.6186, val MSE: 0.3993
Gen 1265/2000  train MSE: 0.6186, val MSE: 0.3993
Gen 1266/2000  train MSE: 0.6186, val MSE: 0.3993
Gen 1267/2000  train MSE: 0.6186, val MSE: 0.3999
Gen 1268/2000  train MSE: 0.6184, val MSE: 0.3999
Gen 1269/2000  train MSE: 0.6184, val MSE: 0.3996
Gen 1270/2000  train MSE: 0.6184, val MSE: 0.3996
Gen 1271/2000  train MSE: 0.6184, val MSE: 0.3996
Gen 1272/2000  train MSE: 0.6184, val MSE: 0.3996
Gen 1273/2000  train MSE: 0.6184, val MSE: 0.3996
Gen 1274/2000  train MSE: 0.6184, val MSE: 0.3996
Gen 1275/2000  train MSE: 0.6184, val MSE: 0.3996
Gen 1276/2000  train MSE: 0.6184, val MSE: 0.3996
Gen 1277/2000  train MSE: 0.6184, val MSE: 0.3996
Gen 1278/2000  train MSE: 0.6183, val MSE: 0.3997
Gen 1279/2000  train MSE: 0.6183, val MSE: 0.3989
Gen 1280/2000  train MSE: 0.6182, val MSE: 0.3994
Gen 1281/2000  train MSE: 0.6182, val MSE: 0.3986
Gen 1282/2000  train MSE: 0.6181, val MSE: 0.3993
Gen 1283/2000  train MSE: 0.6180, val MSE: 0.3990
Gen 1284/2000  train MSE: 0.6180, val MSE: 0.3990
Gen 1285/2000  train MSE: 0.6179, val MSE: 0.3991
Gen 1286/2000  train MSE: 0.6179, val MSE: 0.3991
Gen 1287/2000  train MSE: 0.6179, val MSE: 0.3991
Gen 1288/2000  train MSE: 0.6179, val MSE: 0.3995
Gen 1289/2000  train MSE: 0.6178, val MSE: 0.3989
Gen 1290/2000  train MSE: 0.6178, val MSE: 0.3989
Gen 1291/2000  train MSE: 0.6178, val MSE: 0.3994
Gen 1292/2000  train MSE: 0.6178, val MSE: 0.3994
Gen 1293/2000  train MSE: 0.6178, val MSE: 0.3994
Gen 1294/2000  train MSE: 0.6176, val MSE: 0.3985
Gen 1295/2000  train MSE: 0.6176, val MSE: 0.3985
Gen 1296/2000  train MSE: 0.6175, val MSE: 0.3985
Gen 1297/2000  train MSE: 0.6175, val MSE: 0.3985
Gen 1298/2000  train MSE: 0.6175, val MSE: 0.3985
Gen 1299/2000  train MSE: 0.6175, val MSE: 0.3985
Gen 1300/2000  train MSE: 0.6174, val MSE: 0.3987

--- Refinement @ gen 1300 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1301/2000  train MSE: 0.6178, val MSE: 0.4001
Gen 1302/2000  train MSE: 0.6178, val MSE: 0.4001
Gen 1303/2000  train MSE: 0.6178, val MSE: 0.4001
Gen 1304/2000  train MSE: 0.6178, val MSE: 0.4001
Gen 1305/2000  train MSE: 0.6176, val MSE: 0.3989
Gen 1306/2000  train MSE: 0.6176, val MSE: 0.3989
Gen 1307/2000  train MSE: 0.6176, val MSE: 0.3989
Gen 1308/2000  train MSE: 0.6176, val MSE: 0.3989
Gen 1309/2000  train MSE: 0.6176, val MSE: 0.3989
Gen 1310/2000  train MSE: 0.6176, val MSE: 0.3989
Gen 1311/2000  train MSE: 0.6176, val MSE: 0.3989
Gen 1312/2000  train MSE: 0.6176, val MSE: 0.3989
Gen 1313/2000  train MSE: 0.6176, val MSE: 0.3989
Gen 1314/2000  train MSE: 0.6176, val MSE: 0.3989
Gen 1315/2000  train MSE: 0.6175, val MSE: 0.3995
Gen 1316/2000  train MSE: 0.6175, val MSE: 0.3979
Gen 1317/2000  train MSE: 0.6174, val MSE: 0.3984
Gen 1318/2000  train MSE: 0.6174, val MSE: 0.3984
Gen 1319/2000  train MSE: 0.6174, val MSE: 0.3984
Gen 1320/2000  train MSE: 0.6172, val MSE: 0.3977
Gen 1321/2000  train MSE: 0.6172, val MSE: 0.3977
Gen 1322/2000  train MSE: 0.6172, val MSE: 0.3977
Gen 1323/2000  train MSE: 0.6172, val MSE: 0.3977
Gen 1324/2000  train MSE: 0.6172, val MSE: 0.3977
Gen 1325/2000  train MSE: 0.6172, val MSE: 0.3977
Gen 1326/2000  train MSE: 0.6172, val MSE: 0.3977
Gen 1327/2000  train MSE: 0.6172, val MSE: 0.3977
Gen 1328/2000  train MSE: 0.6172, val MSE: 0.3988
Gen 1329/2000  train MSE: 0.6172, val MSE: 0.3988
Gen 1330/2000  train MSE: 0.6172, val MSE: 0.3988
Gen 1331/2000  train MSE: 0.6172, val MSE: 0.3988
Gen 1332/2000  train MSE: 0.6172, val MSE: 0.3988
Gen 1333/2000  train MSE: 0.6172, val MSE: 0.3988
Gen 1334/2000  train MSE: 0.6172, val MSE: 0.3988
Gen 1335/2000  train MSE: 0.6171, val MSE: 0.3984
Gen 1336/2000  train MSE: 0.6170, val MSE: 0.3972
Gen 1337/2000  train MSE: 0.6170, val MSE: 0.3972
Gen 1338/2000  train MSE: 0.6170, val MSE: 0.3972
Gen 1339/2000  train MSE: 0.6170, val MSE: 0.3972
Gen 1340/2000  train MSE: 0.6170, val MSE: 0.3972
Gen 1341/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1342/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1343/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1344/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1345/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1346/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1347/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1348/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1349/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1350/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1351/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1352/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1353/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1354/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1355/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1356/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1357/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1358/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1359/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1360/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1361/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1362/2000  train MSE: 0.6166, val MSE: 0.3968
Gen 1363/2000  train MSE: 0.6165, val MSE: 0.3969
Gen 1364/2000  train MSE: 0.6165, val MSE: 0.3969
Gen 1365/2000  train MSE: 0.6165, val MSE: 0.3981
Gen 1366/2000  train MSE: 0.6165, val MSE: 0.3981
Gen 1367/2000  train MSE: 0.6165, val MSE: 0.3981
Gen 1368/2000  train MSE: 0.6165, val MSE: 0.3981
Gen 1369/2000  train MSE: 0.6165, val MSE: 0.3981
Gen 1370/2000  train MSE: 0.6165, val MSE: 0.3981
Gen 1371/2000  train MSE: 0.6165, val MSE: 0.3981
Gen 1372/2000  train MSE: 0.6164, val MSE: 0.3960
Gen 1373/2000  train MSE: 0.6164, val MSE: 0.3960
Gen 1374/2000  train MSE: 0.6164, val MSE: 0.3960
Gen 1375/2000  train MSE: 0.6164, val MSE: 0.3960
Gen 1376/2000  train MSE: 0.6163, val MSE: 0.3957
Gen 1377/2000  train MSE: 0.6163, val MSE: 0.3957
Gen 1378/2000  train MSE: 0.6163, val MSE: 0.3957
Gen 1379/2000  train MSE: 0.6163, val MSE: 0.3957
Gen 1380/2000  train MSE: 0.6162, val MSE: 0.3959
Gen 1381/2000  train MSE: 0.6162, val MSE: 0.3959
Gen 1382/2000  train MSE: 0.6162, val MSE: 0.3964
Gen 1383/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1384/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1385/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1386/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1387/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1388/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1389/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1390/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1391/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1392/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1393/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1394/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1395/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1396/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1397/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1398/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1399/2000  train MSE: 0.6157, val MSE: 0.3950
Gen 1400/2000  train MSE: 0.6157, val MSE: 0.3950

--- Refinement @ gen 1400 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1401/2000  train MSE: 0.6166, val MSE: 0.3974
Gen 1402/2000  train MSE: 0.6166, val MSE: 0.3974
Gen 1403/2000  train MSE: 0.6166, val MSE: 0.3974
Gen 1404/2000  train MSE: 0.6166, val MSE: 0.3974
Gen 1405/2000  train MSE: 0.6166, val MSE: 0.3974
Gen 1406/2000  train MSE: 0.6165, val MSE: 0.3969
Gen 1407/2000  train MSE: 0.6165, val MSE: 0.3969
Gen 1408/2000  train MSE: 0.6165, val MSE: 0.3962
Gen 1409/2000  train MSE: 0.6165, val MSE: 0.3962
Gen 1410/2000  train MSE: 0.6165, val MSE: 0.3962
Gen 1411/2000  train MSE: 0.6165, val MSE: 0.3962
Gen 1412/2000  train MSE: 0.6165, val MSE: 0.3962
Gen 1413/2000  train MSE: 0.6165, val MSE: 0.3962
Gen 1414/2000  train MSE: 0.6165, val MSE: 0.3962
Gen 1415/2000  train MSE: 0.6163, val MSE: 0.3938
Gen 1416/2000  train MSE: 0.6163, val MSE: 0.3938
Gen 1417/2000  train MSE: 0.6163, val MSE: 0.3938
Gen 1418/2000  train MSE: 0.6163, val MSE: 0.3938
Gen 1419/2000  train MSE: 0.6163, val MSE: 0.3938
Gen 1420/2000  train MSE: 0.6163, val MSE: 0.3944
Gen 1421/2000  train MSE: 0.6163, val MSE: 0.3944
Gen 1422/2000  train MSE: 0.6163, val MSE: 0.3944
Gen 1423/2000  train MSE: 0.6163, val MSE: 0.3944
Gen 1424/2000  train MSE: 0.6163, val MSE: 0.3944
Gen 1425/2000  train MSE: 0.6163, val MSE: 0.3956
Gen 1426/2000  train MSE: 0.6163, val MSE: 0.3956
Gen 1427/2000  train MSE: 0.6163, val MSE: 0.3956
Gen 1428/2000  train MSE: 0.6163, val MSE: 0.3956
Gen 1429/2000  train MSE: 0.6163, val MSE: 0.3956
Gen 1430/2000  train MSE: 0.6163, val MSE: 0.3956
Gen 1431/2000  train MSE: 0.6163, val MSE: 0.3956
Gen 1432/2000  train MSE: 0.6162, val MSE: 0.3951
Gen 1433/2000  train MSE: 0.6162, val MSE: 0.3951
Gen 1434/2000  train MSE: 0.6162, val MSE: 0.3951
Gen 1435/2000  train MSE: 0.6162, val MSE: 0.3951
Gen 1436/2000  train MSE: 0.6161, val MSE: 0.3958
Gen 1437/2000  train MSE: 0.6161, val MSE: 0.3958
Gen 1438/2000  train MSE: 0.6159, val MSE: 0.3973
Gen 1439/2000  train MSE: 0.6159, val MSE: 0.3973
Gen 1440/2000  train MSE: 0.6159, val MSE: 0.3973
Gen 1441/2000  train MSE: 0.6159, val MSE: 0.3973
Gen 1442/2000  train MSE: 0.6159, val MSE: 0.3973
Gen 1443/2000  train MSE: 0.6157, val MSE: 0.3951
Gen 1444/2000  train MSE: 0.6157, val MSE: 0.3951
Gen 1445/2000  train MSE: 0.6157, val MSE: 0.3951
Gen 1446/2000  train MSE: 0.6157, val MSE: 0.3951
Gen 1447/2000  train MSE: 0.6157, val MSE: 0.3951
Gen 1448/2000  train MSE: 0.6157, val MSE: 0.3951
Gen 1449/2000  train MSE: 0.6157, val MSE: 0.3951
Gen 1450/2000  train MSE: 0.6157, val MSE: 0.3940
Gen 1451/2000  train MSE: 0.6155, val MSE: 0.3949
Gen 1452/2000  train MSE: 0.6155, val MSE: 0.3949
Gen 1453/2000  train MSE: 0.6155, val MSE: 0.3951
Gen 1454/2000  train MSE: 0.6152, val MSE: 0.3939
Gen 1455/2000  train MSE: 0.6152, val MSE: 0.3939
Gen 1456/2000  train MSE: 0.6152, val MSE: 0.3939
Gen 1457/2000  train MSE: 0.6152, val MSE: 0.3939
Gen 1458/2000  train MSE: 0.6152, val MSE: 0.3958
Gen 1459/2000  train MSE: 0.6152, val MSE: 0.3945
Gen 1460/2000  train MSE: 0.6152, val MSE: 0.3945
Gen 1461/2000  train MSE: 0.6152, val MSE: 0.3945
Gen 1462/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1463/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1464/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1465/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1466/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1467/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1468/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1469/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1470/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1471/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1472/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1473/2000  train MSE: 0.6148, val MSE: 0.3942
Gen 1474/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1475/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1476/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1477/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1478/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1479/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1480/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1481/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1482/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1483/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1484/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1485/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1486/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1487/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1488/2000  train MSE: 0.6145, val MSE: 0.3949
Gen 1489/2000  train MSE: 0.6144, val MSE: 0.3939
Gen 1490/2000  train MSE: 0.6144, val MSE: 0.3939
Gen 1491/2000  train MSE: 0.6144, val MSE: 0.3939
Gen 1492/2000  train MSE: 0.6144, val MSE: 0.3939
Gen 1493/2000  train MSE: 0.6144, val MSE: 0.3939
Gen 1494/2000  train MSE: 0.6144, val MSE: 0.3939
Gen 1495/2000  train MSE: 0.6144, val MSE: 0.3939
Gen 1496/2000  train MSE: 0.6143, val MSE: 0.3954
Gen 1497/2000  train MSE: 0.6143, val MSE: 0.3943
Gen 1498/2000  train MSE: 0.6143, val MSE: 0.3943
Gen 1499/2000  train MSE: 0.6141, val MSE: 0.3938
Gen 1500/2000  train MSE: 0.6141, val MSE: 0.3938

--- Refinement @ gen 1500 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1501/2000  train MSE: 0.6144, val MSE: 0.3947
Gen 1502/2000  train MSE: 0.6144, val MSE: 0.3947
Gen 1503/2000  train MSE: 0.6144, val MSE: 0.3947
Gen 1504/2000  train MSE: 0.6141, val MSE: 0.3941
Gen 1505/2000  train MSE: 0.6141, val MSE: 0.3941
Gen 1506/2000  train MSE: 0.6141, val MSE: 0.3941
Gen 1507/2000  train MSE: 0.6141, val MSE: 0.3941
Gen 1508/2000  train MSE: 0.6141, val MSE: 0.3941
Gen 1509/2000  train MSE: 0.6141, val MSE: 0.3941
Gen 1510/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1511/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1512/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1513/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1514/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1515/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1516/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1517/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1518/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1519/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1520/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1521/2000  train MSE: 0.6140, val MSE: 0.3937
Gen 1522/2000  train MSE: 0.6140, val MSE: 0.3941
Gen 1523/2000  train MSE: 0.6140, val MSE: 0.3941
Gen 1524/2000  train MSE: 0.6140, val MSE: 0.3948
Gen 1525/2000  train MSE: 0.6140, val MSE: 0.3939
Gen 1526/2000  train MSE: 0.6140, val MSE: 0.3939
Gen 1527/2000  train MSE: 0.6140, val MSE: 0.3939
Gen 1528/2000  train MSE: 0.6139, val MSE: 0.3936
Gen 1529/2000  train MSE: 0.6139, val MSE: 0.3936
Gen 1530/2000  train MSE: 0.6139, val MSE: 0.3936
Gen 1531/2000  train MSE: 0.6139, val MSE: 0.3936
Gen 1532/2000  train MSE: 0.6139, val MSE: 0.3936
Gen 1533/2000  train MSE: 0.6139, val MSE: 0.3936
Gen 1534/2000  train MSE: 0.6139, val MSE: 0.3936
Gen 1535/2000  train MSE: 0.6138, val MSE: 0.3933
Gen 1536/2000  train MSE: 0.6138, val MSE: 0.3933
Gen 1537/2000  train MSE: 0.6138, val MSE: 0.3932
Gen 1538/2000  train MSE: 0.6137, val MSE: 0.3935
Gen 1539/2000  train MSE: 0.6137, val MSE: 0.3935
Gen 1540/2000  train MSE: 0.6137, val MSE: 0.3935
Gen 1541/2000  train MSE: 0.6137, val MSE: 0.3935
Gen 1542/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1543/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1544/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1545/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1546/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1547/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1548/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1549/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1550/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1551/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1552/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1553/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1554/2000  train MSE: 0.6137, val MSE: 0.3930
Gen 1555/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1556/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1557/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1558/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1559/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1560/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1561/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1562/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1563/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1564/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1565/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1566/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1567/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1568/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1569/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1570/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1571/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1572/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1573/2000  train MSE: 0.6136, val MSE: 0.3925
Gen 1574/2000  train MSE: 0.6135, val MSE: 0.3920
Gen 1575/2000  train MSE: 0.6135, val MSE: 0.3920
Gen 1576/2000  train MSE: 0.6135, val MSE: 0.3920
Gen 1577/2000  train MSE: 0.6135, val MSE: 0.3920
Gen 1578/2000  train MSE: 0.6135, val MSE: 0.3920
Gen 1579/2000  train MSE: 0.6135, val MSE: 0.3920
Gen 1580/2000  train MSE: 0.6135, val MSE: 0.3920
Gen 1581/2000  train MSE: 0.6135, val MSE: 0.3920
Gen 1582/2000  train MSE: 0.6135, val MSE: 0.3920
Gen 1583/2000  train MSE: 0.6135, val MSE: 0.3920
Gen 1584/2000  train MSE: 0.6134, val MSE: 0.3935
Gen 1585/2000  train MSE: 0.6134, val MSE: 0.3935
Gen 1586/2000  train MSE: 0.6134, val MSE: 0.3935
Gen 1587/2000  train MSE: 0.6134, val MSE: 0.3935
Gen 1588/2000  train MSE: 0.6134, val MSE: 0.3936
Gen 1589/2000  train MSE: 0.6134, val MSE: 0.3936
Gen 1590/2000  train MSE: 0.6134, val MSE: 0.3936
Gen 1591/2000  train MSE: 0.6134, val MSE: 0.3936
Gen 1592/2000  train MSE: 0.6134, val MSE: 0.3936
Gen 1593/2000  train MSE: 0.6134, val MSE: 0.3936
Gen 1594/2000  train MSE: 0.6134, val MSE: 0.3936
Gen 1595/2000  train MSE: 0.6134, val MSE: 0.3936
Gen 1596/2000  train MSE: 0.6134, val MSE: 0.3936
Gen 1597/2000  train MSE: 0.6134, val MSE: 0.3936
Gen 1598/2000  train MSE: 0.6133, val MSE: 0.3928
Gen 1599/2000  train MSE: 0.6133, val MSE: 0.3928
Gen 1600/2000  train MSE: 0.6132, val MSE: 0.3926

--- Refinement @ gen 1600 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1601/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1602/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1603/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1604/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1605/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1606/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1607/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1608/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1609/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1610/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1611/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1612/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1613/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1614/2000  train MSE: 0.6136, val MSE: 0.3946
Gen 1615/2000  train MSE: 0.6136, val MSE: 0.3928
Gen 1616/2000  train MSE: 0.6136, val MSE: 0.3928
Gen 1617/2000  train MSE: 0.6136, val MSE: 0.3919
Gen 1618/2000  train MSE: 0.6136, val MSE: 0.3919
Gen 1619/2000  train MSE: 0.6136, val MSE: 0.3919
Gen 1620/2000  train MSE: 0.6134, val MSE: 0.3919
Gen 1621/2000  train MSE: 0.6134, val MSE: 0.3919
Gen 1622/2000  train MSE: 0.6134, val MSE: 0.3919
Gen 1623/2000  train MSE: 0.6134, val MSE: 0.3919
Gen 1624/2000  train MSE: 0.6133, val MSE: 0.3918
Gen 1625/2000  train MSE: 0.6133, val MSE: 0.3918
Gen 1626/2000  train MSE: 0.6132, val MSE: 0.3917
Gen 1627/2000  train MSE: 0.6132, val MSE: 0.3917
Gen 1628/2000  train MSE: 0.6132, val MSE: 0.3917
Gen 1629/2000  train MSE: 0.6132, val MSE: 0.3917
Gen 1630/2000  train MSE: 0.6132, val MSE: 0.3917
Gen 1631/2000  train MSE: 0.6132, val MSE: 0.3917
Gen 1632/2000  train MSE: 0.6132, val MSE: 0.3917
Gen 1633/2000  train MSE: 0.6132, val MSE: 0.3917
Gen 1634/2000  train MSE: 0.6132, val MSE: 0.3917
Gen 1635/2000  train MSE: 0.6132, val MSE: 0.3917
Gen 1636/2000  train MSE: 0.6132, val MSE: 0.3923
Gen 1637/2000  train MSE: 0.6131, val MSE: 0.3914
Gen 1638/2000  train MSE: 0.6131, val MSE: 0.3914
Gen 1639/2000  train MSE: 0.6131, val MSE: 0.3930
Gen 1640/2000  train MSE: 0.6128, val MSE: 0.3919
Gen 1641/2000  train MSE: 0.6128, val MSE: 0.3919
Gen 1642/2000  train MSE: 0.6128, val MSE: 0.3919
Gen 1643/2000  train MSE: 0.6128, val MSE: 0.3919
Gen 1644/2000  train MSE: 0.6128, val MSE: 0.3919
Gen 1645/2000  train MSE: 0.6128, val MSE: 0.3919
Gen 1646/2000  train MSE: 0.6128, val MSE: 0.3919
Gen 1647/2000  train MSE: 0.6128, val MSE: 0.3916
Gen 1648/2000  train MSE: 0.6128, val MSE: 0.3913
Gen 1649/2000  train MSE: 0.6128, val MSE: 0.3913
Gen 1650/2000  train MSE: 0.6127, val MSE: 0.3906
Gen 1651/2000  train MSE: 0.6127, val MSE: 0.3906
Gen 1652/2000  train MSE: 0.6127, val MSE: 0.3906
Gen 1653/2000  train MSE: 0.6127, val MSE: 0.3906
Gen 1654/2000  train MSE: 0.6127, val MSE: 0.3906
Gen 1655/2000  train MSE: 0.6127, val MSE: 0.3906
Gen 1656/2000  train MSE: 0.6127, val MSE: 0.3906
Gen 1657/2000  train MSE: 0.6127, val MSE: 0.3906
Gen 1658/2000  train MSE: 0.6127, val MSE: 0.3907
Gen 1659/2000  train MSE: 0.6127, val MSE: 0.3907
Gen 1660/2000  train MSE: 0.6125, val MSE: 0.3908
Gen 1661/2000  train MSE: 0.6125, val MSE: 0.3908
Gen 1662/2000  train MSE: 0.6125, val MSE: 0.3908
Gen 1663/2000  train MSE: 0.6125, val MSE: 0.3908
Gen 1664/2000  train MSE: 0.6125, val MSE: 0.3908
Gen 1665/2000  train MSE: 0.6125, val MSE: 0.3908
Gen 1666/2000  train MSE: 0.6125, val MSE: 0.3908
Gen 1667/2000  train MSE: 0.6125, val MSE: 0.3908
Gen 1668/2000  train MSE: 0.6125, val MSE: 0.3908
Gen 1669/2000  train MSE: 0.6125, val MSE: 0.3908
Gen 1670/2000  train MSE: 0.6125, val MSE: 0.3908
Gen 1671/2000  train MSE: 0.6124, val MSE: 0.3906
Gen 1672/2000  train MSE: 0.6124, val MSE: 0.3906
Gen 1673/2000  train MSE: 0.6124, val MSE: 0.3906
Gen 1674/2000  train MSE: 0.6124, val MSE: 0.3906
Gen 1675/2000  train MSE: 0.6124, val MSE: 0.3906
Gen 1676/2000  train MSE: 0.6124, val MSE: 0.3906
Gen 1677/2000  train MSE: 0.6124, val MSE: 0.3906
Gen 1678/2000  train MSE: 0.6123, val MSE: 0.3906
Gen 1679/2000  train MSE: 0.6123, val MSE: 0.3906
Gen 1680/2000  train MSE: 0.6123, val MSE: 0.3906
Gen 1681/2000  train MSE: 0.6123, val MSE: 0.3906
Gen 1682/2000  train MSE: 0.6123, val MSE: 0.3906
Gen 1683/2000  train MSE: 0.6123, val MSE: 0.3906
Gen 1684/2000  train MSE: 0.6123, val MSE: 0.3906
Gen 1685/2000  train MSE: 0.6123, val MSE: 0.3906
Gen 1686/2000  train MSE: 0.6123, val MSE: 0.3906
Gen 1687/2000  train MSE: 0.6123, val MSE: 0.3904
Gen 1688/2000  train MSE: 0.6123, val MSE: 0.3904
Gen 1689/2000  train MSE: 0.6123, val MSE: 0.3904
Gen 1690/2000  train MSE: 0.6123, val MSE: 0.3908
Gen 1691/2000  train MSE: 0.6123, val MSE: 0.3908
Gen 1692/2000  train MSE: 0.6123, val MSE: 0.3908
Gen 1693/2000  train MSE: 0.6123, val MSE: 0.3904
Gen 1694/2000  train MSE: 0.6122, val MSE: 0.3890
Gen 1695/2000  train MSE: 0.6122, val MSE: 0.3890
Gen 1696/2000  train MSE: 0.6121, val MSE: 0.3891
Gen 1697/2000  train MSE: 0.6121, val MSE: 0.3891
Gen 1698/2000  train MSE: 0.6120, val MSE: 0.3888
Gen 1699/2000  train MSE: 0.6120, val MSE: 0.3888
Gen 1700/2000  train MSE: 0.6120, val MSE: 0.3888

--- Refinement @ gen 1700 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1701/2000  train MSE: 0.6124, val MSE: 0.3893
Gen 1702/2000  train MSE: 0.6124, val MSE: 0.3893
Gen 1703/2000  train MSE: 0.6124, val MSE: 0.3893
Gen 1704/2000  train MSE: 0.6124, val MSE: 0.3893
Gen 1705/2000  train MSE: 0.6124, val MSE: 0.3893
Gen 1706/2000  train MSE: 0.6124, val MSE: 0.3893
Gen 1707/2000  train MSE: 0.6124, val MSE: 0.3907
Gen 1708/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1709/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1710/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1711/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1712/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1713/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1714/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1715/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1716/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1717/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1718/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1719/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1720/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1721/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1722/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1723/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1724/2000  train MSE: 0.6121, val MSE: 0.3894
Gen 1725/2000  train MSE: 0.6119, val MSE: 0.3896
Gen 1726/2000  train MSE: 0.6119, val MSE: 0.3896
Gen 1727/2000  train MSE: 0.6119, val MSE: 0.3896
Gen 1728/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1729/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1730/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1731/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1732/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1733/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1734/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1735/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1736/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1737/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1738/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1739/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1740/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1741/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1742/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1743/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1744/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1745/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1746/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1747/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1748/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1749/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1750/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1751/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1752/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1753/2000  train MSE: 0.6117, val MSE: 0.3883
Gen 1754/2000  train MSE: 0.6117, val MSE: 0.3888
Gen 1755/2000  train MSE: 0.6117, val MSE: 0.3888
Gen 1756/2000  train MSE: 0.6117, val MSE: 0.3888
Gen 1757/2000  train MSE: 0.6117, val MSE: 0.3888
Gen 1758/2000  train MSE: 0.6117, val MSE: 0.3888
Gen 1759/2000  train MSE: 0.6117, val MSE: 0.3893
Gen 1760/2000  train MSE: 0.6117, val MSE: 0.3893
Gen 1761/2000  train MSE: 0.6116, val MSE: 0.3879
Gen 1762/2000  train MSE: 0.6116, val MSE: 0.3879
Gen 1763/2000  train MSE: 0.6116, val MSE: 0.3879
Gen 1764/2000  train MSE: 0.6116, val MSE: 0.3879
Gen 1765/2000  train MSE: 0.6116, val MSE: 0.3879
Gen 1766/2000  train MSE: 0.6116, val MSE: 0.3879
Gen 1767/2000  train MSE: 0.6115, val MSE: 0.3889
Gen 1768/2000  train MSE: 0.6115, val MSE: 0.3889
Gen 1769/2000  train MSE: 0.6115, val MSE: 0.3889
Gen 1770/2000  train MSE: 0.6115, val MSE: 0.3889
Gen 1771/2000  train MSE: 0.6115, val MSE: 0.3889
Gen 1772/2000  train MSE: 0.6115, val MSE: 0.3889
Gen 1773/2000  train MSE: 0.6115, val MSE: 0.3889
Gen 1774/2000  train MSE: 0.6115, val MSE: 0.3889
Gen 1775/2000  train MSE: 0.6115, val MSE: 0.3889
Gen 1776/2000  train MSE: 0.6115, val MSE: 0.3889
Gen 1777/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1778/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1779/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1780/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1781/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1782/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1783/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1784/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1785/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1786/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1787/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1788/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1789/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1790/2000  train MSE: 0.6113, val MSE: 0.3875
Gen 1791/2000  train MSE: 0.6110, val MSE: 0.3881
Gen 1792/2000  train MSE: 0.6110, val MSE: 0.3881
Gen 1793/2000  train MSE: 0.6110, val MSE: 0.3881
Gen 1794/2000  train MSE: 0.6110, val MSE: 0.3881
Gen 1795/2000  train MSE: 0.6110, val MSE: 0.3881
Gen 1796/2000  train MSE: 0.6110, val MSE: 0.3881
Gen 1797/2000  train MSE: 0.6110, val MSE: 0.3881
Gen 1798/2000  train MSE: 0.6110, val MSE: 0.3881
Gen 1799/2000  train MSE: 0.6110, val MSE: 0.3881
Gen 1800/2000  train MSE: 0.6110, val MSE: 0.3881

--- Refinement @ gen 1800 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1801/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1802/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1803/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1804/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1805/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1806/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1807/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1808/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1809/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1810/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1811/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1812/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1813/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1814/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1815/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1816/2000  train MSE: 0.6118, val MSE: 0.3866
Gen 1817/2000  train MSE: 0.6117, val MSE: 0.3865
Gen 1818/2000  train MSE: 0.6117, val MSE: 0.3865
Gen 1819/2000  train MSE: 0.6117, val MSE: 0.3865
Gen 1820/2000  train MSE: 0.6117, val MSE: 0.3865
Gen 1821/2000  train MSE: 0.6117, val MSE: 0.3865
Gen 1822/2000  train MSE: 0.6117, val MSE: 0.3864
Gen 1823/2000  train MSE: 0.6117, val MSE: 0.3864
Gen 1824/2000  train MSE: 0.6113, val MSE: 0.3866
Gen 1825/2000  train MSE: 0.6113, val MSE: 0.3866
Gen 1826/2000  train MSE: 0.6113, val MSE: 0.3866
Gen 1827/2000  train MSE: 0.6112, val MSE: 0.3858
Gen 1828/2000  train MSE: 0.6112, val MSE: 0.3858
Gen 1829/2000  train MSE: 0.6110, val MSE: 0.3859
Gen 1830/2000  train MSE: 0.6110, val MSE: 0.3859
Gen 1831/2000  train MSE: 0.6109, val MSE: 0.3864
Gen 1832/2000  train MSE: 0.6109, val MSE: 0.3864
Gen 1833/2000  train MSE: 0.6109, val MSE: 0.3864
Gen 1834/2000  train MSE: 0.6109, val MSE: 0.3850
Gen 1835/2000  train MSE: 0.6109, val MSE: 0.3850
Gen 1836/2000  train MSE: 0.6109, val MSE: 0.3850
Gen 1837/2000  train MSE: 0.6109, val MSE: 0.3850
Gen 1838/2000  train MSE: 0.6109, val MSE: 0.3850
Gen 1839/2000  train MSE: 0.6108, val MSE: 0.3852
Gen 1840/2000  train MSE: 0.6108, val MSE: 0.3852
Gen 1841/2000  train MSE: 0.6108, val MSE: 0.3852
Gen 1842/2000  train MSE: 0.6108, val MSE: 0.3852
Gen 1843/2000  train MSE: 0.6108, val MSE: 0.3852
Gen 1844/2000  train MSE: 0.6108, val MSE: 0.3852
Gen 1845/2000  train MSE: 0.6107, val MSE: 0.3857
Gen 1846/2000  train MSE: 0.6107, val MSE: 0.3857
Gen 1847/2000  train MSE: 0.6107, val MSE: 0.3857
Gen 1848/2000  train MSE: 0.6107, val MSE: 0.3857
Gen 1849/2000  train MSE: 0.6105, val MSE: 0.3857
Gen 1850/2000  train MSE: 0.6105, val MSE: 0.3857
Gen 1851/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1852/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1853/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1854/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1855/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1856/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1857/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1858/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1859/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1860/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1861/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1862/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1863/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1864/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1865/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1866/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1867/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1868/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1869/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1870/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1871/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1872/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1873/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1874/2000  train MSE: 0.6105, val MSE: 0.3859
Gen 1875/2000  train MSE: 0.6104, val MSE: 0.3857
Gen 1876/2000  train MSE: 0.6104, val MSE: 0.3857
Gen 1877/2000  train MSE: 0.6104, val MSE: 0.3857
Gen 1878/2000  train MSE: 0.6104, val MSE: 0.3857
Gen 1879/2000  train MSE: 0.6104, val MSE: 0.3851
Gen 1880/2000  train MSE: 0.6104, val MSE: 0.3851
Gen 1881/2000  train MSE: 0.6104, val MSE: 0.3851
Gen 1882/2000  train MSE: 0.6104, val MSE: 0.3851
Gen 1883/2000  train MSE: 0.6104, val MSE: 0.3851
Gen 1884/2000  train MSE: 0.6103, val MSE: 0.3856
Gen 1885/2000  train MSE: 0.6103, val MSE: 0.3856
Gen 1886/2000  train MSE: 0.6103, val MSE: 0.3856
Gen 1887/2000  train MSE: 0.6103, val MSE: 0.3856
Gen 1888/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1889/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1890/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1891/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1892/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1893/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1894/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1895/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1896/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1897/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1898/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1899/2000  train MSE: 0.6100, val MSE: 0.3855
Gen 1900/2000  train MSE: 0.6100, val MSE: 0.3855

--- Refinement @ gen 1900 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
Gen 1901/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1902/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1903/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1904/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1905/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1906/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1907/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1908/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1909/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1910/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1911/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1912/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1913/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1914/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1915/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1916/2000  train MSE: 0.6103, val MSE: 0.3866
Gen 1917/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1918/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1919/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1920/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1921/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1922/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1923/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1924/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1925/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1926/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1927/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1928/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1929/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1930/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1931/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1932/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1933/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1934/2000  train MSE: 0.6101, val MSE: 0.3864
Gen 1935/2000  train MSE: 0.6099, val MSE: 0.3860
Gen 1936/2000  train MSE: 0.6099, val MSE: 0.3860
Gen 1937/2000  train MSE: 0.6099, val MSE: 0.3860
Gen 1938/2000  train MSE: 0.6099, val MSE: 0.3860
Gen 1939/2000  train MSE: 0.6099, val MSE: 0.3860
Gen 1940/2000  train MSE: 0.6099, val MSE: 0.3860
Gen 1941/2000  train MSE: 0.6099, val MSE: 0.3860
Gen 1942/2000  train MSE: 0.6099, val MSE: 0.3860
Gen 1943/2000  train MSE: 0.6099, val MSE: 0.3860
Gen 1944/2000  train MSE: 0.6099, val MSE: 0.3860
Gen 1945/2000  train MSE: 0.6099, val MSE: 0.3843
Gen 1946/2000  train MSE: 0.6099, val MSE: 0.3845
Gen 1947/2000  train MSE: 0.6099, val MSE: 0.3845
Gen 1948/2000  train MSE: 0.6099, val MSE: 0.3845
Gen 1949/2000  train MSE: 0.6099, val MSE: 0.3845
Gen 1950/2000  train MSE: 0.6099, val MSE: 0.3845
Gen 1951/2000  train MSE: 0.6098, val MSE: 0.3844
Gen 1952/2000  train MSE: 0.6098, val MSE: 0.3844
Gen 1953/2000  train MSE: 0.6098, val MSE: 0.3844
Gen 1954/2000  train MSE: 0.6098, val MSE: 0.3844
Gen 1955/2000  train MSE: 0.6098, val MSE: 0.3844
Gen 1956/2000  train MSE: 0.6097, val MSE: 0.3844
Gen 1957/2000  train MSE: 0.6097, val MSE: 0.3844
Gen 1958/2000  train MSE: 0.6096, val MSE: 0.3834
Gen 1959/2000  train MSE: 0.6096, val MSE: 0.3834
Gen 1960/2000  train MSE: 0.6096, val MSE: 0.3834
Gen 1961/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1962/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1963/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1964/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1965/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1966/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1967/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1968/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1969/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1970/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1971/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1972/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1973/2000  train MSE: 0.6096, val MSE: 0.3839
Gen 1974/2000  train MSE: 0.6094, val MSE: 0.3828
Gen 1975/2000  train MSE: 0.6094, val MSE: 0.3828
Gen 1976/2000  train MSE: 0.6094, val MSE: 0.3828
Gen 1977/2000  train MSE: 0.6094, val MSE: 0.3828
Gen 1978/2000  train MSE: 0.6094, val MSE: 0.3828
Gen 1979/2000  train MSE: 0.6094, val MSE: 0.3828
Gen 1980/2000  train MSE: 0.6094, val MSE: 0.3828
Gen 1981/2000  train MSE: 0.6094, val MSE: 0.3828
Gen 1982/2000  train MSE: 0.6094, val MSE: 0.3828
Gen 1983/2000  train MSE: 0.6094, val MSE: 0.3828
Gen 1984/2000  train MSE: 0.6094, val MSE: 0.3828
Gen 1985/2000  train MSE: 0.6094, val MSE: 0.3826
Gen 1986/2000  train MSE: 0.6094, val MSE: 0.3826
Gen 1987/2000  train MSE: 0.6094, val MSE: 0.3826
Gen 1988/2000  train MSE: 0.6094, val MSE: 0.3826
Gen 1989/2000  train MSE: 0.6094, val MSE: 0.3813
Gen 1990/2000  train MSE: 0.6094, val MSE: 0.3813
Gen 1991/2000  train MSE: 0.6094, val MSE: 0.3813
Gen 1992/2000  train MSE: 0.6093, val MSE: 0.3830
Gen 1993/2000  train MSE: 0.6093, val MSE: 0.3830
Gen 1994/2000  train MSE: 0.6091, val MSE: 0.3819
Gen 1995/2000  train MSE: 0.6091, val MSE: 0.3819
Gen 1996/2000  train MSE: 0.6091, val MSE: 0.3819
Gen 1997/2000  train MSE: 0.6091, val MSE: 0.3819
Gen 1998/2000  train MSE: 0.6091, val MSE: 0.3819
Gen 1999/2000  train MSE: 0.6091, val MSE: 0.3819
Gen 2000/2000  train MSE: 0.6090, val MSE: 0.3815

--- Refinement @ gen 2000 ---
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9
refine step :  0
refine step :  1
refine step :  2
refine step :  3
refine step :  4
refine step :  5
refine step :  6
refine step :  7
refine step :  8
refine step :  9

 GA+SGD done!  Final Train MSE: 1.2190, Val MSE: 0.9835
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=23890bde">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Final-hybrid-GA-implementation:-Final-run:-Stop-SGD-refining-after-600-generations-due-to-divergence-of-solutions.">Final hybrid GA implementation: Final run: Stop SGD refining after 600 generations due to divergence of solutions.<a class="anchor-link" href="#Final-hybrid-GA-implementation:-Final-run:-Stop-SGD-refining-after-600-generations-due-to-divergence-of-solutions."></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=db239bc9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Hybrid Genetic Algorithm with Periodic SGD Refinement for FFN Training</span>
<span class="sd">=======================================================================</span>

<span class="sd">This script implements a hybrid optimisation strategy that combines a real-valued</span>
<span class="sd">Genetic Algorithm (GA) with periodic Stochastic Gradient Descent (SGD) refinement</span>
<span class="sd">for training a Feed-Forward Neural Network (FFN). The GA uses BLX- crossover,</span>
<span class="sd">Gaussian mutation, and elitism. Every 100 generations, the top-performing elite</span>
<span class="sd">individuals are further fine-tuned using a fixed number of SGD epochs.</span>

<span class="sd">Key Innovations:</span>
<span class="sd">----------------</span>
<span class="sd">- Hybrid Strategy: Combines global GA exploration with local gradient-based exploitation</span>
<span class="sd">- Periodic Refinement: Every 100 generations, elite individuals undergo 10 epochs of SGD</span>
<span class="sd">- Logging: Logs elite MSEs before and after refinement to a CSV file for ablation and analysis</span>
<span class="sd">- Now stops refining after generation 600</span>

<span class="sd">Core GA Configuration:</span>
<span class="sd">----------------------</span>
<span class="sd">- Population Size: 200</span>
<span class="sd">- Generations: 2000</span>
<span class="sd">- Elitism: top 20% retained per generation</span>
<span class="sd">- Tournament Selection: size 3</span>
<span class="sd">- Crossover: BLX- with  = 0.6</span>
<span class="sd">- Mutation: Gaussian with probability 1%, std 0.01</span>

<span class="sd">SGD Refinement:</span>
<span class="sd">---------------</span>
<span class="sd">- Performed every 100 generations (`REFINE_EVERY`)</span>
<span class="sd">- Runs for 10 epochs (`REFINE_EPOCHS`) on each elite</span>
<span class="sd">- Uses momentum SGD with hyperparameters obtained via Optuna</span>

<span class="sd">Feedforward Network Architecture:</span>
<span class="sd">---------------------------------</span>
<span class="sd">- 2 hidden layers with 24 ReLU units each</span>
<span class="sd">- Xavier weight initialisation</span>
<span class="sd">- Output layer: linear (for regression)</span>

<span class="sd">Output:</span>
<span class="sd">-------</span>
<span class="sd">- Console logs for generation-wise MSE and refinement notifications</span>
<span class="sd">- `elite_mse_log.csv` file containing per-elite MSE before and after refinement</span>
<span class="sd">- Final training and validation MSE</span>
<span class="sd">- Matplotlib plot of training and validation MSE over time</span>

<span class="sd">Usage Notes:</span>
<span class="sd">------------</span>
<span class="sd">- Assumes `X_train`, `y_train`, `X_val`, `y_val` are pre-defined as `torch.Tensor`</span>
<span class="sd">- Designed for regression tasks (uses `nn.MSELoss`)</span>
<span class="sd">- Can be adapted to run refinement more/less frequently or to use other optimisers</span>

<span class="sd">"""</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="c1">#  1) Repro &amp; Device </span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1">#  2) Hyperparameters </span>
<span class="c1"># GA settings</span>
<span class="n">POP_SIZE</span>    <span class="o">=</span> <span class="mi">200</span>
<span class="n">GENERATIONS</span> <span class="o">=</span> <span class="mi">8000</span>
<span class="n">ELITE_FRAC</span>  <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">TOURN_SIZE</span>  <span class="o">=</span> <span class="mi">3</span>
<span class="n">MUT_P</span>       <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">MUT_SD</span>      <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">BLX_ALPHA</span>   <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1"># Local SGD refinement (every REFINE_EVERY gens, on top ELITE_FRAC)</span>
<span class="n">REFINE_EVERY</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">REFINE_EPOCHS</span>  <span class="o">=</span> <span class="mi">10</span> 
<span class="c1"># FFN / SGD settings (from Optuna)</span>
<span class="n">best_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"lr"</span><span class="p">:</span>         <span class="mf">0.013988</span><span class="p">,</span>
    <span class="s2">"momentum"</span><span class="p">:</span>   <span class="mf">0.6346</span><span class="p">,</span>
    <span class="s2">"batch_size"</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s2">"n_layers"</span><span class="p">:</span>   <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"n_units"</span><span class="p">:</span>    <span class="mi">24</span><span class="p">,</span>
    <span class="s2">"activation"</span><span class="p">:</span> <span class="s2">"ReLU"</span>
<span class="p">}</span>

<span class="c1">#  3) DataPrep </span>
<span class="c1"># assume X_train, y_train, X_val, y_val are already in scope as torch.Tensor</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">],</span>
                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X_train_dev</span><span class="p">,</span> <span class="n">y_train_dev</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_val_dev</span><span class="p">,</span>   <span class="n">y_val_dev</span>   <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>   <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1">#  4) Model Builder </span>
<span class="n">arch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">n_layers</span>   <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">],</span>
    <span class="n">n_units</span>    <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">],</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Constructs a feed-forward neural network (FFN) using the architecture specified in `arch`.</span>
<span class="sd">    </span>
<span class="sd">    The model includes:</span>
<span class="sd">    - `n_layers` hidden layers with `n_units` neurons each</span>
<span class="sd">    - Activation function as defined in `arch["activation"]`</span>
<span class="sd">    - Xavier normal weight initialization and zero-initialized biases</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: A PyTorch sequential model moved to the appropriate device.</span>
<span class="sd">    """</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_f</span> <span class="o">=</span> <span class="n">X_train_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"activation"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="s2">"n_layers"</span><span class="p">]):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]),</span> <span class="n">Act</span><span class="p">()]</span>
        <span class="n">in_f</span> <span class="o">=</span> <span class="n">arch</span><span class="p">[</span><span class="s2">"n_units"</span><span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Xavier init</span>
    <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="c1">#  5) GA Helpers </span>
<span class="k">def</span><span class="w"> </span><span class="nf">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Selects one individual from the population using tournament selection.</span>

<span class="sd">    Args:</span>
<span class="sd">        pop (list of np.ndarray): Population of genome vectors.</span>
<span class="sd">        fitness (list of float): Corresponding fitness values (lower is better).</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Genome of the selected individual.</span>
<span class="sd">    """</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pop</span><span class="p">),</span> <span class="n">TOURN_SIZE</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">([</span><span class="n">fitness</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">crossover_and_mutate</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Produces a new child genome via BLX- crossover and Gaussian mutation.</span>

<span class="sd">    - BLX- expands the gene-wise search space around two parents.</span>
<span class="sd">    - Gaussian noise is added to a subset of genes based on mutation probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        p1 (np.ndarray): Parent 1 genome.</span>
<span class="sd">        p2 (np.ndarray): Parent 2 genome.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Mutated child genome.</span>
<span class="sd">    """</span>
    <span class="n">low</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">-</span> <span class="n">BLX_ALPHA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
    <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="n">BLX_ALPHA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span>
    <span class="n">child</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># mutation</span>
    <span class="n">mask</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">MUT_P</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">MUT_SD</span>
    <span class="n">child</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">child</span>

<span class="k">def</span><span class="w"> </span><span class="nf">refine_with_sgd</span><span class="p">(</span><span class="n">genome</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Produces a new child genome via BLX- crossover and Gaussian mutation.</span>

<span class="sd">    - BLX- expands the gene-wise search space around two parents.</span>
<span class="sd">    - Gaussian noise is added to a subset of genes based on mutation probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        p1 (np.ndarray): Parent 1 genome.</span>
<span class="sd">        p2 (np.ndarray): Parent 2 genome.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: Mutated child genome.</span>
<span class="sd">    """</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                         <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="n">optim_sgd</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                          <span class="n">lr</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"lr"</span><span class="p">],</span>
                          <span class="n">momentum</span><span class="o">=</span><span class="n">best_params</span><span class="p">[</span><span class="s2">"momentum"</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">REFINE_EPOCHS</span><span class="p">):</span>
        
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optim_sgd</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optim_sgd</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># write back</span>
    <span class="k">return</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1">#  6) Initialize Population </span>
<span class="n">pop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">POP_SIZE</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="c1">#  7) GA Main Loop </span>
<span class="n">train_curve</span><span class="p">,</span> <span class="n">val_curve</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="c1">###logging of mse values before and after sgd</span>
<span class="n">LOG_FN</span> <span class="o">=</span> <span class="s2">"elite_mse_log.csv"</span>
<span class="c1"># write header: gen,phase,elite0,elite1,</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="c1"># we'll fill in the number of elites after we know ELITE_FRAC and POP_SIZE</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"gen"</span><span class="p">,</span><span class="s2">"phase"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"elite</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">elite_n</span><span class="p">)]</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="n">GRAPH_LOG_FN</span> <span class="o">=</span> <span class="s2">"graph_logs.csv"</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">GRAPH_LOG_FN</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">gf</span><span class="p">:</span>
    <span class="n">gf</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">"gen,mean_pop,mean_elite_pre,mean_elite_post,best_new,mean_children,gen_time</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="c1">#----------------------------------------------------------------------#</span>
<span class="c1">#Start of generations:</span>
<span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">GENERATIONS</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># a) evaluate all fitness</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>
    <span class="c1">#calculate mean fitness of pop</span>
    <span class="n">mean_pop</span> <span class="o">=</span> <span class="n">fitness</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
    <span class="n">train_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span>
    <span class="c1"># validation mse of best</span>
    <span class="n">m_best</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
    <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m_best</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">val_curve</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m_best</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span> <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Gen </span><span class="si">{</span><span class="n">gen</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">GENERATIONS</span><span class="si">}</span><span class="s2">  "</span>
        <span class="sa">f</span><span class="s2">"train MSE: </span><span class="si">{</span><span class="n">fitness</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val MSE: </span><span class="si">{</span><span class="n">val_curve</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># c) elitism</span>
    <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
    <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
    <span class="c1">#Elite fitness meaan pre </span>
    <span class="n">mean_elite_pre</span> <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">elite_idxs</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">elites</span> <span class="o">=</span> <span class="p">[</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">]</span>
    <span class="c1">#Mean elite post, intiialised the same</span>
    <span class="n">mean_elite_post</span> <span class="o">=</span> <span class="n">mean_elite_pre</span>
    <span class="c1">#Fitness of elites by indexing into the fitness array:</span>
    <span class="n">elites_fitness</span> <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">elite_idxs</span><span class="p">]</span>

    <span class="n">mean_of_prerefined_elites</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">elites_fitness</span><span class="p">)</span>
   
    <span class="k">if</span> <span class="n">gen</span> <span class="o">%</span> <span class="n">REFINE_EVERY</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">gen</span> <span class="o">&lt;</span> <span class="mi">699</span><span class="p">:</span>
        <span class="n">elite_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ELITE_FRAC</span> <span class="o">*</span> <span class="n">POP_SIZE</span><span class="p">))</span>
        <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">fitness</span><span class="p">)[:</span><span class="n">elite_n</span><span class="p">]</span>
        <span class="n">pre_vals</span> <span class="o">=</span> <span class="n">fitness</span><span class="p">[</span><span class="n">elite_idxs</span><span class="p">]</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">line</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">gen</span><span class="p">),</span> <span class="s2">"pre"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">pre_vals</span><span class="p">]</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">--- Refinement @ gen </span><span class="si">{</span><span class="n">gen</span><span class="si">}</span><span class="s2"> ---"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">:</span>
            <span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">refine_with_sgd</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">post_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
            <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">post_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">mean_elite_post</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">post_vals</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">line</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">gen</span><span class="p">),</span> <span class="s2">"post"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">post_vals</span><span class="p">]</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="c1"># e) selection + reproduction</span>
    <span class="n">new_pop</span> <span class="o">=</span> <span class="p">[</span> <span class="n">pop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">elite_idxs</span> <span class="p">]</span> 
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_pop</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">POP_SIZE</span><span class="p">:</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">tournament_select</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">crossover_and_mutate</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">)</span>
        <span class="n">new_pop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="n">pop</span> <span class="o">=</span> <span class="n">new_pop</span>
    <span class="c1"># 5) Recompute fitness on the replaced population</span>
    <span class="n">new_fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">new_fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">new_fitness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">new_fitness</span><span class="p">)</span>
    <span class="c1"># best MSE in the new population</span>
    <span class="n">best_new</span> <span class="o">=</span> <span class="n">new_fitness</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="c1"># mean MSE of _just_ the children (everything after the first `elite_n` slots)</span>
    <span class="k">if</span> <span class="n">POP_SIZE</span> <span class="o">&gt;</span> <span class="n">elite_n</span><span class="p">:</span>
        <span class="n">mean_children</span> <span class="o">=</span> <span class="n">new_fitness</span><span class="p">[</span><span class="n">elite_n</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mean_children</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"nan"</span><span class="p">)</span>
    <span class="n">gen_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">GRAPH_LOG_FN</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span> <span class="k">as</span> <span class="n">gf</span><span class="p">:</span>
        <span class="n">gf</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">gen</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">mean_pop</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">mean_elite_pre</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">mean_elite_post</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">best_new</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">mean_children</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">gen_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span>
        <span class="p">)</span>

    <span class="c1">#  very final: recompute fitness on the updated pop </span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">genome</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fitness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">fitness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fitness</span><span class="p">)</span>

<span class="c1">#  8) Final Eval &amp; Plot </span>
<span class="n">best_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">fitness</span><span class="p">))</span>
<span class="n">best_genome</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
<span class="n">best_model</span>  <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">best_genome</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">best_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">final_tr</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model</span><span class="p">(</span><span class="n">X_train_dev</span><span class="p">),</span> <span class="n">y_train_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">final_va</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">best_model</span><span class="p">(</span><span class="n">X_val_dev</span><span class="p">),</span>   <span class="n">y_val_dev</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> GA+SGD done!  Final Train MSE: </span><span class="si">{</span><span class="n">final_tr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Val MSE: </span><span class="si">{</span><span class="n">final_va</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">"checkpoints"</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 1) Save the PyTorch state_dict</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">best_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
           <span class="s2">"checkpoints/best_model_weights.pth"</span><span class="p">)</span>
<span class="c1"># 2) Save the flat genome vector (numpy array)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"checkpoints/best_genome.npy"</span><span class="p">,</span> <span class="n">best_genome</span><span class="p">)</span>
<span class="c1"># 3) (Optional) Save arch + Optuna params so you can rebuild the same model</span>
<span class="n">meta</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"arch"</span><span class="p">:</span> <span class="n">arch</span><span class="p">,</span>
    <span class="s2">"best_params"</span><span class="p">:</span> <span class="n">best_params</span>
<span class="p">}</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"checkpoints/best_model_meta.json"</span><span class="p">,</span><span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">meta</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">" Best model weights, genome and metadata saved to ./checkpoints/"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_curve</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_curve</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">"Val   MSE"</span><span class="p">)</span>
<span class="n">refine_gens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">600</span><span class="p">]</span>
<span class="n">refine_gens</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">refine_gens</span> <span class="k">if</span> <span class="n">g</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_curve</span><span class="p">)]</span>
<span class="n">refine_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">train_curve</span><span class="p">[</span><span class="n">g</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">refine_gens</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">refine_gens</span><span class="p">,</span> <span class="n">refine_vals</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'x'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Refinement with SGD'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Generation"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"GA+PeriodicSGD Refinement"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda

Gen 1/8000  train MSE: 1.0075, val MSE: 1.0054
Gen 2/8000  train MSE: 1.0075, val MSE: 1.0054
Gen 3/8000  train MSE: 1.0075, val MSE: 1.0054
Gen 4/8000  train MSE: 1.0073, val MSE: 1.0051
Gen 5/8000  train MSE: 1.0073, val MSE: 1.0051
Gen 6/8000  train MSE: 1.0073, val MSE: 1.0051
Gen 7/8000  train MSE: 1.0073, val MSE: 1.0051
Gen 8/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 9/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 10/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 11/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 12/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 13/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 14/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 15/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 16/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 17/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 18/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 19/8000  train MSE: 1.0072, val MSE: 1.0051
Gen 20/8000  train MSE: 1.0023, val MSE: 1.0025
Gen 21/8000  train MSE: 1.0011, val MSE: 1.0026
Gen 22/8000  train MSE: 1.0011, val MSE: 1.0026
Gen 23/8000  train MSE: 1.0011, val MSE: 1.0026
Gen 24/8000  train MSE: 1.0011, val MSE: 1.0026
Gen 25/8000  train MSE: 1.0011, val MSE: 1.0026
Gen 26/8000  train MSE: 1.0011, val MSE: 1.0026
Gen 27/8000  train MSE: 1.0011, val MSE: 1.0026
Gen 28/8000  train MSE: 1.0011, val MSE: 1.0026
Gen 29/8000  train MSE: 1.0011, val MSE: 1.0026
Gen 30/8000  train MSE: 1.0009, val MSE: 1.0004
Gen 31/8000  train MSE: 1.0001, val MSE: 1.0015
Gen 32/8000  train MSE: 1.0001, val MSE: 1.0015
Gen 33/8000  train MSE: 1.0001, val MSE: 1.0015
Gen 34/8000  train MSE: 1.0001, val MSE: 1.0015
Gen 35/8000  train MSE: 1.0001, val MSE: 1.0015
Gen 36/8000  train MSE: 1.0001, val MSE: 1.0013
Gen 37/8000  train MSE: 1.0001, val MSE: 1.0013
Gen 38/8000  train MSE: 1.0001, val MSE: 1.0013
Gen 39/8000  train MSE: 0.9995, val MSE: 1.0004
Gen 40/8000  train MSE: 0.9995, val MSE: 1.0004
Gen 41/8000  train MSE: 0.9995, val MSE: 1.0004
Gen 42/8000  train MSE: 0.9995, val MSE: 1.0004
Gen 43/8000  train MSE: 0.9995, val MSE: 1.0004
Gen 44/8000  train MSE: 0.9991, val MSE: 1.0000
Gen 45/8000  train MSE: 0.9991, val MSE: 1.0000
Gen 46/8000  train MSE: 0.9991, val MSE: 1.0000
Gen 47/8000  train MSE: 0.9991, val MSE: 1.0000
Gen 48/8000  train MSE: 0.9991, val MSE: 1.0000
Gen 49/8000  train MSE: 0.9991, val MSE: 1.0000
Gen 50/8000  train MSE: 0.9987, val MSE: 1.0000
Gen 51/8000  train MSE: 0.9987, val MSE: 1.0000
Gen 52/8000  train MSE: 0.9987, val MSE: 1.0000
Gen 53/8000  train MSE: 0.9987, val MSE: 1.0000
Gen 54/8000  train MSE: 0.9987, val MSE: 1.0000
Gen 55/8000  train MSE: 0.9987, val MSE: 1.0000
Gen 56/8000  train MSE: 0.9987, val MSE: 1.0000
Gen 57/8000  train MSE: 0.9987, val MSE: 1.0000
Gen 58/8000  train MSE: 0.9986, val MSE: 0.9985
Gen 59/8000  train MSE: 0.9981, val MSE: 0.9981
Gen 60/8000  train MSE: 0.9981, val MSE: 0.9981
Gen 61/8000  train MSE: 0.9981, val MSE: 0.9981
Gen 62/8000  train MSE: 0.9981, val MSE: 0.9981
Gen 63/8000  train MSE: 0.9981, val MSE: 0.9981
Gen 64/8000  train MSE: 0.9981, val MSE: 0.9996
Gen 65/8000  train MSE: 0.9981, val MSE: 0.9996
Gen 66/8000  train MSE: 0.9981, val MSE: 0.9996
Gen 67/8000  train MSE: 0.9980, val MSE: 0.9987
Gen 68/8000  train MSE: 0.9980, val MSE: 0.9987
Gen 69/8000  train MSE: 0.9979, val MSE: 0.9974
Gen 70/8000  train MSE: 0.9979, val MSE: 0.9974
Gen 71/8000  train MSE: 0.9977, val MSE: 0.9981
Gen 72/8000  train MSE: 0.9977, val MSE: 0.9981
Gen 73/8000  train MSE: 0.9977, val MSE: 0.9981
Gen 74/8000  train MSE: 0.9977, val MSE: 0.9981
Gen 75/8000  train MSE: 0.9977, val MSE: 0.9981
Gen 76/8000  train MSE: 0.9975, val MSE: 0.9986
Gen 77/8000  train MSE: 0.9975, val MSE: 0.9986
Gen 78/8000  train MSE: 0.9975, val MSE: 0.9986
Gen 79/8000  train MSE: 0.9975, val MSE: 0.9978
Gen 80/8000  train MSE: 0.9975, val MSE: 0.9978
Gen 81/8000  train MSE: 0.9975, val MSE: 0.9978
Gen 82/8000  train MSE: 0.9975, val MSE: 0.9978
Gen 83/8000  train MSE: 0.9975, val MSE: 0.9978
Gen 84/8000  train MSE: 0.9975, val MSE: 0.9978
Gen 85/8000  train MSE: 0.9975, val MSE: 0.9979
Gen 86/8000  train MSE: 0.9975, val MSE: 0.9979
Gen 87/8000  train MSE: 0.9975, val MSE: 0.9979
Gen 88/8000  train MSE: 0.9974, val MSE: 0.9980
Gen 89/8000  train MSE: 0.9973, val MSE: 0.9972
Gen 90/8000  train MSE: 0.9973, val MSE: 0.9972
Gen 91/8000  train MSE: 0.9970, val MSE: 0.9978
Gen 92/8000  train MSE: 0.9970, val MSE: 0.9978
Gen 93/8000  train MSE: 0.9970, val MSE: 0.9978
Gen 94/8000  train MSE: 0.9970, val MSE: 0.9978
Gen 95/8000  train MSE: 0.9970, val MSE: 0.9978
Gen 96/8000  train MSE: 0.9970, val MSE: 0.9978
Gen 97/8000  train MSE: 0.9968, val MSE: 0.9974
Gen 98/8000  train MSE: 0.9968, val MSE: 0.9974
Gen 99/8000  train MSE: 0.9968, val MSE: 0.9974
Gen 100/8000  train MSE: 0.9968, val MSE: 0.9974

--- Refinement @ gen 100 ---
Gen 101/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 102/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 103/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 104/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 105/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 106/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 107/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 108/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 109/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 110/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 111/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 112/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 113/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 114/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 115/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 116/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 117/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 118/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 119/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 120/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 121/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 122/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 123/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 124/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 125/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 126/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 127/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 128/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 129/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 130/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 131/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 132/8000  train MSE: 0.9709, val MSE: 0.9603
Gen 133/8000  train MSE: 0.9708, val MSE: 0.9584
Gen 134/8000  train MSE: 0.9704, val MSE: 0.9586
Gen 135/8000  train MSE: 0.9701, val MSE: 0.9581
Gen 136/8000  train MSE: 0.9699, val MSE: 0.9586
Gen 137/8000  train MSE: 0.9695, val MSE: 0.9579
Gen 138/8000  train MSE: 0.9689, val MSE: 0.9583
Gen 139/8000  train MSE: 0.9689, val MSE: 0.9575
Gen 140/8000  train MSE: 0.9687, val MSE: 0.9577
Gen 141/8000  train MSE: 0.9685, val MSE: 0.9571
Gen 142/8000  train MSE: 0.9682, val MSE: 0.9578
Gen 143/8000  train MSE: 0.9682, val MSE: 0.9573
Gen 144/8000  train MSE: 0.9681, val MSE: 0.9577
Gen 145/8000  train MSE: 0.9681, val MSE: 0.9577
Gen 146/8000  train MSE: 0.9679, val MSE: 0.9573
Gen 147/8000  train MSE: 0.9677, val MSE: 0.9563
Gen 148/8000  train MSE: 0.9675, val MSE: 0.9561
Gen 149/8000  train MSE: 0.9675, val MSE: 0.9561
Gen 150/8000  train MSE: 0.9675, val MSE: 0.9560
Gen 151/8000  train MSE: 0.9673, val MSE: 0.9552
Gen 152/8000  train MSE: 0.9673, val MSE: 0.9552
Gen 153/8000  train MSE: 0.9673, val MSE: 0.9552
Gen 154/8000  train MSE: 0.9673, val MSE: 0.9552
Gen 155/8000  train MSE: 0.9672, val MSE: 0.9560
Gen 156/8000  train MSE: 0.9670, val MSE: 0.9558
Gen 157/8000  train MSE: 0.9670, val MSE: 0.9558
Gen 158/8000  train MSE: 0.9670, val MSE: 0.9558
Gen 159/8000  train MSE: 0.9670, val MSE: 0.9550
Gen 160/8000  train MSE: 0.9670, val MSE: 0.9550
Gen 161/8000  train MSE: 0.9670, val MSE: 0.9550
Gen 162/8000  train MSE: 0.9670, val MSE: 0.9557
Gen 163/8000  train MSE: 0.9670, val MSE: 0.9557
Gen 164/8000  train MSE: 0.9669, val MSE: 0.9554
Gen 165/8000  train MSE: 0.9668, val MSE: 0.9555
Gen 166/8000  train MSE: 0.9668, val MSE: 0.9555
Gen 167/8000  train MSE: 0.9667, val MSE: 0.9546
Gen 168/8000  train MSE: 0.9667, val MSE: 0.9546
Gen 169/8000  train MSE: 0.9667, val MSE: 0.9545
Gen 170/8000  train MSE: 0.9667, val MSE: 0.9545
Gen 171/8000  train MSE: 0.9667, val MSE: 0.9545
Gen 172/8000  train MSE: 0.9667, val MSE: 0.9546
Gen 173/8000  train MSE: 0.9667, val MSE: 0.9546
Gen 174/8000  train MSE: 0.9667, val MSE: 0.9546
Gen 175/8000  train MSE: 0.9666, val MSE: 0.9549
Gen 176/8000  train MSE: 0.9666, val MSE: 0.9549
Gen 177/8000  train MSE: 0.9666, val MSE: 0.9549
Gen 178/8000  train MSE: 0.9666, val MSE: 0.9549
Gen 179/8000  train MSE: 0.9666, val MSE: 0.9549
Gen 180/8000  train MSE: 0.9666, val MSE: 0.9546
Gen 181/8000  train MSE: 0.9665, val MSE: 0.9549
Gen 182/8000  train MSE: 0.9665, val MSE: 0.9549
Gen 183/8000  train MSE: 0.9664, val MSE: 0.9552
Gen 184/8000  train MSE: 0.9664, val MSE: 0.9552
Gen 185/8000  train MSE: 0.9664, val MSE: 0.9552
Gen 186/8000  train MSE: 0.9664, val MSE: 0.9552
Gen 187/8000  train MSE: 0.9664, val MSE: 0.9552
Gen 188/8000  train MSE: 0.9664, val MSE: 0.9550
Gen 189/8000  train MSE: 0.9664, val MSE: 0.9550
Gen 190/8000  train MSE: 0.9664, val MSE: 0.9550
Gen 191/8000  train MSE: 0.9663, val MSE: 0.9542
Gen 192/8000  train MSE: 0.9663, val MSE: 0.9542
Gen 193/8000  train MSE: 0.9663, val MSE: 0.9542
Gen 194/8000  train MSE: 0.9662, val MSE: 0.9539
Gen 195/8000  train MSE: 0.9662, val MSE: 0.9539
Gen 196/8000  train MSE: 0.9662, val MSE: 0.9539
Gen 197/8000  train MSE: 0.9662, val MSE: 0.9539
Gen 198/8000  train MSE: 0.9662, val MSE: 0.9539
Gen 199/8000  train MSE: 0.9662, val MSE: 0.9539
Gen 200/8000  train MSE: 0.9661, val MSE: 0.9547

--- Refinement @ gen 200 ---
Gen 201/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 202/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 203/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 204/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 205/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 206/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 207/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 208/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 209/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 210/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 211/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 212/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 213/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 214/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 215/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 216/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 217/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 218/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 219/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 220/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 221/8000  train MSE: 0.8110, val MSE: 0.7011
Gen 222/8000  train MSE: 0.8084, val MSE: 0.6936
Gen 223/8000  train MSE: 0.8078, val MSE: 0.6954
Gen 224/8000  train MSE: 0.8019, val MSE: 0.6884
Gen 225/8000  train MSE: 0.8019, val MSE: 0.6884
Gen 226/8000  train MSE: 0.8012, val MSE: 0.6882
Gen 227/8000  train MSE: 0.8000, val MSE: 0.6857
Gen 228/8000  train MSE: 0.7983, val MSE: 0.6873
Gen 229/8000  train MSE: 0.7976, val MSE: 0.6855
Gen 230/8000  train MSE: 0.7971, val MSE: 0.6854
Gen 231/8000  train MSE: 0.7956, val MSE: 0.6823
Gen 232/8000  train MSE: 0.7948, val MSE: 0.6851
Gen 233/8000  train MSE: 0.7941, val MSE: 0.6769
Gen 234/8000  train MSE: 0.7941, val MSE: 0.6769
Gen 235/8000  train MSE: 0.7938, val MSE: 0.6790
Gen 236/8000  train MSE: 0.7931, val MSE: 0.6818
Gen 237/8000  train MSE: 0.7931, val MSE: 0.6818
Gen 238/8000  train MSE: 0.7894, val MSE: 0.6754
Gen 239/8000  train MSE: 0.7894, val MSE: 0.6754
Gen 240/8000  train MSE: 0.7894, val MSE: 0.6754
Gen 241/8000  train MSE: 0.7894, val MSE: 0.6754
Gen 242/8000  train MSE: 0.7893, val MSE: 0.6741
Gen 243/8000  train MSE: 0.7893, val MSE: 0.6741
Gen 244/8000  train MSE: 0.7893, val MSE: 0.6741
Gen 245/8000  train MSE: 0.7881, val MSE: 0.6749
Gen 246/8000  train MSE: 0.7881, val MSE: 0.6749
Gen 247/8000  train MSE: 0.7881, val MSE: 0.6749
Gen 248/8000  train MSE: 0.7881, val MSE: 0.6749
Gen 249/8000  train MSE: 0.7881, val MSE: 0.6749
Gen 250/8000  train MSE: 0.7881, val MSE: 0.6749
Gen 251/8000  train MSE: 0.7873, val MSE: 0.6751
Gen 252/8000  train MSE: 0.7873, val MSE: 0.6751
Gen 253/8000  train MSE: 0.7873, val MSE: 0.6751
Gen 254/8000  train MSE: 0.7863, val MSE: 0.6746
Gen 255/8000  train MSE: 0.7858, val MSE: 0.6738
Gen 256/8000  train MSE: 0.7853, val MSE: 0.6715
Gen 257/8000  train MSE: 0.7842, val MSE: 0.6696
Gen 258/8000  train MSE: 0.7837, val MSE: 0.6705
Gen 259/8000  train MSE: 0.7833, val MSE: 0.6706
Gen 260/8000  train MSE: 0.7833, val MSE: 0.6699
Gen 261/8000  train MSE: 0.7820, val MSE: 0.6680
Gen 262/8000  train MSE: 0.7820, val MSE: 0.6680
Gen 263/8000  train MSE: 0.7820, val MSE: 0.6680
Gen 264/8000  train MSE: 0.7816, val MSE: 0.6653
Gen 265/8000  train MSE: 0.7816, val MSE: 0.6653
Gen 266/8000  train MSE: 0.7813, val MSE: 0.6665
Gen 267/8000  train MSE: 0.7807, val MSE: 0.6681
Gen 268/8000  train MSE: 0.7807, val MSE: 0.6681
Gen 269/8000  train MSE: 0.7803, val MSE: 0.6675
Gen 270/8000  train MSE: 0.7794, val MSE: 0.6621
Gen 271/8000  train MSE: 0.7794, val MSE: 0.6621
Gen 272/8000  train MSE: 0.7794, val MSE: 0.6621
Gen 273/8000  train MSE: 0.7786, val MSE: 0.6659
Gen 274/8000  train MSE: 0.7786, val MSE: 0.6659
Gen 275/8000  train MSE: 0.7786, val MSE: 0.6659
Gen 276/8000  train MSE: 0.7786, val MSE: 0.6659
Gen 277/8000  train MSE: 0.7786, val MSE: 0.6659
Gen 278/8000  train MSE: 0.7786, val MSE: 0.6659
Gen 279/8000  train MSE: 0.7785, val MSE: 0.6664
Gen 280/8000  train MSE: 0.7785, val MSE: 0.6668
Gen 281/8000  train MSE: 0.7785, val MSE: 0.6668
Gen 282/8000  train MSE: 0.7785, val MSE: 0.6668
Gen 283/8000  train MSE: 0.7785, val MSE: 0.6668
Gen 284/8000  train MSE: 0.7780, val MSE: 0.6615
Gen 285/8000  train MSE: 0.7780, val MSE: 0.6615
Gen 286/8000  train MSE: 0.7780, val MSE: 0.6615
Gen 287/8000  train MSE: 0.7780, val MSE: 0.6615
Gen 288/8000  train MSE: 0.7772, val MSE: 0.6657
Gen 289/8000  train MSE: 0.7772, val MSE: 0.6657
Gen 290/8000  train MSE: 0.7771, val MSE: 0.6657
Gen 291/8000  train MSE: 0.7770, val MSE: 0.6654
Gen 292/8000  train MSE: 0.7768, val MSE: 0.6612
Gen 293/8000  train MSE: 0.7768, val MSE: 0.6612
Gen 294/8000  train MSE: 0.7764, val MSE: 0.6657
Gen 295/8000  train MSE: 0.7764, val MSE: 0.6657
Gen 296/8000  train MSE: 0.7764, val MSE: 0.6657
Gen 297/8000  train MSE: 0.7764, val MSE: 0.6657
Gen 298/8000  train MSE: 0.7764, val MSE: 0.6657
Gen 299/8000  train MSE: 0.7764, val MSE: 0.6657
Gen 300/8000  train MSE: 0.7764, val MSE: 0.6657

--- Refinement @ gen 300 ---
Gen 301/8000  train MSE: 0.7087, val MSE: 0.5411
Gen 302/8000  train MSE: 0.7085, val MSE: 0.5411
Gen 303/8000  train MSE: 0.7085, val MSE: 0.5411
Gen 304/8000  train MSE: 0.7085, val MSE: 0.5411
Gen 305/8000  train MSE: 0.7085, val MSE: 0.5411
Gen 306/8000  train MSE: 0.7085, val MSE: 0.5411
Gen 307/8000  train MSE: 0.7085, val MSE: 0.5411
Gen 308/8000  train MSE: 0.7085, val MSE: 0.5411
Gen 309/8000  train MSE: 0.7085, val MSE: 0.5411
Gen 310/8000  train MSE: 0.7085, val MSE: 0.5411
Gen 311/8000  train MSE: 0.7085, val MSE: 0.5411
Gen 312/8000  train MSE: 0.7083, val MSE: 0.5407
Gen 313/8000  train MSE: 0.7083, val MSE: 0.5407
Gen 314/8000  train MSE: 0.7083, val MSE: 0.5407
Gen 315/8000  train MSE: 0.7075, val MSE: 0.5402
Gen 316/8000  train MSE: 0.7072, val MSE: 0.5400
Gen 317/8000  train MSE: 0.7068, val MSE: 0.5392
Gen 318/8000  train MSE: 0.7058, val MSE: 0.5403
Gen 319/8000  train MSE: 0.7053, val MSE: 0.5366
Gen 320/8000  train MSE: 0.7034, val MSE: 0.5359
Gen 321/8000  train MSE: 0.7015, val MSE: 0.5323
Gen 322/8000  train MSE: 0.7009, val MSE: 0.5303
Gen 323/8000  train MSE: 0.6999, val MSE: 0.5315
Gen 324/8000  train MSE: 0.6973, val MSE: 0.5308
Gen 325/8000  train MSE: 0.6969, val MSE: 0.5316
Gen 326/8000  train MSE: 0.6949, val MSE: 0.5280
Gen 327/8000  train MSE: 0.6939, val MSE: 0.5224
Gen 328/8000  train MSE: 0.6926, val MSE: 0.5259
Gen 329/8000  train MSE: 0.6915, val MSE: 0.5251
Gen 330/8000  train MSE: 0.6910, val MSE: 0.5258
Gen 331/8000  train MSE: 0.6903, val MSE: 0.5233
Gen 332/8000  train MSE: 0.6896, val MSE: 0.5194
Gen 333/8000  train MSE: 0.6885, val MSE: 0.5204
Gen 334/8000  train MSE: 0.6885, val MSE: 0.5212
Gen 335/8000  train MSE: 0.6885, val MSE: 0.5212
Gen 336/8000  train MSE: 0.6877, val MSE: 0.5189
Gen 337/8000  train MSE: 0.6869, val MSE: 0.5183
Gen 338/8000  train MSE: 0.6868, val MSE: 0.5166
Gen 339/8000  train MSE: 0.6856, val MSE: 0.5160
Gen 340/8000  train MSE: 0.6856, val MSE: 0.5160
Gen 341/8000  train MSE: 0.6856, val MSE: 0.5160
Gen 342/8000  train MSE: 0.6856, val MSE: 0.5160
Gen 343/8000  train MSE: 0.6852, val MSE: 0.5133
Gen 344/8000  train MSE: 0.6852, val MSE: 0.5133
Gen 345/8000  train MSE: 0.6843, val MSE: 0.5136
Gen 346/8000  train MSE: 0.6843, val MSE: 0.5136
Gen 347/8000  train MSE: 0.6837, val MSE: 0.5140
Gen 348/8000  train MSE: 0.6837, val MSE: 0.5140
Gen 349/8000  train MSE: 0.6837, val MSE: 0.5140
Gen 350/8000  train MSE: 0.6837, val MSE: 0.5140
Gen 351/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 352/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 353/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 354/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 355/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 356/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 357/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 358/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 359/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 360/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 361/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 362/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 363/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 364/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 365/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 366/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 367/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 368/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 369/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 370/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 371/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 372/8000  train MSE: 0.6811, val MSE: 0.5108
Gen 373/8000  train MSE: 0.6808, val MSE: 0.5110
Gen 374/8000  train MSE: 0.6808, val MSE: 0.5127
Gen 375/8000  train MSE: 0.6802, val MSE: 0.5114
Gen 376/8000  train MSE: 0.6802, val MSE: 0.5114
Gen 377/8000  train MSE: 0.6796, val MSE: 0.5079
Gen 378/8000  train MSE: 0.6791, val MSE: 0.5100
Gen 379/8000  train MSE: 0.6791, val MSE: 0.5100
Gen 380/8000  train MSE: 0.6787, val MSE: 0.5079
Gen 381/8000  train MSE: 0.6787, val MSE: 0.5079
Gen 382/8000  train MSE: 0.6784, val MSE: 0.5052
Gen 383/8000  train MSE: 0.6784, val MSE: 0.5072
Gen 384/8000  train MSE: 0.6784, val MSE: 0.5072
Gen 385/8000  train MSE: 0.6784, val MSE: 0.5072
Gen 386/8000  train MSE: 0.6784, val MSE: 0.5072
Gen 387/8000  train MSE: 0.6780, val MSE: 0.5064
Gen 388/8000  train MSE: 0.6771, val MSE: 0.5059
Gen 389/8000  train MSE: 0.6771, val MSE: 0.5059
Gen 390/8000  train MSE: 0.6771, val MSE: 0.5059
Gen 391/8000  train MSE: 0.6771, val MSE: 0.5059
Gen 392/8000  train MSE: 0.6771, val MSE: 0.5059
Gen 393/8000  train MSE: 0.6771, val MSE: 0.5059
Gen 394/8000  train MSE: 0.6771, val MSE: 0.5023
Gen 395/8000  train MSE: 0.6771, val MSE: 0.5023
Gen 396/8000  train MSE: 0.6771, val MSE: 0.5023
Gen 397/8000  train MSE: 0.6771, val MSE: 0.5023
Gen 398/8000  train MSE: 0.6771, val MSE: 0.5023
Gen 399/8000  train MSE: 0.6771, val MSE: 0.5023
Gen 400/8000  train MSE: 0.6771, val MSE: 0.5023

--- Refinement @ gen 400 ---
Gen 401/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 402/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 403/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 404/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 405/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 406/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 407/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 408/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 409/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 410/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 411/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 412/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 413/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 414/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 415/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 416/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 417/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 418/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 419/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 420/8000  train MSE: 0.6512, val MSE: 0.4522
Gen 421/8000  train MSE: 0.6505, val MSE: 0.4549
Gen 422/8000  train MSE: 0.6478, val MSE: 0.4498
Gen 423/8000  train MSE: 0.6465, val MSE: 0.4463
Gen 424/8000  train MSE: 0.6462, val MSE: 0.4496
Gen 425/8000  train MSE: 0.6442, val MSE: 0.4471
Gen 426/8000  train MSE: 0.6442, val MSE: 0.4471
Gen 427/8000  train MSE: 0.6437, val MSE: 0.4485
Gen 428/8000  train MSE: 0.6428, val MSE: 0.4422
Gen 429/8000  train MSE: 0.6422, val MSE: 0.4451
Gen 430/8000  train MSE: 0.6400, val MSE: 0.4409
Gen 431/8000  train MSE: 0.6396, val MSE: 0.4399
Gen 432/8000  train MSE: 0.6395, val MSE: 0.4394
Gen 433/8000  train MSE: 0.6381, val MSE: 0.4365
Gen 434/8000  train MSE: 0.6379, val MSE: 0.4376
Gen 435/8000  train MSE: 0.6359, val MSE: 0.4371
Gen 436/8000  train MSE: 0.6355, val MSE: 0.4266
Gen 437/8000  train MSE: 0.6355, val MSE: 0.4266
Gen 438/8000  train MSE: 0.6355, val MSE: 0.4266
Gen 439/8000  train MSE: 0.6355, val MSE: 0.4266
Gen 440/8000  train MSE: 0.6355, val MSE: 0.4266
Gen 441/8000  train MSE: 0.6332, val MSE: 0.4276
Gen 442/8000  train MSE: 0.6332, val MSE: 0.4276
Gen 443/8000  train MSE: 0.6332, val MSE: 0.4276
Gen 444/8000  train MSE: 0.6332, val MSE: 0.4276
Gen 445/8000  train MSE: 0.6332, val MSE: 0.4276
Gen 446/8000  train MSE: 0.6332, val MSE: 0.4276
Gen 447/8000  train MSE: 0.6310, val MSE: 0.4266
Gen 448/8000  train MSE: 0.6310, val MSE: 0.4266
Gen 449/8000  train MSE: 0.6310, val MSE: 0.4266
Gen 450/8000  train MSE: 0.6310, val MSE: 0.4266
Gen 451/8000  train MSE: 0.6310, val MSE: 0.4266
Gen 452/8000  train MSE: 0.6310, val MSE: 0.4266
Gen 453/8000  train MSE: 0.6305, val MSE: 0.4276
Gen 454/8000  train MSE: 0.6305, val MSE: 0.4276
Gen 455/8000  train MSE: 0.6303, val MSE: 0.4251
Gen 456/8000  train MSE: 0.6303, val MSE: 0.4251
Gen 457/8000  train MSE: 0.6303, val MSE: 0.4251
Gen 458/8000  train MSE: 0.6303, val MSE: 0.4251
Gen 459/8000  train MSE: 0.6303, val MSE: 0.4251
Gen 460/8000  train MSE: 0.6303, val MSE: 0.4251
Gen 461/8000  train MSE: 0.6303, val MSE: 0.4251
Gen 462/8000  train MSE: 0.6294, val MSE: 0.4260
Gen 463/8000  train MSE: 0.6294, val MSE: 0.4260
Gen 464/8000  train MSE: 0.6294, val MSE: 0.4260
Gen 465/8000  train MSE: 0.6294, val MSE: 0.4260
Gen 466/8000  train MSE: 0.6294, val MSE: 0.4260
Gen 467/8000  train MSE: 0.6294, val MSE: 0.4260
Gen 468/8000  train MSE: 0.6292, val MSE: 0.4214
Gen 469/8000  train MSE: 0.6281, val MSE: 0.4222
Gen 470/8000  train MSE: 0.6281, val MSE: 0.4222
Gen 471/8000  train MSE: 0.6281, val MSE: 0.4222
Gen 472/8000  train MSE: 0.6281, val MSE: 0.4222
Gen 473/8000  train MSE: 0.6281, val MSE: 0.4222
Gen 474/8000  train MSE: 0.6281, val MSE: 0.4222
Gen 475/8000  train MSE: 0.6281, val MSE: 0.4222
Gen 476/8000  train MSE: 0.6281, val MSE: 0.4222
Gen 477/8000  train MSE: 0.6281, val MSE: 0.4222
Gen 478/8000  train MSE: 0.6277, val MSE: 0.4203
Gen 479/8000  train MSE: 0.6277, val MSE: 0.4203
Gen 480/8000  train MSE: 0.6277, val MSE: 0.4203
Gen 481/8000  train MSE: 0.6277, val MSE: 0.4203
Gen 482/8000  train MSE: 0.6277, val MSE: 0.4203
Gen 483/8000  train MSE: 0.6277, val MSE: 0.4203
Gen 484/8000  train MSE: 0.6277, val MSE: 0.4203
Gen 485/8000  train MSE: 0.6277, val MSE: 0.4203
Gen 486/8000  train MSE: 0.6275, val MSE: 0.4204
Gen 487/8000  train MSE: 0.6267, val MSE: 0.4218
Gen 488/8000  train MSE: 0.6265, val MSE: 0.4218
Gen 489/8000  train MSE: 0.6263, val MSE: 0.4207
Gen 490/8000  train MSE: 0.6260, val MSE: 0.4201
Gen 491/8000  train MSE: 0.6260, val MSE: 0.4201
Gen 492/8000  train MSE: 0.6260, val MSE: 0.4201
Gen 493/8000  train MSE: 0.6258, val MSE: 0.4146
Gen 494/8000  train MSE: 0.6253, val MSE: 0.4195
Gen 495/8000  train MSE: 0.6245, val MSE: 0.4185
Gen 496/8000  train MSE: 0.6245, val MSE: 0.4185
Gen 497/8000  train MSE: 0.6244, val MSE: 0.4184
Gen 498/8000  train MSE: 0.6244, val MSE: 0.4184
Gen 499/8000  train MSE: 0.6244, val MSE: 0.4184
Gen 500/8000  train MSE: 0.6242, val MSE: 0.4172

--- Refinement @ gen 500 ---
Gen 501/8000  train MSE: 0.6253, val MSE: 0.4188
Gen 502/8000  train MSE: 0.6252, val MSE: 0.4222
Gen 503/8000  train MSE: 0.6252, val MSE: 0.4222
Gen 504/8000  train MSE: 0.6252, val MSE: 0.4222
Gen 505/8000  train MSE: 0.6249, val MSE: 0.4180
Gen 506/8000  train MSE: 0.6248, val MSE: 0.4194
Gen 507/8000  train MSE: 0.6241, val MSE: 0.4203
Gen 508/8000  train MSE: 0.6241, val MSE: 0.4203
Gen 509/8000  train MSE: 0.6240, val MSE: 0.4209
Gen 510/8000  train MSE: 0.6233, val MSE: 0.4152
Gen 511/8000  train MSE: 0.6232, val MSE: 0.4186
Gen 512/8000  train MSE: 0.6223, val MSE: 0.4156
Gen 513/8000  train MSE: 0.6223, val MSE: 0.4156
Gen 514/8000  train MSE: 0.6221, val MSE: 0.4147
Gen 515/8000  train MSE: 0.6221, val MSE: 0.4147
Gen 516/8000  train MSE: 0.6218, val MSE: 0.4150
Gen 517/8000  train MSE: 0.6218, val MSE: 0.4150
Gen 518/8000  train MSE: 0.6213, val MSE: 0.4111
Gen 519/8000  train MSE: 0.6213, val MSE: 0.4111
Gen 520/8000  train MSE: 0.6211, val MSE: 0.4144
Gen 521/8000  train MSE: 0.6211, val MSE: 0.4144
Gen 522/8000  train MSE: 0.6211, val MSE: 0.4144
Gen 523/8000  train MSE: 0.6211, val MSE: 0.4144
Gen 524/8000  train MSE: 0.6211, val MSE: 0.4144
Gen 525/8000  train MSE: 0.6211, val MSE: 0.4144
Gen 526/8000  train MSE: 0.6208, val MSE: 0.4135
Gen 527/8000  train MSE: 0.6208, val MSE: 0.4135
Gen 528/8000  train MSE: 0.6205, val MSE: 0.4083
Gen 529/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 530/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 531/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 532/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 533/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 534/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 535/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 536/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 537/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 538/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 539/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 540/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 541/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 542/8000  train MSE: 0.6197, val MSE: 0.4134
Gen 543/8000  train MSE: 0.6195, val MSE: 0.4129
Gen 544/8000  train MSE: 0.6189, val MSE: 0.4084
Gen 545/8000  train MSE: 0.6189, val MSE: 0.4084
Gen 546/8000  train MSE: 0.6189, val MSE: 0.4084
Gen 547/8000  train MSE: 0.6189, val MSE: 0.4084
Gen 548/8000  train MSE: 0.6189, val MSE: 0.4084
Gen 549/8000  train MSE: 0.6189, val MSE: 0.4084
Gen 550/8000  train MSE: 0.6189, val MSE: 0.4084
Gen 551/8000  train MSE: 0.6189, val MSE: 0.4084
Gen 552/8000  train MSE: 0.6189, val MSE: 0.4084
Gen 553/8000  train MSE: 0.6189, val MSE: 0.4084
Gen 554/8000  train MSE: 0.6189, val MSE: 0.4084
Gen 555/8000  train MSE: 0.6186, val MSE: 0.4131
Gen 556/8000  train MSE: 0.6186, val MSE: 0.4131
Gen 557/8000  train MSE: 0.6182, val MSE: 0.4125
Gen 558/8000  train MSE: 0.6182, val MSE: 0.4125
Gen 559/8000  train MSE: 0.6182, val MSE: 0.4125
Gen 560/8000  train MSE: 0.6180, val MSE: 0.4106
Gen 561/8000  train MSE: 0.6180, val MSE: 0.4106
Gen 562/8000  train MSE: 0.6180, val MSE: 0.4106
Gen 563/8000  train MSE: 0.6180, val MSE: 0.4106
Gen 564/8000  train MSE: 0.6180, val MSE: 0.4106
Gen 565/8000  train MSE: 0.6180, val MSE: 0.4106
Gen 566/8000  train MSE: 0.6180, val MSE: 0.4106
Gen 567/8000  train MSE: 0.6180, val MSE: 0.4106
Gen 568/8000  train MSE: 0.6180, val MSE: 0.4106
Gen 569/8000  train MSE: 0.6180, val MSE: 0.4106
Gen 570/8000  train MSE: 0.6178, val MSE: 0.4096
Gen 571/8000  train MSE: 0.6170, val MSE: 0.4089
Gen 572/8000  train MSE: 0.6170, val MSE: 0.4089
Gen 573/8000  train MSE: 0.6170, val MSE: 0.4089
Gen 574/8000  train MSE: 0.6170, val MSE: 0.4089
Gen 575/8000  train MSE: 0.6170, val MSE: 0.4089
Gen 576/8000  train MSE: 0.6170, val MSE: 0.4089
Gen 577/8000  train MSE: 0.6170, val MSE: 0.4089
Gen 578/8000  train MSE: 0.6169, val MSE: 0.4073
Gen 579/8000  train MSE: 0.6167, val MSE: 0.4086
Gen 580/8000  train MSE: 0.6165, val MSE: 0.4077
Gen 581/8000  train MSE: 0.6161, val MSE: 0.4086
Gen 582/8000  train MSE: 0.6161, val MSE: 0.4086
Gen 583/8000  train MSE: 0.6161, val MSE: 0.4086
Gen 584/8000  train MSE: 0.6161, val MSE: 0.4086
Gen 585/8000  train MSE: 0.6161, val MSE: 0.4086
Gen 586/8000  train MSE: 0.6161, val MSE: 0.4086
Gen 587/8000  train MSE: 0.6161, val MSE: 0.4086
Gen 588/8000  train MSE: 0.6160, val MSE: 0.4063
Gen 589/8000  train MSE: 0.6160, val MSE: 0.4063
Gen 590/8000  train MSE: 0.6160, val MSE: 0.4063
Gen 591/8000  train MSE: 0.6157, val MSE: 0.4063
Gen 592/8000  train MSE: 0.6156, val MSE: 0.4061
Gen 593/8000  train MSE: 0.6156, val MSE: 0.4061
Gen 594/8000  train MSE: 0.6156, val MSE: 0.4061
Gen 595/8000  train MSE: 0.6153, val MSE: 0.4057
Gen 596/8000  train MSE: 0.6153, val MSE: 0.4057
Gen 597/8000  train MSE: 0.6153, val MSE: 0.4057
Gen 598/8000  train MSE: 0.6152, val MSE: 0.4067
Gen 599/8000  train MSE: 0.6150, val MSE: 0.4050
Gen 600/8000  train MSE: 0.6150, val MSE: 0.4050

--- Refinement @ gen 600 ---
Gen 601/8000  train MSE: 0.6158, val MSE: 0.4066
Gen 602/8000  train MSE: 0.6158, val MSE: 0.4066
Gen 603/8000  train MSE: 0.6158, val MSE: 0.4066
Gen 604/8000  train MSE: 0.6158, val MSE: 0.4066
Gen 605/8000  train MSE: 0.6158, val MSE: 0.4066
Gen 606/8000  train MSE: 0.6151, val MSE: 0.4061
Gen 607/8000  train MSE: 0.6151, val MSE: 0.4061
Gen 608/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 609/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 610/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 611/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 612/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 613/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 614/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 615/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 616/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 617/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 618/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 619/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 620/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 621/8000  train MSE: 0.6150, val MSE: 0.4029
Gen 622/8000  train MSE: 0.6150, val MSE: 0.4027
Gen 623/8000  train MSE: 0.6150, val MSE: 0.4027
Gen 624/8000  train MSE: 0.6150, val MSE: 0.4027
Gen 625/8000  train MSE: 0.6150, val MSE: 0.4031
Gen 626/8000  train MSE: 0.6150, val MSE: 0.4031
Gen 627/8000  train MSE: 0.6145, val MSE: 0.4044
Gen 628/8000  train MSE: 0.6145, val MSE: 0.4044
Gen 629/8000  train MSE: 0.6145, val MSE: 0.4044
Gen 630/8000  train MSE: 0.6141, val MSE: 0.4043
Gen 631/8000  train MSE: 0.6141, val MSE: 0.4043
Gen 632/8000  train MSE: 0.6141, val MSE: 0.4028
Gen 633/8000  train MSE: 0.6138, val MSE: 0.4035
Gen 634/8000  train MSE: 0.6138, val MSE: 0.4035
Gen 635/8000  train MSE: 0.6138, val MSE: 0.4035
Gen 636/8000  train MSE: 0.6138, val MSE: 0.4020
Gen 637/8000  train MSE: 0.6138, val MSE: 0.4020
Gen 638/8000  train MSE: 0.6138, val MSE: 0.4020
Gen 639/8000  train MSE: 0.6136, val MSE: 0.4035
Gen 640/8000  train MSE: 0.6136, val MSE: 0.4035
Gen 641/8000  train MSE: 0.6134, val MSE: 0.4021
Gen 642/8000  train MSE: 0.6134, val MSE: 0.4021
Gen 643/8000  train MSE: 0.6134, val MSE: 0.4021
Gen 644/8000  train MSE: 0.6134, val MSE: 0.4004
Gen 645/8000  train MSE: 0.6134, val MSE: 0.4004
Gen 646/8000  train MSE: 0.6134, val MSE: 0.4026
Gen 647/8000  train MSE: 0.6134, val MSE: 0.4026
Gen 648/8000  train MSE: 0.6134, val MSE: 0.4026
Gen 649/8000  train MSE: 0.6132, val MSE: 0.4013
Gen 650/8000  train MSE: 0.6125, val MSE: 0.3988
Gen 651/8000  train MSE: 0.6125, val MSE: 0.3988
Gen 652/8000  train MSE: 0.6125, val MSE: 0.3988
Gen 653/8000  train MSE: 0.6125, val MSE: 0.3988
Gen 654/8000  train MSE: 0.6125, val MSE: 0.3988
Gen 655/8000  train MSE: 0.6125, val MSE: 0.3988
Gen 656/8000  train MSE: 0.6125, val MSE: 0.3988
Gen 657/8000  train MSE: 0.6125, val MSE: 0.3988
Gen 658/8000  train MSE: 0.6125, val MSE: 0.3988
Gen 659/8000  train MSE: 0.6125, val MSE: 0.3988
Gen 660/8000  train MSE: 0.6123, val MSE: 0.4023
Gen 661/8000  train MSE: 0.6123, val MSE: 0.4023
Gen 662/8000  train MSE: 0.6123, val MSE: 0.4023
Gen 663/8000  train MSE: 0.6123, val MSE: 0.4023
Gen 664/8000  train MSE: 0.6123, val MSE: 0.4005
Gen 665/8000  train MSE: 0.6123, val MSE: 0.4005
Gen 666/8000  train MSE: 0.6123, val MSE: 0.4012
Gen 667/8000  train MSE: 0.6121, val MSE: 0.3998
Gen 668/8000  train MSE: 0.6121, val MSE: 0.3998
Gen 669/8000  train MSE: 0.6121, val MSE: 0.3998
Gen 670/8000  train MSE: 0.6121, val MSE: 0.3998
Gen 671/8000  train MSE: 0.6121, val MSE: 0.3998
Gen 672/8000  train MSE: 0.6119, val MSE: 0.3995
Gen 673/8000  train MSE: 0.6119, val MSE: 0.3995
Gen 674/8000  train MSE: 0.6119, val MSE: 0.3995
Gen 675/8000  train MSE: 0.6119, val MSE: 0.3995
Gen 676/8000  train MSE: 0.6119, val MSE: 0.3995
Gen 677/8000  train MSE: 0.6119, val MSE: 0.3995
Gen 678/8000  train MSE: 0.6118, val MSE: 0.4007
Gen 679/8000  train MSE: 0.6117, val MSE: 0.3996
Gen 680/8000  train MSE: 0.6117, val MSE: 0.3996
Gen 681/8000  train MSE: 0.6115, val MSE: 0.3992
Gen 682/8000  train MSE: 0.6115, val MSE: 0.3990
Gen 683/8000  train MSE: 0.6114, val MSE: 0.3996
Gen 684/8000  train MSE: 0.6114, val MSE: 0.3996
Gen 685/8000  train MSE: 0.6112, val MSE: 0.3972
Gen 686/8000  train MSE: 0.6112, val MSE: 0.3972
Gen 687/8000  train MSE: 0.6112, val MSE: 0.3972
Gen 688/8000  train MSE: 0.6112, val MSE: 0.3972
Gen 689/8000  train MSE: 0.6112, val MSE: 0.3972
Gen 690/8000  train MSE: 0.6112, val MSE: 0.3972
Gen 691/8000  train MSE: 0.6112, val MSE: 0.3972
Gen 692/8000  train MSE: 0.6112, val MSE: 0.3972
Gen 693/8000  train MSE: 0.6112, val MSE: 0.3972
Gen 694/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 695/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 696/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 697/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 698/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 699/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 700/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 701/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 702/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 703/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 704/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 705/8000  train MSE: 0.6112, val MSE: 0.3984
Gen 706/8000  train MSE: 0.6111, val MSE: 0.3989
Gen 707/8000  train MSE: 0.6111, val MSE: 0.3989
Gen 708/8000  train MSE: 0.6111, val MSE: 0.3989
Gen 709/8000  train MSE: 0.6111, val MSE: 0.3989
Gen 710/8000  train MSE: 0.6107, val MSE: 0.3978
Gen 711/8000  train MSE: 0.6107, val MSE: 0.3978
Gen 712/8000  train MSE: 0.6107, val MSE: 0.3978
Gen 713/8000  train MSE: 0.6106, val MSE: 0.3983
Gen 714/8000  train MSE: 0.6106, val MSE: 0.3983
Gen 715/8000  train MSE: 0.6106, val MSE: 0.3983
Gen 716/8000  train MSE: 0.6106, val MSE: 0.3983
Gen 717/8000  train MSE: 0.6106, val MSE: 0.3983
Gen 718/8000  train MSE: 0.6106, val MSE: 0.3983
Gen 719/8000  train MSE: 0.6106, val MSE: 0.3976
Gen 720/8000  train MSE: 0.6104, val MSE: 0.3972
Gen 721/8000  train MSE: 0.6103, val MSE: 0.3978
Gen 722/8000  train MSE: 0.6103, val MSE: 0.3978
Gen 723/8000  train MSE: 0.6103, val MSE: 0.3978
Gen 724/8000  train MSE: 0.6103, val MSE: 0.3978
Gen 725/8000  train MSE: 0.6101, val MSE: 0.3952
Gen 726/8000  train MSE: 0.6101, val MSE: 0.3952
Gen 727/8000  train MSE: 0.6100, val MSE: 0.3960
Gen 728/8000  train MSE: 0.6100, val MSE: 0.3960
Gen 729/8000  train MSE: 0.6100, val MSE: 0.3960
Gen 730/8000  train MSE: 0.6100, val MSE: 0.3960
Gen 731/8000  train MSE: 0.6100, val MSE: 0.3960
Gen 732/8000  train MSE: 0.6100, val MSE: 0.3960
Gen 733/8000  train MSE: 0.6100, val MSE: 0.3960
Gen 734/8000  train MSE: 0.6100, val MSE: 0.3973
Gen 735/8000  train MSE: 0.6096, val MSE: 0.3948
Gen 736/8000  train MSE: 0.6096, val MSE: 0.3948
Gen 737/8000  train MSE: 0.6096, val MSE: 0.3948
Gen 738/8000  train MSE: 0.6096, val MSE: 0.3948
Gen 739/8000  train MSE: 0.6096, val MSE: 0.3948
Gen 740/8000  train MSE: 0.6096, val MSE: 0.3948
Gen 741/8000  train MSE: 0.6095, val MSE: 0.3948
Gen 742/8000  train MSE: 0.6095, val MSE: 0.3948
Gen 743/8000  train MSE: 0.6095, val MSE: 0.3946
Gen 744/8000  train MSE: 0.6094, val MSE: 0.3944
Gen 745/8000  train MSE: 0.6094, val MSE: 0.3944
Gen 746/8000  train MSE: 0.6094, val MSE: 0.3944
Gen 747/8000  train MSE: 0.6094, val MSE: 0.3944
Gen 748/8000  train MSE: 0.6092, val MSE: 0.3929
Gen 749/8000  train MSE: 0.6092, val MSE: 0.3929
Gen 750/8000  train MSE: 0.6092, val MSE: 0.3929
Gen 751/8000  train MSE: 0.6092, val MSE: 0.3929
Gen 752/8000  train MSE: 0.6089, val MSE: 0.3942
Gen 753/8000  train MSE: 0.6089, val MSE: 0.3942
Gen 754/8000  train MSE: 0.6089, val MSE: 0.3942
Gen 755/8000  train MSE: 0.6089, val MSE: 0.3942
Gen 756/8000  train MSE: 0.6089, val MSE: 0.3942
Gen 757/8000  train MSE: 0.6089, val MSE: 0.3942
Gen 758/8000  train MSE: 0.6089, val MSE: 0.3942
Gen 759/8000  train MSE: 0.6089, val MSE: 0.3942
Gen 760/8000  train MSE: 0.6089, val MSE: 0.3942
Gen 761/8000  train MSE: 0.6089, val MSE: 0.3943
Gen 762/8000  train MSE: 0.6089, val MSE: 0.3940
Gen 763/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 764/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 765/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 766/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 767/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 768/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 769/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 770/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 771/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 772/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 773/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 774/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 775/8000  train MSE: 0.6085, val MSE: 0.3932
Gen 776/8000  train MSE: 0.6083, val MSE: 0.3934
Gen 777/8000  train MSE: 0.6083, val MSE: 0.3934
Gen 778/8000  train MSE: 0.6083, val MSE: 0.3934
Gen 779/8000  train MSE: 0.6083, val MSE: 0.3934
Gen 780/8000  train MSE: 0.6083, val MSE: 0.3934
Gen 781/8000  train MSE: 0.6083, val MSE: 0.3934
Gen 782/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 783/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 784/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 785/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 786/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 787/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 788/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 789/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 790/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 791/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 792/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 793/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 794/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 795/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 796/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 797/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 798/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 799/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 800/8000  train MSE: 0.6081, val MSE: 0.3921
Gen 801/8000  train MSE: 0.6081, val MSE: 0.3924
Gen 802/8000  train MSE: 0.6081, val MSE: 0.3912
Gen 803/8000  train MSE: 0.6078, val MSE: 0.3917
Gen 804/8000  train MSE: 0.6078, val MSE: 0.3917
Gen 805/8000  train MSE: 0.6078, val MSE: 0.3917
Gen 806/8000  train MSE: 0.6078, val MSE: 0.3917
Gen 807/8000  train MSE: 0.6078, val MSE: 0.3917
Gen 808/8000  train MSE: 0.6078, val MSE: 0.3917
Gen 809/8000  train MSE: 0.6078, val MSE: 0.3917
Gen 810/8000  train MSE: 0.6078, val MSE: 0.3917
Gen 811/8000  train MSE: 0.6078, val MSE: 0.3917
Gen 812/8000  train MSE: 0.6078, val MSE: 0.3917
Gen 813/8000  train MSE: 0.6078, val MSE: 0.3917
Gen 814/8000  train MSE: 0.6077, val MSE: 0.3920
Gen 815/8000  train MSE: 0.6077, val MSE: 0.3920
Gen 816/8000  train MSE: 0.6077, val MSE: 0.3920
Gen 817/8000  train MSE: 0.6077, val MSE: 0.3920
Gen 818/8000  train MSE: 0.6073, val MSE: 0.3916
Gen 819/8000  train MSE: 0.6073, val MSE: 0.3916
Gen 820/8000  train MSE: 0.6073, val MSE: 0.3916
Gen 821/8000  train MSE: 0.6073, val MSE: 0.3916
Gen 822/8000  train MSE: 0.6073, val MSE: 0.3916
Gen 823/8000  train MSE: 0.6073, val MSE: 0.3916
Gen 824/8000  train MSE: 0.6073, val MSE: 0.3916
Gen 825/8000  train MSE: 0.6073, val MSE: 0.3916
Gen 826/8000  train MSE: 0.6072, val MSE: 0.3918
Gen 827/8000  train MSE: 0.6072, val MSE: 0.3918
Gen 828/8000  train MSE: 0.6072, val MSE: 0.3916
Gen 829/8000  train MSE: 0.6068, val MSE: 0.3879
Gen 830/8000  train MSE: 0.6068, val MSE: 0.3879
Gen 831/8000  train MSE: 0.6068, val MSE: 0.3879
Gen 832/8000  train MSE: 0.6068, val MSE: 0.3879
Gen 833/8000  train MSE: 0.6068, val MSE: 0.3879
Gen 834/8000  train MSE: 0.6068, val MSE: 0.3879
Gen 835/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 836/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 837/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 838/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 839/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 840/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 841/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 842/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 843/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 844/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 845/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 846/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 847/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 848/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 849/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 850/8000  train MSE: 0.6066, val MSE: 0.3892
Gen 851/8000  train MSE: 0.6064, val MSE: 0.3867
Gen 852/8000  train MSE: 0.6063, val MSE: 0.3870
Gen 853/8000  train MSE: 0.6063, val MSE: 0.3870
Gen 854/8000  train MSE: 0.6063, val MSE: 0.3870
Gen 855/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 856/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 857/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 858/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 859/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 860/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 861/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 862/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 863/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 864/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 865/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 866/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 867/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 868/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 869/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 870/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 871/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 872/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 873/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 874/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 875/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 876/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 877/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 878/8000  train MSE: 0.6062, val MSE: 0.3886
Gen 879/8000  train MSE: 0.6060, val MSE: 0.3859
Gen 880/8000  train MSE: 0.6060, val MSE: 0.3866
Gen 881/8000  train MSE: 0.6060, val MSE: 0.3860
Gen 882/8000  train MSE: 0.6058, val MSE: 0.3872
Gen 883/8000  train MSE: 0.6058, val MSE: 0.3872
Gen 884/8000  train MSE: 0.6058, val MSE: 0.3872
Gen 885/8000  train MSE: 0.6057, val MSE: 0.3852
Gen 886/8000  train MSE: 0.6056, val MSE: 0.3853
Gen 887/8000  train MSE: 0.6056, val MSE: 0.3853
Gen 888/8000  train MSE: 0.6054, val MSE: 0.3850
Gen 889/8000  train MSE: 0.6054, val MSE: 0.3850
Gen 890/8000  train MSE: 0.6054, val MSE: 0.3850
Gen 891/8000  train MSE: 0.6052, val MSE: 0.3860
Gen 892/8000  train MSE: 0.6050, val MSE: 0.3841
Gen 893/8000  train MSE: 0.6050, val MSE: 0.3841
Gen 894/8000  train MSE: 0.6050, val MSE: 0.3841
Gen 895/8000  train MSE: 0.6050, val MSE: 0.3841
Gen 896/8000  train MSE: 0.6050, val MSE: 0.3841
Gen 897/8000  train MSE: 0.6050, val MSE: 0.3841
Gen 898/8000  train MSE: 0.6050, val MSE: 0.3841
Gen 899/8000  train MSE: 0.6050, val MSE: 0.3841
Gen 900/8000  train MSE: 0.6050, val MSE: 0.3847
Gen 901/8000  train MSE: 0.6050, val MSE: 0.3847
Gen 902/8000  train MSE: 0.6050, val MSE: 0.3847
Gen 903/8000  train MSE: 0.6047, val MSE: 0.3853
Gen 904/8000  train MSE: 0.6047, val MSE: 0.3853
Gen 905/8000  train MSE: 0.6047, val MSE: 0.3853
Gen 906/8000  train MSE: 0.6047, val MSE: 0.3853
Gen 907/8000  train MSE: 0.6047, val MSE: 0.3853
Gen 908/8000  train MSE: 0.6047, val MSE: 0.3853
Gen 909/8000  train MSE: 0.6047, val MSE: 0.3853
Gen 910/8000  train MSE: 0.6047, val MSE: 0.3853
Gen 911/8000  train MSE: 0.6047, val MSE: 0.3853
Gen 912/8000  train MSE: 0.6047, val MSE: 0.3853
Gen 913/8000  train MSE: 0.6045, val MSE: 0.3858
Gen 914/8000  train MSE: 0.6045, val MSE: 0.3858
Gen 915/8000  train MSE: 0.6045, val MSE: 0.3858
Gen 916/8000  train MSE: 0.6045, val MSE: 0.3858
Gen 917/8000  train MSE: 0.6045, val MSE: 0.3858
Gen 918/8000  train MSE: 0.6045, val MSE: 0.3858
Gen 919/8000  train MSE: 0.6045, val MSE: 0.3858
Gen 920/8000  train MSE: 0.6045, val MSE: 0.3858
Gen 921/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 922/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 923/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 924/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 925/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 926/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 927/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 928/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 929/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 930/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 931/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 932/8000  train MSE: 0.6045, val MSE: 0.3838
Gen 933/8000  train MSE: 0.6043, val MSE: 0.3851
Gen 934/8000  train MSE: 0.6043, val MSE: 0.3851
Gen 935/8000  train MSE: 0.6043, val MSE: 0.3851
Gen 936/8000  train MSE: 0.6042, val MSE: 0.3849
Gen 937/8000  train MSE: 0.6042, val MSE: 0.3849
Gen 938/8000  train MSE: 0.6040, val MSE: 0.3846
Gen 939/8000  train MSE: 0.6040, val MSE: 0.3846
Gen 940/8000  train MSE: 0.6040, val MSE: 0.3846
Gen 941/8000  train MSE: 0.6040, val MSE: 0.3846
Gen 942/8000  train MSE: 0.6040, val MSE: 0.3846
Gen 943/8000  train MSE: 0.6040, val MSE: 0.3846
Gen 944/8000  train MSE: 0.6039, val MSE: 0.3840
Gen 945/8000  train MSE: 0.6039, val MSE: 0.3840
Gen 946/8000  train MSE: 0.6039, val MSE: 0.3840
Gen 947/8000  train MSE: 0.6039, val MSE: 0.3840
Gen 948/8000  train MSE: 0.6038, val MSE: 0.3848
Gen 949/8000  train MSE: 0.6038, val MSE: 0.3848
Gen 950/8000  train MSE: 0.6038, val MSE: 0.3848
Gen 951/8000  train MSE: 0.6038, val MSE: 0.3848
Gen 952/8000  train MSE: 0.6036, val MSE: 0.3835
Gen 953/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 954/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 955/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 956/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 957/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 958/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 959/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 960/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 961/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 962/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 963/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 964/8000  train MSE: 0.6036, val MSE: 0.3833
Gen 965/8000  train MSE: 0.6036, val MSE: 0.3846
Gen 966/8000  train MSE: 0.6036, val MSE: 0.3846
Gen 967/8000  train MSE: 0.6036, val MSE: 0.3846
Gen 968/8000  train MSE: 0.6036, val MSE: 0.3846
Gen 969/8000  train MSE: 0.6036, val MSE: 0.3846
Gen 970/8000  train MSE: 0.6036, val MSE: 0.3846
Gen 971/8000  train MSE: 0.6036, val MSE: 0.3846
Gen 972/8000  train MSE: 0.6035, val MSE: 0.3837
Gen 973/8000  train MSE: 0.6034, val MSE: 0.3843
Gen 974/8000  train MSE: 0.6034, val MSE: 0.3843
Gen 975/8000  train MSE: 0.6034, val MSE: 0.3843
Gen 976/8000  train MSE: 0.6034, val MSE: 0.3843
Gen 977/8000  train MSE: 0.6034, val MSE: 0.3843
Gen 978/8000  train MSE: 0.6034, val MSE: 0.3843
Gen 979/8000  train MSE: 0.6033, val MSE: 0.3836
Gen 980/8000  train MSE: 0.6033, val MSE: 0.3836
Gen 981/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 982/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 983/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 984/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 985/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 986/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 987/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 988/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 989/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 990/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 991/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 992/8000  train MSE: 0.6032, val MSE: 0.3835
Gen 993/8000  train MSE: 0.6031, val MSE: 0.3838
Gen 994/8000  train MSE: 0.6031, val MSE: 0.3830
Gen 995/8000  train MSE: 0.6031, val MSE: 0.3830
Gen 996/8000  train MSE: 0.6031, val MSE: 0.3830
Gen 997/8000  train MSE: 0.6031, val MSE: 0.3830
Gen 998/8000  train MSE: 0.6030, val MSE: 0.3826
Gen 999/8000  train MSE: 0.6028, val MSE: 0.3825
Gen 1000/8000  train MSE: 0.6028, val MSE: 0.3825
Gen 1001/8000  train MSE: 0.6028, val MSE: 0.3825
Gen 1002/8000  train MSE: 0.6028, val MSE: 0.3825
Gen 1003/8000  train MSE: 0.6028, val MSE: 0.3825
Gen 1004/8000  train MSE: 0.6028, val MSE: 0.3825
Gen 1005/8000  train MSE: 0.6028, val MSE: 0.3825
Gen 1006/8000  train MSE: 0.6026, val MSE: 0.3826
Gen 1007/8000  train MSE: 0.6026, val MSE: 0.3826
Gen 1008/8000  train MSE: 0.6026, val MSE: 0.3826
Gen 1009/8000  train MSE: 0.6026, val MSE: 0.3826
Gen 1010/8000  train MSE: 0.6026, val MSE: 0.3826
Gen 1011/8000  train MSE: 0.6026, val MSE: 0.3826
Gen 1012/8000  train MSE: 0.6025, val MSE: 0.3840
Gen 1013/8000  train MSE: 0.6025, val MSE: 0.3840
Gen 1014/8000  train MSE: 0.6025, val MSE: 0.3840
Gen 1015/8000  train MSE: 0.6025, val MSE: 0.3840
Gen 1016/8000  train MSE: 0.6025, val MSE: 0.3840
Gen 1017/8000  train MSE: 0.6025, val MSE: 0.3840
Gen 1018/8000  train MSE: 0.6025, val MSE: 0.3840
Gen 1019/8000  train MSE: 0.6025, val MSE: 0.3840
Gen 1020/8000  train MSE: 0.6025, val MSE: 0.3840
Gen 1021/8000  train MSE: 0.6025, val MSE: 0.3830
Gen 1022/8000  train MSE: 0.6025, val MSE: 0.3830
Gen 1023/8000  train MSE: 0.6025, val MSE: 0.3830
Gen 1024/8000  train MSE: 0.6025, val MSE: 0.3830
Gen 1025/8000  train MSE: 0.6025, val MSE: 0.3830
Gen 1026/8000  train MSE: 0.6024, val MSE: 0.3809
Gen 1027/8000  train MSE: 0.6024, val MSE: 0.3809
Gen 1028/8000  train MSE: 0.6024, val MSE: 0.3809
Gen 1029/8000  train MSE: 0.6024, val MSE: 0.3809
Gen 1030/8000  train MSE: 0.6024, val MSE: 0.3809
Gen 1031/8000  train MSE: 0.6024, val MSE: 0.3809
Gen 1032/8000  train MSE: 0.6024, val MSE: 0.3809
Gen 1033/8000  train MSE: 0.6024, val MSE: 0.3809
Gen 1034/8000  train MSE: 0.6024, val MSE: 0.3809
Gen 1035/8000  train MSE: 0.6024, val MSE: 0.3809
Gen 1036/8000  train MSE: 0.6023, val MSE: 0.3804
Gen 1037/8000  train MSE: 0.6023, val MSE: 0.3804
Gen 1038/8000  train MSE: 0.6023, val MSE: 0.3804
Gen 1039/8000  train MSE: 0.6023, val MSE: 0.3834
Gen 1040/8000  train MSE: 0.6023, val MSE: 0.3834
Gen 1041/8000  train MSE: 0.6023, val MSE: 0.3834
Gen 1042/8000  train MSE: 0.6023, val MSE: 0.3834
Gen 1043/8000  train MSE: 0.6023, val MSE: 0.3834
Gen 1044/8000  train MSE: 0.6022, val MSE: 0.3813
Gen 1045/8000  train MSE: 0.6022, val MSE: 0.3804
Gen 1046/8000  train MSE: 0.6022, val MSE: 0.3806
Gen 1047/8000  train MSE: 0.6021, val MSE: 0.3802
Gen 1048/8000  train MSE: 0.6020, val MSE: 0.3801
Gen 1049/8000  train MSE: 0.6020, val MSE: 0.3801
Gen 1050/8000  train MSE: 0.6019, val MSE: 0.3793
Gen 1051/8000  train MSE: 0.6019, val MSE: 0.3793
Gen 1052/8000  train MSE: 0.6019, val MSE: 0.3793
Gen 1053/8000  train MSE: 0.6019, val MSE: 0.3793
Gen 1054/8000  train MSE: 0.6019, val MSE: 0.3793
Gen 1055/8000  train MSE: 0.6019, val MSE: 0.3793
Gen 1056/8000  train MSE: 0.6019, val MSE: 0.3793
Gen 1057/8000  train MSE: 0.6018, val MSE: 0.3794
Gen 1058/8000  train MSE: 0.6018, val MSE: 0.3794
Gen 1059/8000  train MSE: 0.6016, val MSE: 0.3783
Gen 1060/8000  train MSE: 0.6016, val MSE: 0.3783
Gen 1061/8000  train MSE: 0.6016, val MSE: 0.3783
Gen 1062/8000  train MSE: 0.6016, val MSE: 0.3783
Gen 1063/8000  train MSE: 0.6016, val MSE: 0.3783
Gen 1064/8000  train MSE: 0.6016, val MSE: 0.3783
Gen 1065/8000  train MSE: 0.6016, val MSE: 0.3783
Gen 1066/8000  train MSE: 0.6015, val MSE: 0.3779
Gen 1067/8000  train MSE: 0.6015, val MSE: 0.3779
Gen 1068/8000  train MSE: 0.6015, val MSE: 0.3779
Gen 1069/8000  train MSE: 0.6015, val MSE: 0.3779
Gen 1070/8000  train MSE: 0.6015, val MSE: 0.3779
Gen 1071/8000  train MSE: 0.6015, val MSE: 0.3779
Gen 1072/8000  train MSE: 0.6015, val MSE: 0.3779
Gen 1073/8000  train MSE: 0.6015, val MSE: 0.3766
Gen 1074/8000  train MSE: 0.6015, val MSE: 0.3798
Gen 1075/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1076/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1077/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1078/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1079/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1080/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1081/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1082/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1083/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1084/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1085/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1086/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1087/8000  train MSE: 0.6011, val MSE: 0.3781
Gen 1088/8000  train MSE: 0.6011, val MSE: 0.3766
Gen 1089/8000  train MSE: 0.6011, val MSE: 0.3766
Gen 1090/8000  train MSE: 0.6011, val MSE: 0.3782
Gen 1091/8000  train MSE: 0.6011, val MSE: 0.3782
Gen 1092/8000  train MSE: 0.6011, val MSE: 0.3782
Gen 1093/8000  train MSE: 0.6011, val MSE: 0.3782
Gen 1094/8000  train MSE: 0.6011, val MSE: 0.3782
Gen 1095/8000  train MSE: 0.6010, val MSE: 0.3775
Gen 1096/8000  train MSE: 0.6010, val MSE: 0.3775
Gen 1097/8000  train MSE: 0.6010, val MSE: 0.3775
Gen 1098/8000  train MSE: 0.6010, val MSE: 0.3775
Gen 1099/8000  train MSE: 0.6010, val MSE: 0.3775
Gen 1100/8000  train MSE: 0.6010, val MSE: 0.3775
Gen 1101/8000  train MSE: 0.6010, val MSE: 0.3775
Gen 1102/8000  train MSE: 0.6010, val MSE: 0.3775
Gen 1103/8000  train MSE: 0.6010, val MSE: 0.3763
Gen 1104/8000  train MSE: 0.6009, val MSE: 0.3786
Gen 1105/8000  train MSE: 0.6008, val MSE: 0.3753
Gen 1106/8000  train MSE: 0.6008, val MSE: 0.3753
Gen 1107/8000  train MSE: 0.6008, val MSE: 0.3753
Gen 1108/8000  train MSE: 0.6008, val MSE: 0.3753
Gen 1109/8000  train MSE: 0.6007, val MSE: 0.3761
Gen 1110/8000  train MSE: 0.6007, val MSE: 0.3761
Gen 1111/8000  train MSE: 0.6007, val MSE: 0.3765
Gen 1112/8000  train MSE: 0.6007, val MSE: 0.3765
Gen 1113/8000  train MSE: 0.6007, val MSE: 0.3765
Gen 1114/8000  train MSE: 0.6007, val MSE: 0.3771
Gen 1115/8000  train MSE: 0.6007, val MSE: 0.3771
Gen 1116/8000  train MSE: 0.6007, val MSE: 0.3771
Gen 1117/8000  train MSE: 0.6006, val MSE: 0.3762
Gen 1118/8000  train MSE: 0.6006, val MSE: 0.3762
Gen 1119/8000  train MSE: 0.6006, val MSE: 0.3757
Gen 1120/8000  train MSE: 0.6004, val MSE: 0.3757
Gen 1121/8000  train MSE: 0.6004, val MSE: 0.3757
Gen 1122/8000  train MSE: 0.6004, val MSE: 0.3757
Gen 1123/8000  train MSE: 0.6004, val MSE: 0.3757
Gen 1124/8000  train MSE: 0.6004, val MSE: 0.3757
Gen 1125/8000  train MSE: 0.6004, val MSE: 0.3741
Gen 1126/8000  train MSE: 0.6004, val MSE: 0.3741
Gen 1127/8000  train MSE: 0.6002, val MSE: 0.3746
Gen 1128/8000  train MSE: 0.6002, val MSE: 0.3745
Gen 1129/8000  train MSE: 0.6002, val MSE: 0.3745
Gen 1130/8000  train MSE: 0.6002, val MSE: 0.3745
Gen 1131/8000  train MSE: 0.6002, val MSE: 0.3745
Gen 1132/8000  train MSE: 0.6002, val MSE: 0.3735
Gen 1133/8000  train MSE: 0.6002, val MSE: 0.3735
Gen 1134/8000  train MSE: 0.6002, val MSE: 0.3735
Gen 1135/8000  train MSE: 0.6002, val MSE: 0.3735
Gen 1136/8000  train MSE: 0.6002, val MSE: 0.3735
Gen 1137/8000  train MSE: 0.6001, val MSE: 0.3728
Gen 1138/8000  train MSE: 0.6001, val MSE: 0.3728
Gen 1139/8000  train MSE: 0.6000, val MSE: 0.3738
Gen 1140/8000  train MSE: 0.6000, val MSE: 0.3738
Gen 1141/8000  train MSE: 0.6000, val MSE: 0.3738
Gen 1142/8000  train MSE: 0.6000, val MSE: 0.3738
Gen 1143/8000  train MSE: 0.6000, val MSE: 0.3738
Gen 1144/8000  train MSE: 0.5997, val MSE: 0.3747
Gen 1145/8000  train MSE: 0.5997, val MSE: 0.3747
Gen 1146/8000  train MSE: 0.5997, val MSE: 0.3747
Gen 1147/8000  train MSE: 0.5997, val MSE: 0.3747
Gen 1148/8000  train MSE: 0.5997, val MSE: 0.3747
Gen 1149/8000  train MSE: 0.5997, val MSE: 0.3747
Gen 1150/8000  train MSE: 0.5997, val MSE: 0.3747
Gen 1151/8000  train MSE: 0.5997, val MSE: 0.3747
Gen 1152/8000  train MSE: 0.5997, val MSE: 0.3747
Gen 1153/8000  train MSE: 0.5997, val MSE: 0.3747
Gen 1154/8000  train MSE: 0.5996, val MSE: 0.3735
Gen 1155/8000  train MSE: 0.5996, val MSE: 0.3735
Gen 1156/8000  train MSE: 0.5996, val MSE: 0.3735
Gen 1157/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1158/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1159/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1160/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1161/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1162/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1163/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1164/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1165/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1166/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1167/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1168/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1169/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1170/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1171/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1172/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1173/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1174/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1175/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1176/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1177/8000  train MSE: 0.5995, val MSE: 0.3728
Gen 1178/8000  train MSE: 0.5994, val MSE: 0.3741
Gen 1179/8000  train MSE: 0.5994, val MSE: 0.3741
Gen 1180/8000  train MSE: 0.5994, val MSE: 0.3741
Gen 1181/8000  train MSE: 0.5993, val MSE: 0.3733
Gen 1182/8000  train MSE: 0.5993, val MSE: 0.3733
Gen 1183/8000  train MSE: 0.5993, val MSE: 0.3733
Gen 1184/8000  train MSE: 0.5993, val MSE: 0.3733
Gen 1185/8000  train MSE: 0.5993, val MSE: 0.3733
Gen 1186/8000  train MSE: 0.5993, val MSE: 0.3734
Gen 1187/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1188/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1189/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1190/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1191/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1192/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1193/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1194/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1195/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1196/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1197/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1198/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1199/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1200/8000  train MSE: 0.5990, val MSE: 0.3746
Gen 1201/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1202/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1203/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1204/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1205/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1206/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1207/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1208/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1209/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1210/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1211/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1212/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1213/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1214/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1215/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1216/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1217/8000  train MSE: 0.5990, val MSE: 0.3736
Gen 1218/8000  train MSE: 0.5990, val MSE: 0.3740
Gen 1219/8000  train MSE: 0.5989, val MSE: 0.3729
Gen 1220/8000  train MSE: 0.5989, val MSE: 0.3729
Gen 1221/8000  train MSE: 0.5989, val MSE: 0.3729
Gen 1222/8000  train MSE: 0.5987, val MSE: 0.3739
Gen 1223/8000  train MSE: 0.5987, val MSE: 0.3739
Gen 1224/8000  train MSE: 0.5987, val MSE: 0.3739
Gen 1225/8000  train MSE: 0.5987, val MSE: 0.3739
Gen 1226/8000  train MSE: 0.5987, val MSE: 0.3739
Gen 1227/8000  train MSE: 0.5987, val MSE: 0.3739
Gen 1228/8000  train MSE: 0.5987, val MSE: 0.3735
Gen 1229/8000  train MSE: 0.5986, val MSE: 0.3736
Gen 1230/8000  train MSE: 0.5986, val MSE: 0.3736
Gen 1231/8000  train MSE: 0.5986, val MSE: 0.3734
Gen 1232/8000  train MSE: 0.5986, val MSE: 0.3734
Gen 1233/8000  train MSE: 0.5984, val MSE: 0.3731
Gen 1234/8000  train MSE: 0.5984, val MSE: 0.3731
Gen 1235/8000  train MSE: 0.5984, val MSE: 0.3731
Gen 1236/8000  train MSE: 0.5984, val MSE: 0.3731
Gen 1237/8000  train MSE: 0.5984, val MSE: 0.3731
Gen 1238/8000  train MSE: 0.5984, val MSE: 0.3731
Gen 1239/8000  train MSE: 0.5984, val MSE: 0.3730
Gen 1240/8000  train MSE: 0.5984, val MSE: 0.3730
Gen 1241/8000  train MSE: 0.5984, val MSE: 0.3730
Gen 1242/8000  train MSE: 0.5984, val MSE: 0.3730
Gen 1243/8000  train MSE: 0.5984, val MSE: 0.3730
Gen 1244/8000  train MSE: 0.5984, val MSE: 0.3730
Gen 1245/8000  train MSE: 0.5984, val MSE: 0.3730
Gen 1246/8000  train MSE: 0.5983, val MSE: 0.3724
Gen 1247/8000  train MSE: 0.5983, val MSE: 0.3724
Gen 1248/8000  train MSE: 0.5983, val MSE: 0.3724
Gen 1249/8000  train MSE: 0.5983, val MSE: 0.3724
Gen 1250/8000  train MSE: 0.5983, val MSE: 0.3724
Gen 1251/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1252/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1253/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1254/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1255/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1256/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1257/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1258/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1259/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1260/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1261/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1262/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1263/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1264/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1265/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1266/8000  train MSE: 0.5982, val MSE: 0.3722
Gen 1267/8000  train MSE: 0.5981, val MSE: 0.3725
Gen 1268/8000  train MSE: 0.5981, val MSE: 0.3725
Gen 1269/8000  train MSE: 0.5981, val MSE: 0.3725
Gen 1270/8000  train MSE: 0.5981, val MSE: 0.3725
Gen 1271/8000  train MSE: 0.5981, val MSE: 0.3725
Gen 1272/8000  train MSE: 0.5981, val MSE: 0.3725
Gen 1273/8000  train MSE: 0.5981, val MSE: 0.3725
Gen 1274/8000  train MSE: 0.5981, val MSE: 0.3725
Gen 1275/8000  train MSE: 0.5981, val MSE: 0.3725
Gen 1276/8000  train MSE: 0.5981, val MSE: 0.3730
Gen 1277/8000  train MSE: 0.5981, val MSE: 0.3730
Gen 1278/8000  train MSE: 0.5981, val MSE: 0.3730
Gen 1279/8000  train MSE: 0.5981, val MSE: 0.3730
Gen 1280/8000  train MSE: 0.5981, val MSE: 0.3724
Gen 1281/8000  train MSE: 0.5981, val MSE: 0.3720
Gen 1282/8000  train MSE: 0.5981, val MSE: 0.3720
Gen 1283/8000  train MSE: 0.5981, val MSE: 0.3720
Gen 1284/8000  train MSE: 0.5981, val MSE: 0.3720
Gen 1285/8000  train MSE: 0.5981, val MSE: 0.3720
Gen 1286/8000  train MSE: 0.5981, val MSE: 0.3720
Gen 1287/8000  train MSE: 0.5981, val MSE: 0.3720
Gen 1288/8000  train MSE: 0.5981, val MSE: 0.3720
Gen 1289/8000  train MSE: 0.5979, val MSE: 0.3716
Gen 1290/8000  train MSE: 0.5978, val MSE: 0.3725
Gen 1291/8000  train MSE: 0.5978, val MSE: 0.3725
Gen 1292/8000  train MSE: 0.5978, val MSE: 0.3725
Gen 1293/8000  train MSE: 0.5978, val MSE: 0.3725
Gen 1294/8000  train MSE: 0.5978, val MSE: 0.3725
Gen 1295/8000  train MSE: 0.5978, val MSE: 0.3725
Gen 1296/8000  train MSE: 0.5978, val MSE: 0.3725
Gen 1297/8000  train MSE: 0.5978, val MSE: 0.3725
Gen 1298/8000  train MSE: 0.5978, val MSE: 0.3725
Gen 1299/8000  train MSE: 0.5978, val MSE: 0.3713
Gen 1300/8000  train MSE: 0.5978, val MSE: 0.3713
Gen 1301/8000  train MSE: 0.5978, val MSE: 0.3713
Gen 1302/8000  train MSE: 0.5978, val MSE: 0.3713
Gen 1303/8000  train MSE: 0.5978, val MSE: 0.3713
Gen 1304/8000  train MSE: 0.5978, val MSE: 0.3727
Gen 1305/8000  train MSE: 0.5978, val MSE: 0.3727
Gen 1306/8000  train MSE: 0.5977, val MSE: 0.3723
Gen 1307/8000  train MSE: 0.5977, val MSE: 0.3723
Gen 1308/8000  train MSE: 0.5977, val MSE: 0.3723
Gen 1309/8000  train MSE: 0.5977, val MSE: 0.3723
Gen 1310/8000  train MSE: 0.5977, val MSE: 0.3723
Gen 1311/8000  train MSE: 0.5977, val MSE: 0.3718
Gen 1312/8000  train MSE: 0.5977, val MSE: 0.3718
Gen 1313/8000  train MSE: 0.5977, val MSE: 0.3718
Gen 1314/8000  train MSE: 0.5977, val MSE: 0.3718
Gen 1315/8000  train MSE: 0.5977, val MSE: 0.3718
Gen 1316/8000  train MSE: 0.5977, val MSE: 0.3718
Gen 1317/8000  train MSE: 0.5977, val MSE: 0.3718
Gen 1318/8000  train MSE: 0.5976, val MSE: 0.3736
Gen 1319/8000  train MSE: 0.5976, val MSE: 0.3736
Gen 1320/8000  train MSE: 0.5976, val MSE: 0.3736
Gen 1321/8000  train MSE: 0.5976, val MSE: 0.3736
Gen 1322/8000  train MSE: 0.5976, val MSE: 0.3736
Gen 1323/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1324/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1325/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1326/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1327/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1328/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1329/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1330/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1331/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1332/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1333/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1334/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1335/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1336/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1337/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1338/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1339/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1340/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1341/8000  train MSE: 0.5974, val MSE: 0.3715
Gen 1342/8000  train MSE: 0.5974, val MSE: 0.3728
Gen 1343/8000  train MSE: 0.5974, val MSE: 0.3728
Gen 1344/8000  train MSE: 0.5974, val MSE: 0.3728
Gen 1345/8000  train MSE: 0.5974, val MSE: 0.3728
Gen 1346/8000  train MSE: 0.5974, val MSE: 0.3728
Gen 1347/8000  train MSE: 0.5974, val MSE: 0.3728
Gen 1348/8000  train MSE: 0.5974, val MSE: 0.3728
Gen 1349/8000  train MSE: 0.5974, val MSE: 0.3728
Gen 1350/8000  train MSE: 0.5974, val MSE: 0.3728
Gen 1351/8000  train MSE: 0.5974, val MSE: 0.3728
Gen 1352/8000  train MSE: 0.5973, val MSE: 0.3715
Gen 1353/8000  train MSE: 0.5973, val MSE: 0.3715
Gen 1354/8000  train MSE: 0.5973, val MSE: 0.3715
Gen 1355/8000  train MSE: 0.5973, val MSE: 0.3715
Gen 1356/8000  train MSE: 0.5973, val MSE: 0.3715
Gen 1357/8000  train MSE: 0.5973, val MSE: 0.3715
Gen 1358/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1359/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1360/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1361/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1362/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1363/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1364/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1365/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1366/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1367/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1368/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1369/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1370/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1371/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1372/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1373/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1374/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1375/8000  train MSE: 0.5970, val MSE: 0.3729
Gen 1376/8000  train MSE: 0.5970, val MSE: 0.3724
Gen 1377/8000  train MSE: 0.5970, val MSE: 0.3724
Gen 1378/8000  train MSE: 0.5970, val MSE: 0.3724
Gen 1379/8000  train MSE: 0.5970, val MSE: 0.3713
Gen 1380/8000  train MSE: 0.5969, val MSE: 0.3725
Gen 1381/8000  train MSE: 0.5969, val MSE: 0.3725
Gen 1382/8000  train MSE: 0.5969, val MSE: 0.3728
Gen 1383/8000  train MSE: 0.5969, val MSE: 0.3728
Gen 1384/8000  train MSE: 0.5969, val MSE: 0.3728
Gen 1385/8000  train MSE: 0.5969, val MSE: 0.3728
Gen 1386/8000  train MSE: 0.5969, val MSE: 0.3728
Gen 1387/8000  train MSE: 0.5968, val MSE: 0.3722
Gen 1388/8000  train MSE: 0.5968, val MSE: 0.3712
Gen 1389/8000  train MSE: 0.5968, val MSE: 0.3712
Gen 1390/8000  train MSE: 0.5968, val MSE: 0.3712
Gen 1391/8000  train MSE: 0.5968, val MSE: 0.3717
Gen 1392/8000  train MSE: 0.5968, val MSE: 0.3715
Gen 1393/8000  train MSE: 0.5968, val MSE: 0.3715
Gen 1394/8000  train MSE: 0.5968, val MSE: 0.3715
Gen 1395/8000  train MSE: 0.5967, val MSE: 0.3731
Gen 1396/8000  train MSE: 0.5967, val MSE: 0.3723
Gen 1397/8000  train MSE: 0.5967, val MSE: 0.3723
Gen 1398/8000  train MSE: 0.5967, val MSE: 0.3723
Gen 1399/8000  train MSE: 0.5967, val MSE: 0.3723
Gen 1400/8000  train MSE: 0.5967, val MSE: 0.3723
Gen 1401/8000  train MSE: 0.5967, val MSE: 0.3723
Gen 1402/8000  train MSE: 0.5967, val MSE: 0.3723
Gen 1403/8000  train MSE: 0.5967, val MSE: 0.3720
Gen 1404/8000  train MSE: 0.5967, val MSE: 0.3725
Gen 1405/8000  train MSE: 0.5966, val MSE: 0.3720
Gen 1406/8000  train MSE: 0.5966, val MSE: 0.3718
Gen 1407/8000  train MSE: 0.5966, val MSE: 0.3713
Gen 1408/8000  train MSE: 0.5966, val MSE: 0.3713
Gen 1409/8000  train MSE: 0.5965, val MSE: 0.3728
Gen 1410/8000  train MSE: 0.5965, val MSE: 0.3728
Gen 1411/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1412/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1413/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1414/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1415/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1416/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1417/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1418/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1419/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1420/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1421/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1422/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1423/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1424/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1425/8000  train MSE: 0.5965, val MSE: 0.3712
Gen 1426/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1427/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1428/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1429/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1430/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1431/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1432/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1433/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1434/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1435/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1436/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1437/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1438/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1439/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1440/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1441/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1442/8000  train MSE: 0.5964, val MSE: 0.3712
Gen 1443/8000  train MSE: 0.5964, val MSE: 0.3705
Gen 1444/8000  train MSE: 0.5964, val MSE: 0.3705
Gen 1445/8000  train MSE: 0.5964, val MSE: 0.3705
Gen 1446/8000  train MSE: 0.5964, val MSE: 0.3705
Gen 1447/8000  train MSE: 0.5964, val MSE: 0.3705
Gen 1448/8000  train MSE: 0.5964, val MSE: 0.3705
Gen 1449/8000  train MSE: 0.5964, val MSE: 0.3705
Gen 1450/8000  train MSE: 0.5964, val MSE: 0.3705
Gen 1451/8000  train MSE: 0.5963, val MSE: 0.3710
Gen 1452/8000  train MSE: 0.5962, val MSE: 0.3716
Gen 1453/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1454/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1455/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1456/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1457/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1458/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1459/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1460/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1461/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1462/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1463/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1464/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1465/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1466/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1467/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1468/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1469/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1470/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1471/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1472/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1473/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1474/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1475/8000  train MSE: 0.5961, val MSE: 0.3712
Gen 1476/8000  train MSE: 0.5960, val MSE: 0.3713
Gen 1477/8000  train MSE: 0.5960, val MSE: 0.3713
Gen 1478/8000  train MSE: 0.5960, val MSE: 0.3713
Gen 1479/8000  train MSE: 0.5960, val MSE: 0.3713
Gen 1480/8000  train MSE: 0.5960, val MSE: 0.3713
Gen 1481/8000  train MSE: 0.5960, val MSE: 0.3713
Gen 1482/8000  train MSE: 0.5960, val MSE: 0.3713
Gen 1483/8000  train MSE: 0.5960, val MSE: 0.3713
Gen 1484/8000  train MSE: 0.5960, val MSE: 0.3713
Gen 1485/8000  train MSE: 0.5960, val MSE: 0.3713
Gen 1486/8000  train MSE: 0.5960, val MSE: 0.3713
Gen 1487/8000  train MSE: 0.5959, val MSE: 0.3712
Gen 1488/8000  train MSE: 0.5959, val MSE: 0.3712
Gen 1489/8000  train MSE: 0.5959, val MSE: 0.3712
Gen 1490/8000  train MSE: 0.5959, val MSE: 0.3712
Gen 1491/8000  train MSE: 0.5959, val MSE: 0.3712
Gen 1492/8000  train MSE: 0.5959, val MSE: 0.3712
Gen 1493/8000  train MSE: 0.5959, val MSE: 0.3712
Gen 1494/8000  train MSE: 0.5959, val MSE: 0.3712
Gen 1495/8000  train MSE: 0.5959, val MSE: 0.3712
Gen 1496/8000  train MSE: 0.5959, val MSE: 0.3712
Gen 1497/8000  train MSE: 0.5959, val MSE: 0.3712
Gen 1498/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1499/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1500/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1501/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1502/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1503/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1504/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1505/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1506/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1507/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1508/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1509/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1510/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1511/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1512/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1513/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1514/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1515/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1516/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1517/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1518/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1519/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1520/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1521/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1522/8000  train MSE: 0.5959, val MSE: 0.3697
Gen 1523/8000  train MSE: 0.5958, val MSE: 0.3718
Gen 1524/8000  train MSE: 0.5958, val MSE: 0.3718
Gen 1525/8000  train MSE: 0.5958, val MSE: 0.3718
Gen 1526/8000  train MSE: 0.5958, val MSE: 0.3718
Gen 1527/8000  train MSE: 0.5958, val MSE: 0.3718
Gen 1528/8000  train MSE: 0.5958, val MSE: 0.3698
Gen 1529/8000  train MSE: 0.5958, val MSE: 0.3698
Gen 1530/8000  train MSE: 0.5958, val MSE: 0.3698
Gen 1531/8000  train MSE: 0.5958, val MSE: 0.3698
Gen 1532/8000  train MSE: 0.5958, val MSE: 0.3698
Gen 1533/8000  train MSE: 0.5958, val MSE: 0.3698
Gen 1534/8000  train MSE: 0.5958, val MSE: 0.3698
Gen 1535/8000  train MSE: 0.5958, val MSE: 0.3698
Gen 1536/8000  train MSE: 0.5958, val MSE: 0.3709
Gen 1537/8000  train MSE: 0.5958, val MSE: 0.3709
Gen 1538/8000  train MSE: 0.5958, val MSE: 0.3709
Gen 1539/8000  train MSE: 0.5958, val MSE: 0.3709
Gen 1540/8000  train MSE: 0.5958, val MSE: 0.3709
Gen 1541/8000  train MSE: 0.5958, val MSE: 0.3709
Gen 1542/8000  train MSE: 0.5958, val MSE: 0.3709
Gen 1543/8000  train MSE: 0.5958, val MSE: 0.3709
Gen 1544/8000  train MSE: 0.5957, val MSE: 0.3704
Gen 1545/8000  train MSE: 0.5957, val MSE: 0.3704
Gen 1546/8000  train MSE: 0.5957, val MSE: 0.3704
Gen 1547/8000  train MSE: 0.5957, val MSE: 0.3704
Gen 1548/8000  train MSE: 0.5957, val MSE: 0.3704
Gen 1549/8000  train MSE: 0.5957, val MSE: 0.3704
Gen 1550/8000  train MSE: 0.5957, val MSE: 0.3704
Gen 1551/8000  train MSE: 0.5956, val MSE: 0.3714
Gen 1552/8000  train MSE: 0.5956, val MSE: 0.3714
Gen 1553/8000  train MSE: 0.5956, val MSE: 0.3714
Gen 1554/8000  train MSE: 0.5956, val MSE: 0.3714
Gen 1555/8000  train MSE: 0.5956, val MSE: 0.3714
Gen 1556/8000  train MSE: 0.5956, val MSE: 0.3714
Gen 1557/8000  train MSE: 0.5956, val MSE: 0.3714
Gen 1558/8000  train MSE: 0.5956, val MSE: 0.3714
Gen 1559/8000  train MSE: 0.5956, val MSE: 0.3714
Gen 1560/8000  train MSE: 0.5956, val MSE: 0.3714
Gen 1561/8000  train MSE: 0.5956, val MSE: 0.3714
Gen 1562/8000  train MSE: 0.5956, val MSE: 0.3710
Gen 1563/8000  train MSE: 0.5956, val MSE: 0.3710
Gen 1564/8000  train MSE: 0.5955, val MSE: 0.3705
Gen 1565/8000  train MSE: 0.5955, val MSE: 0.3688
Gen 1566/8000  train MSE: 0.5955, val MSE: 0.3688
Gen 1567/8000  train MSE: 0.5955, val MSE: 0.3688
Gen 1568/8000  train MSE: 0.5955, val MSE: 0.3688
Gen 1569/8000  train MSE: 0.5955, val MSE: 0.3688
Gen 1570/8000  train MSE: 0.5955, val MSE: 0.3688
Gen 1571/8000  train MSE: 0.5955, val MSE: 0.3688
Gen 1572/8000  train MSE: 0.5955, val MSE: 0.3688
Gen 1573/8000  train MSE: 0.5955, val MSE: 0.3688
Gen 1574/8000  train MSE: 0.5955, val MSE: 0.3688
Gen 1575/8000  train MSE: 0.5954, val MSE: 0.3705
Gen 1576/8000  train MSE: 0.5954, val MSE: 0.3705
Gen 1577/8000  train MSE: 0.5954, val MSE: 0.3707
Gen 1578/8000  train MSE: 0.5954, val MSE: 0.3707
Gen 1579/8000  train MSE: 0.5954, val MSE: 0.3707
Gen 1580/8000  train MSE: 0.5954, val MSE: 0.3707
Gen 1581/8000  train MSE: 0.5954, val MSE: 0.3707
Gen 1582/8000  train MSE: 0.5954, val MSE: 0.3701
Gen 1583/8000  train MSE: 0.5954, val MSE: 0.3701
Gen 1584/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1585/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1586/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1587/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1588/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1589/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1590/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1591/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1592/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1593/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1594/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1595/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1596/8000  train MSE: 0.5952, val MSE: 0.3703
Gen 1597/8000  train MSE: 0.5951, val MSE: 0.3704
Gen 1598/8000  train MSE: 0.5951, val MSE: 0.3704
Gen 1599/8000  train MSE: 0.5951, val MSE: 0.3704
Gen 1600/8000  train MSE: 0.5951, val MSE: 0.3704
Gen 1601/8000  train MSE: 0.5951, val MSE: 0.3704
Gen 1602/8000  train MSE: 0.5951, val MSE: 0.3704
Gen 1603/8000  train MSE: 0.5951, val MSE: 0.3704
Gen 1604/8000  train MSE: 0.5951, val MSE: 0.3704
Gen 1605/8000  train MSE: 0.5951, val MSE: 0.3705
Gen 1606/8000  train MSE: 0.5951, val MSE: 0.3705
Gen 1607/8000  train MSE: 0.5951, val MSE: 0.3705
Gen 1608/8000  train MSE: 0.5951, val MSE: 0.3705
Gen 1609/8000  train MSE: 0.5951, val MSE: 0.3705
Gen 1610/8000  train MSE: 0.5951, val MSE: 0.3705
Gen 1611/8000  train MSE: 0.5951, val MSE: 0.3705
Gen 1612/8000  train MSE: 0.5951, val MSE: 0.3705
Gen 1613/8000  train MSE: 0.5951, val MSE: 0.3703
Gen 1614/8000  train MSE: 0.5951, val MSE: 0.3703
Gen 1615/8000  train MSE: 0.5951, val MSE: 0.3703
Gen 1616/8000  train MSE: 0.5951, val MSE: 0.3703
Gen 1617/8000  train MSE: 0.5951, val MSE: 0.3703
Gen 1618/8000  train MSE: 0.5951, val MSE: 0.3703
Gen 1619/8000  train MSE: 0.5951, val MSE: 0.3703
Gen 1620/8000  train MSE: 0.5951, val MSE: 0.3703
Gen 1621/8000  train MSE: 0.5950, val MSE: 0.3702
Gen 1622/8000  train MSE: 0.5950, val MSE: 0.3702
Gen 1623/8000  train MSE: 0.5950, val MSE: 0.3702
Gen 1624/8000  train MSE: 0.5950, val MSE: 0.3705
Gen 1625/8000  train MSE: 0.5950, val MSE: 0.3705
Gen 1626/8000  train MSE: 0.5950, val MSE: 0.3705
Gen 1627/8000  train MSE: 0.5950, val MSE: 0.3705
Gen 1628/8000  train MSE: 0.5950, val MSE: 0.3703
Gen 1629/8000  train MSE: 0.5950, val MSE: 0.3703
Gen 1630/8000  train MSE: 0.5950, val MSE: 0.3703
Gen 1631/8000  train MSE: 0.5950, val MSE: 0.3703
Gen 1632/8000  train MSE: 0.5949, val MSE: 0.3700
Gen 1633/8000  train MSE: 0.5949, val MSE: 0.3700
Gen 1634/8000  train MSE: 0.5949, val MSE: 0.3703
Gen 1635/8000  train MSE: 0.5949, val MSE: 0.3703
Gen 1636/8000  train MSE: 0.5949, val MSE: 0.3703
Gen 1637/8000  train MSE: 0.5949, val MSE: 0.3703
Gen 1638/8000  train MSE: 0.5949, val MSE: 0.3703
Gen 1639/8000  train MSE: 0.5949, val MSE: 0.3703
Gen 1640/8000  train MSE: 0.5949, val MSE: 0.3703
Gen 1641/8000  train MSE: 0.5949, val MSE: 0.3703
Gen 1642/8000  train MSE: 0.5949, val MSE: 0.3703
Gen 1643/8000  train MSE: 0.5949, val MSE: 0.3703
Gen 1644/8000  train MSE: 0.5949, val MSE: 0.3703
Gen 1645/8000  train MSE: 0.5949, val MSE: 0.3700
Gen 1646/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1647/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1648/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1649/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1650/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1651/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1652/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1653/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1654/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1655/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1656/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1657/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1658/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1659/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1660/8000  train MSE: 0.5948, val MSE: 0.3686
Gen 1661/8000  train MSE: 0.5948, val MSE: 0.3698
Gen 1662/8000  train MSE: 0.5948, val MSE: 0.3698
Gen 1663/8000  train MSE: 0.5948, val MSE: 0.3698
Gen 1664/8000  train MSE: 0.5948, val MSE: 0.3698
Gen 1665/8000  train MSE: 0.5948, val MSE: 0.3698
Gen 1666/8000  train MSE: 0.5948, val MSE: 0.3698
Gen 1667/8000  train MSE: 0.5948, val MSE: 0.3698
Gen 1668/8000  train MSE: 0.5948, val MSE: 0.3698
Gen 1669/8000  train MSE: 0.5947, val MSE: 0.3697
Gen 1670/8000  train MSE: 0.5947, val MSE: 0.3697
Gen 1671/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1672/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1673/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1674/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1675/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1676/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1677/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1678/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1679/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1680/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1681/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1682/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1683/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1684/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1685/8000  train MSE: 0.5946, val MSE: 0.3696
Gen 1686/8000  train MSE: 0.5946, val MSE: 0.3704
Gen 1687/8000  train MSE: 0.5946, val MSE: 0.3704
Gen 1688/8000  train MSE: 0.5946, val MSE: 0.3704
Gen 1689/8000  train MSE: 0.5946, val MSE: 0.3704
Gen 1690/8000  train MSE: 0.5946, val MSE: 0.3704
Gen 1691/8000  train MSE: 0.5946, val MSE: 0.3697
Gen 1692/8000  train MSE: 0.5946, val MSE: 0.3697
Gen 1693/8000  train MSE: 0.5946, val MSE: 0.3701
Gen 1694/8000  train MSE: 0.5946, val MSE: 0.3701
Gen 1695/8000  train MSE: 0.5945, val MSE: 0.3701
Gen 1696/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1697/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1698/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1699/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1700/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1701/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1702/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1703/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1704/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1705/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1706/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1707/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1708/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1709/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1710/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1711/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1712/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1713/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1714/8000  train MSE: 0.5944, val MSE: 0.3696
Gen 1715/8000  train MSE: 0.5944, val MSE: 0.3690
Gen 1716/8000  train MSE: 0.5944, val MSE: 0.3685
Gen 1717/8000  train MSE: 0.5944, val MSE: 0.3685
Gen 1718/8000  train MSE: 0.5944, val MSE: 0.3685
Gen 1719/8000  train MSE: 0.5944, val MSE: 0.3685
Gen 1720/8000  train MSE: 0.5944, val MSE: 0.3685
Gen 1721/8000  train MSE: 0.5944, val MSE: 0.3685
Gen 1722/8000  train MSE: 0.5944, val MSE: 0.3685
Gen 1723/8000  train MSE: 0.5944, val MSE: 0.3693
Gen 1724/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1725/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1726/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1727/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1728/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1729/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1730/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1731/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1732/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1733/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1734/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1735/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1736/8000  train MSE: 0.5943, val MSE: 0.3681
Gen 1737/8000  train MSE: 0.5943, val MSE: 0.3679
Gen 1738/8000  train MSE: 0.5943, val MSE: 0.3679
Gen 1739/8000  train MSE: 0.5943, val MSE: 0.3679
Gen 1740/8000  train MSE: 0.5943, val MSE: 0.3687
Gen 1741/8000  train MSE: 0.5942, val MSE: 0.3691
Gen 1742/8000  train MSE: 0.5942, val MSE: 0.3691
Gen 1743/8000  train MSE: 0.5942, val MSE: 0.3691
Gen 1744/8000  train MSE: 0.5942, val MSE: 0.3691
Gen 1745/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1746/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1747/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1748/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1749/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1750/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1751/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1752/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1753/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1754/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1755/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1756/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1757/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1758/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1759/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1760/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1761/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1762/8000  train MSE: 0.5941, val MSE: 0.3680
Gen 1763/8000  train MSE: 0.5941, val MSE: 0.3684
Gen 1764/8000  train MSE: 0.5941, val MSE: 0.3684
Gen 1765/8000  train MSE: 0.5941, val MSE: 0.3684
Gen 1766/8000  train MSE: 0.5941, val MSE: 0.3684
Gen 1767/8000  train MSE: 0.5941, val MSE: 0.3684
Gen 1768/8000  train MSE: 0.5941, val MSE: 0.3684
Gen 1769/8000  train MSE: 0.5941, val MSE: 0.3683
Gen 1770/8000  train MSE: 0.5941, val MSE: 0.3681
Gen 1771/8000  train MSE: 0.5941, val MSE: 0.3688
Gen 1772/8000  train MSE: 0.5941, val MSE: 0.3688
Gen 1773/8000  train MSE: 0.5941, val MSE: 0.3688
Gen 1774/8000  train MSE: 0.5941, val MSE: 0.3684
Gen 1775/8000  train MSE: 0.5941, val MSE: 0.3671
Gen 1776/8000  train MSE: 0.5941, val MSE: 0.3671
Gen 1777/8000  train MSE: 0.5941, val MSE: 0.3671
Gen 1778/8000  train MSE: 0.5940, val MSE: 0.3683
Gen 1779/8000  train MSE: 0.5940, val MSE: 0.3683
Gen 1780/8000  train MSE: 0.5940, val MSE: 0.3683
Gen 1781/8000  train MSE: 0.5940, val MSE: 0.3684
Gen 1782/8000  train MSE: 0.5940, val MSE: 0.3684
Gen 1783/8000  train MSE: 0.5940, val MSE: 0.3679
Gen 1784/8000  train MSE: 0.5940, val MSE: 0.3679
Gen 1785/8000  train MSE: 0.5940, val MSE: 0.3679
Gen 1786/8000  train MSE: 0.5940, val MSE: 0.3679
Gen 1787/8000  train MSE: 0.5940, val MSE: 0.3679
Gen 1788/8000  train MSE: 0.5939, val MSE: 0.3675
Gen 1789/8000  train MSE: 0.5939, val MSE: 0.3671
Gen 1790/8000  train MSE: 0.5939, val MSE: 0.3671
Gen 1791/8000  train MSE: 0.5939, val MSE: 0.3671
Gen 1792/8000  train MSE: 0.5939, val MSE: 0.3671
Gen 1793/8000  train MSE: 0.5939, val MSE: 0.3671
Gen 1794/8000  train MSE: 0.5939, val MSE: 0.3671
Gen 1795/8000  train MSE: 0.5939, val MSE: 0.3671
Gen 1796/8000  train MSE: 0.5939, val MSE: 0.3671
Gen 1797/8000  train MSE: 0.5939, val MSE: 0.3671
Gen 1798/8000  train MSE: 0.5939, val MSE: 0.3671
Gen 1799/8000  train MSE: 0.5938, val MSE: 0.3676
Gen 1800/8000  train MSE: 0.5938, val MSE: 0.3676
Gen 1801/8000  train MSE: 0.5938, val MSE: 0.3676
Gen 1802/8000  train MSE: 0.5938, val MSE: 0.3676
Gen 1803/8000  train MSE: 0.5938, val MSE: 0.3676
Gen 1804/8000  train MSE: 0.5938, val MSE: 0.3676
Gen 1805/8000  train MSE: 0.5938, val MSE: 0.3675
Gen 1806/8000  train MSE: 0.5938, val MSE: 0.3675
Gen 1807/8000  train MSE: 0.5938, val MSE: 0.3675
Gen 1808/8000  train MSE: 0.5938, val MSE: 0.3675
Gen 1809/8000  train MSE: 0.5936, val MSE: 0.3672
Gen 1810/8000  train MSE: 0.5936, val MSE: 0.3672
Gen 1811/8000  train MSE: 0.5936, val MSE: 0.3672
Gen 1812/8000  train MSE: 0.5936, val MSE: 0.3672
Gen 1813/8000  train MSE: 0.5936, val MSE: 0.3672
Gen 1814/8000  train MSE: 0.5936, val MSE: 0.3672
Gen 1815/8000  train MSE: 0.5936, val MSE: 0.3672
Gen 1816/8000  train MSE: 0.5936, val MSE: 0.3672
Gen 1817/8000  train MSE: 0.5936, val MSE: 0.3672
Gen 1818/8000  train MSE: 0.5936, val MSE: 0.3672
Gen 1819/8000  train MSE: 0.5936, val MSE: 0.3672
Gen 1820/8000  train MSE: 0.5936, val MSE: 0.3675
Gen 1821/8000  train MSE: 0.5936, val MSE: 0.3675
Gen 1822/8000  train MSE: 0.5936, val MSE: 0.3675
Gen 1823/8000  train MSE: 0.5936, val MSE: 0.3675
Gen 1824/8000  train MSE: 0.5936, val MSE: 0.3675
Gen 1825/8000  train MSE: 0.5936, val MSE: 0.3675
Gen 1826/8000  train MSE: 0.5936, val MSE: 0.3675
Gen 1827/8000  train MSE: 0.5936, val MSE: 0.3654
Gen 1828/8000  train MSE: 0.5936, val MSE: 0.3654
Gen 1829/8000  train MSE: 0.5936, val MSE: 0.3654
Gen 1830/8000  train MSE: 0.5936, val MSE: 0.3654
Gen 1831/8000  train MSE: 0.5936, val MSE: 0.3654
Gen 1832/8000  train MSE: 0.5936, val MSE: 0.3654
Gen 1833/8000  train MSE: 0.5935, val MSE: 0.3665
Gen 1834/8000  train MSE: 0.5935, val MSE: 0.3665
Gen 1835/8000  train MSE: 0.5935, val MSE: 0.3665
Gen 1836/8000  train MSE: 0.5935, val MSE: 0.3665
Gen 1837/8000  train MSE: 0.5935, val MSE: 0.3665
Gen 1838/8000  train MSE: 0.5935, val MSE: 0.3665
Gen 1839/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1840/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1841/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1842/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1843/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1844/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1845/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1846/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1847/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1848/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1849/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1850/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1851/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1852/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1853/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1854/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1855/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1856/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1857/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1858/8000  train MSE: 0.5934, val MSE: 0.3651
Gen 1859/8000  train MSE: 0.5934, val MSE: 0.3661
Gen 1860/8000  train MSE: 0.5934, val MSE: 0.3661
Gen 1861/8000  train MSE: 0.5934, val MSE: 0.3661
Gen 1862/8000  train MSE: 0.5934, val MSE: 0.3661
Gen 1863/8000  train MSE: 0.5934, val MSE: 0.3661
Gen 1864/8000  train MSE: 0.5934, val MSE: 0.3655
Gen 1865/8000  train MSE: 0.5934, val MSE: 0.3655
Gen 1866/8000  train MSE: 0.5934, val MSE: 0.3655
Gen 1867/8000  train MSE: 0.5934, val MSE: 0.3666
Gen 1868/8000  train MSE: 0.5934, val MSE: 0.3666
Gen 1869/8000  train MSE: 0.5934, val MSE: 0.3666
Gen 1870/8000  train MSE: 0.5934, val MSE: 0.3666
Gen 1871/8000  train MSE: 0.5933, val MSE: 0.3662
Gen 1872/8000  train MSE: 0.5933, val MSE: 0.3651
Gen 1873/8000  train MSE: 0.5933, val MSE: 0.3661
Gen 1874/8000  train MSE: 0.5933, val MSE: 0.3661
Gen 1875/8000  train MSE: 0.5932, val MSE: 0.3652
Gen 1876/8000  train MSE: 0.5932, val MSE: 0.3652
Gen 1877/8000  train MSE: 0.5932, val MSE: 0.3652
Gen 1878/8000  train MSE: 0.5932, val MSE: 0.3652
Gen 1879/8000  train MSE: 0.5932, val MSE: 0.3654
Gen 1880/8000  train MSE: 0.5932, val MSE: 0.3646
Gen 1881/8000  train MSE: 0.5932, val MSE: 0.3646
Gen 1882/8000  train MSE: 0.5932, val MSE: 0.3646
Gen 1883/8000  train MSE: 0.5932, val MSE: 0.3646
Gen 1884/8000  train MSE: 0.5932, val MSE: 0.3646
Gen 1885/8000  train MSE: 0.5932, val MSE: 0.3652
Gen 1886/8000  train MSE: 0.5932, val MSE: 0.3652
Gen 1887/8000  train MSE: 0.5930, val MSE: 0.3655
Gen 1888/8000  train MSE: 0.5930, val MSE: 0.3655
Gen 1889/8000  train MSE: 0.5930, val MSE: 0.3655
Gen 1890/8000  train MSE: 0.5930, val MSE: 0.3655
Gen 1891/8000  train MSE: 0.5930, val MSE: 0.3655
Gen 1892/8000  train MSE: 0.5930, val MSE: 0.3655
Gen 1893/8000  train MSE: 0.5930, val MSE: 0.3655
Gen 1894/8000  train MSE: 0.5929, val MSE: 0.3638
Gen 1895/8000  train MSE: 0.5929, val MSE: 0.3638
Gen 1896/8000  train MSE: 0.5929, val MSE: 0.3638
Gen 1897/8000  train MSE: 0.5929, val MSE: 0.3638
Gen 1898/8000  train MSE: 0.5929, val MSE: 0.3638
Gen 1899/8000  train MSE: 0.5929, val MSE: 0.3638
Gen 1900/8000  train MSE: 0.5929, val MSE: 0.3638
Gen 1901/8000  train MSE: 0.5929, val MSE: 0.3638
Gen 1902/8000  train MSE: 0.5928, val MSE: 0.3659
Gen 1903/8000  train MSE: 0.5928, val MSE: 0.3659
Gen 1904/8000  train MSE: 0.5928, val MSE: 0.3659
Gen 1905/8000  train MSE: 0.5928, val MSE: 0.3659
Gen 1906/8000  train MSE: 0.5928, val MSE: 0.3659
Gen 1907/8000  train MSE: 0.5928, val MSE: 0.3659
Gen 1908/8000  train MSE: 0.5928, val MSE: 0.3659
Gen 1909/8000  train MSE: 0.5928, val MSE: 0.3659
Gen 1910/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1911/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1912/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1913/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1914/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1915/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1916/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1917/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1918/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1919/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1920/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1921/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1922/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1923/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1924/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1925/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1926/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1927/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1928/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1929/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1930/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1931/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1932/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1933/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1934/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1935/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1936/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1937/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1938/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1939/8000  train MSE: 0.5927, val MSE: 0.3642
Gen 1940/8000  train MSE: 0.5927, val MSE: 0.3643
Gen 1941/8000  train MSE: 0.5927, val MSE: 0.3643
Gen 1942/8000  train MSE: 0.5927, val MSE: 0.3643
Gen 1943/8000  train MSE: 0.5927, val MSE: 0.3643
Gen 1944/8000  train MSE: 0.5927, val MSE: 0.3643
Gen 1945/8000  train MSE: 0.5927, val MSE: 0.3643
Gen 1946/8000  train MSE: 0.5926, val MSE: 0.3642
Gen 1947/8000  train MSE: 0.5926, val MSE: 0.3642
Gen 1948/8000  train MSE: 0.5926, val MSE: 0.3642
Gen 1949/8000  train MSE: 0.5926, val MSE: 0.3649
Gen 1950/8000  train MSE: 0.5926, val MSE: 0.3649
Gen 1951/8000  train MSE: 0.5926, val MSE: 0.3649
Gen 1952/8000  train MSE: 0.5925, val MSE: 0.3642
Gen 1953/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1954/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1955/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1956/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1957/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1958/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1959/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1960/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1961/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1962/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1963/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1964/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1965/8000  train MSE: 0.5925, val MSE: 0.3643
Gen 1966/8000  train MSE: 0.5925, val MSE: 0.3644
Gen 1967/8000  train MSE: 0.5925, val MSE: 0.3644
Gen 1968/8000  train MSE: 0.5924, val MSE: 0.3643
Gen 1969/8000  train MSE: 0.5924, val MSE: 0.3643
Gen 1970/8000  train MSE: 0.5924, val MSE: 0.3643
Gen 1971/8000  train MSE: 0.5923, val MSE: 0.3637
Gen 1972/8000  train MSE: 0.5923, val MSE: 0.3637
Gen 1973/8000  train MSE: 0.5923, val MSE: 0.3637
Gen 1974/8000  train MSE: 0.5923, val MSE: 0.3642
Gen 1975/8000  train MSE: 0.5923, val MSE: 0.3642
Gen 1976/8000  train MSE: 0.5923, val MSE: 0.3645
Gen 1977/8000  train MSE: 0.5923, val MSE: 0.3645
Gen 1978/8000  train MSE: 0.5921, val MSE: 0.3633
Gen 1979/8000  train MSE: 0.5921, val MSE: 0.3633
Gen 1980/8000  train MSE: 0.5921, val MSE: 0.3633
Gen 1981/8000  train MSE: 0.5921, val MSE: 0.3633
Gen 1982/8000  train MSE: 0.5921, val MSE: 0.3633
Gen 1983/8000  train MSE: 0.5921, val MSE: 0.3633
Gen 1984/8000  train MSE: 0.5921, val MSE: 0.3633
Gen 1985/8000  train MSE: 0.5921, val MSE: 0.3633
Gen 1986/8000  train MSE: 0.5921, val MSE: 0.3633
Gen 1987/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1988/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1989/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1990/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1991/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1992/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1993/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1994/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1995/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1996/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1997/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1998/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 1999/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 2000/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 2001/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 2002/8000  train MSE: 0.5920, val MSE: 0.3637
Gen 2003/8000  train MSE: 0.5919, val MSE: 0.3634
Gen 2004/8000  train MSE: 0.5919, val MSE: 0.3634
Gen 2005/8000  train MSE: 0.5919, val MSE: 0.3634
Gen 2006/8000  train MSE: 0.5919, val MSE: 0.3634
Gen 2007/8000  train MSE: 0.5919, val MSE: 0.3634
Gen 2008/8000  train MSE: 0.5919, val MSE: 0.3634
Gen 2009/8000  train MSE: 0.5919, val MSE: 0.3642
Gen 2010/8000  train MSE: 0.5919, val MSE: 0.3642
Gen 2011/8000  train MSE: 0.5919, val MSE: 0.3642
Gen 2012/8000  train MSE: 0.5919, val MSE: 0.3640
Gen 2013/8000  train MSE: 0.5919, val MSE: 0.3631
Gen 2014/8000  train MSE: 0.5919, val MSE: 0.3631
Gen 2015/8000  train MSE: 0.5919, val MSE: 0.3631
Gen 2016/8000  train MSE: 0.5919, val MSE: 0.3631
Gen 2017/8000  train MSE: 0.5918, val MSE: 0.3633
Gen 2018/8000  train MSE: 0.5918, val MSE: 0.3633
Gen 2019/8000  train MSE: 0.5918, val MSE: 0.3633
Gen 2020/8000  train MSE: 0.5918, val MSE: 0.3633
Gen 2021/8000  train MSE: 0.5918, val MSE: 0.3635
Gen 2022/8000  train MSE: 0.5917, val MSE: 0.3625
Gen 2023/8000  train MSE: 0.5917, val MSE: 0.3625
Gen 2024/8000  train MSE: 0.5917, val MSE: 0.3625
Gen 2025/8000  train MSE: 0.5917, val MSE: 0.3634
Gen 2026/8000  train MSE: 0.5917, val MSE: 0.3634
Gen 2027/8000  train MSE: 0.5917, val MSE: 0.3634
Gen 2028/8000  train MSE: 0.5917, val MSE: 0.3634
Gen 2029/8000  train MSE: 0.5917, val MSE: 0.3634
Gen 2030/8000  train MSE: 0.5917, val MSE: 0.3634
Gen 2031/8000  train MSE: 0.5917, val MSE: 0.3634
Gen 2032/8000  train MSE: 0.5917, val MSE: 0.3634
Gen 2033/8000  train MSE: 0.5915, val MSE: 0.3621
Gen 2034/8000  train MSE: 0.5915, val MSE: 0.3621
Gen 2035/8000  train MSE: 0.5915, val MSE: 0.3621
Gen 2036/8000  train MSE: 0.5915, val MSE: 0.3621
Gen 2037/8000  train MSE: 0.5914, val MSE: 0.3630
Gen 2038/8000  train MSE: 0.5914, val MSE: 0.3630
Gen 2039/8000  train MSE: 0.5914, val MSE: 0.3630
Gen 2040/8000  train MSE: 0.5914, val MSE: 0.3630
Gen 2041/8000  train MSE: 0.5914, val MSE: 0.3630
Gen 2042/8000  train MSE: 0.5914, val MSE: 0.3630
Gen 2043/8000  train MSE: 0.5914, val MSE: 0.3630
Gen 2044/8000  train MSE: 0.5914, val MSE: 0.3630
Gen 2045/8000  train MSE: 0.5914, val MSE: 0.3630
Gen 2046/8000  train MSE: 0.5914, val MSE: 0.3630
Gen 2047/8000  train MSE: 0.5914, val MSE: 0.3630
Gen 2048/8000  train MSE: 0.5914, val MSE: 0.3617
Gen 2049/8000  train MSE: 0.5914, val MSE: 0.3617
Gen 2050/8000  train MSE: 0.5914, val MSE: 0.3617
Gen 2051/8000  train MSE: 0.5914, val MSE: 0.3617
Gen 2052/8000  train MSE: 0.5914, val MSE: 0.3617
Gen 2053/8000  train MSE: 0.5914, val MSE: 0.3617
Gen 2054/8000  train MSE: 0.5914, val MSE: 0.3617
Gen 2055/8000  train MSE: 0.5914, val MSE: 0.3617
Gen 2056/8000  train MSE: 0.5914, val MSE: 0.3617
Gen 2057/8000  train MSE: 0.5914, val MSE: 0.3616
Gen 2058/8000  train MSE: 0.5914, val MSE: 0.3616
Gen 2059/8000  train MSE: 0.5914, val MSE: 0.3616
Gen 2060/8000  train MSE: 0.5913, val MSE: 0.3615
Gen 2061/8000  train MSE: 0.5913, val MSE: 0.3615
Gen 2062/8000  train MSE: 0.5913, val MSE: 0.3612
Gen 2063/8000  train MSE: 0.5913, val MSE: 0.3612
Gen 2064/8000  train MSE: 0.5913, val MSE: 0.3614
Gen 2065/8000  train MSE: 0.5913, val MSE: 0.3614
Gen 2066/8000  train MSE: 0.5913, val MSE: 0.3616
Gen 2067/8000  train MSE: 0.5913, val MSE: 0.3616
Gen 2068/8000  train MSE: 0.5913, val MSE: 0.3616
Gen 2069/8000  train MSE: 0.5913, val MSE: 0.3616
Gen 2070/8000  train MSE: 0.5912, val MSE: 0.3609
Gen 2071/8000  train MSE: 0.5911, val MSE: 0.3606
Gen 2072/8000  train MSE: 0.5911, val MSE: 0.3606
Gen 2073/8000  train MSE: 0.5911, val MSE: 0.3606
Gen 2074/8000  train MSE: 0.5911, val MSE: 0.3606
Gen 2075/8000  train MSE: 0.5911, val MSE: 0.3606
Gen 2076/8000  train MSE: 0.5911, val MSE: 0.3618
Gen 2077/8000  train MSE: 0.5911, val MSE: 0.3618
Gen 2078/8000  train MSE: 0.5911, val MSE: 0.3618
Gen 2079/8000  train MSE: 0.5911, val MSE: 0.3611
Gen 2080/8000  train MSE: 0.5911, val MSE: 0.3611
Gen 2081/8000  train MSE: 0.5911, val MSE: 0.3611
Gen 2082/8000  train MSE: 0.5911, val MSE: 0.3611
Gen 2083/8000  train MSE: 0.5911, val MSE: 0.3611
Gen 2084/8000  train MSE: 0.5911, val MSE: 0.3611
Gen 2085/8000  train MSE: 0.5911, val MSE: 0.3611
Gen 2086/8000  train MSE: 0.5910, val MSE: 0.3604
Gen 2087/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2088/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2089/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2090/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2091/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2092/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2093/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2094/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2095/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2096/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2097/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2098/8000  train MSE: 0.5910, val MSE: 0.3608
Gen 2099/8000  train MSE: 0.5909, val MSE: 0.3611
Gen 2100/8000  train MSE: 0.5909, val MSE: 0.3611
Gen 2101/8000  train MSE: 0.5909, val MSE: 0.3611
Gen 2102/8000  train MSE: 0.5909, val MSE: 0.3611
Gen 2103/8000  train MSE: 0.5909, val MSE: 0.3611
Gen 2104/8000  train MSE: 0.5909, val MSE: 0.3611
Gen 2105/8000  train MSE: 0.5909, val MSE: 0.3611
Gen 2106/8000  train MSE: 0.5909, val MSE: 0.3611
Gen 2107/8000  train MSE: 0.5909, val MSE: 0.3611
Gen 2108/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2109/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2110/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2111/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2112/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2113/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2114/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2115/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2116/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2117/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2118/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2119/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2120/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2121/8000  train MSE: 0.5908, val MSE: 0.3609
Gen 2122/8000  train MSE: 0.5908, val MSE: 0.3615
Gen 2123/8000  train MSE: 0.5908, val MSE: 0.3615
Gen 2124/8000  train MSE: 0.5908, val MSE: 0.3615
Gen 2125/8000  train MSE: 0.5908, val MSE: 0.3615
Gen 2126/8000  train MSE: 0.5908, val MSE: 0.3615
Gen 2127/8000  train MSE: 0.5907, val MSE: 0.3609
Gen 2128/8000  train MSE: 0.5907, val MSE: 0.3609
Gen 2129/8000  train MSE: 0.5907, val MSE: 0.3609
Gen 2130/8000  train MSE: 0.5907, val MSE: 0.3609
Gen 2131/8000  train MSE: 0.5907, val MSE: 0.3609
Gen 2132/8000  train MSE: 0.5907, val MSE: 0.3609
Gen 2133/8000  train MSE: 0.5907, val MSE: 0.3609
Gen 2134/8000  train MSE: 0.5907, val MSE: 0.3609
Gen 2135/8000  train MSE: 0.5907, val MSE: 0.3609
Gen 2136/8000  train MSE: 0.5907, val MSE: 0.3609
Gen 2137/8000  train MSE: 0.5907, val MSE: 0.3609
Gen 2138/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2139/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2140/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2141/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2142/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2143/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2144/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2145/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2146/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2147/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2148/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2149/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2150/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2151/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2152/8000  train MSE: 0.5907, val MSE: 0.3597
Gen 2153/8000  train MSE: 0.5907, val MSE: 0.3599
Gen 2154/8000  train MSE: 0.5907, val MSE: 0.3599
Gen 2155/8000  train MSE: 0.5907, val MSE: 0.3599
Gen 2156/8000  train MSE: 0.5907, val MSE: 0.3599
Gen 2157/8000  train MSE: 0.5906, val MSE: 0.3608
Gen 2158/8000  train MSE: 0.5906, val MSE: 0.3608
Gen 2159/8000  train MSE: 0.5906, val MSE: 0.3608
Gen 2160/8000  train MSE: 0.5906, val MSE: 0.3608
Gen 2161/8000  train MSE: 0.5906, val MSE: 0.3608
Gen 2162/8000  train MSE: 0.5906, val MSE: 0.3608
Gen 2163/8000  train MSE: 0.5906, val MSE: 0.3608
Gen 2164/8000  train MSE: 0.5906, val MSE: 0.3608
Gen 2165/8000  train MSE: 0.5906, val MSE: 0.3608
Gen 2166/8000  train MSE: 0.5906, val MSE: 0.3608
Gen 2167/8000  train MSE: 0.5905, val MSE: 0.3605
Gen 2168/8000  train MSE: 0.5905, val MSE: 0.3605
Gen 2169/8000  train MSE: 0.5905, val MSE: 0.3605
Gen 2170/8000  train MSE: 0.5905, val MSE: 0.3601
Gen 2171/8000  train MSE: 0.5905, val MSE: 0.3601
Gen 2172/8000  train MSE: 0.5905, val MSE: 0.3601
Gen 2173/8000  train MSE: 0.5905, val MSE: 0.3601
Gen 2174/8000  train MSE: 0.5905, val MSE: 0.3601
Gen 2175/8000  train MSE: 0.5905, val MSE: 0.3594
Gen 2176/8000  train MSE: 0.5905, val MSE: 0.3594
Gen 2177/8000  train MSE: 0.5905, val MSE: 0.3594
Gen 2178/8000  train MSE: 0.5905, val MSE: 0.3595
Gen 2179/8000  train MSE: 0.5905, val MSE: 0.3595
Gen 2180/8000  train MSE: 0.5904, val MSE: 0.3600
Gen 2181/8000  train MSE: 0.5904, val MSE: 0.3600
Gen 2182/8000  train MSE: 0.5904, val MSE: 0.3600
Gen 2183/8000  train MSE: 0.5904, val MSE: 0.3600
Gen 2184/8000  train MSE: 0.5904, val MSE: 0.3600
Gen 2185/8000  train MSE: 0.5904, val MSE: 0.3600
Gen 2186/8000  train MSE: 0.5904, val MSE: 0.3600
Gen 2187/8000  train MSE: 0.5904, val MSE: 0.3600
Gen 2188/8000  train MSE: 0.5904, val MSE: 0.3600
Gen 2189/8000  train MSE: 0.5904, val MSE: 0.3600
Gen 2190/8000  train MSE: 0.5904, val MSE: 0.3598
Gen 2191/8000  train MSE: 0.5904, val MSE: 0.3598
Gen 2192/8000  train MSE: 0.5904, val MSE: 0.3598
Gen 2193/8000  train MSE: 0.5904, val MSE: 0.3598
Gen 2194/8000  train MSE: 0.5904, val MSE: 0.3598
Gen 2195/8000  train MSE: 0.5904, val MSE: 0.3598
Gen 2196/8000  train MSE: 0.5904, val MSE: 0.3598
Gen 2197/8000  train MSE: 0.5904, val MSE: 0.3598
Gen 2198/8000  train MSE: 0.5904, val MSE: 0.3598
Gen 2199/8000  train MSE: 0.5904, val MSE: 0.3598
Gen 2200/8000  train MSE: 0.5904, val MSE: 0.3598
Gen 2201/8000  train MSE: 0.5904, val MSE: 0.3601
Gen 2202/8000  train MSE: 0.5904, val MSE: 0.3601
Gen 2203/8000  train MSE: 0.5904, val MSE: 0.3601
Gen 2204/8000  train MSE: 0.5904, val MSE: 0.3601
Gen 2205/8000  train MSE: 0.5904, val MSE: 0.3601
Gen 2206/8000  train MSE: 0.5904, val MSE: 0.3601
Gen 2207/8000  train MSE: 0.5904, val MSE: 0.3601
Gen 2208/8000  train MSE: 0.5904, val MSE: 0.3601
Gen 2209/8000  train MSE: 0.5904, val MSE: 0.3601
Gen 2210/8000  train MSE: 0.5903, val MSE: 0.3607
Gen 2211/8000  train MSE: 0.5903, val MSE: 0.3607
Gen 2212/8000  train MSE: 0.5903, val MSE: 0.3607
Gen 2213/8000  train MSE: 0.5903, val MSE: 0.3607
Gen 2214/8000  train MSE: 0.5903, val MSE: 0.3607
Gen 2215/8000  train MSE: 0.5903, val MSE: 0.3603
Gen 2216/8000  train MSE: 0.5903, val MSE: 0.3603
Gen 2217/8000  train MSE: 0.5903, val MSE: 0.3603
Gen 2218/8000  train MSE: 0.5903, val MSE: 0.3603
Gen 2219/8000  train MSE: 0.5903, val MSE: 0.3603
Gen 2220/8000  train MSE: 0.5903, val MSE: 0.3603
Gen 2221/8000  train MSE: 0.5903, val MSE: 0.3603
Gen 2222/8000  train MSE: 0.5903, val MSE: 0.3603
Gen 2223/8000  train MSE: 0.5903, val MSE: 0.3603
Gen 2224/8000  train MSE: 0.5903, val MSE: 0.3603
Gen 2225/8000  train MSE: 0.5903, val MSE: 0.3603
Gen 2226/8000  train MSE: 0.5903, val MSE: 0.3607
Gen 2227/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2228/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2229/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2230/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2231/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2232/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2233/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2234/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2235/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2236/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2237/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2238/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2239/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2240/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2241/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2242/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2243/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2244/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2245/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2246/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2247/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2248/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2249/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2250/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2251/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2252/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2253/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2254/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2255/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2256/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2257/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2258/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2259/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2260/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2261/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2262/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2263/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2264/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2265/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2266/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2267/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2268/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2269/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2270/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2271/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2272/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2273/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2274/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2275/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2276/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2277/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2278/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2279/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2280/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2281/8000  train MSE: 0.5901, val MSE: 0.3602
Gen 2282/8000  train MSE: 0.5901, val MSE: 0.3596
Gen 2283/8000  train MSE: 0.5901, val MSE: 0.3600
Gen 2284/8000  train MSE: 0.5901, val MSE: 0.3600
Gen 2285/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2286/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2287/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2288/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2289/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2290/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2291/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2292/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2293/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2294/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2295/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2296/8000  train MSE: 0.5901, val MSE: 0.3601
Gen 2297/8000  train MSE: 0.5901, val MSE: 0.3600
Gen 2298/8000  train MSE: 0.5901, val MSE: 0.3600
Gen 2299/8000  train MSE: 0.5901, val MSE: 0.3600
Gen 2300/8000  train MSE: 0.5901, val MSE: 0.3600
Gen 2301/8000  train MSE: 0.5901, val MSE: 0.3600
Gen 2302/8000  train MSE: 0.5901, val MSE: 0.3597
Gen 2303/8000  train MSE: 0.5901, val MSE: 0.3597
Gen 2304/8000  train MSE: 0.5901, val MSE: 0.3597
Gen 2305/8000  train MSE: 0.5900, val MSE: 0.3589
Gen 2306/8000  train MSE: 0.5900, val MSE: 0.3589
Gen 2307/8000  train MSE: 0.5900, val MSE: 0.3589
Gen 2308/8000  train MSE: 0.5900, val MSE: 0.3589
Gen 2309/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2310/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2311/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2312/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2313/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2314/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2315/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2316/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2317/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2318/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2319/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2320/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2321/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2322/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2323/8000  train MSE: 0.5900, val MSE: 0.3592
Gen 2324/8000  train MSE: 0.5899, val MSE: 0.3592
Gen 2325/8000  train MSE: 0.5899, val MSE: 0.3592
Gen 2326/8000  train MSE: 0.5899, val MSE: 0.3592
Gen 2327/8000  train MSE: 0.5899, val MSE: 0.3592
Gen 2328/8000  train MSE: 0.5899, val MSE: 0.3592
Gen 2329/8000  train MSE: 0.5899, val MSE: 0.3589
Gen 2330/8000  train MSE: 0.5899, val MSE: 0.3589
Gen 2331/8000  train MSE: 0.5899, val MSE: 0.3589
Gen 2332/8000  train MSE: 0.5899, val MSE: 0.3589
Gen 2333/8000  train MSE: 0.5899, val MSE: 0.3589
Gen 2334/8000  train MSE: 0.5899, val MSE: 0.3601
Gen 2335/8000  train MSE: 0.5899, val MSE: 0.3601
Gen 2336/8000  train MSE: 0.5899, val MSE: 0.3601
Gen 2337/8000  train MSE: 0.5899, val MSE: 0.3601
Gen 2338/8000  train MSE: 0.5899, val MSE: 0.3601
Gen 2339/8000  train MSE: 0.5899, val MSE: 0.3601
Gen 2340/8000  train MSE: 0.5899, val MSE: 0.3601
Gen 2341/8000  train MSE: 0.5899, val MSE: 0.3601
Gen 2342/8000  train MSE: 0.5899, val MSE: 0.3601
Gen 2343/8000  train MSE: 0.5899, val MSE: 0.3601
Gen 2344/8000  train MSE: 0.5898, val MSE: 0.3594
Gen 2345/8000  train MSE: 0.5898, val MSE: 0.3594
Gen 2346/8000  train MSE: 0.5898, val MSE: 0.3593
Gen 2347/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2348/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2349/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2350/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2351/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2352/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2353/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2354/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2355/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2356/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2357/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2358/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2359/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2360/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2361/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2362/8000  train MSE: 0.5897, val MSE: 0.3598
Gen 2363/8000  train MSE: 0.5897, val MSE: 0.3596
Gen 2364/8000  train MSE: 0.5897, val MSE: 0.3578
Gen 2365/8000  train MSE: 0.5897, val MSE: 0.3578
Gen 2366/8000  train MSE: 0.5897, val MSE: 0.3578
Gen 2367/8000  train MSE: 0.5897, val MSE: 0.3581
Gen 2368/8000  train MSE: 0.5897, val MSE: 0.3581
Gen 2369/8000  train MSE: 0.5897, val MSE: 0.3581
Gen 2370/8000  train MSE: 0.5897, val MSE: 0.3584
Gen 2371/8000  train MSE: 0.5897, val MSE: 0.3584
Gen 2372/8000  train MSE: 0.5897, val MSE: 0.3584
Gen 2373/8000  train MSE: 0.5897, val MSE: 0.3584
Gen 2374/8000  train MSE: 0.5897, val MSE: 0.3579
Gen 2375/8000  train MSE: 0.5897, val MSE: 0.3579
Gen 2376/8000  train MSE: 0.5897, val MSE: 0.3579
Gen 2377/8000  train MSE: 0.5896, val MSE: 0.3585
Gen 2378/8000  train MSE: 0.5896, val MSE: 0.3585
Gen 2379/8000  train MSE: 0.5896, val MSE: 0.3585
Gen 2380/8000  train MSE: 0.5896, val MSE: 0.3585
Gen 2381/8000  train MSE: 0.5896, val MSE: 0.3585
Gen 2382/8000  train MSE: 0.5896, val MSE: 0.3585
Gen 2383/8000  train MSE: 0.5896, val MSE: 0.3585
Gen 2384/8000  train MSE: 0.5896, val MSE: 0.3585
Gen 2385/8000  train MSE: 0.5896, val MSE: 0.3589
Gen 2386/8000  train MSE: 0.5896, val MSE: 0.3589
Gen 2387/8000  train MSE: 0.5896, val MSE: 0.3589
Gen 2388/8000  train MSE: 0.5896, val MSE: 0.3589
Gen 2389/8000  train MSE: 0.5896, val MSE: 0.3589
Gen 2390/8000  train MSE: 0.5896, val MSE: 0.3589
Gen 2391/8000  train MSE: 0.5896, val MSE: 0.3589
Gen 2392/8000  train MSE: 0.5896, val MSE: 0.3589
Gen 2393/8000  train MSE: 0.5896, val MSE: 0.3589
Gen 2394/8000  train MSE: 0.5895, val MSE: 0.3581
Gen 2395/8000  train MSE: 0.5895, val MSE: 0.3581
Gen 2396/8000  train MSE: 0.5895, val MSE: 0.3581
Gen 2397/8000  train MSE: 0.5895, val MSE: 0.3581
Gen 2398/8000  train MSE: 0.5895, val MSE: 0.3581
Gen 2399/8000  train MSE: 0.5895, val MSE: 0.3581
Gen 2400/8000  train MSE: 0.5895, val MSE: 0.3581
Gen 2401/8000  train MSE: 0.5895, val MSE: 0.3581
Gen 2402/8000  train MSE: 0.5895, val MSE: 0.3580
Gen 2403/8000  train MSE: 0.5895, val MSE: 0.3580
Gen 2404/8000  train MSE: 0.5895, val MSE: 0.3580
Gen 2405/8000  train MSE: 0.5895, val MSE: 0.3580
Gen 2406/8000  train MSE: 0.5895, val MSE: 0.3580
Gen 2407/8000  train MSE: 0.5895, val MSE: 0.3587
Gen 2408/8000  train MSE: 0.5895, val MSE: 0.3587
Gen 2409/8000  train MSE: 0.5895, val MSE: 0.3587
Gen 2410/8000  train MSE: 0.5895, val MSE: 0.3588
Gen 2411/8000  train MSE: 0.5895, val MSE: 0.3588
Gen 2412/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2413/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2414/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2415/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2416/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2417/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2418/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2419/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2420/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2421/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2422/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2423/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2424/8000  train MSE: 0.5894, val MSE: 0.3581
Gen 2425/8000  train MSE: 0.5894, val MSE: 0.3585
Gen 2426/8000  train MSE: 0.5894, val MSE: 0.3585
Gen 2427/8000  train MSE: 0.5894, val MSE: 0.3585
Gen 2428/8000  train MSE: 0.5894, val MSE: 0.3585
Gen 2429/8000  train MSE: 0.5893, val MSE: 0.3569
Gen 2430/8000  train MSE: 0.5893, val MSE: 0.3569
Gen 2431/8000  train MSE: 0.5893, val MSE: 0.3569
Gen 2432/8000  train MSE: 0.5893, val MSE: 0.3569
Gen 2433/8000  train MSE: 0.5893, val MSE: 0.3569
Gen 2434/8000  train MSE: 0.5893, val MSE: 0.3569
Gen 2435/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2436/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2437/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2438/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2439/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2440/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2441/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2442/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2443/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2444/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2445/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2446/8000  train MSE: 0.5892, val MSE: 0.3588
Gen 2447/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2448/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2449/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2450/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2451/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2452/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2453/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2454/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2455/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2456/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2457/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2458/8000  train MSE: 0.5892, val MSE: 0.3577
Gen 2459/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2460/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2461/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2462/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2463/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2464/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2465/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2466/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2467/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2468/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2469/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2470/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2471/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2472/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2473/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2474/8000  train MSE: 0.5891, val MSE: 0.3572
Gen 2475/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2476/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2477/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2478/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2479/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2480/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2481/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2482/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2483/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2484/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2485/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2486/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2487/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2488/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2489/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2490/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2491/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2492/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2493/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2494/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2495/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2496/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2497/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2498/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2499/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2500/8000  train MSE: 0.5890, val MSE: 0.3559
Gen 2501/8000  train MSE: 0.5890, val MSE: 0.3571
Gen 2502/8000  train MSE: 0.5890, val MSE: 0.3571
Gen 2503/8000  train MSE: 0.5889, val MSE: 0.3571
Gen 2504/8000  train MSE: 0.5889, val MSE: 0.3571
Gen 2505/8000  train MSE: 0.5889, val MSE: 0.3571
Gen 2506/8000  train MSE: 0.5889, val MSE: 0.3571
Gen 2507/8000  train MSE: 0.5889, val MSE: 0.3566
Gen 2508/8000  train MSE: 0.5889, val MSE: 0.3566
Gen 2509/8000  train MSE: 0.5889, val MSE: 0.3566
Gen 2510/8000  train MSE: 0.5889, val MSE: 0.3566
Gen 2511/8000  train MSE: 0.5889, val MSE: 0.3566
Gen 2512/8000  train MSE: 0.5889, val MSE: 0.3566
Gen 2513/8000  train MSE: 0.5889, val MSE: 0.3566
Gen 2514/8000  train MSE: 0.5889, val MSE: 0.3566
Gen 2515/8000  train MSE: 0.5889, val MSE: 0.3566
Gen 2516/8000  train MSE: 0.5889, val MSE: 0.3566
Gen 2517/8000  train MSE: 0.5889, val MSE: 0.3566
Gen 2518/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2519/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2520/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2521/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2522/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2523/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2524/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2525/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2526/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2527/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2528/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2529/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2530/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2531/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2532/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2533/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2534/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2535/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2536/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2537/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2538/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2539/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2540/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2541/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2542/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2543/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2544/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2545/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2546/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2547/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2548/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2549/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2550/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2551/8000  train MSE: 0.5889, val MSE: 0.3568
Gen 2552/8000  train MSE: 0.5889, val MSE: 0.3570
Gen 2553/8000  train MSE: 0.5889, val MSE: 0.3570
Gen 2554/8000  train MSE: 0.5889, val MSE: 0.3570
Gen 2555/8000  train MSE: 0.5889, val MSE: 0.3570
Gen 2556/8000  train MSE: 0.5889, val MSE: 0.3570
Gen 2557/8000  train MSE: 0.5889, val MSE: 0.3570
Gen 2558/8000  train MSE: 0.5889, val MSE: 0.3570
Gen 2559/8000  train MSE: 0.5889, val MSE: 0.3570
Gen 2560/8000  train MSE: 0.5889, val MSE: 0.3565
Gen 2561/8000  train MSE: 0.5889, val MSE: 0.3565
Gen 2562/8000  train MSE: 0.5889, val MSE: 0.3565
Gen 2563/8000  train MSE: 0.5889, val MSE: 0.3565
Gen 2564/8000  train MSE: 0.5889, val MSE: 0.3565
Gen 2565/8000  train MSE: 0.5889, val MSE: 0.3572
Gen 2566/8000  train MSE: 0.5889, val MSE: 0.3572
Gen 2567/8000  train MSE: 0.5889, val MSE: 0.3572
Gen 2568/8000  train MSE: 0.5889, val MSE: 0.3572
Gen 2569/8000  train MSE: 0.5889, val MSE: 0.3563
Gen 2570/8000  train MSE: 0.5889, val MSE: 0.3563
Gen 2571/8000  train MSE: 0.5889, val MSE: 0.3563
Gen 2572/8000  train MSE: 0.5889, val MSE: 0.3563
Gen 2573/8000  train MSE: 0.5888, val MSE: 0.3561
Gen 2574/8000  train MSE: 0.5888, val MSE: 0.3561
Gen 2575/8000  train MSE: 0.5888, val MSE: 0.3561
Gen 2576/8000  train MSE: 0.5888, val MSE: 0.3564
Gen 2577/8000  train MSE: 0.5888, val MSE: 0.3564
Gen 2578/8000  train MSE: 0.5888, val MSE: 0.3564
Gen 2579/8000  train MSE: 0.5888, val MSE: 0.3564
Gen 2580/8000  train MSE: 0.5888, val MSE: 0.3564
Gen 2581/8000  train MSE: 0.5888, val MSE: 0.3564
Gen 2582/8000  train MSE: 0.5888, val MSE: 0.3564
Gen 2583/8000  train MSE: 0.5888, val MSE: 0.3557
Gen 2584/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2585/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2586/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2587/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2588/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2589/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2590/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2591/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2592/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2593/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2594/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2595/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2596/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2597/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2598/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2599/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2600/8000  train MSE: 0.5887, val MSE: 0.3561
Gen 2601/8000  train MSE: 0.5887, val MSE: 0.3561
Gen 2602/8000  train MSE: 0.5887, val MSE: 0.3561
Gen 2603/8000  train MSE: 0.5887, val MSE: 0.3561
Gen 2604/8000  train MSE: 0.5887, val MSE: 0.3561
Gen 2605/8000  train MSE: 0.5887, val MSE: 0.3561
Gen 2606/8000  train MSE: 0.5887, val MSE: 0.3561
Gen 2607/8000  train MSE: 0.5887, val MSE: 0.3561
Gen 2608/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2609/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2610/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2611/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2612/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2613/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2614/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2615/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2616/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2617/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2618/8000  train MSE: 0.5887, val MSE: 0.3564
Gen 2619/8000  train MSE: 0.5887, val MSE: 0.3568
Gen 2620/8000  train MSE: 0.5887, val MSE: 0.3568
Gen 2621/8000  train MSE: 0.5887, val MSE: 0.3568
Gen 2622/8000  train MSE: 0.5887, val MSE: 0.3560
Gen 2623/8000  train MSE: 0.5887, val MSE: 0.3560
Gen 2624/8000  train MSE: 0.5887, val MSE: 0.3560
Gen 2625/8000  train MSE: 0.5887, val MSE: 0.3560
Gen 2626/8000  train MSE: 0.5887, val MSE: 0.3560
Gen 2627/8000  train MSE: 0.5887, val MSE: 0.3560
Gen 2628/8000  train MSE: 0.5887, val MSE: 0.3560
Gen 2629/8000  train MSE: 0.5887, val MSE: 0.3560
Gen 2630/8000  train MSE: 0.5887, val MSE: 0.3560
Gen 2631/8000  train MSE: 0.5887, val MSE: 0.3560
Gen 2632/8000  train MSE: 0.5887, val MSE: 0.3560
Gen 2633/8000  train MSE: 0.5886, val MSE: 0.3572
Gen 2634/8000  train MSE: 0.5886, val MSE: 0.3572
Gen 2635/8000  train MSE: 0.5886, val MSE: 0.3572
Gen 2636/8000  train MSE: 0.5886, val MSE: 0.3572
Gen 2637/8000  train MSE: 0.5886, val MSE: 0.3572
Gen 2638/8000  train MSE: 0.5886, val MSE: 0.3572
Gen 2639/8000  train MSE: 0.5886, val MSE: 0.3572
Gen 2640/8000  train MSE: 0.5886, val MSE: 0.3572
Gen 2641/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2642/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2643/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2644/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2645/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2646/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2647/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2648/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2649/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2650/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2651/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2652/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2653/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2654/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2655/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2656/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2657/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2658/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2659/8000  train MSE: 0.5885, val MSE: 0.3569
Gen 2660/8000  train MSE: 0.5885, val MSE: 0.3567
Gen 2661/8000  train MSE: 0.5885, val MSE: 0.3567
Gen 2662/8000  train MSE: 0.5885, val MSE: 0.3567
Gen 2663/8000  train MSE: 0.5885, val MSE: 0.3567
Gen 2664/8000  train MSE: 0.5885, val MSE: 0.3567
Gen 2665/8000  train MSE: 0.5885, val MSE: 0.3567
Gen 2666/8000  train MSE: 0.5885, val MSE: 0.3567
Gen 2667/8000  train MSE: 0.5885, val MSE: 0.3560
Gen 2668/8000  train MSE: 0.5885, val MSE: 0.3567
Gen 2669/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2670/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2671/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2672/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2673/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2674/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2675/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2676/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2677/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2678/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2679/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2680/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2681/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2682/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2683/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2684/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2685/8000  train MSE: 0.5884, val MSE: 0.3548
Gen 2686/8000  train MSE: 0.5884, val MSE: 0.3560
Gen 2687/8000  train MSE: 0.5884, val MSE: 0.3560
Gen 2688/8000  train MSE: 0.5884, val MSE: 0.3560
Gen 2689/8000  train MSE: 0.5884, val MSE: 0.3560
Gen 2690/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2691/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2692/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2693/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2694/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2695/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2696/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2697/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2698/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2699/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2700/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2701/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2702/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2703/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2704/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2705/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2706/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2707/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2708/8000  train MSE: 0.5883, val MSE: 0.3562
Gen 2709/8000  train MSE: 0.5883, val MSE: 0.3552
Gen 2710/8000  train MSE: 0.5883, val MSE: 0.3556
Gen 2711/8000  train MSE: 0.5883, val MSE: 0.3556
Gen 2712/8000  train MSE: 0.5882, val MSE: 0.3557
Gen 2713/8000  train MSE: 0.5882, val MSE: 0.3557
Gen 2714/8000  train MSE: 0.5882, val MSE: 0.3557
Gen 2715/8000  train MSE: 0.5882, val MSE: 0.3557
Gen 2716/8000  train MSE: 0.5882, val MSE: 0.3557
Gen 2717/8000  train MSE: 0.5882, val MSE: 0.3557
Gen 2718/8000  train MSE: 0.5881, val MSE: 0.3548
Gen 2719/8000  train MSE: 0.5881, val MSE: 0.3548
Gen 2720/8000  train MSE: 0.5881, val MSE: 0.3548
Gen 2721/8000  train MSE: 0.5881, val MSE: 0.3556
Gen 2722/8000  train MSE: 0.5881, val MSE: 0.3556
Gen 2723/8000  train MSE: 0.5881, val MSE: 0.3556
Gen 2724/8000  train MSE: 0.5881, val MSE: 0.3556
Gen 2725/8000  train MSE: 0.5881, val MSE: 0.3556
Gen 2726/8000  train MSE: 0.5881, val MSE: 0.3556
Gen 2727/8000  train MSE: 0.5881, val MSE: 0.3556
Gen 2728/8000  train MSE: 0.5881, val MSE: 0.3556
Gen 2729/8000  train MSE: 0.5881, val MSE: 0.3556
Gen 2730/8000  train MSE: 0.5881, val MSE: 0.3556
Gen 2731/8000  train MSE: 0.5881, val MSE: 0.3556
Gen 2732/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2733/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2734/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2735/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2736/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2737/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2738/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2739/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2740/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2741/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2742/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2743/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2744/8000  train MSE: 0.5881, val MSE: 0.3549
Gen 2745/8000  train MSE: 0.5881, val MSE: 0.3543
Gen 2746/8000  train MSE: 0.5881, val MSE: 0.3543
Gen 2747/8000  train MSE: 0.5881, val MSE: 0.3543
Gen 2748/8000  train MSE: 0.5881, val MSE: 0.3543
Gen 2749/8000  train MSE: 0.5881, val MSE: 0.3543
Gen 2750/8000  train MSE: 0.5880, val MSE: 0.3549
Gen 2751/8000  train MSE: 0.5880, val MSE: 0.3549
Gen 2752/8000  train MSE: 0.5880, val MSE: 0.3549
Gen 2753/8000  train MSE: 0.5880, val MSE: 0.3549
Gen 2754/8000  train MSE: 0.5880, val MSE: 0.3549
Gen 2755/8000  train MSE: 0.5880, val MSE: 0.3553
Gen 2756/8000  train MSE: 0.5880, val MSE: 0.3550
Gen 2757/8000  train MSE: 0.5880, val MSE: 0.3550
Gen 2758/8000  train MSE: 0.5880, val MSE: 0.3550
Gen 2759/8000  train MSE: 0.5880, val MSE: 0.3550
Gen 2760/8000  train MSE: 0.5879, val MSE: 0.3555
Gen 2761/8000  train MSE: 0.5879, val MSE: 0.3555
Gen 2762/8000  train MSE: 0.5879, val MSE: 0.3555
Gen 2763/8000  train MSE: 0.5879, val MSE: 0.3555
Gen 2764/8000  train MSE: 0.5879, val MSE: 0.3555
Gen 2765/8000  train MSE: 0.5879, val MSE: 0.3555
Gen 2766/8000  train MSE: 0.5879, val MSE: 0.3555
Gen 2767/8000  train MSE: 0.5879, val MSE: 0.3555
Gen 2768/8000  train MSE: 0.5879, val MSE: 0.3550
Gen 2769/8000  train MSE: 0.5879, val MSE: 0.3553
Gen 2770/8000  train MSE: 0.5879, val MSE: 0.3553
Gen 2771/8000  train MSE: 0.5879, val MSE: 0.3553
Gen 2772/8000  train MSE: 0.5879, val MSE: 0.3553
Gen 2773/8000  train MSE: 0.5879, val MSE: 0.3553
Gen 2774/8000  train MSE: 0.5879, val MSE: 0.3546
Gen 2775/8000  train MSE: 0.5878, val MSE: 0.3539
Gen 2776/8000  train MSE: 0.5878, val MSE: 0.3539
Gen 2777/8000  train MSE: 0.5878, val MSE: 0.3539
Gen 2778/8000  train MSE: 0.5878, val MSE: 0.3539
Gen 2779/8000  train MSE: 0.5878, val MSE: 0.3539
Gen 2780/8000  train MSE: 0.5878, val MSE: 0.3546
Gen 2781/8000  train MSE: 0.5878, val MSE: 0.3546
Gen 2782/8000  train MSE: 0.5878, val MSE: 0.3546
Gen 2783/8000  train MSE: 0.5878, val MSE: 0.3546
Gen 2784/8000  train MSE: 0.5878, val MSE: 0.3546
Gen 2785/8000  train MSE: 0.5878, val MSE: 0.3546
Gen 2786/8000  train MSE: 0.5878, val MSE: 0.3546
Gen 2787/8000  train MSE: 0.5878, val MSE: 0.3546
Gen 2788/8000  train MSE: 0.5878, val MSE: 0.3546
Gen 2789/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2790/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2791/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2792/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2793/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2794/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2795/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2796/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2797/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2798/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2799/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2800/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2801/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2802/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2803/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2804/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2805/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2806/8000  train MSE: 0.5877, val MSE: 0.3534
Gen 2807/8000  train MSE: 0.5877, val MSE: 0.3544
Gen 2808/8000  train MSE: 0.5877, val MSE: 0.3544
Gen 2809/8000  train MSE: 0.5876, val MSE: 0.3534
Gen 2810/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2811/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2812/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2813/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2814/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2815/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2816/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2817/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2818/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2819/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2820/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2821/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2822/8000  train MSE: 0.5876, val MSE: 0.3538
Gen 2823/8000  train MSE: 0.5876, val MSE: 0.3545
Gen 2824/8000  train MSE: 0.5876, val MSE: 0.3545
Gen 2825/8000  train MSE: 0.5876, val MSE: 0.3545
Gen 2826/8000  train MSE: 0.5876, val MSE: 0.3545
Gen 2827/8000  train MSE: 0.5876, val MSE: 0.3545
Gen 2828/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2829/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2830/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2831/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2832/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2833/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2834/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2835/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2836/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2837/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2838/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2839/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2840/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2841/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2842/8000  train MSE: 0.5875, val MSE: 0.3537
Gen 2843/8000  train MSE: 0.5875, val MSE: 0.3530
Gen 2844/8000  train MSE: 0.5875, val MSE: 0.3530
Gen 2845/8000  train MSE: 0.5875, val MSE: 0.3530
Gen 2846/8000  train MSE: 0.5875, val MSE: 0.3530
Gen 2847/8000  train MSE: 0.5875, val MSE: 0.3535
Gen 2848/8000  train MSE: 0.5875, val MSE: 0.3535
Gen 2849/8000  train MSE: 0.5875, val MSE: 0.3535
Gen 2850/8000  train MSE: 0.5875, val MSE: 0.3535
Gen 2851/8000  train MSE: 0.5875, val MSE: 0.3535
Gen 2852/8000  train MSE: 0.5874, val MSE: 0.3534
Gen 2853/8000  train MSE: 0.5874, val MSE: 0.3534
Gen 2854/8000  train MSE: 0.5874, val MSE: 0.3534
Gen 2855/8000  train MSE: 0.5874, val MSE: 0.3534
Gen 2856/8000  train MSE: 0.5874, val MSE: 0.3534
Gen 2857/8000  train MSE: 0.5874, val MSE: 0.3534
Gen 2858/8000  train MSE: 0.5874, val MSE: 0.3534
Gen 2859/8000  train MSE: 0.5874, val MSE: 0.3534
Gen 2860/8000  train MSE: 0.5874, val MSE: 0.3531
Gen 2861/8000  train MSE: 0.5874, val MSE: 0.3531
Gen 2862/8000  train MSE: 0.5874, val MSE: 0.3531
Gen 2863/8000  train MSE: 0.5874, val MSE: 0.3531
Gen 2864/8000  train MSE: 0.5874, val MSE: 0.3531
Gen 2865/8000  train MSE: 0.5874, val MSE: 0.3531
Gen 2866/8000  train MSE: 0.5873, val MSE: 0.3533
Gen 2867/8000  train MSE: 0.5873, val MSE: 0.3533
Gen 2868/8000  train MSE: 0.5873, val MSE: 0.3533
Gen 2869/8000  train MSE: 0.5873, val MSE: 0.3533
Gen 2870/8000  train MSE: 0.5873, val MSE: 0.3533
Gen 2871/8000  train MSE: 0.5873, val MSE: 0.3533
Gen 2872/8000  train MSE: 0.5873, val MSE: 0.3533
Gen 2873/8000  train MSE: 0.5873, val MSE: 0.3533
Gen 2874/8000  train MSE: 0.5873, val MSE: 0.3533
Gen 2875/8000  train MSE: 0.5873, val MSE: 0.3533
Gen 2876/8000  train MSE: 0.5873, val MSE: 0.3533
Gen 2877/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2878/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2879/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2880/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2881/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2882/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2883/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2884/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2885/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2886/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2887/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2888/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2889/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2890/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2891/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2892/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2893/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2894/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2895/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2896/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2897/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2898/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2899/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2900/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2901/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2902/8000  train MSE: 0.5873, val MSE: 0.3535
Gen 2903/8000  train MSE: 0.5873, val MSE: 0.3536
Gen 2904/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2905/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2906/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2907/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2908/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2909/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2910/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2911/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2912/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2913/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2914/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2915/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2916/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2917/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2918/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2919/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2920/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2921/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2922/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2923/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2924/8000  train MSE: 0.5873, val MSE: 0.3530
Gen 2925/8000  train MSE: 0.5872, val MSE: 0.3534
Gen 2926/8000  train MSE: 0.5872, val MSE: 0.3534
Gen 2927/8000  train MSE: 0.5872, val MSE: 0.3534
Gen 2928/8000  train MSE: 0.5872, val MSE: 0.3534
Gen 2929/8000  train MSE: 0.5872, val MSE: 0.3534
Gen 2930/8000  train MSE: 0.5872, val MSE: 0.3543
Gen 2931/8000  train MSE: 0.5872, val MSE: 0.3545
Gen 2932/8000  train MSE: 0.5872, val MSE: 0.3545
Gen 2933/8000  train MSE: 0.5872, val MSE: 0.3528
Gen 2934/8000  train MSE: 0.5872, val MSE: 0.3528
Gen 2935/8000  train MSE: 0.5872, val MSE: 0.3528
Gen 2936/8000  train MSE: 0.5872, val MSE: 0.3535
Gen 2937/8000  train MSE: 0.5872, val MSE: 0.3535
Gen 2938/8000  train MSE: 0.5872, val MSE: 0.3535
Gen 2939/8000  train MSE: 0.5872, val MSE: 0.3535
Gen 2940/8000  train MSE: 0.5872, val MSE: 0.3535
Gen 2941/8000  train MSE: 0.5872, val MSE: 0.3535
Gen 2942/8000  train MSE: 0.5872, val MSE: 0.3535
Gen 2943/8000  train MSE: 0.5872, val MSE: 0.3535
Gen 2944/8000  train MSE: 0.5871, val MSE: 0.3535
Gen 2945/8000  train MSE: 0.5871, val MSE: 0.3535
Gen 2946/8000  train MSE: 0.5871, val MSE: 0.3535
Gen 2947/8000  train MSE: 0.5871, val MSE: 0.3532
Gen 2948/8000  train MSE: 0.5871, val MSE: 0.3532
Gen 2949/8000  train MSE: 0.5871, val MSE: 0.3532
Gen 2950/8000  train MSE: 0.5871, val MSE: 0.3532
Gen 2951/8000  train MSE: 0.5871, val MSE: 0.3532
Gen 2952/8000  train MSE: 0.5871, val MSE: 0.3532
Gen 2953/8000  train MSE: 0.5871, val MSE: 0.3527
Gen 2954/8000  train MSE: 0.5871, val MSE: 0.3527
Gen 2955/8000  train MSE: 0.5871, val MSE: 0.3527
Gen 2956/8000  train MSE: 0.5871, val MSE: 0.3527
Gen 2957/8000  train MSE: 0.5871, val MSE: 0.3527
Gen 2958/8000  train MSE: 0.5871, val MSE: 0.3527
Gen 2959/8000  train MSE: 0.5871, val MSE: 0.3527
Gen 2960/8000  train MSE: 0.5871, val MSE: 0.3527
Gen 2961/8000  train MSE: 0.5871, val MSE: 0.3527
Gen 2962/8000  train MSE: 0.5871, val MSE: 0.3539
Gen 2963/8000  train MSE: 0.5871, val MSE: 0.3539
Gen 2964/8000  train MSE: 0.5871, val MSE: 0.3539
Gen 2965/8000  train MSE: 0.5871, val MSE: 0.3539
Gen 2966/8000  train MSE: 0.5871, val MSE: 0.3539
Gen 2967/8000  train MSE: 0.5870, val MSE: 0.3536
Gen 2968/8000  train MSE: 0.5870, val MSE: 0.3536
Gen 2969/8000  train MSE: 0.5870, val MSE: 0.3536
Gen 2970/8000  train MSE: 0.5870, val MSE: 0.3532
Gen 2971/8000  train MSE: 0.5870, val MSE: 0.3532
Gen 2972/8000  train MSE: 0.5870, val MSE: 0.3532
Gen 2973/8000  train MSE: 0.5870, val MSE: 0.3532
Gen 2974/8000  train MSE: 0.5869, val MSE: 0.3534
Gen 2975/8000  train MSE: 0.5869, val MSE: 0.3534
Gen 2976/8000  train MSE: 0.5869, val MSE: 0.3534
Gen 2977/8000  train MSE: 0.5869, val MSE: 0.3534
Gen 2978/8000  train MSE: 0.5869, val MSE: 0.3534
Gen 2979/8000  train MSE: 0.5869, val MSE: 0.3534
Gen 2980/8000  train MSE: 0.5869, val MSE: 0.3534
Gen 2981/8000  train MSE: 0.5869, val MSE: 0.3534
Gen 2982/8000  train MSE: 0.5869, val MSE: 0.3534
Gen 2983/8000  train MSE: 0.5869, val MSE: 0.3531
Gen 2984/8000  train MSE: 0.5869, val MSE: 0.3531
Gen 2985/8000  train MSE: 0.5869, val MSE: 0.3531
Gen 2986/8000  train MSE: 0.5869, val MSE: 0.3531
Gen 2987/8000  train MSE: 0.5869, val MSE: 0.3531
Gen 2988/8000  train MSE: 0.5868, val MSE: 0.3527
Gen 2989/8000  train MSE: 0.5868, val MSE: 0.3527
Gen 2990/8000  train MSE: 0.5868, val MSE: 0.3527
Gen 2991/8000  train MSE: 0.5868, val MSE: 0.3527
Gen 2992/8000  train MSE: 0.5868, val MSE: 0.3527
Gen 2993/8000  train MSE: 0.5868, val MSE: 0.3527
Gen 2994/8000  train MSE: 0.5868, val MSE: 0.3527
Gen 2995/8000  train MSE: 0.5868, val MSE: 0.3527
Gen 2996/8000  train MSE: 0.5868, val MSE: 0.3527
Gen 2997/8000  train MSE: 0.5868, val MSE: 0.3521
Gen 2998/8000  train MSE: 0.5868, val MSE: 0.3521
Gen 2999/8000  train MSE: 0.5868, val MSE: 0.3521
Gen 3000/8000  train MSE: 0.5868, val MSE: 0.3521
Gen 3001/8000  train MSE: 0.5868, val MSE: 0.3521
Gen 3002/8000  train MSE: 0.5868, val MSE: 0.3521
Gen 3003/8000  train MSE: 0.5868, val MSE: 0.3521
Gen 3004/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3005/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3006/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3007/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3008/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3009/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3010/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3011/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3012/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3013/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3014/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3015/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3016/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3017/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3018/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3019/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3020/8000  train MSE: 0.5867, val MSE: 0.3521
Gen 3021/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3022/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3023/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3024/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3025/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3026/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3027/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3028/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3029/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3030/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3031/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3032/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3033/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3034/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3035/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3036/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3037/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3038/8000  train MSE: 0.5867, val MSE: 0.3524
Gen 3039/8000  train MSE: 0.5867, val MSE: 0.3522
Gen 3040/8000  train MSE: 0.5867, val MSE: 0.3522
Gen 3041/8000  train MSE: 0.5867, val MSE: 0.3527
Gen 3042/8000  train MSE: 0.5867, val MSE: 0.3527
Gen 3043/8000  train MSE: 0.5867, val MSE: 0.3527
Gen 3044/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3045/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3046/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3047/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3048/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3049/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3050/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3051/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3052/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3053/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3054/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3055/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3056/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3057/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3058/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3059/8000  train MSE: 0.5867, val MSE: 0.3520
Gen 3060/8000  train MSE: 0.5866, val MSE: 0.3518
Gen 3061/8000  train MSE: 0.5866, val MSE: 0.3518
Gen 3062/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3063/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3064/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3065/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3066/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3067/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3068/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3069/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3070/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3071/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3072/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3073/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3074/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3075/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3076/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3077/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3078/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3079/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3080/8000  train MSE: 0.5865, val MSE: 0.3518
Gen 3081/8000  train MSE: 0.5865, val MSE: 0.3519
Gen 3082/8000  train MSE: 0.5865, val MSE: 0.3519
Gen 3083/8000  train MSE: 0.5865, val MSE: 0.3519
Gen 3084/8000  train MSE: 0.5865, val MSE: 0.3519
Gen 3085/8000  train MSE: 0.5865, val MSE: 0.3519
Gen 3086/8000  train MSE: 0.5865, val MSE: 0.3519
Gen 3087/8000  train MSE: 0.5865, val MSE: 0.3519
Gen 3088/8000  train MSE: 0.5865, val MSE: 0.3519
Gen 3089/8000  train MSE: 0.5865, val MSE: 0.3519
Gen 3090/8000  train MSE: 0.5865, val MSE: 0.3519
Gen 3091/8000  train MSE: 0.5864, val MSE: 0.3522
Gen 3092/8000  train MSE: 0.5864, val MSE: 0.3522
Gen 3093/8000  train MSE: 0.5864, val MSE: 0.3522
Gen 3094/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3095/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3096/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3097/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3098/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3099/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3100/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3101/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3102/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3103/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3104/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3105/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3106/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3107/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3108/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3109/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3110/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3111/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3112/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3113/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3114/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3115/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3116/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3117/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3118/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3119/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3120/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3121/8000  train MSE: 0.5864, val MSE: 0.3525
Gen 3122/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3123/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3124/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3125/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3126/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3127/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3128/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3129/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3130/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3131/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3132/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3133/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3134/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3135/8000  train MSE: 0.5864, val MSE: 0.3531
Gen 3136/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3137/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3138/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3139/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3140/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3141/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3142/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3143/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3144/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3145/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3146/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3147/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3148/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3149/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3150/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3151/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3152/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3153/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3154/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3155/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3156/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3157/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3158/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3159/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3160/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3161/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3162/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3163/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3164/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3165/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3166/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3167/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3168/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3169/8000  train MSE: 0.5863, val MSE: 0.3530
Gen 3170/8000  train MSE: 0.5862, val MSE: 0.3524
Gen 3171/8000  train MSE: 0.5862, val MSE: 0.3524
Gen 3172/8000  train MSE: 0.5862, val MSE: 0.3524
Gen 3173/8000  train MSE: 0.5862, val MSE: 0.3524
Gen 3174/8000  train MSE: 0.5862, val MSE: 0.3524
Gen 3175/8000  train MSE: 0.5862, val MSE: 0.3524
Gen 3176/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3177/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3178/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3179/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3180/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3181/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3182/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3183/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3184/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3185/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3186/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3187/8000  train MSE: 0.5862, val MSE: 0.3530
Gen 3188/8000  train MSE: 0.5862, val MSE: 0.3530
Gen 3189/8000  train MSE: 0.5862, val MSE: 0.3530
Gen 3190/8000  train MSE: 0.5862, val MSE: 0.3530
Gen 3191/8000  train MSE: 0.5862, val MSE: 0.3530
Gen 3192/8000  train MSE: 0.5862, val MSE: 0.3520
Gen 3193/8000  train MSE: 0.5862, val MSE: 0.3520
Gen 3194/8000  train MSE: 0.5862, val MSE: 0.3520
Gen 3195/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3196/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3197/8000  train MSE: 0.5862, val MSE: 0.3528
Gen 3198/8000  train MSE: 0.5862, val MSE: 0.3521
Gen 3199/8000  train MSE: 0.5862, val MSE: 0.3521
Gen 3200/8000  train MSE: 0.5862, val MSE: 0.3521
Gen 3201/8000  train MSE: 0.5862, val MSE: 0.3521
Gen 3202/8000  train MSE: 0.5862, val MSE: 0.3521
Gen 3203/8000  train MSE: 0.5862, val MSE: 0.3521
Gen 3204/8000  train MSE: 0.5862, val MSE: 0.3521
Gen 3205/8000  train MSE: 0.5862, val MSE: 0.3521
Gen 3206/8000  train MSE: 0.5862, val MSE: 0.3521
Gen 3207/8000  train MSE: 0.5862, val MSE: 0.3521
Gen 3208/8000  train MSE: 0.5862, val MSE: 0.3521
Gen 3209/8000  train MSE: 0.5861, val MSE: 0.3520
Gen 3210/8000  train MSE: 0.5861, val MSE: 0.3520
Gen 3211/8000  train MSE: 0.5861, val MSE: 0.3520
Gen 3212/8000  train MSE: 0.5861, val MSE: 0.3520
Gen 3213/8000  train MSE: 0.5861, val MSE: 0.3520
Gen 3214/8000  train MSE: 0.5861, val MSE: 0.3520
Gen 3215/8000  train MSE: 0.5861, val MSE: 0.3520
Gen 3216/8000  train MSE: 0.5861, val MSE: 0.3519
Gen 3217/8000  train MSE: 0.5861, val MSE: 0.3519
Gen 3218/8000  train MSE: 0.5861, val MSE: 0.3517
Gen 3219/8000  train MSE: 0.5861, val MSE: 0.3517
Gen 3220/8000  train MSE: 0.5861, val MSE: 0.3517
Gen 3221/8000  train MSE: 0.5861, val MSE: 0.3517
Gen 3222/8000  train MSE: 0.5861, val MSE: 0.3517
Gen 3223/8000  train MSE: 0.5861, val MSE: 0.3517
Gen 3224/8000  train MSE: 0.5861, val MSE: 0.3517
Gen 3225/8000  train MSE: 0.5861, val MSE: 0.3517
Gen 3226/8000  train MSE: 0.5860, val MSE: 0.3514
Gen 3227/8000  train MSE: 0.5860, val MSE: 0.3514
Gen 3228/8000  train MSE: 0.5860, val MSE: 0.3517
Gen 3229/8000  train MSE: 0.5860, val MSE: 0.3517
Gen 3230/8000  train MSE: 0.5860, val MSE: 0.3517
Gen 3231/8000  train MSE: 0.5860, val MSE: 0.3517
Gen 3232/8000  train MSE: 0.5860, val MSE: 0.3517
Gen 3233/8000  train MSE: 0.5860, val MSE: 0.3517
Gen 3234/8000  train MSE: 0.5860, val MSE: 0.3517
Gen 3235/8000  train MSE: 0.5860, val MSE: 0.3517
Gen 3236/8000  train MSE: 0.5860, val MSE: 0.3517
Gen 3237/8000  train MSE: 0.5860, val MSE: 0.3517
Gen 3238/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3239/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3240/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3241/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3242/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3243/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3244/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3245/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3246/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3247/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3248/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3249/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3250/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3251/8000  train MSE: 0.5860, val MSE: 0.3502
Gen 3252/8000  train MSE: 0.5859, val MSE: 0.3506
Gen 3253/8000  train MSE: 0.5859, val MSE: 0.3506
Gen 3254/8000  train MSE: 0.5859, val MSE: 0.3506
Gen 3255/8000  train MSE: 0.5859, val MSE: 0.3506
Gen 3256/8000  train MSE: 0.5859, val MSE: 0.3506
Gen 3257/8000  train MSE: 0.5859, val MSE: 0.3506
Gen 3258/8000  train MSE: 0.5859, val MSE: 0.3506
Gen 3259/8000  train MSE: 0.5859, val MSE: 0.3506
Gen 3260/8000  train MSE: 0.5859, val MSE: 0.3506
Gen 3261/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3262/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3263/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3264/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3265/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3266/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3267/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3268/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3269/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3270/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3271/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3272/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3273/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3274/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3275/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3276/8000  train MSE: 0.5859, val MSE: 0.3512
Gen 3277/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3278/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3279/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3280/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3281/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3282/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3283/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3284/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3285/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3286/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3287/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3288/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3289/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3290/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3291/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3292/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3293/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3294/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3295/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3296/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3297/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3298/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3299/8000  train MSE: 0.5858, val MSE: 0.3509
Gen 3300/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3301/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3302/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3303/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3304/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3305/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3306/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3307/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3308/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3309/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3310/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3311/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3312/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3313/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3314/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3315/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3316/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3317/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3318/8000  train MSE: 0.5856, val MSE: 0.3501
Gen 3319/8000  train MSE: 0.5856, val MSE: 0.3500
Gen 3320/8000  train MSE: 0.5856, val MSE: 0.3500
Gen 3321/8000  train MSE: 0.5856, val MSE: 0.3500
Gen 3322/8000  train MSE: 0.5856, val MSE: 0.3500
Gen 3323/8000  train MSE: 0.5856, val MSE: 0.3500
Gen 3324/8000  train MSE: 0.5856, val MSE: 0.3500
Gen 3325/8000  train MSE: 0.5856, val MSE: 0.3500
Gen 3326/8000  train MSE: 0.5856, val MSE: 0.3500
Gen 3327/8000  train MSE: 0.5856, val MSE: 0.3500
Gen 3328/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3329/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3330/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3331/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3332/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3333/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3334/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3335/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3336/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3337/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3338/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3339/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3340/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3341/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3342/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3343/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3344/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3345/8000  train MSE: 0.5855, val MSE: 0.3497
Gen 3346/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3347/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3348/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3349/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3350/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3351/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3352/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3353/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3354/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3355/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3356/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3357/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3358/8000  train MSE: 0.5855, val MSE: 0.3499
Gen 3359/8000  train MSE: 0.5855, val MSE: 0.3493
Gen 3360/8000  train MSE: 0.5854, val MSE: 0.3493
Gen 3361/8000  train MSE: 0.5854, val MSE: 0.3493
Gen 3362/8000  train MSE: 0.5854, val MSE: 0.3493
Gen 3363/8000  train MSE: 0.5854, val MSE: 0.3493
Gen 3364/8000  train MSE: 0.5854, val MSE: 0.3493
Gen 3365/8000  train MSE: 0.5854, val MSE: 0.3493
Gen 3366/8000  train MSE: 0.5854, val MSE: 0.3493
Gen 3367/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3368/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3369/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3370/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3371/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3372/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3373/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3374/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3375/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3376/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3377/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3378/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3379/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3380/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3381/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3382/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3383/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3384/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3385/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3386/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3387/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3388/8000  train MSE: 0.5854, val MSE: 0.3490
Gen 3389/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3390/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3391/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3392/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3393/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3394/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3395/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3396/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3397/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3398/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3399/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3400/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3401/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3402/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3403/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3404/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3405/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3406/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3407/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3408/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3409/8000  train MSE: 0.5853, val MSE: 0.3495
Gen 3410/8000  train MSE: 0.5853, val MSE: 0.3494
Gen 3411/8000  train MSE: 0.5853, val MSE: 0.3494
Gen 3412/8000  train MSE: 0.5853, val MSE: 0.3494
Gen 3413/8000  train MSE: 0.5853, val MSE: 0.3494
Gen 3414/8000  train MSE: 0.5853, val MSE: 0.3494
Gen 3415/8000  train MSE: 0.5853, val MSE: 0.3494
Gen 3416/8000  train MSE: 0.5853, val MSE: 0.3494
Gen 3417/8000  train MSE: 0.5853, val MSE: 0.3494
Gen 3418/8000  train MSE: 0.5853, val MSE: 0.3494
Gen 3419/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3420/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3421/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3422/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3423/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3424/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3425/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3426/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3427/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3428/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3429/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3430/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3431/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3432/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3433/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3434/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3435/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3436/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3437/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3438/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3439/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3440/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3441/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3442/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3443/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3444/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3445/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3446/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3447/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3448/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3449/8000  train MSE: 0.5852, val MSE: 0.3484
Gen 3450/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3451/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3452/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3453/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3454/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3455/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3456/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3457/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3458/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3459/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3460/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3461/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3462/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3463/8000  train MSE: 0.5852, val MSE: 0.3482
Gen 3464/8000  train MSE: 0.5851, val MSE: 0.3486
Gen 3465/8000  train MSE: 0.5851, val MSE: 0.3486
Gen 3466/8000  train MSE: 0.5851, val MSE: 0.3486
Gen 3467/8000  train MSE: 0.5851, val MSE: 0.3486
Gen 3468/8000  train MSE: 0.5851, val MSE: 0.3485
Gen 3469/8000  train MSE: 0.5851, val MSE: 0.3489
Gen 3470/8000  train MSE: 0.5851, val MSE: 0.3478
Gen 3471/8000  train MSE: 0.5851, val MSE: 0.3478
Gen 3472/8000  train MSE: 0.5851, val MSE: 0.3478
Gen 3473/8000  train MSE: 0.5851, val MSE: 0.3485
Gen 3474/8000  train MSE: 0.5850, val MSE: 0.3484
Gen 3475/8000  train MSE: 0.5850, val MSE: 0.3484
Gen 3476/8000  train MSE: 0.5850, val MSE: 0.3484
Gen 3477/8000  train MSE: 0.5850, val MSE: 0.3484
Gen 3478/8000  train MSE: 0.5850, val MSE: 0.3484
Gen 3479/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3480/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3481/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3482/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3483/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3484/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3485/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3486/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3487/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3488/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3489/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3490/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3491/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3492/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3493/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3494/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3495/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3496/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3497/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3498/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3499/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3500/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3501/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3502/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3503/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3504/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3505/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3506/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3507/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3508/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3509/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3510/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3511/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3512/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3513/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3514/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3515/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3516/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3517/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3518/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3519/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3520/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3521/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3522/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3523/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3524/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3525/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3526/8000  train MSE: 0.5850, val MSE: 0.3485
Gen 3527/8000  train MSE: 0.5850, val MSE: 0.3481
Gen 3528/8000  train MSE: 0.5850, val MSE: 0.3481
Gen 3529/8000  train MSE: 0.5850, val MSE: 0.3481
Gen 3530/8000  train MSE: 0.5850, val MSE: 0.3481
Gen 3531/8000  train MSE: 0.5850, val MSE: 0.3481
Gen 3532/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3533/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3534/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3535/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3536/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3537/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3538/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3539/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3540/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3541/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3542/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3543/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3544/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3545/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3546/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3547/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3548/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3549/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3550/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3551/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3552/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3553/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3554/8000  train MSE: 0.5849, val MSE: 0.3487
Gen 3555/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3556/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3557/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3558/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3559/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3560/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3561/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3562/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3563/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3564/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3565/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3566/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3567/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3568/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3569/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3570/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3571/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3572/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3573/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3574/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3575/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3576/8000  train MSE: 0.5849, val MSE: 0.3489
Gen 3577/8000  train MSE: 0.5849, val MSE: 0.3489
Gen 3578/8000  train MSE: 0.5849, val MSE: 0.3489
Gen 3579/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3580/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3581/8000  train MSE: 0.5849, val MSE: 0.3478
Gen 3582/8000  train MSE: 0.5849, val MSE: 0.3483
Gen 3583/8000  train MSE: 0.5849, val MSE: 0.3483
Gen 3584/8000  train MSE: 0.5849, val MSE: 0.3483
Gen 3585/8000  train MSE: 0.5849, val MSE: 0.3483
Gen 3586/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3587/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3588/8000  train MSE: 0.5849, val MSE: 0.3484
Gen 3589/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3590/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3591/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3592/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3593/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3594/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3595/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3596/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3597/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3598/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3599/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3600/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3601/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3602/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3603/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3604/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3605/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3606/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3607/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3608/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3609/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3610/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3611/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3612/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3613/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3614/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3615/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3616/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3617/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3618/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3619/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3620/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3621/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3622/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3623/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3624/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3625/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3626/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3627/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3628/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3629/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3630/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3631/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3632/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3633/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3634/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3635/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3636/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3637/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3638/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3639/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3640/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3641/8000  train MSE: 0.5848, val MSE: 0.3488
Gen 3642/8000  train MSE: 0.5848, val MSE: 0.3479
Gen 3643/8000  train MSE: 0.5848, val MSE: 0.3479
Gen 3644/8000  train MSE: 0.5848, val MSE: 0.3480
Gen 3645/8000  train MSE: 0.5848, val MSE: 0.3480
Gen 3646/8000  train MSE: 0.5848, val MSE: 0.3480
Gen 3647/8000  train MSE: 0.5848, val MSE: 0.3480
Gen 3648/8000  train MSE: 0.5848, val MSE: 0.3480
Gen 3649/8000  train MSE: 0.5848, val MSE: 0.3480
Gen 3650/8000  train MSE: 0.5848, val MSE: 0.3480
Gen 3651/8000  train MSE: 0.5848, val MSE: 0.3480
Gen 3652/8000  train MSE: 0.5848, val MSE: 0.3479
Gen 3653/8000  train MSE: 0.5848, val MSE: 0.3479
Gen 3654/8000  train MSE: 0.5847, val MSE: 0.3478
Gen 3655/8000  train MSE: 0.5847, val MSE: 0.3478
Gen 3656/8000  train MSE: 0.5847, val MSE: 0.3480
Gen 3657/8000  train MSE: 0.5847, val MSE: 0.3480
Gen 3658/8000  train MSE: 0.5847, val MSE: 0.3480
Gen 3659/8000  train MSE: 0.5847, val MSE: 0.3483
Gen 3660/8000  train MSE: 0.5847, val MSE: 0.3483
Gen 3661/8000  train MSE: 0.5847, val MSE: 0.3483
Gen 3662/8000  train MSE: 0.5847, val MSE: 0.3483
Gen 3663/8000  train MSE: 0.5847, val MSE: 0.3483
Gen 3664/8000  train MSE: 0.5847, val MSE: 0.3483
Gen 3665/8000  train MSE: 0.5847, val MSE: 0.3483
Gen 3666/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3667/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3668/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3669/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3670/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3671/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3672/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3673/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3674/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3675/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3676/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3677/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3678/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3679/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3680/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3681/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3682/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3683/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3684/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3685/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3686/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3687/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3688/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3689/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3690/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3691/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3692/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3693/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3694/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3695/8000  train MSE: 0.5847, val MSE: 0.3481
Gen 3696/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3697/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3698/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3699/8000  train MSE: 0.5847, val MSE: 0.3482
Gen 3700/8000  train MSE: 0.5846, val MSE: 0.3481
Gen 3701/8000  train MSE: 0.5846, val MSE: 0.3481
Gen 3702/8000  train MSE: 0.5846, val MSE: 0.3481
Gen 3703/8000  train MSE: 0.5846, val MSE: 0.3481
Gen 3704/8000  train MSE: 0.5846, val MSE: 0.3481
Gen 3705/8000  train MSE: 0.5846, val MSE: 0.3476
Gen 3706/8000  train MSE: 0.5846, val MSE: 0.3476
Gen 3707/8000  train MSE: 0.5846, val MSE: 0.3476
Gen 3708/8000  train MSE: 0.5846, val MSE: 0.3476
Gen 3709/8000  train MSE: 0.5846, val MSE: 0.3476
Gen 3710/8000  train MSE: 0.5846, val MSE: 0.3476
Gen 3711/8000  train MSE: 0.5846, val MSE: 0.3476
Gen 3712/8000  train MSE: 0.5846, val MSE: 0.3476
Gen 3713/8000  train MSE: 0.5846, val MSE: 0.3476
Gen 3714/8000  train MSE: 0.5846, val MSE: 0.3476
Gen 3715/8000  train MSE: 0.5846, val MSE: 0.3476
Gen 3716/8000  train MSE: 0.5846, val MSE: 0.3475
Gen 3717/8000  train MSE: 0.5846, val MSE: 0.3475
Gen 3718/8000  train MSE: 0.5846, val MSE: 0.3475
Gen 3719/8000  train MSE: 0.5846, val MSE: 0.3475
Gen 3720/8000  train MSE: 0.5846, val MSE: 0.3475
Gen 3721/8000  train MSE: 0.5846, val MSE: 0.3475
Gen 3722/8000  train MSE: 0.5846, val MSE: 0.3475
Gen 3723/8000  train MSE: 0.5846, val MSE: 0.3475
Gen 3724/8000  train MSE: 0.5846, val MSE: 0.3473
Gen 3725/8000  train MSE: 0.5846, val MSE: 0.3477
Gen 3726/8000  train MSE: 0.5846, val MSE: 0.3483
Gen 3727/8000  train MSE: 0.5846, val MSE: 0.3483
Gen 3728/8000  train MSE: 0.5846, val MSE: 0.3483
Gen 3729/8000  train MSE: 0.5846, val MSE: 0.3483
Gen 3730/8000  train MSE: 0.5846, val MSE: 0.3483
Gen 3731/8000  train MSE: 0.5846, val MSE: 0.3483
Gen 3732/8000  train MSE: 0.5846, val MSE: 0.3483
Gen 3733/8000  train MSE: 0.5846, val MSE: 0.3478
Gen 3734/8000  train MSE: 0.5846, val MSE: 0.3478
Gen 3735/8000  train MSE: 0.5846, val MSE: 0.3478
Gen 3736/8000  train MSE: 0.5846, val MSE: 0.3475
Gen 3737/8000  train MSE: 0.5846, val MSE: 0.3482
Gen 3738/8000  train MSE: 0.5846, val MSE: 0.3474
Gen 3739/8000  train MSE: 0.5846, val MSE: 0.3474
Gen 3740/8000  train MSE: 0.5845, val MSE: 0.3478
Gen 3741/8000  train MSE: 0.5845, val MSE: 0.3478
Gen 3742/8000  train MSE: 0.5845, val MSE: 0.3478
Gen 3743/8000  train MSE: 0.5845, val MSE: 0.3478
Gen 3744/8000  train MSE: 0.5845, val MSE: 0.3479
Gen 3745/8000  train MSE: 0.5845, val MSE: 0.3474
Gen 3746/8000  train MSE: 0.5845, val MSE: 0.3474
Gen 3747/8000  train MSE: 0.5845, val MSE: 0.3474
Gen 3748/8000  train MSE: 0.5845, val MSE: 0.3473
Gen 3749/8000  train MSE: 0.5845, val MSE: 0.3473
Gen 3750/8000  train MSE: 0.5845, val MSE: 0.3473
Gen 3751/8000  train MSE: 0.5845, val MSE: 0.3473
Gen 3752/8000  train MSE: 0.5844, val MSE: 0.3470
Gen 3753/8000  train MSE: 0.5844, val MSE: 0.3470
Gen 3754/8000  train MSE: 0.5844, val MSE: 0.3470
Gen 3755/8000  train MSE: 0.5844, val MSE: 0.3470
Gen 3756/8000  train MSE: 0.5844, val MSE: 0.3470
Gen 3757/8000  train MSE: 0.5844, val MSE: 0.3470
Gen 3758/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3759/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3760/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3761/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3762/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3763/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3764/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3765/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3766/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3767/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3768/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3769/8000  train MSE: 0.5844, val MSE: 0.3472
Gen 3770/8000  train MSE: 0.5844, val MSE: 0.3475
Gen 3771/8000  train MSE: 0.5844, val MSE: 0.3475
Gen 3772/8000  train MSE: 0.5844, val MSE: 0.3467
Gen 3773/8000  train MSE: 0.5844, val MSE: 0.3467
Gen 3774/8000  train MSE: 0.5844, val MSE: 0.3467
Gen 3775/8000  train MSE: 0.5844, val MSE: 0.3467
Gen 3776/8000  train MSE: 0.5844, val MSE: 0.3467
Gen 3777/8000  train MSE: 0.5844, val MSE: 0.3467
Gen 3778/8000  train MSE: 0.5843, val MSE: 0.3475
Gen 3779/8000  train MSE: 0.5843, val MSE: 0.3475
Gen 3780/8000  train MSE: 0.5843, val MSE: 0.3475
Gen 3781/8000  train MSE: 0.5843, val MSE: 0.3475
Gen 3782/8000  train MSE: 0.5843, val MSE: 0.3475
Gen 3783/8000  train MSE: 0.5843, val MSE: 0.3475
Gen 3784/8000  train MSE: 0.5843, val MSE: 0.3477
Gen 3785/8000  train MSE: 0.5843, val MSE: 0.3477
Gen 3786/8000  train MSE: 0.5843, val MSE: 0.3477
Gen 3787/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3788/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3789/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3790/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3791/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3792/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3793/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3794/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3795/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3796/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3797/8000  train MSE: 0.5843, val MSE: 0.3478
Gen 3798/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3799/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3800/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3801/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3802/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3803/8000  train MSE: 0.5843, val MSE: 0.3474
Gen 3804/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3805/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3806/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3807/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3808/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3809/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3810/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3811/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3812/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3813/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3814/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3815/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3816/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3817/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3818/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3819/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3820/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3821/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3822/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3823/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3824/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3825/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3826/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3827/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3828/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3829/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3830/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3831/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3832/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3833/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3834/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3835/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3836/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3837/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3838/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3839/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3840/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3841/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3842/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3843/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3844/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3845/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3846/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3847/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3848/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3849/8000  train MSE: 0.5842, val MSE: 0.3481
Gen 3850/8000  train MSE: 0.5841, val MSE: 0.3476
Gen 3851/8000  train MSE: 0.5841, val MSE: 0.3476
Gen 3852/8000  train MSE: 0.5841, val MSE: 0.3476
Gen 3853/8000  train MSE: 0.5841, val MSE: 0.3476
Gen 3854/8000  train MSE: 0.5841, val MSE: 0.3476
Gen 3855/8000  train MSE: 0.5841, val MSE: 0.3476
Gen 3856/8000  train MSE: 0.5841, val MSE: 0.3476
Gen 3857/8000  train MSE: 0.5841, val MSE: 0.3476
Gen 3858/8000  train MSE: 0.5841, val MSE: 0.3476
Gen 3859/8000  train MSE: 0.5841, val MSE: 0.3476
Gen 3860/8000  train MSE: 0.5841, val MSE: 0.3477
Gen 3861/8000  train MSE: 0.5840, val MSE: 0.3472
Gen 3862/8000  train MSE: 0.5840, val MSE: 0.3472
Gen 3863/8000  train MSE: 0.5840, val MSE: 0.3472
Gen 3864/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3865/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3866/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3867/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3868/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3869/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3870/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3871/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3872/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3873/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3874/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3875/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3876/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3877/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3878/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3879/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3880/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3881/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3882/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3883/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3884/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3885/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3886/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3887/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3888/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3889/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3890/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3891/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3892/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3893/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3894/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3895/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3896/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3897/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3898/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3899/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3900/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3901/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3902/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3903/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3904/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3905/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3906/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3907/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3908/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3909/8000  train MSE: 0.5840, val MSE: 0.3476
Gen 3910/8000  train MSE: 0.5840, val MSE: 0.3477
Gen 3911/8000  train MSE: 0.5840, val MSE: 0.3477
Gen 3912/8000  train MSE: 0.5840, val MSE: 0.3477
Gen 3913/8000  train MSE: 0.5840, val MSE: 0.3477
Gen 3914/8000  train MSE: 0.5839, val MSE: 0.3474
Gen 3915/8000  train MSE: 0.5839, val MSE: 0.3474
Gen 3916/8000  train MSE: 0.5839, val MSE: 0.3478
Gen 3917/8000  train MSE: 0.5839, val MSE: 0.3478
Gen 3918/8000  train MSE: 0.5839, val MSE: 0.3473
Gen 3919/8000  train MSE: 0.5839, val MSE: 0.3473
Gen 3920/8000  train MSE: 0.5839, val MSE: 0.3473
Gen 3921/8000  train MSE: 0.5839, val MSE: 0.3473
Gen 3922/8000  train MSE: 0.5839, val MSE: 0.3473
Gen 3923/8000  train MSE: 0.5839, val MSE: 0.3473
Gen 3924/8000  train MSE: 0.5839, val MSE: 0.3473
Gen 3925/8000  train MSE: 0.5839, val MSE: 0.3473
Gen 3926/8000  train MSE: 0.5839, val MSE: 0.3473
Gen 3927/8000  train MSE: 0.5838, val MSE: 0.3468
Gen 3928/8000  train MSE: 0.5838, val MSE: 0.3468
Gen 3929/8000  train MSE: 0.5838, val MSE: 0.3468
Gen 3930/8000  train MSE: 0.5838, val MSE: 0.3468
Gen 3931/8000  train MSE: 0.5838, val MSE: 0.3468
Gen 3932/8000  train MSE: 0.5838, val MSE: 0.3471
Gen 3933/8000  train MSE: 0.5838, val MSE: 0.3471
Gen 3934/8000  train MSE: 0.5838, val MSE: 0.3467
Gen 3935/8000  train MSE: 0.5838, val MSE: 0.3467
Gen 3936/8000  train MSE: 0.5838, val MSE: 0.3467
Gen 3937/8000  train MSE: 0.5838, val MSE: 0.3467
Gen 3938/8000  train MSE: 0.5838, val MSE: 0.3467
Gen 3939/8000  train MSE: 0.5838, val MSE: 0.3467
Gen 3940/8000  train MSE: 0.5838, val MSE: 0.3467
Gen 3941/8000  train MSE: 0.5838, val MSE: 0.3469
Gen 3942/8000  train MSE: 0.5838, val MSE: 0.3469
Gen 3943/8000  train MSE: 0.5838, val MSE: 0.3469
Gen 3944/8000  train MSE: 0.5838, val MSE: 0.3469
Gen 3945/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3946/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3947/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3948/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3949/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3950/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3951/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3952/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3953/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3954/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3955/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3956/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3957/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3958/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3959/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3960/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3961/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3962/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3963/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3964/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3965/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3966/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3967/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3968/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3969/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3970/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3971/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3972/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3973/8000  train MSE: 0.5837, val MSE: 0.3473
Gen 3974/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3975/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3976/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3977/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3978/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3979/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3980/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3981/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3982/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3983/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3984/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3985/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3986/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3987/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3988/8000  train MSE: 0.5837, val MSE: 0.3466
Gen 3989/8000  train MSE: 0.5837, val MSE: 0.3464
Gen 3990/8000  train MSE: 0.5837, val MSE: 0.3464
Gen 3991/8000  train MSE: 0.5837, val MSE: 0.3464
Gen 3992/8000  train MSE: 0.5837, val MSE: 0.3464
Gen 3993/8000  train MSE: 0.5837, val MSE: 0.3464
Gen 3994/8000  train MSE: 0.5837, val MSE: 0.3463
Gen 3995/8000  train MSE: 0.5837, val MSE: 0.3463
Gen 3996/8000  train MSE: 0.5837, val MSE: 0.3463
Gen 3997/8000  train MSE: 0.5837, val MSE: 0.3463
Gen 3998/8000  train MSE: 0.5837, val MSE: 0.3467
Gen 3999/8000  train MSE: 0.5837, val MSE: 0.3467
Gen 4000/8000  train MSE: 0.5837, val MSE: 0.3467
Gen 4001/8000  train MSE: 0.5837, val MSE: 0.3468
Gen 4002/8000  train MSE: 0.5837, val MSE: 0.3468
Gen 4003/8000  train MSE: 0.5837, val MSE: 0.3468
Gen 4004/8000  train MSE: 0.5837, val MSE: 0.3468
Gen 4005/8000  train MSE: 0.5837, val MSE: 0.3468
Gen 4006/8000  train MSE: 0.5837, val MSE: 0.3468
Gen 4007/8000  train MSE: 0.5836, val MSE: 0.3462
Gen 4008/8000  train MSE: 0.5836, val MSE: 0.3462
Gen 4009/8000  train MSE: 0.5836, val MSE: 0.3462
Gen 4010/8000  train MSE: 0.5836, val MSE: 0.3459
Gen 4011/8000  train MSE: 0.5836, val MSE: 0.3459
Gen 4012/8000  train MSE: 0.5836, val MSE: 0.3459
Gen 4013/8000  train MSE: 0.5836, val MSE: 0.3459
Gen 4014/8000  train MSE: 0.5836, val MSE: 0.3459
Gen 4015/8000  train MSE: 0.5836, val MSE: 0.3457
Gen 4016/8000  train MSE: 0.5836, val MSE: 0.3457
Gen 4017/8000  train MSE: 0.5836, val MSE: 0.3457
Gen 4018/8000  train MSE: 0.5836, val MSE: 0.3458
Gen 4019/8000  train MSE: 0.5836, val MSE: 0.3458
Gen 4020/8000  train MSE: 0.5836, val MSE: 0.3458
Gen 4021/8000  train MSE: 0.5836, val MSE: 0.3458
Gen 4022/8000  train MSE: 0.5836, val MSE: 0.3458
Gen 4023/8000  train MSE: 0.5836, val MSE: 0.3458
Gen 4024/8000  train MSE: 0.5836, val MSE: 0.3452
Gen 4025/8000  train MSE: 0.5836, val MSE: 0.3452
Gen 4026/8000  train MSE: 0.5836, val MSE: 0.3452
Gen 4027/8000  train MSE: 0.5835, val MSE: 0.3458
Gen 4028/8000  train MSE: 0.5835, val MSE: 0.3458
Gen 4029/8000  train MSE: 0.5835, val MSE: 0.3458
Gen 4030/8000  train MSE: 0.5835, val MSE: 0.3458
Gen 4031/8000  train MSE: 0.5835, val MSE: 0.3458
Gen 4032/8000  train MSE: 0.5835, val MSE: 0.3458
Gen 4033/8000  train MSE: 0.5835, val MSE: 0.3454
Gen 4034/8000  train MSE: 0.5835, val MSE: 0.3454
Gen 4035/8000  train MSE: 0.5835, val MSE: 0.3454
Gen 4036/8000  train MSE: 0.5835, val MSE: 0.3454
Gen 4037/8000  train MSE: 0.5835, val MSE: 0.3454
Gen 4038/8000  train MSE: 0.5835, val MSE: 0.3454
Gen 4039/8000  train MSE: 0.5835, val MSE: 0.3454
Gen 4040/8000  train MSE: 0.5835, val MSE: 0.3454
Gen 4041/8000  train MSE: 0.5834, val MSE: 0.3454
Gen 4042/8000  train MSE: 0.5834, val MSE: 0.3454
Gen 4043/8000  train MSE: 0.5834, val MSE: 0.3454
Gen 4044/8000  train MSE: 0.5834, val MSE: 0.3454
Gen 4045/8000  train MSE: 0.5834, val MSE: 0.3455
Gen 4046/8000  train MSE: 0.5834, val MSE: 0.3455
Gen 4047/8000  train MSE: 0.5834, val MSE: 0.3455
Gen 4048/8000  train MSE: 0.5834, val MSE: 0.3455
Gen 4049/8000  train MSE: 0.5834, val MSE: 0.3451
Gen 4050/8000  train MSE: 0.5834, val MSE: 0.3451
Gen 4051/8000  train MSE: 0.5834, val MSE: 0.3451
Gen 4052/8000  train MSE: 0.5834, val MSE: 0.3451
Gen 4053/8000  train MSE: 0.5834, val MSE: 0.3451
Gen 4054/8000  train MSE: 0.5834, val MSE: 0.3451
Gen 4055/8000  train MSE: 0.5833, val MSE: 0.3454
Gen 4056/8000  train MSE: 0.5833, val MSE: 0.3454
Gen 4057/8000  train MSE: 0.5833, val MSE: 0.3454
Gen 4058/8000  train MSE: 0.5833, val MSE: 0.3454
Gen 4059/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4060/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4061/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4062/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4063/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4064/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4065/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4066/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4067/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4068/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4069/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4070/8000  train MSE: 0.5833, val MSE: 0.3455
Gen 4071/8000  train MSE: 0.5833, val MSE: 0.3452
Gen 4072/8000  train MSE: 0.5833, val MSE: 0.3452
Gen 4073/8000  train MSE: 0.5833, val MSE: 0.3452
Gen 4074/8000  train MSE: 0.5833, val MSE: 0.3452
Gen 4075/8000  train MSE: 0.5833, val MSE: 0.3452
Gen 4076/8000  train MSE: 0.5833, val MSE: 0.3452
Gen 4077/8000  train MSE: 0.5833, val MSE: 0.3452
Gen 4078/8000  train MSE: 0.5832, val MSE: 0.3445
Gen 4079/8000  train MSE: 0.5832, val MSE: 0.3445
Gen 4080/8000  train MSE: 0.5832, val MSE: 0.3445
Gen 4081/8000  train MSE: 0.5832, val MSE: 0.3445
Gen 4082/8000  train MSE: 0.5832, val MSE: 0.3445
Gen 4083/8000  train MSE: 0.5832, val MSE: 0.3445
Gen 4084/8000  train MSE: 0.5832, val MSE: 0.3445
Gen 4085/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4086/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4087/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4088/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4089/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4090/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4091/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4092/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4093/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4094/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4095/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4096/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4097/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4098/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4099/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4100/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4101/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4102/8000  train MSE: 0.5832, val MSE: 0.3449
Gen 4103/8000  train MSE: 0.5832, val MSE: 0.3450
Gen 4104/8000  train MSE: 0.5832, val MSE: 0.3450
Gen 4105/8000  train MSE: 0.5832, val MSE: 0.3450
Gen 4106/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4107/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4108/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4109/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4110/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4111/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4112/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4113/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4114/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4115/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4116/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4117/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4118/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4119/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4120/8000  train MSE: 0.5831, val MSE: 0.3455
Gen 4121/8000  train MSE: 0.5831, val MSE: 0.3454
Gen 4122/8000  train MSE: 0.5831, val MSE: 0.3454
Gen 4123/8000  train MSE: 0.5831, val MSE: 0.3454
Gen 4124/8000  train MSE: 0.5831, val MSE: 0.3454
Gen 4125/8000  train MSE: 0.5831, val MSE: 0.3454
Gen 4126/8000  train MSE: 0.5831, val MSE: 0.3454
Gen 4127/8000  train MSE: 0.5831, val MSE: 0.3454
Gen 4128/8000  train MSE: 0.5830, val MSE: 0.3440
Gen 4129/8000  train MSE: 0.5830, val MSE: 0.3440
Gen 4130/8000  train MSE: 0.5830, val MSE: 0.3440
Gen 4131/8000  train MSE: 0.5830, val MSE: 0.3440
Gen 4132/8000  train MSE: 0.5830, val MSE: 0.3440
Gen 4133/8000  train MSE: 0.5830, val MSE: 0.3440
Gen 4134/8000  train MSE: 0.5830, val MSE: 0.3440
Gen 4135/8000  train MSE: 0.5830, val MSE: 0.3448
Gen 4136/8000  train MSE: 0.5830, val MSE: 0.3448
Gen 4137/8000  train MSE: 0.5830, val MSE: 0.3448
Gen 4138/8000  train MSE: 0.5830, val MSE: 0.3448
Gen 4139/8000  train MSE: 0.5830, val MSE: 0.3448
Gen 4140/8000  train MSE: 0.5830, val MSE: 0.3448
Gen 4141/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4142/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4143/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4144/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4145/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4146/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4147/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4148/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4149/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4150/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4151/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4152/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4153/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4154/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4155/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4156/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4157/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4158/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4159/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4160/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4161/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4162/8000  train MSE: 0.5830, val MSE: 0.3458
Gen 4163/8000  train MSE: 0.5830, val MSE: 0.3453
Gen 4164/8000  train MSE: 0.5830, val MSE: 0.3453
Gen 4165/8000  train MSE: 0.5830, val MSE: 0.3453
Gen 4166/8000  train MSE: 0.5830, val MSE: 0.3453
Gen 4167/8000  train MSE: 0.5830, val MSE: 0.3453
Gen 4168/8000  train MSE: 0.5830, val MSE: 0.3453
Gen 4169/8000  train MSE: 0.5830, val MSE: 0.3453
Gen 4170/8000  train MSE: 0.5830, val MSE: 0.3453
Gen 4171/8000  train MSE: 0.5830, val MSE: 0.3453
Gen 4172/8000  train MSE: 0.5830, val MSE: 0.3453
Gen 4173/8000  train MSE: 0.5830, val MSE: 0.3457
Gen 4174/8000  train MSE: 0.5830, val MSE: 0.3457
Gen 4175/8000  train MSE: 0.5830, val MSE: 0.3457
Gen 4176/8000  train MSE: 0.5830, val MSE: 0.3457
Gen 4177/8000  train MSE: 0.5830, val MSE: 0.3454
Gen 4178/8000  train MSE: 0.5830, val MSE: 0.3454
Gen 4179/8000  train MSE: 0.5830, val MSE: 0.3454
Gen 4180/8000  train MSE: 0.5830, val MSE: 0.3452
Gen 4181/8000  train MSE: 0.5830, val MSE: 0.3452
Gen 4182/8000  train MSE: 0.5830, val MSE: 0.3452
Gen 4183/8000  train MSE: 0.5830, val MSE: 0.3452
Gen 4184/8000  train MSE: 0.5829, val MSE: 0.3453
Gen 4185/8000  train MSE: 0.5829, val MSE: 0.3453
Gen 4186/8000  train MSE: 0.5829, val MSE: 0.3453
Gen 4187/8000  train MSE: 0.5829, val MSE: 0.3453
Gen 4188/8000  train MSE: 0.5829, val MSE: 0.3453
Gen 4189/8000  train MSE: 0.5829, val MSE: 0.3453
Gen 4190/8000  train MSE: 0.5829, val MSE: 0.3453
Gen 4191/8000  train MSE: 0.5829, val MSE: 0.3453
Gen 4192/8000  train MSE: 0.5829, val MSE: 0.3460
Gen 4193/8000  train MSE: 0.5829, val MSE: 0.3460
Gen 4194/8000  train MSE: 0.5829, val MSE: 0.3460
Gen 4195/8000  train MSE: 0.5829, val MSE: 0.3448
Gen 4196/8000  train MSE: 0.5829, val MSE: 0.3448
Gen 4197/8000  train MSE: 0.5829, val MSE: 0.3448
Gen 4198/8000  train MSE: 0.5829, val MSE: 0.3456
Gen 4199/8000  train MSE: 0.5829, val MSE: 0.3456
Gen 4200/8000  train MSE: 0.5829, val MSE: 0.3450
Gen 4201/8000  train MSE: 0.5829, val MSE: 0.3450
Gen 4202/8000  train MSE: 0.5829, val MSE: 0.3450
Gen 4203/8000  train MSE: 0.5829, val MSE: 0.3455
Gen 4204/8000  train MSE: 0.5829, val MSE: 0.3455
Gen 4205/8000  train MSE: 0.5828, val MSE: 0.3451
Gen 4206/8000  train MSE: 0.5828, val MSE: 0.3451
Gen 4207/8000  train MSE: 0.5828, val MSE: 0.3451
Gen 4208/8000  train MSE: 0.5828, val MSE: 0.3451
Gen 4209/8000  train MSE: 0.5828, val MSE: 0.3451
Gen 4210/8000  train MSE: 0.5828, val MSE: 0.3451
Gen 4211/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4212/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4213/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4214/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4215/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4216/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4217/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4218/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4219/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4220/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4221/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4222/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4223/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4224/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4225/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4226/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4227/8000  train MSE: 0.5828, val MSE: 0.3450
Gen 4228/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4229/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4230/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4231/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4232/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4233/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4234/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4235/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4236/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4237/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4238/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4239/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4240/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4241/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4242/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4243/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4244/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4245/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4246/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4247/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4248/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4249/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4250/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4251/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4252/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4253/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4254/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4255/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4256/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4257/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4258/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4259/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4260/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4261/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4262/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4263/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4264/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4265/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4266/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4267/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4268/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4269/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4270/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4271/8000  train MSE: 0.5828, val MSE: 0.3452
Gen 4272/8000  train MSE: 0.5828, val MSE: 0.3465
Gen 4273/8000  train MSE: 0.5828, val MSE: 0.3465
Gen 4274/8000  train MSE: 0.5828, val MSE: 0.3465
Gen 4275/8000  train MSE: 0.5828, val MSE: 0.3465
Gen 4276/8000  train MSE: 0.5828, val MSE: 0.3465
Gen 4277/8000  train MSE: 0.5828, val MSE: 0.3465
Gen 4278/8000  train MSE: 0.5828, val MSE: 0.3465
Gen 4279/8000  train MSE: 0.5828, val MSE: 0.3465
Gen 4280/8000  train MSE: 0.5827, val MSE: 0.3453
Gen 4281/8000  train MSE: 0.5827, val MSE: 0.3453
Gen 4282/8000  train MSE: 0.5827, val MSE: 0.3453
Gen 4283/8000  train MSE: 0.5827, val MSE: 0.3453
Gen 4284/8000  train MSE: 0.5827, val MSE: 0.3453
Gen 4285/8000  train MSE: 0.5827, val MSE: 0.3453
Gen 4286/8000  train MSE: 0.5827, val MSE: 0.3453
Gen 4287/8000  train MSE: 0.5827, val MSE: 0.3453
Gen 4288/8000  train MSE: 0.5827, val MSE: 0.3461
Gen 4289/8000  train MSE: 0.5827, val MSE: 0.3461
Gen 4290/8000  train MSE: 0.5827, val MSE: 0.3461
Gen 4291/8000  train MSE: 0.5827, val MSE: 0.3455
Gen 4292/8000  train MSE: 0.5827, val MSE: 0.3455
Gen 4293/8000  train MSE: 0.5827, val MSE: 0.3455
Gen 4294/8000  train MSE: 0.5827, val MSE: 0.3455
Gen 4295/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4296/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4297/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4298/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4299/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4300/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4301/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4302/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4303/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4304/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4305/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4306/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4307/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4308/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4309/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4310/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4311/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4312/8000  train MSE: 0.5827, val MSE: 0.3456
Gen 4313/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4314/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4315/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4316/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4317/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4318/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4319/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4320/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4321/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4322/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4323/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4324/8000  train MSE: 0.5826, val MSE: 0.3460
Gen 4325/8000  train MSE: 0.5826, val MSE: 0.3460
Gen 4326/8000  train MSE: 0.5826, val MSE: 0.3460
Gen 4327/8000  train MSE: 0.5826, val MSE: 0.3460
Gen 4328/8000  train MSE: 0.5826, val MSE: 0.3460
Gen 4329/8000  train MSE: 0.5826, val MSE: 0.3460
Gen 4330/8000  train MSE: 0.5826, val MSE: 0.3460
Gen 4331/8000  train MSE: 0.5826, val MSE: 0.3460
Gen 4332/8000  train MSE: 0.5826, val MSE: 0.3460
Gen 4333/8000  train MSE: 0.5826, val MSE: 0.3460
Gen 4334/8000  train MSE: 0.5826, val MSE: 0.3460
Gen 4335/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4336/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4337/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4338/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4339/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4340/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4341/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4342/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4343/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4344/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4345/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4346/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4347/8000  train MSE: 0.5826, val MSE: 0.3459
Gen 4348/8000  train MSE: 0.5826, val MSE: 0.3458
Gen 4349/8000  train MSE: 0.5826, val MSE: 0.3458
Gen 4350/8000  train MSE: 0.5826, val MSE: 0.3458
Gen 4351/8000  train MSE: 0.5826, val MSE: 0.3458
Gen 4352/8000  train MSE: 0.5826, val MSE: 0.3458
Gen 4353/8000  train MSE: 0.5826, val MSE: 0.3458
Gen 4354/8000  train MSE: 0.5826, val MSE: 0.3458
Gen 4355/8000  train MSE: 0.5826, val MSE: 0.3458
Gen 4356/8000  train MSE: 0.5826, val MSE: 0.3458
Gen 4357/8000  train MSE: 0.5826, val MSE: 0.3458
Gen 4358/8000  train MSE: 0.5826, val MSE: 0.3458
Gen 4359/8000  train MSE: 0.5825, val MSE: 0.3457
Gen 4360/8000  train MSE: 0.5825, val MSE: 0.3457
Gen 4361/8000  train MSE: 0.5825, val MSE: 0.3457
Gen 4362/8000  train MSE: 0.5825, val MSE: 0.3457
Gen 4363/8000  train MSE: 0.5825, val MSE: 0.3457
Gen 4364/8000  train MSE: 0.5825, val MSE: 0.3457
Gen 4365/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4366/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4367/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4368/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4369/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4370/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4371/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4372/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4373/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4374/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4375/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4376/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4377/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4378/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4379/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4380/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4381/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4382/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4383/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4384/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4385/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4386/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4387/8000  train MSE: 0.5825, val MSE: 0.3455
Gen 4388/8000  train MSE: 0.5825, val MSE: 0.3458
Gen 4389/8000  train MSE: 0.5825, val MSE: 0.3458
Gen 4390/8000  train MSE: 0.5825, val MSE: 0.3458
Gen 4391/8000  train MSE: 0.5825, val MSE: 0.3458
Gen 4392/8000  train MSE: 0.5825, val MSE: 0.3458
Gen 4393/8000  train MSE: 0.5825, val MSE: 0.3458
Gen 4394/8000  train MSE: 0.5825, val MSE: 0.3458
Gen 4395/8000  train MSE: 0.5825, val MSE: 0.3458
Gen 4396/8000  train MSE: 0.5825, val MSE: 0.3458
Gen 4397/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4398/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4399/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4400/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4401/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4402/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4403/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4404/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4405/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4406/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4407/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4408/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4409/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4410/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4411/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4412/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4413/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4414/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4415/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4416/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4417/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4418/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4419/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4420/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4421/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4422/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4423/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4424/8000  train MSE: 0.5825, val MSE: 0.3446
Gen 4425/8000  train MSE: 0.5825, val MSE: 0.3459
Gen 4426/8000  train MSE: 0.5825, val MSE: 0.3459
Gen 4427/8000  train MSE: 0.5825, val MSE: 0.3459
Gen 4428/8000  train MSE: 0.5825, val MSE: 0.3459
Gen 4429/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4430/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4431/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4432/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4433/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4434/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4435/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4436/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4437/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4438/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4439/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4440/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4441/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4442/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4443/8000  train MSE: 0.5824, val MSE: 0.3457
Gen 4444/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4445/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4446/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4447/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4448/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4449/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4450/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4451/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4452/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4453/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4454/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4455/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4456/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4457/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4458/8000  train MSE: 0.5824, val MSE: 0.3459
Gen 4459/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4460/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4461/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4462/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4463/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4464/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4465/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4466/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4467/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4468/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4469/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4470/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4471/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4472/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4473/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4474/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4475/8000  train MSE: 0.5824, val MSE: 0.3454
Gen 4476/8000  train MSE: 0.5823, val MSE: 0.3458
Gen 4477/8000  train MSE: 0.5823, val MSE: 0.3458
Gen 4478/8000  train MSE: 0.5823, val MSE: 0.3458
Gen 4479/8000  train MSE: 0.5823, val MSE: 0.3458
Gen 4480/8000  train MSE: 0.5823, val MSE: 0.3458
Gen 4481/8000  train MSE: 0.5823, val MSE: 0.3458
Gen 4482/8000  train MSE: 0.5823, val MSE: 0.3458
Gen 4483/8000  train MSE: 0.5823, val MSE: 0.3458
Gen 4484/8000  train MSE: 0.5823, val MSE: 0.3458
Gen 4485/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4486/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4487/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4488/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4489/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4490/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4491/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4492/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4493/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4494/8000  train MSE: 0.5823, val MSE: 0.3459
Gen 4495/8000  train MSE: 0.5823, val MSE: 0.3459
Gen 4496/8000  train MSE: 0.5823, val MSE: 0.3459
Gen 4497/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4498/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4499/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4500/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4501/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4502/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4503/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4504/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4505/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4506/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4507/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4508/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4509/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4510/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4511/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4512/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4513/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4514/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4515/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4516/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4517/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4518/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4519/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4520/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4521/8000  train MSE: 0.5823, val MSE: 0.3457
Gen 4522/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4523/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4524/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4525/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4526/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4527/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4528/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4529/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4530/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4531/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4532/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4533/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4534/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4535/8000  train MSE: 0.5823, val MSE: 0.3460
Gen 4536/8000  train MSE: 0.5823, val MSE: 0.3454
Gen 4537/8000  train MSE: 0.5823, val MSE: 0.3454
Gen 4538/8000  train MSE: 0.5823, val MSE: 0.3454
Gen 4539/8000  train MSE: 0.5823, val MSE: 0.3454
Gen 4540/8000  train MSE: 0.5823, val MSE: 0.3454
Gen 4541/8000  train MSE: 0.5823, val MSE: 0.3454
Gen 4542/8000  train MSE: 0.5823, val MSE: 0.3454
Gen 4543/8000  train MSE: 0.5823, val MSE: 0.3454
Gen 4544/8000  train MSE: 0.5823, val MSE: 0.3454
Gen 4545/8000  train MSE: 0.5823, val MSE: 0.3456
Gen 4546/8000  train MSE: 0.5823, val MSE: 0.3456
Gen 4547/8000  train MSE: 0.5823, val MSE: 0.3456
Gen 4548/8000  train MSE: 0.5823, val MSE: 0.3458
Gen 4549/8000  train MSE: 0.5823, val MSE: 0.3458
Gen 4550/8000  train MSE: 0.5822, val MSE: 0.3456
Gen 4551/8000  train MSE: 0.5822, val MSE: 0.3456
Gen 4552/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4553/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4554/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4555/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4556/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4557/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4558/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4559/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4560/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4561/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4562/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4563/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4564/8000  train MSE: 0.5822, val MSE: 0.3458
Gen 4565/8000  train MSE: 0.5822, val MSE: 0.3458
Gen 4566/8000  train MSE: 0.5822, val MSE: 0.3458
Gen 4567/8000  train MSE: 0.5822, val MSE: 0.3458
Gen 4568/8000  train MSE: 0.5822, val MSE: 0.3458
Gen 4569/8000  train MSE: 0.5822, val MSE: 0.3458
Gen 4570/8000  train MSE: 0.5822, val MSE: 0.3458
Gen 4571/8000  train MSE: 0.5822, val MSE: 0.3458
Gen 4572/8000  train MSE: 0.5822, val MSE: 0.3459
Gen 4573/8000  train MSE: 0.5822, val MSE: 0.3459
Gen 4574/8000  train MSE: 0.5822, val MSE: 0.3459
Gen 4575/8000  train MSE: 0.5822, val MSE: 0.3459
Gen 4576/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4577/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4578/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4579/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4580/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4581/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4582/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4583/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4584/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4585/8000  train MSE: 0.5822, val MSE: 0.3461
Gen 4586/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4587/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4588/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4589/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4590/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4591/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4592/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4593/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4594/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4595/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4596/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4597/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4598/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4599/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4600/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4601/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4602/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4603/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4604/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4605/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4606/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4607/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4608/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4609/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4610/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4611/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4612/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4613/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4614/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4615/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4616/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4617/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4618/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4619/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4620/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4621/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4622/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4623/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4624/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4625/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4626/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4627/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4628/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4629/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4630/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4631/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4632/8000  train MSE: 0.5821, val MSE: 0.3451
Gen 4633/8000  train MSE: 0.5821, val MSE: 0.3450
Gen 4634/8000  train MSE: 0.5821, val MSE: 0.3450
Gen 4635/8000  train MSE: 0.5821, val MSE: 0.3450
Gen 4636/8000  train MSE: 0.5821, val MSE: 0.3450
Gen 4637/8000  train MSE: 0.5821, val MSE: 0.3450
Gen 4638/8000  train MSE: 0.5821, val MSE: 0.3450
Gen 4639/8000  train MSE: 0.5821, val MSE: 0.3450
Gen 4640/8000  train MSE: 0.5821, val MSE: 0.3450
Gen 4641/8000  train MSE: 0.5821, val MSE: 0.3450
Gen 4642/8000  train MSE: 0.5821, val MSE: 0.3450
Gen 4643/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4644/8000  train MSE: 0.5821, val MSE: 0.3457
Gen 4645/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4646/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4647/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4648/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4649/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4650/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4651/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4652/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4653/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4654/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4655/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4656/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4657/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4658/8000  train MSE: 0.5820, val MSE: 0.3457
Gen 4659/8000  train MSE: 0.5820, val MSE: 0.3456
Gen 4660/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4661/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4662/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4663/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4664/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4665/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4666/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4667/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4668/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4669/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4670/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4671/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4672/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4673/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4674/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4675/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4676/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4677/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4678/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4679/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4680/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4681/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4682/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4683/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4684/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4685/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4686/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4687/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4688/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4689/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4690/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4691/8000  train MSE: 0.5820, val MSE: 0.3442
Gen 4692/8000  train MSE: 0.5820, val MSE: 0.3443
Gen 4693/8000  train MSE: 0.5820, val MSE: 0.3443
Gen 4694/8000  train MSE: 0.5820, val MSE: 0.3443
Gen 4695/8000  train MSE: 0.5820, val MSE: 0.3443
Gen 4696/8000  train MSE: 0.5820, val MSE: 0.3443
Gen 4697/8000  train MSE: 0.5820, val MSE: 0.3443
Gen 4698/8000  train MSE: 0.5820, val MSE: 0.3443
Gen 4699/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4700/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4701/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4702/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4703/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4704/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4705/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4706/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4707/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4708/8000  train MSE: 0.5819, val MSE: 0.3440
Gen 4709/8000  train MSE: 0.5819, val MSE: 0.3440
Gen 4710/8000  train MSE: 0.5819, val MSE: 0.3440
Gen 4711/8000  train MSE: 0.5819, val MSE: 0.3440
Gen 4712/8000  train MSE: 0.5819, val MSE: 0.3440
Gen 4713/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4714/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4715/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4716/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4717/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4718/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4719/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4720/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4721/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4722/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4723/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4724/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4725/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4726/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4727/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4728/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4729/8000  train MSE: 0.5819, val MSE: 0.3439
Gen 4730/8000  train MSE: 0.5819, val MSE: 0.3446
Gen 4731/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4732/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4733/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4734/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4735/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4736/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4737/8000  train MSE: 0.5819, val MSE: 0.3442
Gen 4738/8000  train MSE: 0.5818, val MSE: 0.3442
Gen 4739/8000  train MSE: 0.5818, val MSE: 0.3442
Gen 4740/8000  train MSE: 0.5818, val MSE: 0.3442
Gen 4741/8000  train MSE: 0.5818, val MSE: 0.3442
Gen 4742/8000  train MSE: 0.5818, val MSE: 0.3442
Gen 4743/8000  train MSE: 0.5818, val MSE: 0.3442
Gen 4744/8000  train MSE: 0.5818, val MSE: 0.3442
Gen 4745/8000  train MSE: 0.5818, val MSE: 0.3442
Gen 4746/8000  train MSE: 0.5818, val MSE: 0.3442
Gen 4747/8000  train MSE: 0.5818, val MSE: 0.3449
Gen 4748/8000  train MSE: 0.5818, val MSE: 0.3449
Gen 4749/8000  train MSE: 0.5818, val MSE: 0.3449
Gen 4750/8000  train MSE: 0.5818, val MSE: 0.3449
Gen 4751/8000  train MSE: 0.5818, val MSE: 0.3449
Gen 4752/8000  train MSE: 0.5818, val MSE: 0.3449
Gen 4753/8000  train MSE: 0.5818, val MSE: 0.3441
Gen 4754/8000  train MSE: 0.5818, val MSE: 0.3441
Gen 4755/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4756/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4757/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4758/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4759/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4760/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4761/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4762/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4763/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4764/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4765/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4766/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4767/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4768/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4769/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4770/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4771/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4772/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4773/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4774/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4775/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4776/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4777/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4778/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4779/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4780/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4781/8000  train MSE: 0.5818, val MSE: 0.3444
Gen 4782/8000  train MSE: 0.5818, val MSE: 0.3450
Gen 4783/8000  train MSE: 0.5818, val MSE: 0.3450
Gen 4784/8000  train MSE: 0.5818, val MSE: 0.3450
Gen 4785/8000  train MSE: 0.5818, val MSE: 0.3450
Gen 4786/8000  train MSE: 0.5818, val MSE: 0.3450
Gen 4787/8000  train MSE: 0.5818, val MSE: 0.3450
Gen 4788/8000  train MSE: 0.5818, val MSE: 0.3450
Gen 4789/8000  train MSE: 0.5818, val MSE: 0.3450
Gen 4790/8000  train MSE: 0.5818, val MSE: 0.3446
Gen 4791/8000  train MSE: 0.5818, val MSE: 0.3446
Gen 4792/8000  train MSE: 0.5818, val MSE: 0.3446
Gen 4793/8000  train MSE: 0.5818, val MSE: 0.3446
Gen 4794/8000  train MSE: 0.5818, val MSE: 0.3447
Gen 4795/8000  train MSE: 0.5818, val MSE: 0.3447
Gen 4796/8000  train MSE: 0.5818, val MSE: 0.3447
Gen 4797/8000  train MSE: 0.5817, val MSE: 0.3447
Gen 4798/8000  train MSE: 0.5817, val MSE: 0.3447
Gen 4799/8000  train MSE: 0.5817, val MSE: 0.3447
Gen 4800/8000  train MSE: 0.5817, val MSE: 0.3447
Gen 4801/8000  train MSE: 0.5817, val MSE: 0.3447
Gen 4802/8000  train MSE: 0.5817, val MSE: 0.3447
Gen 4803/8000  train MSE: 0.5817, val MSE: 0.3447
Gen 4804/8000  train MSE: 0.5817, val MSE: 0.3447
Gen 4805/8000  train MSE: 0.5817, val MSE: 0.3447
Gen 4806/8000  train MSE: 0.5817, val MSE: 0.3447
Gen 4807/8000  train MSE: 0.5817, val MSE: 0.3446
Gen 4808/8000  train MSE: 0.5817, val MSE: 0.3446
Gen 4809/8000  train MSE: 0.5817, val MSE: 0.3446
Gen 4810/8000  train MSE: 0.5817, val MSE: 0.3446
Gen 4811/8000  train MSE: 0.5817, val MSE: 0.3446
Gen 4812/8000  train MSE: 0.5817, val MSE: 0.3446
Gen 4813/8000  train MSE: 0.5817, val MSE: 0.3446
Gen 4814/8000  train MSE: 0.5817, val MSE: 0.3446
Gen 4815/8000  train MSE: 0.5817, val MSE: 0.3446
Gen 4816/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4817/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4818/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4819/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4820/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4821/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4822/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4823/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4824/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4825/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4826/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4827/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4828/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4829/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4830/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4831/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4832/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4833/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4834/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4835/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4836/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4837/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4838/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4839/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4840/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4841/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4842/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4843/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4844/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4845/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4846/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4847/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4848/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4849/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4850/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4851/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4852/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4853/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4854/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4855/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4856/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4857/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4858/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4859/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4860/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4861/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4862/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4863/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4864/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4865/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4866/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4867/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4868/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4869/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4870/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4871/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4872/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4873/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4874/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4875/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4876/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4877/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4878/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4879/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4880/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4881/8000  train MSE: 0.5816, val MSE: 0.3445
Gen 4882/8000  train MSE: 0.5816, val MSE: 0.3444
Gen 4883/8000  train MSE: 0.5816, val MSE: 0.3444
Gen 4884/8000  train MSE: 0.5816, val MSE: 0.3444
Gen 4885/8000  train MSE: 0.5816, val MSE: 0.3444
Gen 4886/8000  train MSE: 0.5816, val MSE: 0.3444
Gen 4887/8000  train MSE: 0.5816, val MSE: 0.3444
Gen 4888/8000  train MSE: 0.5816, val MSE: 0.3444
Gen 4889/8000  train MSE: 0.5816, val MSE: 0.3444
Gen 4890/8000  train MSE: 0.5816, val MSE: 0.3444
Gen 4891/8000  train MSE: 0.5816, val MSE: 0.3444
Gen 4892/8000  train MSE: 0.5816, val MSE: 0.3444
Gen 4893/8000  train MSE: 0.5815, val MSE: 0.3445
Gen 4894/8000  train MSE: 0.5815, val MSE: 0.3445
Gen 4895/8000  train MSE: 0.5815, val MSE: 0.3445
Gen 4896/8000  train MSE: 0.5815, val MSE: 0.3445
Gen 4897/8000  train MSE: 0.5815, val MSE: 0.3445
Gen 4898/8000  train MSE: 0.5815, val MSE: 0.3445
Gen 4899/8000  train MSE: 0.5815, val MSE: 0.3445
Gen 4900/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4901/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4902/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4903/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4904/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4905/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4906/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4907/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4908/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4909/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4910/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4911/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4912/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4913/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4914/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4915/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4916/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4917/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4918/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4919/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4920/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4921/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4922/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4923/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4924/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4925/8000  train MSE: 0.5815, val MSE: 0.3443
Gen 4926/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4927/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4928/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4929/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4930/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4931/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4932/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4933/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4934/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4935/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4936/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4937/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4938/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4939/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4940/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4941/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4942/8000  train MSE: 0.5815, val MSE: 0.3442
Gen 4943/8000  train MSE: 0.5815, val MSE: 0.3438
Gen 4944/8000  train MSE: 0.5815, val MSE: 0.3438
Gen 4945/8000  train MSE: 0.5815, val MSE: 0.3438
Gen 4946/8000  train MSE: 0.5815, val MSE: 0.3438
Gen 4947/8000  train MSE: 0.5815, val MSE: 0.3438
Gen 4948/8000  train MSE: 0.5815, val MSE: 0.3438
Gen 4949/8000  train MSE: 0.5815, val MSE: 0.3438
Gen 4950/8000  train MSE: 0.5815, val MSE: 0.3438
Gen 4951/8000  train MSE: 0.5815, val MSE: 0.3438
Gen 4952/8000  train MSE: 0.5815, val MSE: 0.3438
Gen 4953/8000  train MSE: 0.5815, val MSE: 0.3438
Gen 4954/8000  train MSE: 0.5815, val MSE: 0.3440
Gen 4955/8000  train MSE: 0.5815, val MSE: 0.3440
Gen 4956/8000  train MSE: 0.5815, val MSE: 0.3440
Gen 4957/8000  train MSE: 0.5815, val MSE: 0.3440
Gen 4958/8000  train MSE: 0.5815, val MSE: 0.3440
Gen 4959/8000  train MSE: 0.5815, val MSE: 0.3440
Gen 4960/8000  train MSE: 0.5815, val MSE: 0.3440
Gen 4961/8000  train MSE: 0.5814, val MSE: 0.3440
Gen 4962/8000  train MSE: 0.5814, val MSE: 0.3440
Gen 4963/8000  train MSE: 0.5814, val MSE: 0.3440
Gen 4964/8000  train MSE: 0.5814, val MSE: 0.3440
Gen 4965/8000  train MSE: 0.5814, val MSE: 0.3448
Gen 4966/8000  train MSE: 0.5814, val MSE: 0.3439
Gen 4967/8000  train MSE: 0.5814, val MSE: 0.3439
Gen 4968/8000  train MSE: 0.5814, val MSE: 0.3439
Gen 4969/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4970/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4971/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4972/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4973/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4974/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4975/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4976/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4977/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4978/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4979/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4980/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4981/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4982/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4983/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4984/8000  train MSE: 0.5814, val MSE: 0.3434
Gen 4985/8000  train MSE: 0.5814, val MSE: 0.3438
Gen 4986/8000  train MSE: 0.5814, val MSE: 0.3438
Gen 4987/8000  train MSE: 0.5814, val MSE: 0.3438
Gen 4988/8000  train MSE: 0.5814, val MSE: 0.3439
Gen 4989/8000  train MSE: 0.5814, val MSE: 0.3439
Gen 4990/8000  train MSE: 0.5814, val MSE: 0.3439
Gen 4991/8000  train MSE: 0.5814, val MSE: 0.3439
Gen 4992/8000  train MSE: 0.5814, val MSE: 0.3439
Gen 4993/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 4994/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 4995/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 4996/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 4997/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 4998/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 4999/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 5000/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 5001/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 5002/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 5003/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 5004/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 5005/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 5006/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 5007/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 5008/8000  train MSE: 0.5813, val MSE: 0.3428
Gen 5009/8000  train MSE: 0.5813, val MSE: 0.3435
Gen 5010/8000  train MSE: 0.5813, val MSE: 0.3435
Gen 5011/8000  train MSE: 0.5813, val MSE: 0.3435
Gen 5012/8000  train MSE: 0.5813, val MSE: 0.3435
Gen 5013/8000  train MSE: 0.5813, val MSE: 0.3435
Gen 5014/8000  train MSE: 0.5813, val MSE: 0.3435
Gen 5015/8000  train MSE: 0.5813, val MSE: 0.3435
Gen 5016/8000  train MSE: 0.5813, val MSE: 0.3435
Gen 5017/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5018/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5019/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5020/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5021/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5022/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5023/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5024/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5025/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5026/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5027/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5028/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5029/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5030/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5031/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5032/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5033/8000  train MSE: 0.5813, val MSE: 0.3424
Gen 5034/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5035/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5036/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5037/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5038/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5039/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5040/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5041/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5042/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5043/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5044/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5045/8000  train MSE: 0.5813, val MSE: 0.3431
Gen 5046/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5047/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5048/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5049/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5050/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5051/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5052/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5053/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5054/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5055/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5056/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5057/8000  train MSE: 0.5812, val MSE: 0.3435
Gen 5058/8000  train MSE: 0.5812, val MSE: 0.3435
Gen 5059/8000  train MSE: 0.5812, val MSE: 0.3435
Gen 5060/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5061/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5062/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5063/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5064/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5065/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5066/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5067/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5068/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5069/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5070/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5071/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5072/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5073/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5074/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5075/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5076/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5077/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5078/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5079/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5080/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5081/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5082/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5083/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5084/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5085/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5086/8000  train MSE: 0.5812, val MSE: 0.3431
Gen 5087/8000  train MSE: 0.5812, val MSE: 0.3426
Gen 5088/8000  train MSE: 0.5812, val MSE: 0.3426
Gen 5089/8000  train MSE: 0.5812, val MSE: 0.3426
Gen 5090/8000  train MSE: 0.5812, val MSE: 0.3426
Gen 5091/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5092/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5093/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5094/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5095/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5096/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5097/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5098/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5099/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5100/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5101/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5102/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5103/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5104/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5105/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5106/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5107/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5108/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5109/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5110/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5111/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5112/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5113/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5114/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5115/8000  train MSE: 0.5812, val MSE: 0.3429
Gen 5116/8000  train MSE: 0.5812, val MSE: 0.3427
Gen 5117/8000  train MSE: 0.5811, val MSE: 0.3426
Gen 5118/8000  train MSE: 0.5811, val MSE: 0.3426
Gen 5119/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5120/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5121/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5122/8000  train MSE: 0.5811, val MSE: 0.3429
Gen 5123/8000  train MSE: 0.5811, val MSE: 0.3429
Gen 5124/8000  train MSE: 0.5811, val MSE: 0.3429
Gen 5125/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5126/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5127/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5128/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5129/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5130/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5131/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5132/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5133/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5134/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5135/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5136/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5137/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5138/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5139/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5140/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5141/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5142/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5143/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5144/8000  train MSE: 0.5811, val MSE: 0.3424
Gen 5145/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5146/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5147/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5148/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5149/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5150/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5151/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5152/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5153/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5154/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5155/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5156/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5157/8000  train MSE: 0.5811, val MSE: 0.3425
Gen 5158/8000  train MSE: 0.5811, val MSE: 0.3435
Gen 5159/8000  train MSE: 0.5811, val MSE: 0.3435
Gen 5160/8000  train MSE: 0.5811, val MSE: 0.3435
Gen 5161/8000  train MSE: 0.5811, val MSE: 0.3427
Gen 5162/8000  train MSE: 0.5811, val MSE: 0.3427
Gen 5163/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5164/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5165/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5166/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5167/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5168/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5169/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5170/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5171/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5172/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5173/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5174/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5175/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5176/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5177/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5178/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5179/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5180/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5181/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5182/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5183/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5184/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5185/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5186/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5187/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5188/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5189/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5190/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5191/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5192/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5193/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5194/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5195/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5196/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5197/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5198/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5199/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5200/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5201/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5202/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5203/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5204/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5205/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5206/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5207/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5208/8000  train MSE: 0.5810, val MSE: 0.3420
Gen 5209/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5210/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5211/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5212/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5213/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5214/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5215/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5216/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5217/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5218/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5219/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5220/8000  train MSE: 0.5810, val MSE: 0.3415
Gen 5221/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5222/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5223/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5224/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5225/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5226/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5227/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5228/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5229/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5230/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5231/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5232/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5233/8000  train MSE: 0.5810, val MSE: 0.3421
Gen 5234/8000  train MSE: 0.5809, val MSE: 0.3412
Gen 5235/8000  train MSE: 0.5809, val MSE: 0.3412
Gen 5236/8000  train MSE: 0.5809, val MSE: 0.3412
Gen 5237/8000  train MSE: 0.5809, val MSE: 0.3412
Gen 5238/8000  train MSE: 0.5809, val MSE: 0.3412
Gen 5239/8000  train MSE: 0.5809, val MSE: 0.3412
Gen 5240/8000  train MSE: 0.5809, val MSE: 0.3412
Gen 5241/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5242/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5243/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5244/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5245/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5246/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5247/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5248/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5249/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5250/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5251/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5252/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5253/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5254/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5255/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5256/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5257/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5258/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5259/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5260/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5261/8000  train MSE: 0.5809, val MSE: 0.3416
Gen 5262/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5263/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5264/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5265/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5266/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5267/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5268/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5269/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5270/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5271/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5272/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5273/8000  train MSE: 0.5809, val MSE: 0.3417
Gen 5274/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5275/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5276/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5277/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5278/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5279/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5280/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5281/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5282/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5283/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5284/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5285/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5286/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5287/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5288/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5289/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5290/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5291/8000  train MSE: 0.5809, val MSE: 0.3418
Gen 5292/8000  train MSE: 0.5808, val MSE: 0.3412
Gen 5293/8000  train MSE: 0.5808, val MSE: 0.3412
Gen 5294/8000  train MSE: 0.5808, val MSE: 0.3412
Gen 5295/8000  train MSE: 0.5808, val MSE: 0.3412
Gen 5296/8000  train MSE: 0.5808, val MSE: 0.3412
Gen 5297/8000  train MSE: 0.5808, val MSE: 0.3412
Gen 5298/8000  train MSE: 0.5808, val MSE: 0.3412
Gen 5299/8000  train MSE: 0.5808, val MSE: 0.3412
Gen 5300/8000  train MSE: 0.5808, val MSE: 0.3412
Gen 5301/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5302/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5303/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5304/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5305/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5306/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5307/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5308/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5309/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5310/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5311/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5312/8000  train MSE: 0.5808, val MSE: 0.3413
Gen 5313/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5314/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5315/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5316/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5317/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5318/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5319/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5320/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5321/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5322/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5323/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5324/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5325/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5326/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5327/8000  train MSE: 0.5808, val MSE: 0.3416
Gen 5328/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5329/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5330/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5331/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5332/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5333/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5334/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5335/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5336/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5337/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5338/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5339/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5340/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5341/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5342/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5343/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5344/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5345/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5346/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5347/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5348/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5349/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5350/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5351/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5352/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5353/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5354/8000  train MSE: 0.5807, val MSE: 0.3412
Gen 5355/8000  train MSE: 0.5807, val MSE: 0.3412
Gen 5356/8000  train MSE: 0.5807, val MSE: 0.3412
Gen 5357/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5358/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5359/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5360/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5361/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5362/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5363/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5364/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5365/8000  train MSE: 0.5807, val MSE: 0.3405
Gen 5366/8000  train MSE: 0.5807, val MSE: 0.3402
Gen 5367/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5368/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5369/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5370/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5371/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5372/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5373/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5374/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5375/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5376/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5377/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5378/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5379/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5380/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5381/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5382/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5383/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5384/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5385/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5386/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5387/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5388/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5389/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5390/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5391/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5392/8000  train MSE: 0.5807, val MSE: 0.3410
Gen 5393/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5394/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5395/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5396/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5397/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5398/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5399/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5400/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5401/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5402/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5403/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5404/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5405/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5406/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5407/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5408/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5409/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5410/8000  train MSE: 0.5806, val MSE: 0.3407
Gen 5411/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5412/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5413/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5414/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5415/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5416/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5417/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5418/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5419/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5420/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5421/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5422/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5423/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5424/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5425/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5426/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5427/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5428/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5429/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5430/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5431/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5432/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5433/8000  train MSE: 0.5806, val MSE: 0.3403
Gen 5434/8000  train MSE: 0.5806, val MSE: 0.3413
Gen 5435/8000  train MSE: 0.5806, val MSE: 0.3404
Gen 5436/8000  train MSE: 0.5806, val MSE: 0.3404
Gen 5437/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5438/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5439/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5440/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5441/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5442/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5443/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5444/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5445/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5446/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5447/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5448/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5449/8000  train MSE: 0.5806, val MSE: 0.3405
Gen 5450/8000  train MSE: 0.5806, val MSE: 0.3408
Gen 5451/8000  train MSE: 0.5806, val MSE: 0.3408
Gen 5452/8000  train MSE: 0.5806, val MSE: 0.3408
Gen 5453/8000  train MSE: 0.5806, val MSE: 0.3408
Gen 5454/8000  train MSE: 0.5806, val MSE: 0.3408
Gen 5455/8000  train MSE: 0.5806, val MSE: 0.3408
Gen 5456/8000  train MSE: 0.5806, val MSE: 0.3408
Gen 5457/8000  train MSE: 0.5806, val MSE: 0.3408
Gen 5458/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5459/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5460/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5461/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5462/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5463/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5464/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5465/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5466/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5467/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5468/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5469/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5470/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5471/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5472/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5473/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5474/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5475/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5476/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5477/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5478/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5479/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5480/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5481/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5482/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5483/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5484/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5485/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5486/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5487/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5488/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5489/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5490/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5491/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5492/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5493/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5494/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5495/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5496/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5497/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5498/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5499/8000  train MSE: 0.5805, val MSE: 0.3408
Gen 5500/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5501/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5502/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5503/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5504/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5505/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5506/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5507/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5508/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5509/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5510/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5511/8000  train MSE: 0.5805, val MSE: 0.3403
Gen 5512/8000  train MSE: 0.5804, val MSE: 0.3415
Gen 5513/8000  train MSE: 0.5804, val MSE: 0.3415
Gen 5514/8000  train MSE: 0.5804, val MSE: 0.3415
Gen 5515/8000  train MSE: 0.5804, val MSE: 0.3415
Gen 5516/8000  train MSE: 0.5804, val MSE: 0.3415
Gen 5517/8000  train MSE: 0.5804, val MSE: 0.3415
Gen 5518/8000  train MSE: 0.5804, val MSE: 0.3400
Gen 5519/8000  train MSE: 0.5804, val MSE: 0.3400
Gen 5520/8000  train MSE: 0.5804, val MSE: 0.3400
Gen 5521/8000  train MSE: 0.5804, val MSE: 0.3400
Gen 5522/8000  train MSE: 0.5804, val MSE: 0.3400
Gen 5523/8000  train MSE: 0.5804, val MSE: 0.3400
Gen 5524/8000  train MSE: 0.5804, val MSE: 0.3400
Gen 5525/8000  train MSE: 0.5804, val MSE: 0.3400
Gen 5526/8000  train MSE: 0.5804, val MSE: 0.3406
Gen 5527/8000  train MSE: 0.5804, val MSE: 0.3406
Gen 5528/8000  train MSE: 0.5804, val MSE: 0.3406
Gen 5529/8000  train MSE: 0.5804, val MSE: 0.3406
Gen 5530/8000  train MSE: 0.5804, val MSE: 0.3406
Gen 5531/8000  train MSE: 0.5804, val MSE: 0.3406
Gen 5532/8000  train MSE: 0.5804, val MSE: 0.3406
Gen 5533/8000  train MSE: 0.5804, val MSE: 0.3406
Gen 5534/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5535/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5536/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5537/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5538/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5539/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5540/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5541/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5542/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5543/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5544/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5545/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5546/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5547/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5548/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5549/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5550/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5551/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5552/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5553/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5554/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5555/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5556/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5557/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5558/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5559/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5560/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5561/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5562/8000  train MSE: 0.5804, val MSE: 0.3407
Gen 5563/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5564/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5565/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5566/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5567/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5568/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5569/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5570/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5571/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5572/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5573/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5574/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5575/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5576/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5577/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5578/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5579/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5580/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5581/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5582/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5583/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5584/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5585/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5586/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5587/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5588/8000  train MSE: 0.5804, val MSE: 0.3397
Gen 5589/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5590/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5591/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5592/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5593/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5594/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5595/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5596/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5597/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5598/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5599/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5600/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5601/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5602/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5603/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5604/8000  train MSE: 0.5804, val MSE: 0.3409
Gen 5605/8000  train MSE: 0.5803, val MSE: 0.3397
Gen 5606/8000  train MSE: 0.5803, val MSE: 0.3397
Gen 5607/8000  train MSE: 0.5803, val MSE: 0.3397
Gen 5608/8000  train MSE: 0.5803, val MSE: 0.3397
Gen 5609/8000  train MSE: 0.5803, val MSE: 0.3397
Gen 5610/8000  train MSE: 0.5803, val MSE: 0.3397
Gen 5611/8000  train MSE: 0.5803, val MSE: 0.3397
Gen 5612/8000  train MSE: 0.5803, val MSE: 0.3397
Gen 5613/8000  train MSE: 0.5803, val MSE: 0.3407
Gen 5614/8000  train MSE: 0.5803, val MSE: 0.3407
Gen 5615/8000  train MSE: 0.5803, val MSE: 0.3407
Gen 5616/8000  train MSE: 0.5803, val MSE: 0.3407
Gen 5617/8000  train MSE: 0.5803, val MSE: 0.3407
Gen 5618/8000  train MSE: 0.5803, val MSE: 0.3407
Gen 5619/8000  train MSE: 0.5803, val MSE: 0.3407
Gen 5620/8000  train MSE: 0.5803, val MSE: 0.3407
Gen 5621/8000  train MSE: 0.5803, val MSE: 0.3407
Gen 5622/8000  train MSE: 0.5803, val MSE: 0.3407
Gen 5623/8000  train MSE: 0.5803, val MSE: 0.3407
Gen 5624/8000  train MSE: 0.5803, val MSE: 0.3400
Gen 5625/8000  train MSE: 0.5803, val MSE: 0.3400
Gen 5626/8000  train MSE: 0.5803, val MSE: 0.3400
Gen 5627/8000  train MSE: 0.5803, val MSE: 0.3398
Gen 5628/8000  train MSE: 0.5803, val MSE: 0.3398
Gen 5629/8000  train MSE: 0.5803, val MSE: 0.3398
Gen 5630/8000  train MSE: 0.5803, val MSE: 0.3398
Gen 5631/8000  train MSE: 0.5803, val MSE: 0.3398
Gen 5632/8000  train MSE: 0.5803, val MSE: 0.3398
Gen 5633/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5634/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5635/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5636/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5637/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5638/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5639/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5640/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5641/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5642/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5643/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5644/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5645/8000  train MSE: 0.5802, val MSE: 0.3406
Gen 5646/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5647/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5648/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5649/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5650/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5651/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5652/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5653/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5654/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5655/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5656/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5657/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5658/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5659/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5660/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5661/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5662/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5663/8000  train MSE: 0.5802, val MSE: 0.3395
Gen 5664/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5665/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5666/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5667/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5668/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5669/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5670/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5671/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5672/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5673/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5674/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5675/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5676/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5677/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5678/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5679/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5680/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5681/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5682/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5683/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5684/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5685/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5686/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5687/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5688/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5689/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5690/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5691/8000  train MSE: 0.5802, val MSE: 0.3400
Gen 5692/8000  train MSE: 0.5802, val MSE: 0.3407
Gen 5693/8000  train MSE: 0.5801, val MSE: 0.3404
Gen 5694/8000  train MSE: 0.5801, val MSE: 0.3404
Gen 5695/8000  train MSE: 0.5801, val MSE: 0.3404
Gen 5696/8000  train MSE: 0.5801, val MSE: 0.3404
Gen 5697/8000  train MSE: 0.5801, val MSE: 0.3404
Gen 5698/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5699/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5700/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5701/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5702/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5703/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5704/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5705/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5706/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5707/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5708/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5709/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5710/8000  train MSE: 0.5801, val MSE: 0.3403
Gen 5711/8000  train MSE: 0.5801, val MSE: 0.3402
Gen 5712/8000  train MSE: 0.5801, val MSE: 0.3402
Gen 5713/8000  train MSE: 0.5801, val MSE: 0.3402
Gen 5714/8000  train MSE: 0.5801, val MSE: 0.3402
Gen 5715/8000  train MSE: 0.5801, val MSE: 0.3402
Gen 5716/8000  train MSE: 0.5801, val MSE: 0.3402
Gen 5717/8000  train MSE: 0.5801, val MSE: 0.3402
Gen 5718/8000  train MSE: 0.5801, val MSE: 0.3402
Gen 5719/8000  train MSE: 0.5801, val MSE: 0.3402
Gen 5720/8000  train MSE: 0.5801, val MSE: 0.3402
Gen 5721/8000  train MSE: 0.5801, val MSE: 0.3402
Gen 5722/8000  train MSE: 0.5801, val MSE: 0.3399
Gen 5723/8000  train MSE: 0.5801, val MSE: 0.3399
Gen 5724/8000  train MSE: 0.5801, val MSE: 0.3399
Gen 5725/8000  train MSE: 0.5801, val MSE: 0.3399
Gen 5726/8000  train MSE: 0.5801, val MSE: 0.3399
Gen 5727/8000  train MSE: 0.5801, val MSE: 0.3399
Gen 5728/8000  train MSE: 0.5801, val MSE: 0.3399
Gen 5729/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5730/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5731/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5732/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5733/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5734/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5735/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5736/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5737/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5738/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5739/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5740/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5741/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5742/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5743/8000  train MSE: 0.5800, val MSE: 0.3395
Gen 5744/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5745/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5746/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5747/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5748/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5749/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5750/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5751/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5752/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5753/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5754/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5755/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5756/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5757/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5758/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5759/8000  train MSE: 0.5799, val MSE: 0.3390
Gen 5760/8000  train MSE: 0.5799, val MSE: 0.3393
Gen 5761/8000  train MSE: 0.5799, val MSE: 0.3393
Gen 5762/8000  train MSE: 0.5799, val MSE: 0.3393
Gen 5763/8000  train MSE: 0.5799, val MSE: 0.3393
Gen 5764/8000  train MSE: 0.5799, val MSE: 0.3393
Gen 5765/8000  train MSE: 0.5799, val MSE: 0.3393
Gen 5766/8000  train MSE: 0.5799, val MSE: 0.3393
Gen 5767/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5768/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5769/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5770/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5771/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5772/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5773/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5774/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5775/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5776/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5777/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5778/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5779/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5780/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5781/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5782/8000  train MSE: 0.5798, val MSE: 0.3390
Gen 5783/8000  train MSE: 0.5798, val MSE: 0.3393
Gen 5784/8000  train MSE: 0.5798, val MSE: 0.3393
Gen 5785/8000  train MSE: 0.5798, val MSE: 0.3393
Gen 5786/8000  train MSE: 0.5798, val MSE: 0.3397
Gen 5787/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5788/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5789/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5790/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5791/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5792/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5793/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5794/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5795/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5796/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5797/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5798/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5799/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5800/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5801/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5802/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5803/8000  train MSE: 0.5798, val MSE: 0.3394
Gen 5804/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5805/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5806/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5807/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5808/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5809/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5810/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5811/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5812/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5813/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5814/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5815/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5816/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5817/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5818/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5819/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5820/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5821/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5822/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5823/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5824/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5825/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5826/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5827/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5828/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5829/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5830/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5831/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5832/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5833/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5834/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5835/8000  train MSE: 0.5798, val MSE: 0.3389
Gen 5836/8000  train MSE: 0.5797, val MSE: 0.3390
Gen 5837/8000  train MSE: 0.5797, val MSE: 0.3385
Gen 5838/8000  train MSE: 0.5797, val MSE: 0.3385
Gen 5839/8000  train MSE: 0.5797, val MSE: 0.3385
Gen 5840/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5841/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5842/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5843/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5844/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5845/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5846/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5847/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5848/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5849/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5850/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5851/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5852/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5853/8000  train MSE: 0.5797, val MSE: 0.3388
Gen 5854/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5855/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5856/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5857/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5858/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5859/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5860/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5861/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5862/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5863/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5864/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5865/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5866/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5867/8000  train MSE: 0.5796, val MSE: 0.3394
Gen 5868/8000  train MSE: 0.5796, val MSE: 0.3387
Gen 5869/8000  train MSE: 0.5796, val MSE: 0.3387
Gen 5870/8000  train MSE: 0.5796, val MSE: 0.3387
Gen 5871/8000  train MSE: 0.5796, val MSE: 0.3387
Gen 5872/8000  train MSE: 0.5796, val MSE: 0.3387
Gen 5873/8000  train MSE: 0.5796, val MSE: 0.3386
Gen 5874/8000  train MSE: 0.5796, val MSE: 0.3386
Gen 5875/8000  train MSE: 0.5796, val MSE: 0.3386
Gen 5876/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5877/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5878/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5879/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5880/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5881/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5882/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5883/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5884/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5885/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5886/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5887/8000  train MSE: 0.5796, val MSE: 0.3385
Gen 5888/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5889/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5890/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5891/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5892/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5893/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5894/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5895/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5896/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5897/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5898/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5899/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5900/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5901/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5902/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5903/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5904/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5905/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5906/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5907/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5908/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5909/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5910/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5911/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5912/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5913/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5914/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5915/8000  train MSE: 0.5795, val MSE: 0.3385
Gen 5916/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5917/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5918/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5919/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5920/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5921/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5922/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5923/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5924/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5925/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5926/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5927/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5928/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5929/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5930/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5931/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5932/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5933/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5934/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5935/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5936/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5937/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5938/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5939/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5940/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5941/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5942/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5943/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5944/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5945/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5946/8000  train MSE: 0.5795, val MSE: 0.3389
Gen 5947/8000  train MSE: 0.5795, val MSE: 0.3388
Gen 5948/8000  train MSE: 0.5795, val MSE: 0.3388
Gen 5949/8000  train MSE: 0.5795, val MSE: 0.3388
Gen 5950/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5951/8000  train MSE: 0.5795, val MSE: 0.3385
Gen 5952/8000  train MSE: 0.5795, val MSE: 0.3385
Gen 5953/8000  train MSE: 0.5795, val MSE: 0.3385
Gen 5954/8000  train MSE: 0.5795, val MSE: 0.3385
Gen 5955/8000  train MSE: 0.5795, val MSE: 0.3385
Gen 5956/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5957/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5958/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5959/8000  train MSE: 0.5795, val MSE: 0.3390
Gen 5960/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5961/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5962/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5963/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5964/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5965/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5966/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5967/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5968/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5969/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5970/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5971/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5972/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5973/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5974/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5975/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5976/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5977/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5978/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5979/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5980/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5981/8000  train MSE: 0.5794, val MSE: 0.3392
Gen 5982/8000  train MSE: 0.5794, val MSE: 0.3389
Gen 5983/8000  train MSE: 0.5794, val MSE: 0.3389
Gen 5984/8000  train MSE: 0.5794, val MSE: 0.3389
Gen 5985/8000  train MSE: 0.5794, val MSE: 0.3389
Gen 5986/8000  train MSE: 0.5794, val MSE: 0.3389
Gen 5987/8000  train MSE: 0.5794, val MSE: 0.3390
Gen 5988/8000  train MSE: 0.5794, val MSE: 0.3390
Gen 5989/8000  train MSE: 0.5794, val MSE: 0.3393
Gen 5990/8000  train MSE: 0.5794, val MSE: 0.3393
Gen 5991/8000  train MSE: 0.5794, val MSE: 0.3389
Gen 5992/8000  train MSE: 0.5794, val MSE: 0.3389
Gen 5993/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 5994/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 5995/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 5996/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 5997/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 5998/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 5999/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 6000/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 6001/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 6002/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 6003/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 6004/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 6005/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 6006/8000  train MSE: 0.5794, val MSE: 0.3391
Gen 6007/8000  train MSE: 0.5793, val MSE: 0.3389
Gen 6008/8000  train MSE: 0.5793, val MSE: 0.3389
Gen 6009/8000  train MSE: 0.5793, val MSE: 0.3389
Gen 6010/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6011/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6012/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6013/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6014/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6015/8000  train MSE: 0.5793, val MSE: 0.3381
Gen 6016/8000  train MSE: 0.5793, val MSE: 0.3381
Gen 6017/8000  train MSE: 0.5793, val MSE: 0.3381
Gen 6018/8000  train MSE: 0.5793, val MSE: 0.3381
Gen 6019/8000  train MSE: 0.5793, val MSE: 0.3381
Gen 6020/8000  train MSE: 0.5793, val MSE: 0.3381
Gen 6021/8000  train MSE: 0.5793, val MSE: 0.3381
Gen 6022/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6023/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6024/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6025/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6026/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6027/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6028/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6029/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6030/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6031/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6032/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6033/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6034/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6035/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6036/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6037/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6038/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6039/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6040/8000  train MSE: 0.5793, val MSE: 0.3387
Gen 6041/8000  train MSE: 0.5793, val MSE: 0.3395
Gen 6042/8000  train MSE: 0.5793, val MSE: 0.3395
Gen 6043/8000  train MSE: 0.5793, val MSE: 0.3395
Gen 6044/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6045/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6046/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6047/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6048/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6049/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6050/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6051/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6052/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6053/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6054/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6055/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6056/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6057/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6058/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6059/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6060/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6061/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6062/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6063/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6064/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6065/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6066/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6067/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6068/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6069/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6070/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6071/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6072/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6073/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6074/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6075/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6076/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6077/8000  train MSE: 0.5792, val MSE: 0.3389
Gen 6078/8000  train MSE: 0.5792, val MSE: 0.3388
Gen 6079/8000  train MSE: 0.5792, val MSE: 0.3388
Gen 6080/8000  train MSE: 0.5792, val MSE: 0.3388
Gen 6081/8000  train MSE: 0.5792, val MSE: 0.3388
Gen 6082/8000  train MSE: 0.5792, val MSE: 0.3388
Gen 6083/8000  train MSE: 0.5792, val MSE: 0.3388
Gen 6084/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6085/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6086/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6087/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6088/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6089/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6090/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6091/8000  train MSE: 0.5792, val MSE: 0.3393
Gen 6092/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6093/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6094/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6095/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6096/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6097/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6098/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6099/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6100/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6101/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6102/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6103/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6104/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6105/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6106/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6107/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6108/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6109/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6110/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6111/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6112/8000  train MSE: 0.5792, val MSE: 0.3391
Gen 6113/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6114/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6115/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6116/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6117/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6118/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6119/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6120/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6121/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6122/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6123/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6124/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6125/8000  train MSE: 0.5792, val MSE: 0.3386
Gen 6126/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6127/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6128/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6129/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6130/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6131/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6132/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6133/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6134/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6135/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6136/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6137/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6138/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6139/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6140/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6141/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6142/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6143/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6144/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6145/8000  train MSE: 0.5791, val MSE: 0.3390
Gen 6146/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6147/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6148/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6149/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6150/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6151/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6152/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6153/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6154/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6155/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6156/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6157/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6158/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6159/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6160/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6161/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6162/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6163/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6164/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6165/8000  train MSE: 0.5791, val MSE: 0.3389
Gen 6166/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6167/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6168/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6169/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6170/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6171/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6172/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6173/8000  train MSE: 0.5791, val MSE: 0.3386
Gen 6174/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6175/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6176/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6177/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6178/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6179/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6180/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6181/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6182/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6183/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6184/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6185/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6186/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6187/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6188/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6189/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6190/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6191/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6192/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6193/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6194/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6195/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6196/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6197/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6198/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6199/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6200/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6201/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6202/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6203/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6204/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6205/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6206/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6207/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6208/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6209/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6210/8000  train MSE: 0.5791, val MSE: 0.3394
Gen 6211/8000  train MSE: 0.5791, val MSE: 0.3384
Gen 6212/8000  train MSE: 0.5791, val MSE: 0.3395
Gen 6213/8000  train MSE: 0.5791, val MSE: 0.3395
Gen 6214/8000  train MSE: 0.5791, val MSE: 0.3395
Gen 6215/8000  train MSE: 0.5791, val MSE: 0.3395
Gen 6216/8000  train MSE: 0.5791, val MSE: 0.3395
Gen 6217/8000  train MSE: 0.5791, val MSE: 0.3395
Gen 6218/8000  train MSE: 0.5791, val MSE: 0.3395
Gen 6219/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6220/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6221/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6222/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6223/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6224/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6225/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6226/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6227/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6228/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6229/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6230/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6231/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6232/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6233/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6234/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6235/8000  train MSE: 0.5790, val MSE: 0.3391
Gen 6236/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6237/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6238/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6239/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6240/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6241/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6242/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6243/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6244/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6245/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6246/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6247/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6248/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6249/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6250/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6251/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6252/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6253/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6254/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6255/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6256/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6257/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6258/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6259/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6260/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6261/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6262/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6263/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6264/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6265/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6266/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6267/8000  train MSE: 0.5790, val MSE: 0.3390
Gen 6268/8000  train MSE: 0.5790, val MSE: 0.3390
Gen 6269/8000  train MSE: 0.5790, val MSE: 0.3390
Gen 6270/8000  train MSE: 0.5790, val MSE: 0.3390
Gen 6271/8000  train MSE: 0.5790, val MSE: 0.3390
Gen 6272/8000  train MSE: 0.5790, val MSE: 0.3390
Gen 6273/8000  train MSE: 0.5790, val MSE: 0.3390
Gen 6274/8000  train MSE: 0.5790, val MSE: 0.3390
Gen 6275/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6276/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6277/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6278/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6279/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6280/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6281/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6282/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6283/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6284/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6285/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6286/8000  train MSE: 0.5790, val MSE: 0.3389
Gen 6287/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6288/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6289/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6290/8000  train MSE: 0.5790, val MSE: 0.3385
Gen 6291/8000  train MSE: 0.5790, val MSE: 0.3390
Gen 6292/8000  train MSE: 0.5789, val MSE: 0.3387
Gen 6293/8000  train MSE: 0.5789, val MSE: 0.3387
Gen 6294/8000  train MSE: 0.5789, val MSE: 0.3387
Gen 6295/8000  train MSE: 0.5789, val MSE: 0.3387
Gen 6296/8000  train MSE: 0.5789, val MSE: 0.3387
Gen 6297/8000  train MSE: 0.5789, val MSE: 0.3387
Gen 6298/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6299/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6300/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6301/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6302/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6303/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6304/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6305/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6306/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6307/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6308/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6309/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6310/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6311/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6312/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6313/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6314/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6315/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6316/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6317/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6318/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6319/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6320/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6321/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6322/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6323/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6324/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6325/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6326/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6327/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6328/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6329/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6330/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6331/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6332/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6333/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6334/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6335/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6336/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6337/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6338/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6339/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6340/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6341/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6342/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6343/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6344/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6345/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6346/8000  train MSE: 0.5789, val MSE: 0.3384
Gen 6347/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6348/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6349/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6350/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6351/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6352/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6353/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6354/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6355/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6356/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6357/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6358/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6359/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6360/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6361/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6362/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6363/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6364/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6365/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6366/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6367/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6368/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6369/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6370/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6371/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6372/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6373/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6374/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6375/8000  train MSE: 0.5789, val MSE: 0.3378
Gen 6376/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6377/8000  train MSE: 0.5789, val MSE: 0.3386
Gen 6378/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6379/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6380/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6381/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6382/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6383/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6384/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6385/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6386/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6387/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6388/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6389/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6390/8000  train MSE: 0.5788, val MSE: 0.3385
Gen 6391/8000  train MSE: 0.5788, val MSE: 0.3379
Gen 6392/8000  train MSE: 0.5788, val MSE: 0.3379
Gen 6393/8000  train MSE: 0.5788, val MSE: 0.3379
Gen 6394/8000  train MSE: 0.5788, val MSE: 0.3379
Gen 6395/8000  train MSE: 0.5788, val MSE: 0.3379
Gen 6396/8000  train MSE: 0.5788, val MSE: 0.3379
Gen 6397/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6398/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6399/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6400/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6401/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6402/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6403/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6404/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6405/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6406/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6407/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6408/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6409/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6410/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6411/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6412/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6413/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6414/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6415/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6416/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6417/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6418/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6419/8000  train MSE: 0.5788, val MSE: 0.3387
Gen 6420/8000  train MSE: 0.5788, val MSE: 0.3379
Gen 6421/8000  train MSE: 0.5788, val MSE: 0.3379
Gen 6422/8000  train MSE: 0.5788, val MSE: 0.3383
Gen 6423/8000  train MSE: 0.5788, val MSE: 0.3383
Gen 6424/8000  train MSE: 0.5788, val MSE: 0.3383
Gen 6425/8000  train MSE: 0.5788, val MSE: 0.3383
Gen 6426/8000  train MSE: 0.5788, val MSE: 0.3384
Gen 6427/8000  train MSE: 0.5788, val MSE: 0.3384
Gen 6428/8000  train MSE: 0.5788, val MSE: 0.3384
Gen 6429/8000  train MSE: 0.5788, val MSE: 0.3384
Gen 6430/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6431/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6432/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6433/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6434/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6435/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6436/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6437/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6438/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6439/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6440/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6441/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6442/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6443/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6444/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6445/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6446/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6447/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6448/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6449/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6450/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6451/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6452/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6453/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6454/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6455/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6456/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6457/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6458/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6459/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6460/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6461/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6462/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6463/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6464/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6465/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6466/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6467/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6468/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6469/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6470/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6471/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6472/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6473/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6474/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6475/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6476/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6477/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6478/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6479/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6480/8000  train MSE: 0.5788, val MSE: 0.3378
Gen 6481/8000  train MSE: 0.5788, val MSE: 0.3383
Gen 6482/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6483/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6484/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6485/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6486/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6487/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6488/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6489/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6490/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6491/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6492/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6493/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6494/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6495/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6496/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6497/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6498/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6499/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6500/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6501/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6502/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6503/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6504/8000  train MSE: 0.5787, val MSE: 0.3380
Gen 6505/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6506/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6507/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6508/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6509/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6510/8000  train MSE: 0.5787, val MSE: 0.3377
Gen 6511/8000  train MSE: 0.5787, val MSE: 0.3377
Gen 6512/8000  train MSE: 0.5787, val MSE: 0.3377
Gen 6513/8000  train MSE: 0.5787, val MSE: 0.3377
Gen 6514/8000  train MSE: 0.5787, val MSE: 0.3384
Gen 6515/8000  train MSE: 0.5787, val MSE: 0.3384
Gen 6516/8000  train MSE: 0.5787, val MSE: 0.3384
Gen 6517/8000  train MSE: 0.5787, val MSE: 0.3384
Gen 6518/8000  train MSE: 0.5787, val MSE: 0.3384
Gen 6519/8000  train MSE: 0.5787, val MSE: 0.3384
Gen 6520/8000  train MSE: 0.5787, val MSE: 0.3384
Gen 6521/8000  train MSE: 0.5787, val MSE: 0.3384
Gen 6522/8000  train MSE: 0.5787, val MSE: 0.3384
Gen 6523/8000  train MSE: 0.5787, val MSE: 0.3375
Gen 6524/8000  train MSE: 0.5787, val MSE: 0.3375
Gen 6525/8000  train MSE: 0.5787, val MSE: 0.3375
Gen 6526/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6527/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6528/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6529/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6530/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6531/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6532/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6533/8000  train MSE: 0.5787, val MSE: 0.3379
Gen 6534/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6535/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6536/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6537/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6538/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6539/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6540/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6541/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6542/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6543/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6544/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6545/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6546/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6547/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6548/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6549/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6550/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6551/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6552/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6553/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6554/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6555/8000  train MSE: 0.5786, val MSE: 0.3381
Gen 6556/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6557/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6558/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6559/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6560/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6561/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6562/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6563/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6564/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6565/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6566/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6567/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6568/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6569/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6570/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6571/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6572/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6573/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6574/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6575/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6576/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6577/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6578/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6579/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6580/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6581/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6582/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6583/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6584/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6585/8000  train MSE: 0.5786, val MSE: 0.3376
Gen 6586/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6587/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6588/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6589/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6590/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6591/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6592/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6593/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6594/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6595/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6596/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6597/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6598/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6599/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6600/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6601/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6602/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6603/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6604/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6605/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6606/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6607/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6608/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6609/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6610/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6611/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6612/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6613/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6614/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6615/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6616/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6617/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6618/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6619/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6620/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6621/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6622/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6623/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6624/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6625/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6626/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6627/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6628/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6629/8000  train MSE: 0.5785, val MSE: 0.3385
Gen 6630/8000  train MSE: 0.5785, val MSE: 0.3382
Gen 6631/8000  train MSE: 0.5785, val MSE: 0.3382
Gen 6632/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6633/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6634/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6635/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6636/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6637/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6638/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6639/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6640/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6641/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6642/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6643/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6644/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6645/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6646/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6647/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6648/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6649/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6650/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6651/8000  train MSE: 0.5785, val MSE: 0.3379
Gen 6652/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6653/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6654/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6655/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6656/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6657/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6658/8000  train MSE: 0.5785, val MSE: 0.3375
Gen 6659/8000  train MSE: 0.5785, val MSE: 0.3373
Gen 6660/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6661/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6662/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6663/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6664/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6665/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6666/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6667/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6668/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6669/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6670/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6671/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6672/8000  train MSE: 0.5784, val MSE: 0.3382
Gen 6673/8000  train MSE: 0.5784, val MSE: 0.3382
Gen 6674/8000  train MSE: 0.5784, val MSE: 0.3382
Gen 6675/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6676/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6677/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6678/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6679/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6680/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6681/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6682/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6683/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6684/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6685/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6686/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6687/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6688/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6689/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6690/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6691/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6692/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6693/8000  train MSE: 0.5784, val MSE: 0.3380
Gen 6694/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6695/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6696/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6697/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6698/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6699/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6700/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6701/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6702/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6703/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6704/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6705/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6706/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6707/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6708/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6709/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6710/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6711/8000  train MSE: 0.5784, val MSE: 0.3384
Gen 6712/8000  train MSE: 0.5784, val MSE: 0.3384
Gen 6713/8000  train MSE: 0.5784, val MSE: 0.3384
Gen 6714/8000  train MSE: 0.5784, val MSE: 0.3384
Gen 6715/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6716/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6717/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6718/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6719/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6720/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6721/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6722/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6723/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6724/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6725/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6726/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6727/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6728/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6729/8000  train MSE: 0.5784, val MSE: 0.3378
Gen 6730/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6731/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6732/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6733/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6734/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6735/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6736/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6737/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6738/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6739/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6740/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6741/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6742/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6743/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6744/8000  train MSE: 0.5784, val MSE: 0.3375
Gen 6745/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6746/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6747/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6748/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6749/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6750/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6751/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6752/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6753/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6754/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6755/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6756/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6757/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6758/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6759/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6760/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6761/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6762/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6763/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6764/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6765/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6766/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6767/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6768/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6769/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6770/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6771/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6772/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6773/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6774/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6775/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6776/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6777/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6778/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6779/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6780/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6781/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6782/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6783/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6784/8000  train MSE: 0.5783, val MSE: 0.3376
Gen 6785/8000  train MSE: 0.5783, val MSE: 0.3378
Gen 6786/8000  train MSE: 0.5783, val MSE: 0.3378
Gen 6787/8000  train MSE: 0.5783, val MSE: 0.3378
Gen 6788/8000  train MSE: 0.5783, val MSE: 0.3378
Gen 6789/8000  train MSE: 0.5783, val MSE: 0.3378
Gen 6790/8000  train MSE: 0.5783, val MSE: 0.3378
Gen 6791/8000  train MSE: 0.5783, val MSE: 0.3378
Gen 6792/8000  train MSE: 0.5783, val MSE: 0.3377
Gen 6793/8000  train MSE: 0.5783, val MSE: 0.3377
Gen 6794/8000  train MSE: 0.5783, val MSE: 0.3377
Gen 6795/8000  train MSE: 0.5783, val MSE: 0.3377
Gen 6796/8000  train MSE: 0.5783, val MSE: 0.3377
Gen 6797/8000  train MSE: 0.5783, val MSE: 0.3377
Gen 6798/8000  train MSE: 0.5783, val MSE: 0.3377
Gen 6799/8000  train MSE: 0.5783, val MSE: 0.3377
Gen 6800/8000  train MSE: 0.5783, val MSE: 0.3377
Gen 6801/8000  train MSE: 0.5783, val MSE: 0.3377
Gen 6802/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6803/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6804/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6805/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6806/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6807/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6808/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6809/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6810/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6811/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6812/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6813/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6814/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6815/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6816/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6817/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6818/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6819/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6820/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6821/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6822/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6823/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6824/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6825/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6826/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6827/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6828/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6829/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6830/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6831/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6832/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6833/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6834/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6835/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6836/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6837/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6838/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6839/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6840/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6841/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6842/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6843/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6844/8000  train MSE: 0.5783, val MSE: 0.3375
Gen 6845/8000  train MSE: 0.5782, val MSE: 0.3379
Gen 6846/8000  train MSE: 0.5782, val MSE: 0.3379
Gen 6847/8000  train MSE: 0.5782, val MSE: 0.3379
Gen 6848/8000  train MSE: 0.5782, val MSE: 0.3379
Gen 6849/8000  train MSE: 0.5782, val MSE: 0.3379
Gen 6850/8000  train MSE: 0.5782, val MSE: 0.3379
Gen 6851/8000  train MSE: 0.5782, val MSE: 0.3379
Gen 6852/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6853/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6854/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6855/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6856/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6857/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6858/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6859/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6860/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6861/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6862/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6863/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6864/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6865/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6866/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6867/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6868/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6869/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6870/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6871/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6872/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6873/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6874/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6875/8000  train MSE: 0.5782, val MSE: 0.3375
Gen 6876/8000  train MSE: 0.5782, val MSE: 0.3373
Gen 6877/8000  train MSE: 0.5782, val MSE: 0.3373
Gen 6878/8000  train MSE: 0.5782, val MSE: 0.3373
Gen 6879/8000  train MSE: 0.5782, val MSE: 0.3373
Gen 6880/8000  train MSE: 0.5782, val MSE: 0.3370
Gen 6881/8000  train MSE: 0.5782, val MSE: 0.3370
Gen 6882/8000  train MSE: 0.5782, val MSE: 0.3370
Gen 6883/8000  train MSE: 0.5782, val MSE: 0.3370
Gen 6884/8000  train MSE: 0.5781, val MSE: 0.3374
Gen 6885/8000  train MSE: 0.5781, val MSE: 0.3374
Gen 6886/8000  train MSE: 0.5781, val MSE: 0.3374
Gen 6887/8000  train MSE: 0.5781, val MSE: 0.3374
Gen 6888/8000  train MSE: 0.5781, val MSE: 0.3374
Gen 6889/8000  train MSE: 0.5781, val MSE: 0.3374
Gen 6890/8000  train MSE: 0.5781, val MSE: 0.3374
Gen 6891/8000  train MSE: 0.5781, val MSE: 0.3374
Gen 6892/8000  train MSE: 0.5781, val MSE: 0.3374
Gen 6893/8000  train MSE: 0.5781, val MSE: 0.3374
Gen 6894/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6895/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6896/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6897/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6898/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6899/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6900/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6901/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6902/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6903/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6904/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6905/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6906/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6907/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6908/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6909/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6910/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6911/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6912/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6913/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6914/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6915/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6916/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6917/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6918/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6919/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6920/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6921/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6922/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6923/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6924/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6925/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6926/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6927/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6928/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6929/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6930/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6931/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6932/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6933/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6934/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6935/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6936/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6937/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6938/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6939/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6940/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6941/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6942/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6943/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6944/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6945/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6946/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6947/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6948/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6949/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6950/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6951/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6952/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6953/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6954/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6955/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6956/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6957/8000  train MSE: 0.5781, val MSE: 0.3376
Gen 6958/8000  train MSE: 0.5781, val MSE: 0.3371
Gen 6959/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6960/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6961/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6962/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6963/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6964/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6965/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6966/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6967/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6968/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6969/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6970/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6971/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6972/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6973/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6974/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6975/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6976/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6977/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6978/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6979/8000  train MSE: 0.5780, val MSE: 0.3371
Gen 6980/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6981/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6982/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6983/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6984/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6985/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6986/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6987/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6988/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6989/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6990/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6991/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6992/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6993/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6994/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6995/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6996/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6997/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6998/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 6999/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7000/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7001/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7002/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7003/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7004/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7005/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7006/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7007/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7008/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7009/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7010/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7011/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7012/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7013/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7014/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7015/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7016/8000  train MSE: 0.5780, val MSE: 0.3374
Gen 7017/8000  train MSE: 0.5780, val MSE: 0.3371
Gen 7018/8000  train MSE: 0.5780, val MSE: 0.3371
Gen 7019/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7020/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7021/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7022/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7023/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7024/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7025/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7026/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7027/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7028/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7029/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7030/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7031/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7032/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7033/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7034/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7035/8000  train MSE: 0.5780, val MSE: 0.3367
Gen 7036/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7037/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7038/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7039/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7040/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7041/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7042/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7043/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7044/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7045/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7046/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7047/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7048/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7049/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7050/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7051/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7052/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7053/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7054/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7055/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7056/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7057/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7058/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7059/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7060/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7061/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7062/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7063/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7064/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7065/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7066/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7067/8000  train MSE: 0.5779, val MSE: 0.3371
Gen 7068/8000  train MSE: 0.5779, val MSE: 0.3365
Gen 7069/8000  train MSE: 0.5779, val MSE: 0.3365
Gen 7070/8000  train MSE: 0.5779, val MSE: 0.3366
Gen 7071/8000  train MSE: 0.5779, val MSE: 0.3366
Gen 7072/8000  train MSE: 0.5779, val MSE: 0.3365
Gen 7073/8000  train MSE: 0.5779, val MSE: 0.3365
Gen 7074/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7075/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7076/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7077/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7078/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7079/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7080/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7081/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7082/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7083/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7084/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7085/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7086/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7087/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7088/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7089/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7090/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7091/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7092/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7093/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7094/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7095/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7096/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7097/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7098/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7099/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7100/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7101/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7102/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7103/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7104/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7105/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7106/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7107/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7108/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7109/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7110/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7111/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7112/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7113/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7114/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7115/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7116/8000  train MSE: 0.5778, val MSE: 0.3373
Gen 7117/8000  train MSE: 0.5778, val MSE: 0.3370
Gen 7118/8000  train MSE: 0.5778, val MSE: 0.3370
Gen 7119/8000  train MSE: 0.5778, val MSE: 0.3370
Gen 7120/8000  train MSE: 0.5778, val MSE: 0.3370
Gen 7121/8000  train MSE: 0.5778, val MSE: 0.3370
Gen 7122/8000  train MSE: 0.5778, val MSE: 0.3370
Gen 7123/8000  train MSE: 0.5778, val MSE: 0.3370
Gen 7124/8000  train MSE: 0.5778, val MSE: 0.3367
Gen 7125/8000  train MSE: 0.5778, val MSE: 0.3367
Gen 7126/8000  train MSE: 0.5778, val MSE: 0.3367
Gen 7127/8000  train MSE: 0.5778, val MSE: 0.3367
Gen 7128/8000  train MSE: 0.5778, val MSE: 0.3367
Gen 7129/8000  train MSE: 0.5778, val MSE: 0.3367
Gen 7130/8000  train MSE: 0.5778, val MSE: 0.3367
Gen 7131/8000  train MSE: 0.5778, val MSE: 0.3367
Gen 7132/8000  train MSE: 0.5778, val MSE: 0.3367
Gen 7133/8000  train MSE: 0.5778, val MSE: 0.3367
Gen 7134/8000  train MSE: 0.5778, val MSE: 0.3366
Gen 7135/8000  train MSE: 0.5778, val MSE: 0.3366
Gen 7136/8000  train MSE: 0.5778, val MSE: 0.3365
Gen 7137/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7138/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7139/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7140/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7141/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7142/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7143/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7144/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7145/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7146/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7147/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7148/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7149/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7150/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7151/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7152/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7153/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7154/8000  train MSE: 0.5777, val MSE: 0.3368
Gen 7155/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7156/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7157/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7158/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7159/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7160/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7161/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7162/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7163/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7164/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7165/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7166/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7167/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7168/8000  train MSE: 0.5777, val MSE: 0.3361
Gen 7169/8000  train MSE: 0.5777, val MSE: 0.3371
Gen 7170/8000  train MSE: 0.5777, val MSE: 0.3371
Gen 7171/8000  train MSE: 0.5777, val MSE: 0.3371
Gen 7172/8000  train MSE: 0.5777, val MSE: 0.3371
Gen 7173/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7174/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7175/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7176/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7177/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7178/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7179/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7180/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7181/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7182/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7183/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7184/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7185/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7186/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7187/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7188/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7189/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7190/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7191/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7192/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7193/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7194/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7195/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7196/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7197/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7198/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7199/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7200/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7201/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7202/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7203/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7204/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7205/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7206/8000  train MSE: 0.5777, val MSE: 0.3362
Gen 7207/8000  train MSE: 0.5777, val MSE: 0.3362
Gen 7208/8000  train MSE: 0.5777, val MSE: 0.3362
Gen 7209/8000  train MSE: 0.5777, val MSE: 0.3362
Gen 7210/8000  train MSE: 0.5777, val MSE: 0.3362
Gen 7211/8000  train MSE: 0.5777, val MSE: 0.3362
Gen 7212/8000  train MSE: 0.5777, val MSE: 0.3362
Gen 7213/8000  train MSE: 0.5777, val MSE: 0.3370
Gen 7214/8000  train MSE: 0.5777, val MSE: 0.3367
Gen 7215/8000  train MSE: 0.5777, val MSE: 0.3367
Gen 7216/8000  train MSE: 0.5777, val MSE: 0.3367
Gen 7217/8000  train MSE: 0.5777, val MSE: 0.3367
Gen 7218/8000  train MSE: 0.5777, val MSE: 0.3367
Gen 7219/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7220/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7221/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7222/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7223/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7224/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7225/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7226/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7227/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7228/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7229/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7230/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7231/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7232/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7233/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7234/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7235/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7236/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7237/8000  train MSE: 0.5777, val MSE: 0.3365
Gen 7238/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7239/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7240/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7241/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7242/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7243/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7244/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7245/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7246/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7247/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7248/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7249/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7250/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7251/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7252/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7253/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7254/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7255/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7256/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7257/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7258/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7259/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7260/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7261/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7262/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7263/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7264/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7265/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7266/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7267/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7268/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7269/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7270/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7271/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7272/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7273/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7274/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7275/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7276/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7277/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7278/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7279/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7280/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7281/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7282/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7283/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7284/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7285/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7286/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7287/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7288/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7289/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7290/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7291/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7292/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7293/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7294/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7295/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7296/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7297/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7298/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7299/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7300/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7301/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7302/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7303/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7304/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7305/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7306/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7307/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7308/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7309/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7310/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7311/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7312/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7313/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7314/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7315/8000  train MSE: 0.5776, val MSE: 0.3361
Gen 7316/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7317/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7318/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7319/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7320/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7321/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7322/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7323/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7324/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7325/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7326/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7327/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7328/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7329/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7330/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7331/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7332/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7333/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7334/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7335/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7336/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7337/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7338/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7339/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7340/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7341/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7342/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7343/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7344/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7345/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7346/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7347/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7348/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7349/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7350/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7351/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7352/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7353/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7354/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7355/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7356/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7357/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7358/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7359/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7360/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7361/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7362/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7363/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7364/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7365/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7366/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7367/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7368/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7369/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7370/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7371/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7372/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7373/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7374/8000  train MSE: 0.5776, val MSE: 0.3357
Gen 7375/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7376/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7377/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7378/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7379/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7380/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7381/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7382/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7383/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7384/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7385/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7386/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7387/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7388/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7389/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7390/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7391/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7392/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7393/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7394/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7395/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7396/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7397/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7398/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7399/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7400/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7401/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7402/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7403/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7404/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7405/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7406/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7407/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7408/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7409/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7410/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7411/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7412/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7413/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7414/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7415/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7416/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7417/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7418/8000  train MSE: 0.5776, val MSE: 0.3363
Gen 7419/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7420/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7421/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7422/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7423/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7424/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7425/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7426/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7427/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7428/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7429/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7430/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7431/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7432/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7433/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7434/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7435/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7436/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7437/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7438/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7439/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7440/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7441/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7442/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7443/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7444/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7445/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7446/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7447/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7448/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7449/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7450/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7451/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7452/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7453/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7454/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7455/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7456/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7457/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7458/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7459/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7460/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7461/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7462/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7463/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7464/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7465/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7466/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7467/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7468/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7469/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7470/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7471/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7472/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7473/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7474/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7475/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7476/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7477/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7478/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7479/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7480/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7481/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7482/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7483/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7484/8000  train MSE: 0.5775, val MSE: 0.3365
Gen 7485/8000  train MSE: 0.5775, val MSE: 0.3362
Gen 7486/8000  train MSE: 0.5775, val MSE: 0.3362
Gen 7487/8000  train MSE: 0.5775, val MSE: 0.3362
Gen 7488/8000  train MSE: 0.5775, val MSE: 0.3362
Gen 7489/8000  train MSE: 0.5775, val MSE: 0.3364
Gen 7490/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7491/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7492/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7493/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7494/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7495/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7496/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7497/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7498/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7499/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7500/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7501/8000  train MSE: 0.5774, val MSE: 0.3360
Gen 7502/8000  train MSE: 0.5774, val MSE: 0.3360
Gen 7503/8000  train MSE: 0.5774, val MSE: 0.3360
Gen 7504/8000  train MSE: 0.5774, val MSE: 0.3362
Gen 7505/8000  train MSE: 0.5774, val MSE: 0.3362
Gen 7506/8000  train MSE: 0.5774, val MSE: 0.3357
Gen 7507/8000  train MSE: 0.5774, val MSE: 0.3357
Gen 7508/8000  train MSE: 0.5774, val MSE: 0.3357
Gen 7509/8000  train MSE: 0.5774, val MSE: 0.3357
Gen 7510/8000  train MSE: 0.5774, val MSE: 0.3357
Gen 7511/8000  train MSE: 0.5774, val MSE: 0.3357
Gen 7512/8000  train MSE: 0.5774, val MSE: 0.3357
Gen 7513/8000  train MSE: 0.5774, val MSE: 0.3357
Gen 7514/8000  train MSE: 0.5774, val MSE: 0.3357
Gen 7515/8000  train MSE: 0.5774, val MSE: 0.3357
Gen 7516/8000  train MSE: 0.5774, val MSE: 0.3357
Gen 7517/8000  train MSE: 0.5774, val MSE: 0.3363
Gen 7518/8000  train MSE: 0.5774, val MSE: 0.3363
Gen 7519/8000  train MSE: 0.5774, val MSE: 0.3363
Gen 7520/8000  train MSE: 0.5774, val MSE: 0.3363
Gen 7521/8000  train MSE: 0.5774, val MSE: 0.3363
Gen 7522/8000  train MSE: 0.5774, val MSE: 0.3363
Gen 7523/8000  train MSE: 0.5774, val MSE: 0.3363
Gen 7524/8000  train MSE: 0.5774, val MSE: 0.3363
Gen 7525/8000  train MSE: 0.5774, val MSE: 0.3363
Gen 7526/8000  train MSE: 0.5774, val MSE: 0.3363
Gen 7527/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7528/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7529/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7530/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7531/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7532/8000  train MSE: 0.5774, val MSE: 0.3359
Gen 7533/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7534/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7535/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7536/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7537/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7538/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7539/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7540/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7541/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7542/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7543/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7544/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7545/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7546/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7547/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7548/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7549/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7550/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7551/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7552/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7553/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7554/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7555/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7556/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7557/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7558/8000  train MSE: 0.5774, val MSE: 0.3354
Gen 7559/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7560/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7561/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7562/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7563/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7564/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7565/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7566/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7567/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7568/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7569/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7570/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7571/8000  train MSE: 0.5774, val MSE: 0.3358
Gen 7572/8000  train MSE: 0.5773, val MSE: 0.3359
Gen 7573/8000  train MSE: 0.5773, val MSE: 0.3359
Gen 7574/8000  train MSE: 0.5773, val MSE: 0.3359
Gen 7575/8000  train MSE: 0.5773, val MSE: 0.3359
Gen 7576/8000  train MSE: 0.5773, val MSE: 0.3359
Gen 7577/8000  train MSE: 0.5773, val MSE: 0.3359
Gen 7578/8000  train MSE: 0.5773, val MSE: 0.3359
Gen 7579/8000  train MSE: 0.5773, val MSE: 0.3359
Gen 7580/8000  train MSE: 0.5773, val MSE: 0.3359
Gen 7581/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7582/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7583/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7584/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7585/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7586/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7587/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7588/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7589/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7590/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7591/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7592/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7593/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7594/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7595/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7596/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7597/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7598/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7599/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7600/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7601/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7602/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7603/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7604/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7605/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7606/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7607/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7608/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7609/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7610/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7611/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7612/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7613/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7614/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7615/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7616/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7617/8000  train MSE: 0.5773, val MSE: 0.3351
Gen 7618/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7619/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7620/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7621/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7622/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7623/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7624/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7625/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7626/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7627/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7628/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7629/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7630/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7631/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7632/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7633/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7634/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7635/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7636/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7637/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7638/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7639/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7640/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7641/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7642/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7643/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7644/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7645/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7646/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7647/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7648/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7649/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7650/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7651/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7652/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7653/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7654/8000  train MSE: 0.5773, val MSE: 0.3356
Gen 7655/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7656/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7657/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7658/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7659/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7660/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7661/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7662/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7663/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7664/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7665/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7666/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7667/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7668/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7669/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7670/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7671/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7672/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7673/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7674/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7675/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7676/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7677/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7678/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7679/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7680/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7681/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7682/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7683/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7684/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7685/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7686/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7687/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7688/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7689/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7690/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7691/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7692/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7693/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7694/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7695/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7696/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7697/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7698/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7699/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7700/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7701/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7702/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7703/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7704/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7705/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7706/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7707/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7708/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7709/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7710/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7711/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7712/8000  train MSE: 0.5772, val MSE: 0.3355
Gen 7713/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7714/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7715/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7716/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7717/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7718/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7719/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7720/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7721/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7722/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7723/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7724/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7725/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7726/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7727/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7728/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7729/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7730/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7731/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7732/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7733/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7734/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7735/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7736/8000  train MSE: 0.5772, val MSE: 0.3351
Gen 7737/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7738/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7739/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7740/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7741/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7742/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7743/8000  train MSE: 0.5772, val MSE: 0.3359
Gen 7744/8000  train MSE: 0.5772, val MSE: 0.3359
Gen 7745/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7746/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7747/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7748/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7749/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7750/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7751/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7752/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7753/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7754/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7755/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7756/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7757/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7758/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7759/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7760/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7761/8000  train MSE: 0.5772, val MSE: 0.3357
Gen 7762/8000  train MSE: 0.5772, val MSE: 0.3353
Gen 7763/8000  train MSE: 0.5772, val MSE: 0.3353
Gen 7764/8000  train MSE: 0.5772, val MSE: 0.3353
Gen 7765/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7766/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7767/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7768/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7769/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7770/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7771/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7772/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7773/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7774/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7775/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7776/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7777/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7778/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7779/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7780/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7781/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7782/8000  train MSE: 0.5772, val MSE: 0.3361
Gen 7783/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7784/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7785/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7786/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7787/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7788/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7789/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7790/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7791/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7792/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7793/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7794/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7795/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7796/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7797/8000  train MSE: 0.5772, val MSE: 0.3356
Gen 7798/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7799/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7800/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7801/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7802/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7803/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7804/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7805/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7806/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7807/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7808/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7809/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7810/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7811/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7812/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7813/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7814/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7815/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7816/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7817/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7818/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7819/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7820/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7821/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7822/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7823/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7824/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7825/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7826/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7827/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7828/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7829/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7830/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7831/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7832/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7833/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7834/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7835/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7836/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7837/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7838/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7839/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7840/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7841/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7842/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7843/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7844/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7845/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7846/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7847/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7848/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7849/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7850/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7851/8000  train MSE: 0.5771, val MSE: 0.3357
Gen 7852/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7853/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7854/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7855/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7856/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7857/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7858/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7859/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7860/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7861/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7862/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7863/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7864/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7865/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7866/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7867/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7868/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7869/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7870/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7871/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7872/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7873/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7874/8000  train MSE: 0.5771, val MSE: 0.3354
Gen 7875/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7876/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7877/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7878/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7879/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7880/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7881/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7882/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7883/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7884/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7885/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7886/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7887/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7888/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7889/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7890/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7891/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7892/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7893/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7894/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7895/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7896/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7897/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7898/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7899/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7900/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7901/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7902/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7903/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7904/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7905/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7906/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7907/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7908/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7909/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7910/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7911/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7912/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7913/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7914/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7915/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7916/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7917/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7918/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7919/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7920/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7921/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7922/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7923/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7924/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7925/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7926/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7927/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7928/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7929/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7930/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7931/8000  train MSE: 0.5770, val MSE: 0.3351
Gen 7932/8000  train MSE: 0.5770, val MSE: 0.3357
Gen 7933/8000  train MSE: 0.5770, val MSE: 0.3357
Gen 7934/8000  train MSE: 0.5770, val MSE: 0.3356
Gen 7935/8000  train MSE: 0.5770, val MSE: 0.3356
Gen 7936/8000  train MSE: 0.5770, val MSE: 0.3356
Gen 7937/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7938/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7939/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7940/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7941/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7942/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7943/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7944/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7945/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7946/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7947/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7948/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7949/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7950/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7951/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7952/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7953/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7954/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7955/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7956/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7957/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7958/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7959/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7960/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7961/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7962/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7963/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7964/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7965/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7966/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7967/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7968/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7969/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7970/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7971/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7972/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7973/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7974/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7975/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7976/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7977/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7978/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7979/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7980/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7981/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7982/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7983/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7984/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7985/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7986/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7987/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7988/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7989/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7990/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7991/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7992/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7993/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7994/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7995/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7996/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7997/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7998/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 7999/8000  train MSE: 0.5770, val MSE: 0.3352
Gen 8000/8000  train MSE: 0.5770, val MSE: 0.3352

 GA+SGD done!  Final Train MSE: 0.5770, Val MSE: 0.3351
 Best model weights, genome and metadata saved to ./checkpoints/
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=36b5a71a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Final-test-set-evaluation-of-all-three-optimisers-best-models:">Final test set evaluation of all three optimisers best models:<a class="anchor-link" href="#Final-test-set-evaluation-of-all-three-optimisers-best-models:"></a></h4><h4 id="NOTE-:-All-cells-before-this-must-be-run-before-running-this-one.">NOTE : All cells before this must be run before running this one.<a class="anchor-link" href="#NOTE-:-All-cells-before-this-must-be-run-before-running-this-one."></a></h4>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=993aa8dc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#  3) Evaluate each checkpointed model </span>
<span class="sd">"""</span>
<span class="sd">Evaluates the performance of each trained model (GA, SGD, L-BFGS) on the test set.</span>

<span class="sd">For each method:</span>
<span class="sd">- Loads model architecture and weights from checkpoint</span>
<span class="sd">- Runs forward pass on test data (scaled)</span>
<span class="sd">- Converts predictions back to original scale</span>
<span class="sd">- Computes test MSE and RMSE in both scaled and original units</span>
<span class="sd">- Logs results and saves predictions for downstream use</span>
<span class="sd">"""</span>


<span class="c1">#  Evaluate each checkpointed model </span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">all_preds</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">rmse_results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">method</span><span class="p">,</span> <span class="n">ckpt_dir</span><span class="p">,</span> <span class="n">weight_file</span><span class="p">,</span> <span class="n">meta_file</span> <span class="ow">in</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">"GA"</span><span class="p">,</span>    <span class="s2">"checkpoints"</span><span class="p">,</span>         <span class="s2">"best_model_weights.pth"</span><span class="p">,</span>  <span class="s2">"best_model_meta.json"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"SGD"</span><span class="p">,</span>   <span class="s2">"checkpoints_sgd"</span><span class="p">,</span>     <span class="s2">"sgd_final_weights.pth"</span><span class="p">,</span>   <span class="s2">"sgd_model_meta.json"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"L-BFGS"</span><span class="p">,</span><span class="s2">"checkpoints_lbfgs"</span><span class="p">,</span>   <span class="s2">"lbfgs_final_weights.pth"</span><span class="p">,</span> <span class="s2">"lbfgs_model_meta.json"</span><span class="p">),</span>
<span class="p">]:</span>
    <span class="c1"># load architecture meta</span>
    <span class="n">meta</span>   <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ckpt_dir</span><span class="p">,</span> <span class="n">meta_file</span><span class="p">),</span> <span class="s2">"r"</span><span class="p">))</span>
    <span class="n">arch</span>   <span class="o">=</span> <span class="n">meta</span><span class="p">[</span><span class="s2">"arch"</span><span class="p">]</span>
    <span class="n">model</span>  <span class="o">=</span> <span class="n">build_model_from_arch</span><span class="p">(</span><span class="n">arch</span><span class="p">)</span>
    <span class="c1"># load weights</span>
    <span class="n">state</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ckpt_dir</span><span class="p">,</span> <span class="n">weight_file</span><span class="p">),</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="c1"># forward on test set</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">y_pred_scaled</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">y_pred_orig</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_scaled</span><span class="p">)</span>
    <span class="n">y_true_orig</span> <span class="o">=</span> <span class="n">y_test_np</span>
    <span class="c1"># MSE in scaled and original units</span>
    <span class="n">mse_scaled</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_pred_scaled</span> <span class="o">-</span> <span class="n">y_test</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">mse_original</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_pred_orig</span>   <span class="o">-</span> <span class="n">y_true_orig</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">rmse_original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse_original</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">method</span><span class="si">:</span><span class="s2">6s</span><span class="si">}</span><span class="s2">  Test MSE (scaled):   </span><span class="si">{</span><span class="n">mse_scaled</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">method</span><span class="si">:</span><span class="s2">6s</span><span class="si">}</span><span class="s2">  Test MSE (original): </span><span class="si">{</span><span class="n">mse_original</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">method</span><span class="si">:</span><span class="s2">6s</span><span class="si">}</span><span class="s2">  Test RMSE (original): </span><span class="si">{</span><span class="n">rmse_original</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">method</span><span class="p">,</span> <span class="n">mse_scaled</span><span class="p">,</span> <span class="n">mse_original</span><span class="p">))</span>
    <span class="n">rmse_results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">method</span><span class="p">,</span> <span class="n">rmse_original</span><span class="p">))</span>
    <span class="n">all_preds</span><span class="p">[</span><span class="n">method</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_orig</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

<span class="c1">#  5) Barplot of test MSE (original units) </span>
<span class="n">methods</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">orig_mses</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">results</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">methods</span><span class="p">,</span> <span class="n">orig_mses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">"C0"</span><span class="p">,</span><span class="s2">"C1"</span><span class="p">,</span><span class="s2">"C2"</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Test MSE (original units)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Test-set Performance by Method"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">"--"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">#  5b) Barplot of test RMSE (original units) </span>
<span class="n">methods_rmse</span><span class="p">,</span> <span class="n">rmses</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">rmse_results</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">methods_rmse</span><span class="p">,</span> <span class="n">rmses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">"C0"</span><span class="p">,</span> <span class="s2">"C1"</span><span class="p">,</span> <span class="s2">"C2"</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Test RMSE (original units)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Test-set RMSE by Method"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">"--"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>GA      Test MSE (scaled):   0.332427
GA      Test MSE (original): 67.030509
GA      Test RMSE (original): 8.1872

SGD     Test MSE (scaled):   0.504205
SGD     Test MSE (original): 101.667889
SGD     Test RMSE (original): 10.0830

L-BFGS  Test MSE (scaled):   0.354872
L-BFGS  Test MSE (original): 71.556357
L-BFGS  Test RMSE (original): 8.4591

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
